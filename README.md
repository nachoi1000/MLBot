# MLBot: Asistente Experto en "An Introduction to Statistical Learning"

**MLBot** es un agente de RAG (Generaci√≥n Aumentada por Recuperaci√≥n) que act√∫a como un asistente experto en el libro "An Introduction to Statistical Learning with Applications in Python". Construido con LangGraph y la API de OpenAI, te permite resolver dudas y consultar conceptos del libro como si hablaras con un especialista.

El objetivo de este proyecto consist√≠o en poder procesar grandes documentos, con muhca informaci√≥n dificil obtener de manera convencional, y luego implementar un metodo de retrieval, que mejor se adapte a la configuraci√≥n del documento.

Para alcanzar estos objetivos me apoye en 3 partes fundamentales:

- Procesamiento del documento utilizando **Docling** para obtener un formato mas "estructurado" como lo es .md y la posibilidad de poder scrapear formulas matematicas con notacion LaTeX.

- Retrieval de contexto utilizando una estrategia de **Parent Child**. Esta estrategia era la mas fiel al conjunto de datos enfrentados, ya que los teoremas matematicos y/o formulas matematicas, dificilmente "quepan" en un solo chunk, por ende, al tener un .md con una estructura tan bien definida en headers, se pudo utilizar dicha estructura a la hora de pensar en "Parents" y en cada "Parent" si chunkear por tama√±o fijo. El retrieval se hace por "child" chunks y lo que se utiliza como contexto, son los "parents" de dichos child chunks.

- Workflow para RAG utilizando LangGraph.

[![Langchain](https://img.shields.io/badge/LangChain-LangGraph-blue.svg)](https://langchain-ai.github.io/langgraph/)
[![Python](https://img.shields.io/badge/Python-3.10+-yellow.svg)](https://www.python.org/)
[![FastAPI](https://img.shields.io/badge/FastAPI-0.100+-green.svg)](https://fastapi.tiangolo.com/)

![Diagrama del Agente](docs/agent_mermaid_diagram.png)

## ‚ú® Caracter√≠sticas Principales

* **Respuestas Basadas en el Libro**: Todas las respuestas se generan utilizando exclusivamente el contenido del libro, asegurando fidelidad y precisi√≥n.
* **Comprensi√≥n del Contexto**: El agente puede manejar preguntas de seguimiento y repreguntas, manteniendo el hilo de la conversaci√≥n.
* **Estrategia de Retrieval Avanzada**: Utiliza una arquitectura "Parent-Child" para encontrar los fragmentos m√°s relevantes (hijos) y darles contexto completo (padres).
* **Soporte para F√≥rmulas Matem√°ticas**: Gracias al procesamiento con `docling`, el agente tiene acceso a las f√≥rmulas en formato LaTeX del libro.
* **Evaluaci√≥n Rigurosa**: La calidad del sistema ha sido medida con el framework **RAGAS**, evaluando m√©tricas clave como fidelidad, relevancia y precisi√≥n del contexto.

---

## üß† ¬øC√≥mo Funciona? (Arquitectura)

El sistema se basa en un flujo de RAG directo, orquestado con LangGraph.

### 1. Procesamiento del Documento
#### Pasar de .pdf a .md
Se utilizo mediante google colab una notebook con GPU para utilizar docling. El codigo de la notebook esta en el archivo colab_notebook.ipynb.

En esta estapa ademas del texto del .pdf se "scrapeo" la gran cantidad de formulas matem√°ticas que tenia el documento, con un formato LaTex, gracias a docling.

Para el procesamiento de imagenes, decidi no considerarlas, ya que en el documento mismo, cada imagen tenia bajo una descripci√≥n y explicaci√≥n del contenido que reflejaba la imagen.

Luego de obtener el archivo .md, se obtiene la estructura del .pdf (table of contents) y se limpian los headers (#, ## como ###) del .md anterior, para armar una estructura de headers basandome en la table of contents. 

Aqui utilice la notebook pdftoc_to_md.ipynb para generar el documento **PDF-GenAI-Challenge_2.md** el cual fue utilizado para luego chunkear, vectorizar y almacenar en una base de datos vectorial.

#### Generacion de Chunk, Vector, Almcenambiento en Base de datos Vectorial y Estategia de Retrieval
La estrategia de procesamiento elegida es **parent-child**.

Al ser un documento t√©cnico:

Implement√© trozos peque√±os (chunks) para la b√∫squeda: Para que la b√∫squeda sem√°ntica sea precisa, los fragmentos de texto (y sus embeddings) deben ser muy espec√≠ficos. Si un usuario pregunta sobre regularizaci√≥n L1, quieres encontrar el p√°rrafo exacto que lo define, no un cap√≠tulo entero de 20 p√°ginas sobre regresi√≥n.

Utilic√© trozos grandes (chunks) para la generaci√≥n: Una vez que se encuentra ese p√°rrafo espec√≠fico, el LLM necesita m√°s contexto para dar una buena respuesta. ¬øA qu√© modelo se aplica? ¬øCu√°l era el problema que se intentaba solucionar? Este contexto se encuentra en las secciones y cap√≠tulos circundantes.

Al tener un documento en un formato .md con los headers bien definidos decidi implementar un retrieval de Parent-Child.

Para ello los chunks fueron geenrados con la siguiente l√≥gica:
- Se realizo una limpieza de cacracteres en el .md
- Se implemento un MarkdownHeaderTextSplitter para obtener los Parent Documents.
- Se implemento un RecursiveCharacterTextSplitter para generar los Child Documents
- se utilizo Chroma como vectorstore, el cual utiliza text-embedding-3-small como modelo de embedding por default.
- El retriever utilizado fue ParentDocumentRetriever

Se utilizaron componentes de Langchain en todo este proceso.

El detalle de este proceso se ve en el archivo **retrieve_factory.py** 

#### Flujo del Agente (LangGraph)

Para la soluci√≥n de este challenge se implemento un agente con un flujo de RAG directo.

El agente sigue la logica:

- Recibir Pregunta: El usuario env√≠a una consulta.

- Rewriter: Mediante LLM se genera un "enriquericmiento" del input del usuario para reformular la pregunta y que tenga mayor significado con la historia de la conversacion.

- Retriever: El sistema siempre utiliza la pregunta, posterior salida del Rewriter, para buscar los documentos m√°s relevantes en la base de datos vectorial.

- Generator: Los documentos encontrados se inyectan en el prompt junto con la pregunta original.
El LLM recibe el prompt aumentado y genera una respuesta basada exclusivamente en la informaci√≥n proporcionada.

- Retornar respuesta como chunks utilizados (si es necesario).

Este agente fue implementado con **LangGraph**, debido a la simpleza como a la flexibilidad de poder escalar la solucion a que implemente alg√∫n tipo de flujo mas complejo, o la utilizaci√≥n de distintas otras tools.

---

## üìä Evaluaci√≥n de Calidad (RAGAS)

Para garantizar la fiabilidad del agente, se realiz√≥ una evaluaci√≥n cuantitativa utilizando el framework **RAGAS**. Se gener√≥ un dataset de preguntas y respuestas verificadas (`ground_truth`) y se midieron las siguientes m√©tricas:

Para la evaluaci√≥n del modelo se realizaron 2 tests. Buscando medir **Fidelidad**, **Precision del Contexto**, **Recuperacion** y **Relevancia** utilizando **RAGAS**

Fidelidad (Faithfulness) ‚Üí faithfulness

Relevancia (Answer Relevancy) ‚Üí answer_relevancy

Precisi√≥n del Contexto (Context Precision) ‚Üí context_precision

Recuperaci√≥n del Contexto (Context Recall) ‚Üí context_recall

En este paso, primero mediante la utilizacion de un llm genere 2 listas, una enfocada a f√≥rmulas matem√°ticas y otra enfocada en conceptos generales. Genere las preguntas y su ground_truth, 4 por cada capitulo, tanto en ingl√©s como en espa√±ol.

Luego utilic√© el agente para obtener las respuesta y los contextos utilizados.

Con toda esta informacion, almacenada en .json implemente las evaluaciones solicitadas.

Las metricas son buenas, pero si hubo casos donde problem√°ticos (donde el JSON de entrada estaba mal formado o incompleto), aparece el campo error con IndexError: list index out of range, el cual afect√≥ las metricas.

Queda pendiente para poder medir correctamente **Context Recall** agregar las "references" de las pruebas, para poder comparar el contexto recuperado con dichas referencias.

Se pueden ver estos detalles en los archivos dentro de la carpeta **tests**

---

## üõ†Ô∏è Tech Stack

* **Orquestaci√≥n del Agente**: [Langgraph](https://langchain-ai.github.io/langgraph/)
* **Servidor Web**: [FastAPI](https://fastapi.tiangolo.com/)
* **Modelos de Lenguaje**: [OpenAI API](https://platform.openai.com/)
* **Base de Datos Vectorial**: [Chroma](https://github.com/chroma-core/chroma)
* **Evaluacion del Modelo**: [RAGAS](https://docs.ragas.io/en/stable/)
* **Lenguaje**: Python 3.10+

---

## ‚öôÔ∏è Configuraci√≥n e Instalaci√≥n

Sigue estos pasos para poner en marcha el proyecto.

### Prerrequisitos

Aseg√∫rate de tener instalado lo siguiente:
* [Python 3.10+](https://www.python.org/downloads/)

### Pasos

1.  **Clona el repositorio:**
    ```bash
    git clone [https://github.com/nachoi1000/MLBot.git](https://github.com/nachoi1000/MLBot.git)
    cd MLBot
    ```

2.  **Crea y configura tu archivo de entorno:**
    Crea un archivo llamado `.env` en la ra√≠z del proyecto a partir de la plantilla `env.Sample`. Deber√°s completar las siguientes variables:
    ```env
    # Claves de APIs (ver la secci√≥n 'Obtener Claves de API')
    OPENAI_API_KEY="tu_api_key_de_openai"

    ```

3.  **Ejecuta el start file**:
    
    Dependiendo el sistema operativo ejecuta el start.sh o start.ps1.
    el mismo archivo se encargara de:
    - Levantar el entorno de python.
    - Ejecutar el app.py para ejecutar la aplicacion.

---

## üîë Obtener Claves de API

Para que el proyecto funcione, necesita acceso a servicios externos a trav√©s de claves de API.

### OpenAI API Key

Necesitar√°s una clave de OpenAI para que el agente pueda pensar y procesar el lenguaje.
1.  Ve a [platform.openai.com/api-keys](https://platform.openai.com/api-keys).
2.  Inicia sesi√≥n y crea una nueva "secret key".
3.  Copia la clave y p√©gala en la variable `OPENAI_API_KEY` de tu archivo `.env`.

---

## üöÄ Ejecutar el Proyecto

Los scripts de inicio construyen las im√°genes de Docker y levantan los contenedores de la aplicaci√≥n y la base de datos.

* En **Linux** o **macOS**:
    ```bash
    ./start.sh
    ```

* En **Windows**:
    ```powershell
    ./start.ps1
    ```

Una vez ejecutado, los servicios estar√°n disponibles en:

* **Aplicaci√≥n MLBot**: `http://localhost:8000`

---

## üîå Uso de la API

FastAPI genera autom√°ticamente una documentaci√≥n interactiva para que puedas probar los endpoints f√°cilmente.

* **Documentaci√≥n con Swagger UI**: `http://localhost:8000/docs`
* **Documentaci√≥n con ReDoc**: `http://localhost:8000/redoc`

## Ejemplo de utilizaci√≥n

![alt text](docs/1.png) ![alt text](docs/2.png) ![alt text](docs/3.png) ![alt text](docs/4.png) ![alt text](docs/5.png) ![alt text](docs/6.png) ![alt text](docs/7.png)