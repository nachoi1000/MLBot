# MLBot: Asistente Experto en "An Introduction to Statistical Learning"

**MLBot** es un agente de RAG (Generaci√≥n Aumentada por Recuperaci√≥n) que act√∫a como un asistente experto en el libro "An Introduction to Statistical Learning with Applications in Python". Construido con LangGraph y la API de OpenAI, te permite resolver dudas y consultar conceptos del libro como si hablaras con un especialista.

[![Langchain](https://img.shields.io/badge/LangChain-LangGraph-blue.svg)](https://langchain-ai.github.io/langgraph/)
[![Python](https://img.shields.io/badge/Python-3.10+-yellow.svg)](https://www.python.org/)
[![FastAPI](https://img.shields.io/badge/FastAPI-0.100+-green.svg)](https://fastapi.tiangolo.com/)

![Diagrama del Agente](docs/agent_mermaid_diagram.png)

## ‚ú® Caracter√≠sticas Principales

* **Respuestas Basadas en el Libro**: Todas las respuestas se generan utilizando exclusivamente el contenido del libro, asegurando fidelidad y precisi√≥n.
* **Comprensi√≥n del Contexto**: El agente puede manejar preguntas de seguimiento y repreguntas, manteniendo el hilo de la conversaci√≥n.
* **Estrategia de Retrieval Avanzada**: Utiliza una arquitectura "Parent-Child" para encontrar los fragmentos m√°s relevantes (hijos) y darles contexto completo (padres).
* **Soporte para F√≥rmulas Matem√°ticas**: Gracias al procesamiento con `docling`, el agente tiene acceso a las f√≥rmulas en formato LaTeX del libro.
* **Evaluaci√≥n Rigurosa**: La calidad del sistema ha sido medida con el framework **RAGAS**, evaluando m√©tricas clave como fidelidad, relevancia y precisi√≥n del contexto.

---

## üß† ¬øC√≥mo Funciona? (Arquitectura)

El sistema se basa en un flujo de RAG directo, orquestado con LangGraph.

### 1. Procesamiento del Documento
#### Pasar de .pdf a .md
Se utilizo mediante google colab una notebook con GPU para utilizar docling.
El codigo de la notebook esta en el archivo colab_notebook.ipynb.
En esta estapa ademas del texto del .pdf se "scrapeo" la gran cantidad de formulas matematicas que tenia el documento, con un formato LaTex, gracias a docling.
Con las imagenes, decidi no considerarlas en el procesamiento, ya que en el documento, cada imagen tenia bajo una descripcion y explicacion del contenido que reflejaba la imagen.

Luego de obtener el archivo .md, se obtiene la estructura del .pdf (table of contents) y se limpian los headers (#, ## como ###) del .md anterior, para arnmar los headers del .md basandome en la table of contents. Aqui utilice la notebook pdftoc_to_md.ipynb para generar el documento **PDF-GenAI-Challenge_2.md** el cual fue utilizado para luego chunkear, vectorizar y almacenar en una base de datos vectorial.

#### Generacion de Chunk, Vector, Almcenambiento en Base de datos Vectorial y Estategia de Retrieval
La estrategia de procesamiento elegida es "parent-child".
Al ser un documento tecnico por ende:
Necesitas trozos peque√±os (chunks) para la b√∫squeda: Para que la b√∫squeda sem√°ntica sea precisa, los fragmentos de texto (y sus embeddings) deben ser muy espec√≠ficos. Si un usuario pregunta sobre regularizaci√≥n L1, quieres encontrar el p√°rrafo exacto que lo define, no un cap√≠tulo entero de 20 p√°ginas sobre regresi√≥n.

Necesitas trozos grandes (chunks) para la generaci√≥n: Una vez que encuentras ese p√°rrafo espec√≠fico, el LLM necesita m√°s contexto para dar una buena respuesta. ¬øA qu√© modelo se aplica? ¬øCu√°l era el problema que se intentaba solucionar? Este contexto se encuentra en las secciones y cap√≠tulos circundantes.

Al tener un documento en un formato .md con los headers bien definidos decidi implementar un retrieval de Parent-Child.

Para ello los chunks fueron geenrados con la siguioente logica:
1- Se realizo una limpieza de cacracteres en el .md
2- Se implemento un MarkdownHeaderTextSplitter para obtener los Parent Documents.
3- Se implemento un RecursiveCharacterTextSplitter para generar los Child Documents
4- se utilizo Chroma como vectorstore, el cual utiliza text-embedding-3-small como modelo de embedding por default.
5- El retriever utilizado fue ParentDocumentRetriever

Se utilizaron componentes de Langchain en todo este proceso.

El detalle de este proceso se ve en el archivo **retrieve_factory.py** 

#### Flujo del Agente (LangGraph)

Para la solucion de este challenge se implemento un agente con un flujo de RAG directo.

El agente sigue la logica:

- Recibir Pregunta: El usuario env√≠a una consulta.

- Rewriter: Mediante LLM se genera un "enriquericmiento" del input del usuario para reformular la pregunta y que tenga mayor significado con la historia de la conversacion.

- Retriever: El sistema siempre utiliza la pregunta, posterior salida del Rewriter, para buscar los documentos m√°s relevantes en la base de datos vectorial.

- Generator: Los documentos encontrados se inyectan en el prompt junto con la pregunta original.
El LLM recibe el prompt aumentado y genera una respuesta basada exclusivamente en la informaci√≥n proporcionada.

- Retornar respuesta como chunks utilizados (si es necesario).

Este agente fue implementado con **LangGraph**, debido a la simpleza como a la flexibilidad de poder escalar la solucion a que implemente algun tipo de flujo mas complejo, o la utilizacion de distintas otras tools.

---

## üìä Evaluaci√≥n de Calidad (RAGAS)

Para garantizar la fiabilidad del agente, se realiz√≥ una evaluaci√≥n cuantitativa utilizando el framework **RAGAS**. Se gener√≥ un dataset de preguntas y respuestas verificadas (`ground_truth`) y se midieron las siguientes m√©tricas:

Para la evaluacion del modelo se realizaron 2 tests. Buscando medir **Fidelidad**, **Precision del Contexto**, **Recuperacion** y **Relevancia** utilizando **RAGAS**

Fidelidad (Faithfulness) ‚Üí faithfulness

Relevancia (Answer Relevancy) ‚Üí answer_relevancy

Precisi√≥n del Contexto (Context Precision) ‚Üí context_precision

Recuperaci√≥n del Contexto (Context Recall) ‚Üí context_recall

Para esto primero mediante la utilizacion de un llm genere 2 listas, una enfocada a formulas matematicas y otra enfocada mas a conceptos generales. Genere las preguntas y su gorund_truth, 4 por cada capitulo, tanto en ingles como en espa√±ol.

Luego utilice el agente para obtener las respuesta y los chunks.

Y con esta informacion, almacenada en .json implemente las evaluaciones solicitadas.
Las metricas son buenas, pero si hubo casos donde problem√°ticos (donde el JSON de entrada estaba mal formado o incompleto), aparece el campo error con IndexError: list index out of range, el cual afect√≥ las metricas.

Se pueden ver estos detalles en los archivos dentro de la carpeta **tests**

---

## üõ†Ô∏è Tech Stack

* **Orquestaci√≥n del Agente**: [Langgraph](https://langchain-ai.github.io/langgraph/)
* **Servidor Web**: [FastAPI](https://fastapi.tiangolo.com/)
* **Modelos de Lenguaje**: [OpenAI API](https://platform.openai.com/)
* **Base de Datos Vectorial**: [Chroma](https://github.com/chroma-core/chroma)
* **Evaluacion del Modelo**: [RAGAS](https://docs.ragas.io/en/stable/)
* **Lenguaje**: Python 3.10+

---

## ‚öôÔ∏è Configuraci√≥n e Instalaci√≥n

Sigue estos pasos para poner en marcha el proyecto.

### Prerrequisitos

Aseg√∫rate de tener instalado lo siguiente:
* [Python 3.10+](https://www.python.org/downloads/)

### Pasos

1.  **Clona el repositorio:**
    ```bash
    git clone [https://github.com/nachoi1000/scanntech_rag.git](https://github.com/nachoi1000/scanntech_rag.git)
    cd scanntech_rag
    ```

2.  **Crea y configura tu archivo de entorno:**
    Crea un archivo llamado `.env` en la ra√≠z del proyecto a partir de la plantilla `env.Sample`. Deber√°s completar las siguientes variables:
    ```env
    # Claves de APIs (ver la secci√≥n 'Obtener Claves de API')
    OPENAI_API_KEY="tu_api_key_de_openai"

    ```

3.  **Ejecuta el start file**:
    
    Dependiendo el sistema operativo ejecuta el start.sh o start.ps1.
    el mismo archivo se encargara de:
    - Levantar el entorno de python.
    - Ejecutar el app.py para ejecutar la aplicacion.

---

## üîë Obtener Claves de API

Para que el proyecto funcione, necesita acceso a servicios externos a trav√©s de claves de API.

### OpenAI API Key

Necesitar√°s una clave de OpenAI para que el agente pueda pensar y procesar el lenguaje.
1.  Ve a [platform.openai.com/api-keys](https://platform.openai.com/api-keys).
2.  Inicia sesi√≥n y crea una nueva "secret key".
3.  Copia la clave y p√©gala en la variable `OPENAI_API_KEY` de tu archivo `.env`.

---

## üöÄ Ejecutar el Proyecto

Los scripts de inicio construyen las im√°genes de Docker y levantan los contenedores de la aplicaci√≥n y la base de datos.

* En **Linux** o **macOS**:
    ```bash
    ./start.sh
    ```

* En **Windows**:
    ```powershell
    ./start.ps1
    ```

Una vez ejecutado, los servicios estar√°n disponibles en:

* **Aplicaci√≥n MLBot**: `http://localhost:8000`

---

## üîå Uso de la API

FastAPI genera autom√°ticamente una documentaci√≥n interactiva para que puedas probar los endpoints f√°cilmente.

* **Documentaci√≥n con Swagger UI**: `http://localhost:8000/docs`
* **Documentaci√≥n con ReDoc**: `http://localhost:8000/redoc`

## Ejemplo de utilizaci√≥n

![alt text](docs/1.png) ![alt text](docs/2.png) ![alt text](docs/3.png) ![alt text](docs/4.png) ![alt text](docs/5.png) ![alt text](docs/6.png) ![alt text](docs/7.png)