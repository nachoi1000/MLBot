import logging
import os
from dotenv import load_dotenv
from typing import List, TypedDict
from langchain_openai import ChatOpenAI
from langchain.prompts import ChatPromptTemplate, MessagesPlaceholder
from langchain_core.messages import BaseMessage, HumanMessage
from langgraph.graph import END, StateGraph
from langgraph.checkpoint.memory import MemorySaver
from utils.retriever_factory import MarkdownParentRetrieverSetup
from utils.file_manager import FileManager

logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')


def create_agent_graph():
    """
    Funci√≥n que encapsula la creaci√≥n y compilaci√≥n del grafo del agente.
    """
    load_dotenv()
    llm = ChatOpenAI(model_name=os.getenv("OPENAI_MODEL"), temperature=0)
    file_manager = FileManager()
    rag_system_prompt = file_manager.load_md_file("prompt/rag.md")

    # --- Retriever ---
    retriever_factory = MarkdownParentRetrieverSetup(
        file_path="data/PDF-GenAI-Challenge_2.md",
        collection_name="rag_langgraph_prod_v2"
    )
    retriever = retriever_factory.get_retriever()

    # --- Definici√≥n de Estado y Nodos (sin cambios) ---
    class GraphState(TypedDict):
        input: str
        chat_history: List[BaseMessage]
        question: str
        documents: List[str]

    def rewrite_question(state: GraphState):
        logging.info("--- üß† REESCRIBIENDO PREGUNTA ---")
        # ... (c√≥digo del nodo sin cambios)
        prompt = ChatPromptTemplate.from_messages(
            [
                ("system", "Dada una conversaci√≥n y una √∫ltima pregunta del usuario, reformula la √∫ltima pregunta para que sea una pregunta aut√≥noma."),
                MessagesPlaceholder(variable_name="chat_history"),
                ("human", "{input}"),
            ]
        )
        rewriter_chain = prompt | llm
        rephrased_question = rewriter_chain.invoke(state)
        return {"question": rephrased_question.content}

    def retrieve_documents(state: GraphState):
        logging.info("--- üìö RECUPERANDO DOCUMENTOS ---")
        documents = retriever.invoke(state["question"])
        return {"documents": documents}

    def generate_answer(state: GraphState):
        logging.info("--- üó£Ô∏è GENERANDO RESPUESTA ---")
        prompt = ChatPromptTemplate.from_messages(
            [
                ("system", rag_system_prompt),
                MessagesPlaceholder(variable_name="chat_history"),
                ("human", "{input}"),
            ]
        )
        rag_chain = prompt | llm
        context_str = "\n".join([doc.page_content for doc in state["documents"]])
        generation = rag_chain.invoke({
            "context": context_str,
            "input": state["input"],
            "chat_history": state["chat_history"]
        })
        # IMPORTANTE: Ya no actualizamos el historial aqu√≠, LangGraph lo har√° por nosotros.
        return {"chat_history": [generation]} # Devolvemos solo el nuevo mensaje

    # --- Construcci√≥n y Compilaci√≥n del Grafo ---
    workflow = StateGraph(GraphState)
    workflow.add_node("rewriter", rewrite_question)
    workflow.add_node("retriever", retrieve_documents)
    workflow.add_node("generator", generate_answer)

    workflow.set_entry_point("rewriter")
    workflow.add_edge("rewriter", "retriever")
    workflow.add_edge("retriever", "generator")
    workflow.add_edge("generator", END)
    
    memory = MemorySaver()
    app = workflow.compile(checkpointer=memory)
    print("‚úÖ Grafo compilado y listo para ser usado.")
    return app

# Creamos la instancia del agente una sola vez cuando el m√≥dulo se carga
agent_app = create_agent_graph()

# El bloque if __name__ == "__main__": es √∫til para probar este archivo de forma aislada
if __name__ == "__main__":
    print("Probando el agente de forma aislada...")
    config = {"configurable": {"thread_id": "test-thread-1"}}
    # Primera pregunta
    for _ in agent_app.stream({"input": "Explica el concepto de 'dropout'", "chat_history": []}, config):
        pass
    final_state = agent_app.get_state(config)
    print(f"ü§ñ AI: {final_state.values['chat_history'][-1].content}")
    # Segunda pregunta
    for _ in agent_app.stream({"input": "¬øy por qu√© es importante?"}, config):
        pass
    final_state = agent_app.get_state(config)
    print(f"ü§ñ AI: {final_state.values['chat_history'][-1].content}")