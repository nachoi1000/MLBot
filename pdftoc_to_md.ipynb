{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e04d2a10",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install pymupdf\n",
    "import fitz\n",
    "doc = fitz.open(\"data/PDF-GenAI-Challenge.pdf\")\n",
    "toc = doc.get_toc()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "453d64d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "toc = [[1, 'Preface', 3],\n",
    " [1, 'Contents', 5],\n",
    " [1, '1 Introduction', 12],\n",
    " [2, 'An Overview of Statistical Learning', 12],\n",
    " [3, 'Wage Data', 12],\n",
    " [3, 'Stock Market Data', 13],\n",
    " [3, 'Gene Expression Data', 14],\n",
    " [2, 'A Brief History of Statistical Learning', 16],\n",
    " [2, 'This Book', 17],\n",
    " [2, 'Who Should Read This Book?', 19],\n",
    " [2, 'Notation and Simple Matrix Algebra', 19],\n",
    " [2, 'Organization of This Book', 22],\n",
    " [2, 'Data Sets Used in Labs and Exercises', 23],\n",
    " [2, 'Book Website', 24],\n",
    " [2, 'Acknowledgements', 24],\n",
    " [1, '2 Statistical Learning', 25],\n",
    " [2, '2.1 What Is Statistical Learning?', 25],\n",
    " [3, '2.1.1 Why Estimate f?', 27],\n",
    " [3, '2.1.2 How Do We Estimate f?', 30],\n",
    " [3,\n",
    "  '2.1.3 The Trade-Off Between Prediction Accuracy and Model Interpretability',\n",
    "  33],\n",
    " [3, '2.1.4 Supervised Versus Unsupervised Learning', 35],\n",
    " [3, '2.1.5 Regression Versus Classification Problems', 37],\n",
    " [2, '2.2 Assessing Model Accuracy', 37],\n",
    " [3, '2.2.1 Measuring the Quality of Fit', 38],\n",
    " [3, '2.2.2 The Bias-Variance Trade-Off', 41],\n",
    " [3, '2.2.3 The Classification Setting', 44],\n",
    " [2, '2.3 Lab: Introduction to Python', 50],\n",
    " [3, '2.3.1 Getting Started', 50],\n",
    " [3, '2.3.2 Basic Commands', 50],\n",
    " [3, '2.3.3 Introduction to Numerical Python', 52],\n",
    " [3, '2.3.4 Graphics', 58],\n",
    " [3, '2.3.5 Sequences and Slice Notation', 61],\n",
    " [3, '2.3.6 Indexing Data', 61],\n",
    " [3, '2.3.7 Loading Data', 65],\n",
    " [3, '2.3.8 For Loops', 69],\n",
    " [3, '2.3.9 Additional Graphical and Numerical Summaries', 71],\n",
    " [2, '2.4 Exercises', 73],\n",
    " [3, 'Conceptual', 73],\n",
    " [3, 'Applied', 75],\n",
    " [1, '3 Linear Regression', 78],\n",
    " [2, '3.1 Simple Linear Regression', 79],\n",
    " [3, '3.1.1 Estimating the Coefficients', 80],\n",
    " [3, '3.1.2 Assessing the Accuracy of the Coefficient Estimates', 81],\n",
    " [3, '3.1.3 Assessing the Accuracy of the Model', 86],\n",
    " [2, '3.2 Multiple Linear Regression', 89],\n",
    " [3, '3.2.1 Estimating the Regression Coefficients', 90],\n",
    " [3, '3.2.2 Some Important Questions', 92],\n",
    " [2, '3.3 Other Considerations in the Regression Model', 100],\n",
    " [3, '3.3.1 Qualitative Predictors', 100],\n",
    " [3, '3.3.2 Extensions of the Linear Model', 103],\n",
    " [3, '3.3.3 Potential Problems', 109],\n",
    " [2, '3.4 The Marketing Plan', 118],\n",
    " [2, '3.5 Comparison of Linear Regression with K-Nearest Neighbors', 120],\n",
    " [2, '3.6 Lab: Linear Regression', 125],\n",
    " [3, '3.6.1 Importing packages', 125],\n",
    " [3, '3.6.2 Simple Linear Regression', 126],\n",
    " [3, '3.6.3 Multiple Linear Regression', 131],\n",
    " [3, '3.6.4 Multivariate Goodness of Fit', 132],\n",
    " [3, '3.6.5 Interaction Terms', 133],\n",
    " [3, '3.6.6 Non-linear Transformations of the Predictors', 134],\n",
    " [3, '3.6.7 Qualitative Predictors', 135],\n",
    " [2, '3.7 Exercises', 136],\n",
    " [3, 'Conceptual', 136],\n",
    " [3, 'Applied', 138],\n",
    " [1, '4 Classification', 144],\n",
    " [2, '4.1 An Overview of Classification', 144],\n",
    " [2, '4.2 Why Not Linear Regression?', 145],\n",
    " [2, '4.3 Logistic Regression', 147],\n",
    " [3, '4.3.1 The Logistic Model', 148],\n",
    " [3, '4.3.2 Estimating the Regression Coefficients', 149],\n",
    " [3, '4.3.3 Making Predictions', 150],\n",
    " [3, '4.3.4 Multiple Logistic Regression', 151],\n",
    " [3, '4.3.5 Multinomial Logistic Regression', 153],\n",
    " [2, '4.4 Generative Models for Classification', 155],\n",
    " [3, '4.4.1 Linear Discriminant Analysis for p = 1', 156],\n",
    " [3, '4.4.2 Linear Discriminant Analysis for p >1', 159],\n",
    " [3, '4.4.3 Quadratic Discriminant Analysis', 165],\n",
    " [3, '4.4.4 Naive Bayes', 167],\n",
    " [2, '4.5 A Comparison of Classification Methods', 170],\n",
    " [3, '4.5.1 An Analytical Comparison', 170],\n",
    " [3, '4.5.2 An Empirical Comparison', 173],\n",
    " [2, '4.6 Generalized Linear Models', 176],\n",
    " [3, '4.6.1 Linear Regression on the Bikeshare Data', 176],\n",
    " [3, '4.6.2 Poisson Regression on the Bikeshare Data', 178],\n",
    " [3, '4.6.3 Generalized Linear Models in Greater Generality', 181],\n",
    " [2, '4.7 Lab: Logistic Regression, LDA, QDA, and KNN', 182],\n",
    " [3, '4.7.1 The Stock Market Data', 182],\n",
    " [3, '4.7.2 Logistic Regression', 183],\n",
    " [3, '4.7.3 Linear Discriminant Analysis', 188],\n",
    " [3, '4.7.4 Quadratic Discriminant Analysis', 190],\n",
    " [3, '4.7.5 Naive Bayes', 191],\n",
    " [3, '4.7.6 K-Nearest Neighbors', 192],\n",
    " [3, '4.7.7 Linear and Poisson Regression on the Bikeshare Data', 197],\n",
    " [2, '4.8 Exercises', 202],\n",
    " [3, 'Conceptual', 202],\n",
    " [3, 'Applied', 205],\n",
    " [1, '5 Resampling Methods', 209],\n",
    " [2, '5.1 Cross-Validation', 210],\n",
    " [3, '5.1.1 The Validation Set Approach', 210],\n",
    " [3, '5.1.2 Leave-One-Out Cross-Validation', 212],\n",
    " [3, '5.1.3 k-Fold Cross-Validation', 214],\n",
    " [3, '5.1.4 Bias-Variance Trade-Off for k-Fold Cross-Validation', 216],\n",
    " [3, '5.1.5 Cross-Validation on Classification Problems', 217],\n",
    " [2, '5.2 The Bootstrap', 220],\n",
    " [2, '5.3 Lab: Cross-Validation and the Bootstrap', 223],\n",
    " [3, '5.3.1 The Validation Set Approach', 224],\n",
    " [3, '5.3.2 Cross-Validation', 225],\n",
    " [3, '5.3.3 The Bootstrap', 228],\n",
    " [2, '5.4 Exercises', 232],\n",
    " [3, 'Conceptual', 232],\n",
    " [3, 'Applied', 233],\n",
    " [1, '6 Linear Model Selection and Regularization', 237],\n",
    " [2, '6.1 Subset Selection', 239],\n",
    " [3, '6.1.1 Best Subset Selection', 239],\n",
    " [3, '6.1.2 Stepwise Selection', 241],\n",
    " [3, '6.1.3 Choosing the Optimal Model', 243],\n",
    " [2, '6.2 Shrinkage Methods', 248],\n",
    " [3, '6.2.1 Ridge Regression', 248],\n",
    " [3, '6.2.2 The Lasso', 252],\n",
    " [3, '6.2.3 Selecting the Tuning Parameter', 260],\n",
    " [2, '6.3 Dimension Reduction Methods', 261],\n",
    " [3, '6.3.1 Principal Components Regression', 262],\n",
    " [3, '6.3.2 Partial Least Squares', 268],\n",
    " [2, '6.4 Considerations in High Dimensions', 270],\n",
    " [3, '6.4.1 High-Dimensional Data', 270],\n",
    " [3, '6.4.2 What Goes Wrong in High Dimensions?', 271],\n",
    " [3, '6.4.3 Regression in High Dimensions', 273],\n",
    " [3, '6.4.4 Interpreting Results in High Dimensions', 274],\n",
    " [2, '6.5 Lab: Linear Models and Regularization Methods', 275],\n",
    " [3, '6.5.1 Subset Selection Methods', 276],\n",
    " [3, '6.5.2 Ridge Regression and the Lasso', 281],\n",
    " [3, '6.5.3 PCR and PLS Regression', 288],\n",
    " [2, '6.6 Exercises', 291],\n",
    " [3, 'Conceptual', 291],\n",
    " [3, 'Applied', 294],\n",
    " [1, '7 Moving Beyond Linearity', 297],\n",
    " [2, '7.1 Polynomial Regression', 298],\n",
    " [2, '7.2 Step Functions', 300],\n",
    " [2, '7.3 Basis Functions', 301],\n",
    " [2, '7.4 Regression Splines', 302],\n",
    " [3, '7.4.1 Piecewise Polynomials', 302],\n",
    " [3, '7.4.2 Constraints and Splines', 304],\n",
    " [3, '7.4.3 The Spline Basis Representation', 304],\n",
    " [3, '7.4.4 Choosing the Number and Locations of the Knots', 305],\n",
    " [3, '7.4.5 Comparison to Polynomial Regression', 307],\n",
    " [2, '7.5 Smoothing Splines', 308],\n",
    " [3, '7.5.1 An Overview of Smoothing Splines', 308],\n",
    " [3, '7.5.2 Choosing the Smoothing Parameter Î» ', 309],\n",
    " [2, '7.6 Local Regression', 311],\n",
    " [2, '7.7 Generalized Additive Models', 313],\n",
    " [3, '7.7.1 GAMs for Regression Problems', 314],\n",
    " [3, '7.7.2 GAMs for Classification Problems', 316],\n",
    " [2, '7.8 Lab: Non-Linear Modeling', 317],\n",
    " [3, '7.8.1 Polynomial Regression and Step Functions', 318],\n",
    " [3, '7.8.2 Splines', 323],\n",
    " [3, '7.8.3 Smoothing Splines and GAMs', 325],\n",
    " [3, '7.8.4 Local Regression', 332],\n",
    " [2, '7.9 Exercises', 333],\n",
    " [3, 'Conceptual', 333],\n",
    " [3, 'Applied', 335],\n",
    " [1, '8 Tree-Based Methods', 338],\n",
    " [2, '8.1 The Basics of Decision Trees', 338],\n",
    " [3, '8.1.1 Regression Trees', 338],\n",
    " [3, '8.1.2 Classification Trees', 344],\n",
    " [3, '8.1.3 Trees Versus Linear Models', 348],\n",
    " [3, '8.1.4 Advantages and Disadvantages of Trees', 348],\n",
    " [2,\n",
    "  '8.2 Bagging, Random Forests, Boosting, and Bayesian Additive Regression Trees',\n",
    "  350],\n",
    " [3, '8.2.1 Bagging', 350],\n",
    " [3, '8.2.2 Random Forests', 353],\n",
    " [3, '8.2.3 Boosting', 354],\n",
    " [3, '8.2.4 Bayesian Additive Regression Trees', 357],\n",
    " [3, '8.2.5 Summary of Tree Ensemble Methods', 360],\n",
    " [2, '8.3 Lab: Tree-Based Methods', 361],\n",
    " [3, '8.3.1 Fitting Classification Trees', 362],\n",
    " [3, '8.3.2 Fitting Regression Trees', 365],\n",
    " [3, '8.3.3 Bagging and Random Forests', 367],\n",
    " [3, '8.3.4 Boosting', 368],\n",
    " [3, '8.3.5 Bayesian Additive Regression Trees', 369],\n",
    " [2, '8.4 Exercises', 370],\n",
    " [3, 'Conceptual', 370],\n",
    " [3, 'Applied', 371],\n",
    " [1, '9 Support Vector Machines', 374],\n",
    " [2, '9.1 Maximal Margin Classifier', 374],\n",
    " [3, '9.1.1 What Is a Hyperplane?', 375],\n",
    " [3, '9.1.2 Classification Using a Separating Hyperplane', 375],\n",
    " [3, '9.1.3 The Maximal Margin Classifier', 377],\n",
    " [3, '9.1.4 Construction of the Maximal Margin Classifier', 379],\n",
    " [3, '9.1.5 The Non-separable Case', 379],\n",
    " [2, '9.2 Support Vector Classifiers', 380],\n",
    " [3, '9.2.1 Overview of the Support Vector Classifier', 380],\n",
    " [3, '9.2.2 Details of the Support Vector Classifier', 381],\n",
    " [2, '9.3 Support Vector Machines', 384],\n",
    " [3, '9.3.1 Classification with Non-Linear Decision Boundaries', 385],\n",
    " [3, '9.3.2 The Support Vector Machine', 386],\n",
    " [3, '9.3.3 An Application to the Heart Disease Data', 389],\n",
    " [2, '9.4 SVMs with More than Two Classes', 390],\n",
    " [3, '9.4.1 One-Versus-One Classification', 391],\n",
    " [3, '9.4.2 One-Versus-All Classification', 391],\n",
    " [2, '9.5 Relationship to Logistic Regression', 391],\n",
    " [2, '9.6 Lab: Support Vector Machines', 394],\n",
    " [3, '9.6.1 Support Vector Classifier', 394],\n",
    " [3, '9.6.2 Support Vector Machine', 397],\n",
    " [3, '9.6.3 ROC Curves', 399],\n",
    " [3, '9.6.4 SVM with Multiple Classes', 400],\n",
    " [3, '9.6.5 Application to Gene Expression Data', 401],\n",
    " [2, '9.7 Exercises', 402],\n",
    " [3, 'Conceptual', 402],\n",
    " [3, 'Applied', 403],\n",
    " [1, '10 Deep Learning', 406],\n",
    " [2, '10.1 Single Layer Neural Networks', 407],\n",
    " [2, '10.2 Multilayer Neural Networks', 409],\n",
    " [2, '10.3 Convolutional Neural Networks', 413],\n",
    " [3, '10.3.1 Convolution Layers', 414],\n",
    " [3, '10.3.2 Pooling Layers', 417],\n",
    " [3, '10.3.3 Architecture of a Convolutional Neural Network', 417],\n",
    " [3, '10.3.4 Data Augmentation', 418],\n",
    " [3, '10.3.5 Results Using a Pretrained Classifier', 419],\n",
    " [2, '10.4 Document Classification', 420],\n",
    " [2, '10.5 Recurrent Neural Networks', 423],\n",
    " [3, '10.5.1 Sequential Models for Document Classification', 425],\n",
    " [3, '10.5.2 Time Series Forecasting', 427],\n",
    " [3, '10.5.3 Summary of RNNs', 431],\n",
    " [2, '10.6 When to Use Deep Learning', 432],\n",
    " [2, '10.7 Fitting a Neural Network', 434],\n",
    " [3, '10.7.1 Backpropagation', 435],\n",
    " [3, '10.7.2 Regularization and Stochastic Gradient Descent', 436],\n",
    " [3, '10.7.3 Dropout Learning', 438],\n",
    " [3, '10.7.4 Network Tuning', 438],\n",
    " [2, '10.8 Interpolation and Double Descent', 439],\n",
    " [2, '10.9 Lab: Deep Learning', 442],\n",
    " [3, '10.9.1 Single Layer Network on Hitters Data', 444],\n",
    " [3, '10.9.2 Multilayer Network on the MNIST Digit Data', 451],\n",
    " [3, '10.9.3 Convolutional Neural Networks', 455],\n",
    " [3, '10.9.4 Using Pretrained CNN Models', 459],\n",
    " [3, '10.9.5 IMDB Document Classification', 461],\n",
    " [3, '10.9.6 Recurrent Neural Networks', 465],\n",
    " [2, '10.10 Exercises', 472],\n",
    " [3, 'Conceptual', 472],\n",
    " [3, 'Applied', 473],\n",
    " [1, '11 Survival Analysis and Censored Data', 475],\n",
    " [2, '11.1 Survival and Censoring Times', 476],\n",
    " [2, '11.2 A Closer Look at Censoring', 476],\n",
    " [2, '11.3 The Kaplanâ€“Meier Survival Curve', 478],\n",
    " [2, '11.4 The Log-Rank Test', 480],\n",
    " [2, '11.5 Regression Models With a Survival Response', 482],\n",
    " [3, '11.5.1 The Hazard Function', 482],\n",
    " [3, '11.5.2 Proportional Hazards', 484],\n",
    " [3, '11.5.3 Example: Brain Cancer Data', 488],\n",
    " [3, '11.5.4 Example: Publication Data', 488],\n",
    " [2, '11.6 Shrinkage for the Cox Model', 490],\n",
    " [2, '11.7 Additional Topics', 492],\n",
    " [3, '11.7.1 Area Under the Curve for Survival Analysis', 492],\n",
    " [3, '11.7.2 Choice of Time Scale', 493],\n",
    " [3, '11.7.3 Time-Dependent Covariates', 494],\n",
    " [3, '11.7.4 Checking the Proportional Hazards Assumption', 494],\n",
    " [3, '11.7.5 Survival Trees', 494],\n",
    " [2, '11.8 Lab: Survival Analysis', 495],\n",
    " [3, '11.8.1 Brain Cancer Data', 495],\n",
    " [3, '11.8.2 Publication Data', 499],\n",
    " [3, '11.8.3 Call Center Data', 500],\n",
    " [2, '11.9 Exercises', 504],\n",
    " [3, 'Conceptual', 504],\n",
    " [3, 'Applied', 508],\n",
    " [1, '12 Unsupervised Learning', 509],\n",
    " [2, '12.1 The Challenge of Unsupervised Learning', 509],\n",
    " [2, '12.2 Principal Components Analysis', 510],\n",
    " [3, '12.2.1 What Are Principal Components?', 511],\n",
    " [3, '12.2.2 Another Interpretation of Principal Components', 514],\n",
    " [3, '12.2.3 The Proportion of Variance Explained', 516],\n",
    " [3, '12.2.4 More on PCA', 518],\n",
    " [3, '12.2.5 Other Uses for Principal Components', 521],\n",
    " [2, '12.3 Missing Values and Matrix Completion', 521],\n",
    " [2, '12.4 Clustering Methods', 526],\n",
    " [3, '12.4.1 K-Means Clustering', 527],\n",
    " [3, '12.4.2 Hierarchical Clustering', 531],\n",
    " [3, '12.4.3 Practical Issues in Clustering', 538],\n",
    " [2, '12.5 Lab: Unsupervised Learning', 541],\n",
    " [3, '12.5.1 Principal Components Analysis', 541],\n",
    " [3, '12.5.2 Matrix Completion', 545],\n",
    " [3, '12.5.3 Clustering', 548],\n",
    " [3, '12.5.4 NCI60 Data Example', 552],\n",
    " [2, '12.6 Exercises', 558],\n",
    " [3, 'Conceptual', 558],\n",
    " [3, 'Applied', 560],\n",
    " [1, '13 Multiple Testing', 563],\n",
    " [2, '13.1 A Quick Review of Hypothesis Testing', 564],\n",
    " [3, '13.1.1 Testing a Hypothesis', 564],\n",
    " [3, '13.1.2 Type I and Type II Errors', 568],\n",
    " [2, '13.2 The Challenge of Multiple Testing', 569],\n",
    " [2, '13.3 The Family-Wise Error Rate', 571],\n",
    " [3, '13.3.1 What is the Family-Wise Error Rate?', 571],\n",
    " [3, '13.3.2 Approaches to Control the Family-Wise Error Rate', 573],\n",
    " [3, '13.3.3 Trade-Off Between the FWER and Power', 578],\n",
    " [2, '13.4 The False Discovery Rate', 579],\n",
    " [3, '13.4.1 Intuition for the False Discovery Rate', 579],\n",
    " [3, '13.4.2 The Benjaminiâ€“Hochberg Procedure', 581],\n",
    " [2, '13.5 A Re-Sampling Approach to p-Values and False Discovery Rates', 583],\n",
    " [3, '13.5.1 A Re-Sampling Approach to the p-Value', 584],\n",
    " [3, '13.5.2 A Re-Sampling Approach to the False Discovery Rate', 585],\n",
    " [3, '13.5.3 When Are Re-Sampling Approaches Useful?', 587],\n",
    " [2, '13.6 Lab: Multiple Testing', 589],\n",
    " [3, '13.6.1 Review of Hypothesis Tests', 589],\n",
    " [3, '13.6.2 Family-Wise Error Rate', 591],\n",
    " [3, '13.6.3 False Discovery Rate', 594],\n",
    " [3, '13.6.4 A Re-Sampling Approach', 596],\n",
    " [2, '13.7 Exercises', 599],\n",
    " [3, 'Conceptual', 599],\n",
    " [3, 'Applied', 601],\n",
    " [1, 'Index', 603]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "131af0c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ðŸ“„ Guardado en PDF-GenAI-Challenge_2.md\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "def clean_md_headings(md_text: str) -> str:\n",
    "    \"\"\"\n",
    "    Elimina todos los '#' al inicio de las lÃ­neas en el .md.\n",
    "    \"\"\"\n",
    "    return re.sub(r'^\\s*#+\\s*', '', md_text, flags=re.MULTILINE)\n",
    "\n",
    "\n",
    "\n",
    "def insert_outline_in_md(md_path, output_path):\n",
    "    # 2. Leer el .md y limpiar headings previos\n",
    "    with open(md_path, \"r\", encoding=\"utf-8\") as f:\n",
    "        md_text = f.read()\n",
    "    md_text = clean_md_headings(md_text)\n",
    "\n",
    "    # 4. Guardar resultado\n",
    "    with open(output_path, \"w\", encoding=\"utf-8\") as f:\n",
    "        f.write(md_text)\n",
    "\n",
    "    print(f\"\\nðŸ“„ Guardado en {output_path}\")\n",
    "\n",
    "insert_outline_in_md(md_path=\"data/PDF-GenAI-Challenge.md\", output_path=\"PDF-GenAI-Challenge_2.md\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83d63135",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
