user_input,retrieved_contexts,response,reference,faithfulness,answer_relevancy,context_precision,context_recall
What is the primary motivation for supervised learning?,"[""### 2.1.4 Supervised Versus Unsupervised Learning  \nMost statistical learning problems fall into one of two categories: supervised or unsupervised . The examples that we have discussed so far in this chapter all fall into the supervised learning domain. For each observation of the predictor measurement(s) x i , i = 1         n there is an associated response measurement y i . We wish to fit a model that relates the response to the predictors, with the aim of accurately predicting the response for future observations (prediction) or better understanding the relationship between the response and the predictors (inference). Many classical statistical learning methods such as linear regression and logistic regression (Chapter 4), as well as more modern approaches such as GAM, boosting, and support vector machines, operate in the supervised learning domain. The vast majority of this book is devoted to this setting.  \nBy contrast, unsupervised learning describes the somewhat more challenging situation in which for every observation i = 1         n , we observe a vector of measurements x i but no associated response y i . It is not possible to fit a linear regression model, since there is no response variable to predict. In this setting, we are in some sense working blind; the situation is referred to as unsupervised because we lack a response variable that can supervise our analysis. What sort of statistical analysis is possible? We can seek to understand the relationships between the variables or between the observations. One statistical learning tool that we may use in this setting is cluster analysis , or clustering. The goal of cluster analysis is to ascertain, on the basis of x 1         x n , whether the observations fall into relatively distinct groups. For example, in a market segmentation study we might observe multiple characteristics (variables) for potential customers, such as zip code, family income, and shopping habits. We might believe that the customers fall into different groups, such as big spenders versus low spenders. If the information about each customer's spending patterns were available, then a supervised analysis would be possible. However, this information is not available-that is, we do not know whether each potential customer is a big spender or not. In this setting, we can try to cluster the customers on the basis of the variables measured, in order to identify supervised unsupervised  \nlogistic regression cluster analysis  \nFIGURE 2.8. A clustering data set involving three groups. Each group is shown using a different colored symbol. Left: The three groups are well-separated. In this setting, a clustering approach should successfully identify the three groups. Right: There is some overlap among the groups. Now the clustering task is more challenging.  \n<!-- image -->  \ndistinct groups of potential customers. Identifying such groups can be of interest because it might be that the groups differ with respect to some property of interest, such as spending habits.  \nIn the examples shown in Figure 2.8, there are only two variables, and so one can simply visually inspect the scatterplots of the observations in order to identify clusters. However, in practice, we often encounter data sets that contain many more than two variables. In this case, we cannot easily plot the observations. For instance, if there are p variables in our data set, then p ( p -1) glyph[triangleleft] 2 distinct scatterplots can be made, and visual inspection is simply not a viable way to identify clusters. For this reason, automated clustering methods are important. We discuss clustering and other unsupervised learning approaches in Chapter 12.  \nFigure 2.8 provides a simple illustration of the clustering problem. We have plotted 150 observations with measurements on two variables, X 1 and X 2 . Each observation corresponds to one of three distinct groups. For illustrative purposes, we have plotted the members of each group using different colors and symbols. However, in practice the group memberships are unknown, and the goal is to determine the group to which each observation belongs. In the left-hand panel of Figure 2.8, this is a relatively easy task because the groups are well-separated. By contrast, the right-hand panel illustrates a more challenging setting in which there is some overlap between the groups. A clustering method could not be expected to assign all of the overlapping points to their correct group (blue, green, or orange).  \nMany problems fall naturally into the supervised or unsupervised learning paradigms. However, sometimes the question of whether an analysis should be considered supervised or unsupervised is less clear-cut. For instance, suppose that we have a set of n observations. For m of the observations, where m&lt;n , we have both predictor measurements and a response  \nmeasurement. For the remaining n -m observations, we have predictor measurements but no response measurement. Such a scenario can arise if the predictors can be measured relatively cheaply but the corresponding responses are much more expensive to collect. We refer to this setting as a semi-supervised learning problem. In this setting, we wish to use a statistical learning method that can incorporate the m observations for which response measurements are available as well as the n -m observations for which they are not. Although this is an interesting topic, it is beyond the scope of this book."", ""## 12.1 The Challenge of Unsupervised Learning  \nSupervised learning is a well-understood area. In fact, if you have read the preceding chapters in this book, then you should by now have a good grasp of supervised learning. For instance, if you are asked to predict a binary outcome from a data set, you have a very well developed set of tools at your disposal (such as logistic regression, linear discriminant analysis, classification trees, support vector machines, and more) as well as a clear  \nhttps://doi.org/10.1007/978-3-031-38747-0\\_12  \nunderstanding of how to assess the quality of the results obtained (using cross-validation, validation on an independent test set, and so forth).  \nTechniques for unsupervised learning are of growing importance in a number of fields. A cancer researcher might assay gene expression levels in 100 patients with breast cancer. He or she might then look for subgroups among the breast cancer samples, or among the genes, in order to obtain a better understanding of the disease. An online shopping site might try to identify groups of shoppers with similar browsing and purchase histories, as well as items that are of particular interest to the shoppers within each group. Then an individual shopper can be preferentially shown the items in which he or she is particularly likely to be interested, based on the purchase histories of similar shoppers. A search engine might choose which search results to display to a particular individual based on the click histories of other individuals with similar search patterns. These statistical learning tasks, and many more, can be performed via unsupervised learning techniques.  \nIn contrast, unsupervised learning is often much more challenging. The exercise tends to be more subjective, and there is no simple goal for the analysis, such as prediction of a response. Unsupervised learning is often performed as part of an exploratory data analysis . Furthermore, it can be hard to assess the results obtained from unsupervised learning methods, since there is no universally accepted mechanism for performing crossvalidation or validating results on an independent data set. The reason for this difference is simple. If we fit a predictive model using a supervised learning technique, then it is possible to check our work by seeing how well our model predicts the response Y on observations not used in fitting the model. However, in unsupervised learning, there is no way to check our work because we don't know the true answer-the problem is unsupervised."", ""## A Brief History of Statistical Learning  \nThough the term statistical learning is fairly new, many of the concepts that underlie the field were developed long ago. At the beginning of the nineteenth century, the method of least squares was developed, implementing the earliest form of what is now known as linear regression . The approach was first successfully applied to problems in astronomy. Linear regression is used for predicting quantitative values, such as an individual's salary. In order to predict qualitative values, such as whether a patient survives or dies, or whether the stock market increases or decreases, linear discriminant analysis was proposed in 1936. In the 1940s, various authors put forth an alternative approach, logistic regression . In the early 1970s, the term generalized linear model was developed to describe an entire class of statistical learning methods that include both linear and logistic regression as special cases.  \nSince that time, statistical learning has emerged as a new subfield in statistics, focused on supervised and unsupervised modeling and prediction. In recent years, progress in statistical learning has been marked by the increasing availability of powerful and relatively user-friendly software, such as the popular and freely available Python system. This has the potential to continue the transformation of the field from a set of techniques used and  \nBy the end of the 1970s, many more techniques for learning from data were available. However, they were almost exclusively linear methods because fitting non-linear relationships was computationally difficult at the time. By the 1980s, computing technology had finally improved sufficiently that non-linear methods were no longer computationally prohibitive. In the mid 1980s, classification and regression trees were developed, followed shortly by generalized additive models . Neural networks gained popularity in the 1980s, and support vector machines arose in the 1990s.  \ndeveloped by statisticians and computer scientists to an essential toolkit for a much broader community."", '# 12 Unsupervised Learning  \n<!-- image -->  \nMost of this book concerns supervised learning methods such as regression and classification. In the supervised learning setting, we typically have access to a set of p features X 1  X 2         X p , measured on n observations, and a response Y also measured on those same n observations. The goal is then to predict Y using X 1  X 2         X p .  \nThis chapter will instead focus on unsupervised learning , a set of statistical tools intended for the setting in which we have only a set of features X 1  X 2         X p measured on n observations. We are not interested in prediction, because we do not have an associated response variable Y . Rather, the goal is to discover interesting things about the measurements on X 1  X 2         X p . Is there an informative way to visualize the data? Can we discover subgroups among the variables or among the observations? Unsupervised learning refers to a diverse set of techniques for answering questions such as these. In this chapter, we will focus on two particular types of unsupervised learning: principal components analysis , a tool used for data visualization or data pre-processing before supervised techniques are applied, and clustering , a broad class of methods for discovering unknown subgroups in data.', ""## This Book  \nThe Elements of Statistical Learning (ESL) by Hastie, Tibshirani, and Friedman was first published in 2001. Since that time, it has become an important reference on the fundamentals of statistical machine learning. Its success derives from its comprehensive and detailed treatment of many important topics in statistical learning, as well as the fact that (relative to many upper-level statistics textbooks) it is accessible to a wide audience. However, the greatest factor behind the success of ESL has been its topical nature. At the time of its publication, interest in the field of statistical learning was starting to explode. ESL provided one of the first accessible and comprehensive introductions to the topic.  \nIn recent years, new and improved software packages have significantly eased the implementation burden for many statistical learning methods. At the same time, there has been growing recognition across a number of fields, from business to health care to genetics to the social sciences and beyond, that statistical learning is a powerful tool with important practical applications. As a result, the field has moved from one of primarily academic interest to a mainstream discipline, with an enormous potential audience. This trend will surely continue with the increasing availability of enormous quantities of data and the software to analyze it.  \nSince ESL was first published, the field of statistical learning has continued to flourish. The field's expansion has taken two forms. The most obvious growth has involved the development of new and improved statistical learning approaches aimed at answering a range of scientific questions across a number of fields. However, the field of statistical learning has also expanded its audience. In the 1990s, increases in computational power generated a surge of interest in the field from non-statisticians who were eager to use cutting-edge statistical tools to analyze their data. Unfortunately, the highly technical nature of these approaches meant that the user community remained primarily restricted to experts in statistics, computer science, and related fields with the training (and time) to understand and implement them.  \nThe purpose of An Introduction to Statistical Learning (ISL) is to facilitate the transition of statistical learning from an academic to a mainstream field. ISL is not intended to replace ESL, which is a far more comprehensive text both in terms of the number of approaches considered and the depth to which they are explored. We consider ESL to be an important companion for professionals (with graduate degrees in statistics, machine learning, or related fields) who need to understand the technical details behind statistical learning approaches. However, the community of users of statistical learning techniques has expanded to include individuals with a wider range of interests and backgrounds. Therefore, there is a place for a less technical and more accessible version of ESL.  \nIn teaching these topics over the years, we have discovered that they are of interest to master's and PhD students in fields as disparate as business administration, biology, and computer science, as well as to quantitativelyoriented upper-division undergraduates. It is important for this diverse group to be able to understand the models, intuitions, and strengths and weaknesses of the various approaches. But for this audience, many of the technical details behind statistical learning methods, such as optimization algorithms and theoretical properties, are not of primary interest. We believe that these students do not need a deep understanding of these aspects in order to become informed users of the various methodologies, and in order to contribute to their chosen fields through the use of statistical learning tools.  \nISL is based on the following four premises.  \n1. Many statistical learning methods are relevant and useful in a wide range of academic and non-academic disciplines, beyond just the statistical sciences. We believe that many contemporary statistical learning procedures should, and will, become as widely available and used as is currently the case for classical methods such as linear regression. As a result, rather than attempting to consider every possible approach (an impossible task), we have concentrated on presenting the methods that we believe are most widely applicable.\n2. Statistical learning should not be viewed as a series of black boxes. No single approach will perform well in all possible applications. Without understanding all of the cogs inside the box, or the interaction between those cogs, it is impossible to select the best box. Hence, we have attempted to carefully describe the model, intuition, assumptions, and trade-offs behind each of the methods that we consider.\n3. While it is important to know what job is performed by each cog, it is not necessary to have the skills to construct the machine inside the box! Thus, we have minimized discussion of technical details related to fitting procedures and theoretical properties. We assume that the reader is comfortable with basic mathematical concepts, but we do not assume a graduate degree in the mathematical sciences. For instance, we have almost completely avoided the use of matrix algebra, and it is possible to understand the entire book without a detailed knowledge of matrices and vectors.\n4. We presume that the reader is interested in applying statistical learning methods to real-world problems. In order to facilitate this, as well as to motivate the techniques discussed, we have devoted a section within each chapter to computer labs. In each lab, we walk the reader through a realistic application of the methods considered in that chapter. When we have taught this material in our courses, we have allocated roughly one-third of classroom time to working through the labs, and we have found them to be extremely useful. Many of the less computationally-oriented students who were initially intimidated by the labs got the hang of things over the course of the quarter or semester. This book originally appeared (2013, second edition 2021)\n5. 8 1. Introduction  \nwith computer labs written in the R language. Since then, there has been increasing demand for Python implementations of the important techniques in statistical learning. Consequently, this version has labs in Python . There are a rapidly growing number of Python packages available, and by examination of the imports at the beginning of each lab, readers will see that we have carefully selected and used the most appropriate. We have also supplied some additional code and functionality in our package ISLP . However, the labs in ISL are selfcontained, and can be skipped if the reader wishes to use a different software package or does not wish to apply the methods discussed to real-world problems."", '# 1 Introduction  \n## An Overview of Statistical Learning  \nStatistical learning refers to a vast set of tools for understanding data . These tools can be classified as supervised or unsupervised . Broadly speaking, supervised statistical learning involves building a statistical model for predicting, or estimating, an output based on one or more inputs . Problems of this nature occur in fields as diverse as business, medicine, astrophysics, and public policy. With unsupervised statistical learning, there are inputs but no supervising output; nevertheless we can learn relationships and structure from such data. To provide an illustration of some applications of statistical learning, we briefly discuss three real-world data sets that are considered in this book.']","The primary motivation for supervised learning is to fit a model that relates the response to the predictors, with the aim of accurately predicting the response for future observations (prediction) or better understanding the relationship between the response and the predictors (inference).","The primary motivation for supervised learning is to build a statistical model for predicting or estimating an output based on one or more inputs. This is useful in scenarios where we have a set of features (predictors) and a known outcome (response) and we want to predict the outcome for new, unseen data.",1.0,,0.91666666664375,1.0
What are the two main types of problems that supervised learning addresses?,"[""### 2.1.4 Supervised Versus Unsupervised Learning  \nMost statistical learning problems fall into one of two categories: supervised or unsupervised . The examples that we have discussed so far in this chapter all fall into the supervised learning domain. For each observation of the predictor measurement(s) x i , i = 1         n there is an associated response measurement y i . We wish to fit a model that relates the response to the predictors, with the aim of accurately predicting the response for future observations (prediction) or better understanding the relationship between the response and the predictors (inference). Many classical statistical learning methods such as linear regression and logistic regression (Chapter 4), as well as more modern approaches such as GAM, boosting, and support vector machines, operate in the supervised learning domain. The vast majority of this book is devoted to this setting.  \nBy contrast, unsupervised learning describes the somewhat more challenging situation in which for every observation i = 1         n , we observe a vector of measurements x i but no associated response y i . It is not possible to fit a linear regression model, since there is no response variable to predict. In this setting, we are in some sense working blind; the situation is referred to as unsupervised because we lack a response variable that can supervise our analysis. What sort of statistical analysis is possible? We can seek to understand the relationships between the variables or between the observations. One statistical learning tool that we may use in this setting is cluster analysis , or clustering. The goal of cluster analysis is to ascertain, on the basis of x 1         x n , whether the observations fall into relatively distinct groups. For example, in a market segmentation study we might observe multiple characteristics (variables) for potential customers, such as zip code, family income, and shopping habits. We might believe that the customers fall into different groups, such as big spenders versus low spenders. If the information about each customer's spending patterns were available, then a supervised analysis would be possible. However, this information is not available-that is, we do not know whether each potential customer is a big spender or not. In this setting, we can try to cluster the customers on the basis of the variables measured, in order to identify supervised unsupervised  \nlogistic regression cluster analysis  \nFIGURE 2.8. A clustering data set involving three groups. Each group is shown using a different colored symbol. Left: The three groups are well-separated. In this setting, a clustering approach should successfully identify the three groups. Right: There is some overlap among the groups. Now the clustering task is more challenging.  \n<!-- image -->  \ndistinct groups of potential customers. Identifying such groups can be of interest because it might be that the groups differ with respect to some property of interest, such as spending habits.  \nIn the examples shown in Figure 2.8, there are only two variables, and so one can simply visually inspect the scatterplots of the observations in order to identify clusters. However, in practice, we often encounter data sets that contain many more than two variables. In this case, we cannot easily plot the observations. For instance, if there are p variables in our data set, then p ( p -1) glyph[triangleleft] 2 distinct scatterplots can be made, and visual inspection is simply not a viable way to identify clusters. For this reason, automated clustering methods are important. We discuss clustering and other unsupervised learning approaches in Chapter 12.  \nFigure 2.8 provides a simple illustration of the clustering problem. We have plotted 150 observations with measurements on two variables, X 1 and X 2 . Each observation corresponds to one of three distinct groups. For illustrative purposes, we have plotted the members of each group using different colors and symbols. However, in practice the group memberships are unknown, and the goal is to determine the group to which each observation belongs. In the left-hand panel of Figure 2.8, this is a relatively easy task because the groups are well-separated. By contrast, the right-hand panel illustrates a more challenging setting in which there is some overlap between the groups. A clustering method could not be expected to assign all of the overlapping points to their correct group (blue, green, or orange).  \nMany problems fall naturally into the supervised or unsupervised learning paradigms. However, sometimes the question of whether an analysis should be considered supervised or unsupervised is less clear-cut. For instance, suppose that we have a set of n observations. For m of the observations, where m&lt;n , we have both predictor measurements and a response  \nmeasurement. For the remaining n -m observations, we have predictor measurements but no response measurement. Such a scenario can arise if the predictors can be measured relatively cheaply but the corresponding responses are much more expensive to collect. We refer to this setting as a semi-supervised learning problem. In this setting, we wish to use a statistical learning method that can incorporate the m observations for which response measurements are available as well as the n -m observations for which they are not. Although this is an interesting topic, it is beyond the scope of this book."", ""## 4.1 An Overview of Classification  \nClassification problems occur often, perhaps even more so than regression problems. Some examples include:  \nqualitative classification  \nclassifier logistic  \nregression linear discriminant analysis quadratic discriminant analysis naive Bayes K -nearest neighbors generalized linear models Poisson regression  \n1. A person arrives at the emergency room with a set of symptoms that could possibly be attributed to one of three medical conditions. Which of the three conditions does the individual have?\n2. An online banking service must be able to determine whether or not a transaction being performed on the site is fraudulent, on the basis of the user's IP address, past transaction history, and so forth.\n3. On the basis of DNA sequence data for a number of patients with and without a given disease, a biologist would like to figure out which DNA mutations are deleterious (disease-causing) and which are not.  \nJust as in the regression setting, in the classification setting we have a set of training observations ( x 1  y 1 )         ( x n  y n ) that we can use to build a classifier. We want our classifier to perform well not only on the training data, but also on test observations that were not used to train the classifier.  \nIt is worth noting that Figure 4.1 displays a very pronounced relationship between the predictor balance and the response default . In most real applications, the relationship between the predictor and the response will not be nearly so strong. However, for the sake of illustrating the classification procedures discussed in this chapter, we use an example in which the relationship between the predictor and the response is somewhat exaggerated.  \nIn this chapter, we will illustrate the concept of classification using the simulated Default data set. We are interested in predicting whether an individual will default on his or her credit card payment, on the basis of annual income and monthly credit card balance. The data set is displayed in Figure 4.1. In the left-hand panel of Figure 4.1, we have plotted annual income and monthly credit card balance for a subset of 10  000 individuals. The individuals who defaulted in a given month are shown in orange, and those who did not in blue. (The overall default rate is about 3 %, so we have plotted only a fraction of the individuals who did not default.) It appears that individuals who defaulted tended to have higher credit card balances than those who did not. In the center and right-hand panels of Figure 4.1, two pairs of boxplots are shown. The first shows the distribution of balance split by the binary default variable; the second is a similar plot for income . In this chapter, we learn how to build a model to predict default ( Y ) for any given value of balance ( X 1 ) and income ( X 2 ). Since Y is not quantitative, the simple linear regression model of Chapter 3 is not a good choice: we will elaborate on this further in Section 4.2."", ""### 2.1.5 Regression Versus Classification Problems  \nVariables can be characterized as either quantitative or qualitative (also known as categorical ). Quantitative variables take on numerical values. Examples include a person's age, height, or income, the value of a house, and the price of a stock. In contrast, qualitative variables take on values in one of K different classes , or categories. Examples of qualitative variables include a person's marital status (married or not), the brand of product purchased (brand A, B, or C), whether a person defaults on a debt (yes or no), or a cancer diagnosis (Acute Myelogenous Leukemia, Acute Lymphoblastic Leukemia, or No Leukemia). We tend to refer to problems with a quantitative response as regression problems, while those involving a qualitative response are often referred to as classification problems. However, the distinction is not always that crisp. Least squares linear regression (Chapter 3) is used with a quantitative response, whereas logistic regression (Chapter 4) is typically used with a qualitative (two-class, or binary ) response. Thus, despite its name, logistic regression is a classification method. But since it estimates class probabilities, it can be thought of as a regression method as well. Some statistical methods, such as K -nearest neighbors (Chapters 2 and 4) and boosting (Chapter 8), can be used in the case of either quantitative or qualitative responses.  \nWe tend to select statistical learning methods on the basis of whether the response is quantitative or qualitative; i.e. we might use linear regression when quantitative and logistic regression when qualitative. However, whether the predictors are qualitative or quantitative is generally considered less important. Most of the statistical learning methods discussed in this book can be applied regardless of the predictor variable type, provided that any qualitative predictors are properly coded before the analysis is performed. This is discussed in Chapter 3."", ""## 12.1 The Challenge of Unsupervised Learning  \nSupervised learning is a well-understood area. In fact, if you have read the preceding chapters in this book, then you should by now have a good grasp of supervised learning. For instance, if you are asked to predict a binary outcome from a data set, you have a very well developed set of tools at your disposal (such as logistic regression, linear discriminant analysis, classification trees, support vector machines, and more) as well as a clear  \nhttps://doi.org/10.1007/978-3-031-38747-0\\_12  \nunderstanding of how to assess the quality of the results obtained (using cross-validation, validation on an independent test set, and so forth).  \nTechniques for unsupervised learning are of growing importance in a number of fields. A cancer researcher might assay gene expression levels in 100 patients with breast cancer. He or she might then look for subgroups among the breast cancer samples, or among the genes, in order to obtain a better understanding of the disease. An online shopping site might try to identify groups of shoppers with similar browsing and purchase histories, as well as items that are of particular interest to the shoppers within each group. Then an individual shopper can be preferentially shown the items in which he or she is particularly likely to be interested, based on the purchase histories of similar shoppers. A search engine might choose which search results to display to a particular individual based on the click histories of other individuals with similar search patterns. These statistical learning tasks, and many more, can be performed via unsupervised learning techniques.  \nIn contrast, unsupervised learning is often much more challenging. The exercise tends to be more subjective, and there is no simple goal for the analysis, such as prediction of a response. Unsupervised learning is often performed as part of an exploratory data analysis . Furthermore, it can be hard to assess the results obtained from unsupervised learning methods, since there is no universally accepted mechanism for performing crossvalidation or validating results on an independent data set. The reason for this difference is simple. If we fit a predictive model using a supervised learning technique, then it is possible to check our work by seeing how well our model predicts the response Y on observations not used in fitting the model. However, in unsupervised learning, there is no way to check our work because we don't know the true answer-the problem is unsupervised."", '# 1 Introduction  \n## An Overview of Statistical Learning  \nStatistical learning refers to a vast set of tools for understanding data . These tools can be classified as supervised or unsupervised . Broadly speaking, supervised statistical learning involves building a statistical model for predicting, or estimating, an output based on one or more inputs . Problems of this nature occur in fields as diverse as business, medicine, astrophysics, and public policy. With unsupervised statistical learning, there are inputs but no supervising output; nevertheless we can learn relationships and structure from such data. To provide an illustration of some applications of statistical learning, we briefly discuss three real-world data sets that are considered in this book.', '# 12 Unsupervised Learning  \n<!-- image -->  \nMost of this book concerns supervised learning methods such as regression and classification. In the supervised learning setting, we typically have access to a set of p features X 1  X 2         X p , measured on n observations, and a response Y also measured on those same n observations. The goal is then to predict Y using X 1  X 2         X p .  \nThis chapter will instead focus on unsupervised learning , a set of statistical tools intended for the setting in which we have only a set of features X 1  X 2         X p measured on n observations. We are not interested in prediction, because we do not have an associated response variable Y . Rather, the goal is to discover interesting things about the measurements on X 1  X 2         X p . Is there an informative way to visualize the data? Can we discover subgroups among the variables or among the observations? Unsupervised learning refers to a diverse set of techniques for answering questions such as these. In this chapter, we will focus on two particular types of unsupervised learning: principal components analysis , a tool used for data visualization or data pre-processing before supervised techniques are applied, and clustering , a broad class of methods for discovering unknown subgroups in data.']","Supervised learning primarily addresses two main types of problems: **regression** and **classification**.

1. **Regression Problems**: These involve predicting a quantitative response. Examples of quantitative variables include a person's age, height, or income. In regression problems, the goal is to predict a numerical value based on input features.

2. **Classification Problems**: These involve predicting a qualitative response, also known as categorical. Examples of qualitative variables include a person's marital status (married or not) or whether a person defaults on a debt (yes or no). In classification problems, the goal is to assign input data into one of several predefined categories or classes.","Supervised learning addresses two main types of problems: regression and classification. Regression problems involve predicting a continuous or quantitative output, while classification problems involve predicting a qualitative or categorical output.",1.0,,0.9266666666481335,1.0
Describe the difference between training data and test data in the context of machine learning.,"['## 5.1 Cross-Validation  \nIn Chapter 2 we discuss the distinction between the test error rate and the training error rate . The test error is the average error that results from using a statistical learning method to predict the response on a new observationthat is, a measurement that was not used in training the method. Given a data set, the use of a particular statistical learning method is warranted if it results in a low test error. The test error can be easily calculated if a designated test set is available. Unfortunately, this is usually not the case. In contrast, the training error can be easily calculated by applying the statistical learning method to the observations used in its training. But as we saw in Chapter 2, the training error rate often is quite different from the test error rate, and in particular the former can dramatically underestimate the latter.  \nIn Sections 5.1.1-5.1.4, for simplicity we assume that we are interested in performing regression with a quantitative response. In Section 5.1.5 we consider the case of classification with a qualitative response. As we will see, the key concepts remain the same regardless of whether the response is quantitative or qualitative.  \nIn the absence of a very large designated test set that can be used to directly estimate the test error rate, a number of techniques can be used to estimate this quantity using the available training data. Some methods make a mathematical adjustment to the training error rate in order to estimate the test error rate. Such approaches are discussed in Chapter 6. In this section, we instead consider a class of methods that estimate the test error rate by holding out a subset of the training observations from the fitting process, and then applying the statistical learning method to those held out observations.', '### Applied  \n8. In this exercise, we will generate simulated data, and will then use this data to perform forward and backward stepwise selection.\n2. (a) Create a random number generator and use its normal() method to generate a predictor X of length n = 100 , as well as a noise vector /epsilon1 of length n = 100 .\n3. (b) Generate a response vector Y of length n = 100 according to the model  \n$$Y = \\beta _ { 0 } + \\beta _ { 1 } X + \\beta _ { 2 } X ^ { 2 } + \\beta _ { 3 } X ^ { 3 } + \\epsilon ,$$  \nwhere β 0 , β 1 , β 2 , and β 3 are constants of your choice.  \n- (c) Use forward stepwise selection in order to select a model containing the predictors XX 2         X 10 . What is the model obtained according to C p ? Report the coefficients of the model obtained.\n- (d) Repeat (c), using backwards stepwise selection. How does your answer compare to the results in (c)?\n- (e) Now fit a lasso model to the simulated data, again using XX 2         X 10 as predictors. Use cross-validation to select the optimal value of λ . Create plots of the cross-validation error as a function of λ . Report the resulting coefficient estimates, and discuss the results obtained.\n- (f) Now generate a response vector Y according to the model  \n$$Y = \\beta _ { 0 } + \\beta _ { 7 } X ^ { 7 } + \\epsilon ,$$  \nand perform forward stepwise selection and the lasso. Discuss the results obtained.  \n9. In this exercise, we will predict the number of applications received using the other variables in the College data set.\n2. (a) Split the data set into a training set and a test set.\n3. (b) Fit a linear model using least squares on the training set, and report the test error obtained.\n4. (c) Fit a ridge regression model on the training set, with λ chosen by cross-validation. Report the test error obtained.\n5. (d) Fit a lasso model on the training set, with λ chosen by crossvalidation. Report the test error obtained, along with the number of non-zero coefficient estimates.\n6. (e) Fit a PCR model on the training set, with M chosen by crossvalidation. Report the test error obtained, along with the value of M selected by cross-validation.\n7. (f) Fit a PLS model on the training set, with M chosen by crossvalidation. Report the test error obtained, along with the value of M selected by cross-validation.  \n- (g) Comment on the results obtained. How accurately can we predict the number of college applications received? Is there much difference among the test errors resulting from these five approaches?\n10. We have seen that as the number of features used in a model increases, the training error will necessarily decrease, but the test error may not. We will now explore this in a simulated data set.\n- (a) Generate a data set with p = 20 features, n = 1  000 observations, and an associated quantitative response vector generated according to the model  \n$$Y = X \\beta + \\epsilon ,$$  \nwhere β has some elements that are exactly equal to zero.  \n- (b) Split your data set into a training set containing 100 observations and a test set containing 900 observations.\n- (c) Perform best subset selection on the training set, and plot the training set MSE associated with the best model of each size.\n- (d) Plot the test set MSE associated with the best model of each size.\n- (e) For which model size does the test set MSE take on its minimum value? Comment on your results. If it takes on its minimum value for a model containing only an intercept or a model containing all of the features, then play around with the way that you are generating the data in (a) until you come up with a scenario in which the test set MSE is minimized for an intermediate model size.\n- (f) How does the model at which the test set MSE is minimized compare to the true model used to generate the data? Comment on the coefficient values.\n- (g) Create a plot displaying √ ∑ p j =1 ( β j -ˆ β r j ) 2 for a range of values of r , where ˆ β r j is the j th coefficient estimate for the best model containing r coefficients. Comment on what you observe. How does this compare to the test MSE plot from (d)?\n11. We will now try to predict per capita crime rate in the Boston data set.\n- (a) Try out some of the regression methods explored in this chapter, such as best subset selection, the lasso, ridge regression, and PCR. Present and discuss results for the approaches that you consider.\n- (b) Propose a model (or set of models) that seem to perform well on this data set, and justify your answer. Make sure that you are evaluating model performance using validation set error, crossvalidation, or some other reasonable alternative, as opposed to using training error.  \n- (c) Does your chosen model involve all of the features in the data set? Why or why not?', ""### 2.2.1 Measuring the Quality of Fit  \nIn order to evaluate the performance of a statistical learning method on a given data set, we need some way to measure how well its predictions actually match the observed data. That is, we need to quantify the extent to which the predicted response value for a given observation is close to the true response value for that observation. In the regression setting, the most commonly-used measure is the mean squared error (MSE), given by  \n$$M S E = \\frac { 1 } { n } \\sum _ { i = 1 } ^ { n } ( y _ { i } - \\hat { f } ( x _ { i } ) ) ^ { 2 } , \\quad \\ \\ ( 2 . 5 ) ^ { \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\$$  \nwhere ˆ f ( x i ) is the prediction that ˆ f gives for the i th observation. The MSE will be small if the predicted responses are very close to the true responses, and will be large if for some of the observations, the predicted and true responses differ substantially.  \nTo state it more mathematically, suppose that we fit our statistical learning method on our training observations ¶ ( x 1  y 1 )  ( x 2  y 2 )         ( x n  y n ) ♦ , and we obtain the estimate ˆ f . We can then compute ˆ f ( x 1 )  ˆ f ( x 2 )         ˆ f ( x n ) .  \nThe MSE in (2.5) is computed using the training data that was used to fit the model, and so should more accurately be referred to as the training MSE . But in general, we do not really care how well the method works on the training data. Rather, we are interested in the accuracy of the predictions that we obtain when we apply our method to previously unseen test data . Why is this what we care about? Suppose that we are interested in developing an algorithm to predict a stock's price based on previous stock returns. We can train the method using stock returns from the past 6 months. But we don't really care how well our method predicts last week's stock price. We instead care about how well it will predict tomorrow's price or next month's price. On a similar note, suppose that we have clinical measurements (e.g. weight, blood pressure, height, age, family history of disease) for a number of patients, as well as information about whether each patient has diabetes. We can use these patients to train a statistical learning method to predict risk of diabetes based on clinical measurements. In practice, we want this method to accurately predict diabetes risk for future patients based on their clinical measurements. We are not very interested in whether or not the method accurately predicts diabetes risk for patients used to train the model, since we already know which of those patients have diabetes.  \nmean squared error training MSE  \ntest data  \nFIGURE 2.9. Left: Data simulated from f , shown in black. Three estimates of f are shown: the linear regression line (orange curve), and two smoothing spline fits (blue and green curves). Right: Training MSE (grey curve), test MSE (red curve), and minimum possible test MSE over all methods (dashed line). Squares represent the training and test MSEs for the three fits shown in the left-hand panel.  \n<!-- image -->  \nIf these are approximately equal to y 1  y 2         y n , then the training MSE given by (2.5) is small. However, we are really not interested in whether ˆ f ( x i ) ≈ y i ; instead, we want to know whether ˆ f ( x 0 ) is approximately equal to y 0 , where ( x 0  y 0 ) is a previously unseen test observation not used to train the statistical learning method . We want to choose the method that gives the lowest test MSE , as opposed to the lowest training MSE. In other words, if we had a large number of test observations, we could compute  \n$$\\ A v e ( y _ { 0 } - \\hat { f } ( x _ { 0 } ) ) ^ { 2 } , \\\\ \\quad \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\$$  \nHow can we go about trying to select a method that minimizes the test MSE? In some settings, we may have a test data set available-that is, we may have access to a set of observations that were not used to train the statistical learning method. We can then simply evaluate (2.6) on the test observations, and select the learning method for which the test MSE is smallest. But what if no test observations are available? In that case, one might imagine simply selecting a statistical learning method that minimizes the training MSE (2.5). This seems like it might be a sensible approach, since the training MSE and the test MSE appear to be closely related. Unfortunately, there is a fundamental problem with this strategy: there is no guarantee that the method with the lowest training MSE will also have the lowest test MSE. Roughly speaking, the problem is that many statistical methods specifically estimate coefficients so as to minimize the training set MSE. For these methods, the training set MSE can be quite small, but the test MSE is often much larger.  \nthe average squared prediction error for these test observations ( x 0  y 0 ) . We'd like to select the model for which this quantity is as small as possible.  \nFigure 2.9 illustrates this phenomenon on a simple example. In the lefthand panel of Figure 2.9, we have generated observations from (2.1) with test MSE  \nthe true f given by the black curve. The orange, blue and green curves illustrate three possible estimates for f obtained using methods with increasing levels of flexibility. The orange line is the linear regression fit, which is relatively inflexible. The blue and green curves were produced using smoothing splines , discussed in Chapter 7, with different levels of smoothness. It is clear that as the level of flexibility increases, the curves fit the observed data more closely. The green curve is the most flexible and matches the data very well; however, we observe that it fits the true f (shown in black) poorly because it is too wiggly. By adjusting the level of flexibility of the smoothing spline fit, we can produce many different fits to this data.  \nIn this example, we know the true function f , and so we can also compute the test MSE over a very large test set, as a function of flexibility. (Of course, in general f is unknown, so this will not be possible.) The test MSE is displayed using the red curve in the right-hand panel of Figure 2.9. As with the training MSE, the test MSE initially declines as the level of flexibility increases. However, at some point the test MSE levels off and then starts to increase again. Consequently, the orange and green curves both have high test MSE. The blue curve minimizes the test MSE, which should not be surprising given that visually it appears to estimate f the best in the left-hand panel of Figure 2.9. The horizontal dashed line indicates Var ( /epsilon1 ) , the irreducible error in (2.3), which corresponds to the lowest achievable test MSE among all possible methods. Hence, the smoothing spline represented by the blue curve is close to optimal.  \nWe now move on to the right-hand panel of Figure 2.9. The grey curve displays the average training MSE as a function of flexibility, or more formally the degrees of freedom , for a number of smoothing splines. The degrees of freedom is a quantity that summarizes the flexibility of a curve; it is discussed more fully in Chapter 7. The orange, blue and green squares indicate the MSEs associated with the corresponding curves in the lefthand panel. A more restricted and hence smoother curve has fewer degrees of freedom than a wiggly curve-note that in Figure 2.9, linear regression is at the most restrictive end, with two degrees of freedom. The training MSE declines monotonically as flexibility increases. In this example the true f is non-linear, and so the orange linear fit is not flexible enough to estimate f well. The green curve has the lowest training MSE of all three methods, since it corresponds to the most flexible of the three curves fit in the left-hand panel.  \nIn the right-hand panel of Figure 2.9, as the flexibility of the statistical learning method increases, we observe a monotone decrease in the training MSE and a U-shape in the test MSE. This is a fundamental property of statistical learning that holds regardless of the particular data set at hand and regardless of the statistical method being used. As model flexibility increases, the training MSE will decrease, but the test MSE may not. When a given method yields a small training MSE but a large test MSE, we are said to be overfitting the data. This happens because our statistical learning procedure is working too hard to find patterns in the training data, and may be picking up some patterns that are just caused by random chance rather than by true properties of the unknown function f . When we overfit the training data, the test MSE will be very large because the supposed smoothing spline  \ndegrees of freedom  \nFIGURE 2.10. Details are as in Figure 2.9, using a different true f that is much closer to linear. In this setting, linear regression provides a very good fit to the data.  \n<!-- image -->  \npatterns that the method found in the training data simply don't exist in the test data. Note that regardless of whether or not overfitting has occurred, we almost always expect the training MSE to be smaller than the test MSE because most statistical learning methods either directly or indirectly seek to minimize the training MSE. Overfitting refers specifically to the case in which a less flexible model would have yielded a smaller test MSE.  \nIn practice, one can usually compute the training MSE with relative ease, but estimating the test MSE is considerably more difficult because usually no test data are available. As the previous three examples illustrate, the flexibility level corresponding to the model with the minimal test MSE can vary considerably among data sets. Throughout this book, we discuss a variety of approaches that can be used in practice to estimate this minimum point. One important method is cross-validation (Chapter 5), which is a method for estimating the test MSE using the training data.  \nFigure 2.10 provides another example in which the true f is approximately linear. Again we observe that the training MSE decreases monotonically as the model flexibility increases, and that there is a U-shape in the test MSE. However, because the truth is close to linear, the test MSE only decreases slightly before increasing again, so that the orange least squares fit is substantially better than the highly flexible green curve. Finally, Figure 2.11 displays an example in which f is highly non-linear. The training and test MSE curves still exhibit the same general patterns, but now there is a rapid decrease in both curves before the test MSE starts to increase slowly."", ""### 4.4.2 Linear Discriminant Analysis for p &gt; 1  \nWe now extend the LDA classifier to the case of multiple predictors. To do this, we will assume that X = ( X 1  X 2         X p ) is drawn from a multivariate Gaussian (or multivariate normal) distribution, with a class-specific mean vector and a common covariance matrix. We begin with a brief review of this distribution.  \n$$\\text {is defined as} \\\\ f ( x ) = \\frac { 1 } { ( 2 \\pi ) ^ { p / 2 } | \\Sigma | ^ { 1 / 2 } } \\exp \\left ( - \\frac { 1 } { 2 } ( x - \\mu ) ^ { T } \\Sigma ^ { - 1 } ( x - \\mu ) \\right ) . \\\\ \\text {In the case of } \\eta \\geq 1 \\, \\underset { p } { \\underset { p } { \\underset { p } { \\underset { p } { \\underset { p } { \\underset { p } { \\underset { p } { \\underset { p } { \\underset { p } { \\underset { p } { \\underset { p } { \\underset { p } { \\underset { p } { \\underset { p } { \\underset { p } { \\underset { p } { \\underset { p } { \\underset { p } { \\underset { p } { \\underset { p } { \\underset { p } { \\underset { p } { \\underset { p } { \\underset { p } { \\underset { p } { \\underset { p } { \\underset { p } { \\underset { p } { \\underset { p } { \\underset { p } { \\underset { p } { \\underset { p } { \\underset { p } { \\underset { p } { \\underset { p } { \\underset { p } { \\underset { p } { \\underset { p } { \\underset { p } { \\underset { p } { \\underset { p } { \\underset { p } { \\underset { p } { \\underset { p } { \\underset { p } { \\underset { p } { \\underset { p } { \\underset { p } { \\underset { p } { \\underset { p } { \\underset { p } { \\underset { p } { \\underset { p } { \\underset { p } { \\underset { p } { \\underset { p } { \\underset { p } { \\underset { p } { \\underset { p } { \\underset { p } { \\underset { p } { \\underset { p } { \\underset { p } { \\underset { p } { \\underset { p } { \\underset { p } { \\underset { p } { \\underset { p } { \\underset { p } { \\underset { p } { \\underset { p } { \\underset { p } { \\underset { p } { \\underset { p } { \\underset { p } { \\underset { p } { \\underset { p } { \\underset { p } { \\underset { p } { \\underset { p } { \\underset { p } { \\underset { p } { \\underset { p } { \\underset { p } { \\underset { p } { \\underset { p } { \\underset { p } { \\underset { p } { \\underset { p } { \\underset { p } { \\underset { p } { \\underset { p } { \\underset { p } { \\underset { p } { \\underset { p } { \\underset { p } { \\underset { p } { \\underset { p } { \\underset { p } { \\underset { p } { \\underset { p } { \\underset { p } { \\underset { p } { \\underset { p } { \\underset { p } { \\underset { p } { \\underset { p } { \\underset { p } { \\underset { p } { \\underset { p } { \\underset { p } { \\underset { p } { \\underset { p } { \\underset { p } { \\underset { p } { \\underset { p } { \\underset { p } { \\underset { p } { \\underset { p } { \\underset { p } { \\underset { p } { \\underset { p } { \\underset { p } { \\underset { p } { \\underset { p } { \\underset { p } { \\underset { p } { \\underset { p } { \\underset { p } { \\underset { p } { \\underset { p } { \\underset { p } { \\underset { p } { \\underset { p } { \\underset { p } { \\underset { p } { \\underset { p } { \\underset { p } { \\underset { p } { \\underset { p } { \\underset { p } { \\underset { p } { \\underset { p } { \\underset { p } { \\underset { p } { \\underset { p } { \\underset { p } { \\underset { p } { \\underset { p } { \\underset { p } { \\underset { p } { \\underset { p } { \\underset { p } { \\underset { p } { \\underset { p } { \\underset { p } { \\underset { p } { \\underset { p } { \\underset { p } { \\underset { p } { \\underset { p } { \\underset { p } { \\underset { p } { \\underset { p } { \\underset { p } { \\underset { p } { \\underset { p } { \\underset { p } { \\underset { p } { \\underset { p } { \\underset { p } { \\underset { p } { \\underset { p } { \\underset { p } { \\underset { p } { \\underset { p } { \\underset { p } { \\underset { p } { \\underset { p } { \\underset { p } { \\underset { p } { \\underset { p } { \\underset { p } { \\underset { p } { \\underset { p } { \\underset { p } { \\underset { p } { \\underset { p } { \\underset { p } { \\underset { p } { \\underset { p } { \\underset { p } { \\underset { p } { \\underset { p } { \\underset { p } { \\underset { p } { \\underset { p } { \\underset { p } { \\underset { p } { \\underset { p } { \\underset { p } { \\underset { p } { \\underset { p } { \\underset { p } { \\underset { p } { \\underset { p } { \\underset { p } { \\underset { p } { \\underset { p } { \\underset { p } { \\underset { p } { \\underset { p } { \\underset { p } { \\underset { p } { \\underset { p } { \\underset { p } { \\underset { p } { \\underset { p } { \\underset { p } { \\underset { p } { \\underset { p } { \\underset { p } { \\underset { p } { \\underset { p } { \\underset { p } { \\underset { p } { \\underset { p } { \\underset { p } { \\underset { p } { \\underset { p } { \\underset { p } { \\underset { p } { \\underset { p } { \\underset { p } { \\underset { p } { \\underset { p } { \\underset { p } { \\underset { p } { \\underset { p } { \\underset { p } { \\underset { p } { \\underset { p } { \\underset { p } { \\underset { p } { \\underset { p } { \\underset { p } { \\underset { p } { \\underset { p } { \\underset { p } { \\underset { p } { \\underset { p } { \\underset { p } { \\underset { p } { \\underset { p } { \\underset { p } { \\underset { p } { \\underset { p } { \\underset { p } { \\underset { p } { \\underset { p } { \\underset { p } { \\underset { p } { \\underset { p } { \\underset { p } { \\underset { p } { \\underset { p } { \\underset { p } { \\underset { p } { \\underset { p } { \\underset { p } { \\underset { p } { \\underset { p } { \\underset { p } { \\underset { p } { \\underset { p } { \\underset { p } { \\underset { p } { \\underset { p } { \\underset { p } { \\underset { p } { \\underset { p } { \\underset { p } { \\underset { p } { \\underset { p } { \\underset { p } { \\underset { p } { \\underset { p } { \\underset { p } { \\underset { p } { \\underset { p } { \\underset { p } { \\underset { p } { \\underset { p } { \\underset { p } { \\underset { p } { \\underset { p } { \\underset { p } { \\underset { p } { \\underset { p } { \\underset { p } { \\underset { p } { \\underset { p } { \\underset { p } { \\underset { p } { \\underset { p } { \\underset { p } { \\underset { p } { \\underset { p } { \\underset { p } { \\underset { p } { \\underset { p } { \\underset { p } { \\underset { p } { \\underset { p } { \\underset { p } { \\underset { p } { \\underset { p } { \\underset { p } { \\underset { p } { \\underset { p } { \\underset { p } { \\underset { p } { \\underset { p } { \\underset { p } { \\underset { p } { \\underset { p } { \\underset { p } { \\underset { p } { \\underset { p } { \\underset { p } { \\underset { p } { \\underset { p } { \\underset { p } { \\underset { p } { \\underset { p } { \\underset { p } { \\underset { p } { \\underset { p } { \\underset { p } { \\underset { p } { \\underset { p } { \\underset { p } { \\underset { p } { \\underset { p } { \\underset { p } { \\underset { p } { \\underset { p } { \\underset { p } { \\underset { p } { \\underset { p } { \\underset { p } { \\underset { p } { \\underset { p } { \\underset { p } { \\underset { p } { \\underset { p } { \\underset { p } { \\underset { p } { \\underset { p } { \\underset { p } { \\underset { p } { \\underset { p } { \\underset { p } { \\underset { p } { \\underset { p } { \\underset { p } { \\underset { p } { \\underset { p } { \\underset { p } { \\underset { p } { \\underset { p } { \\underset { p } { \\underset { p } { \\underset { p } { \\underset { p } { \\underset { p } { \\underset { p } { \\underset { p } { \\underset { p } { \\underset { p } { \\underset { p } { \\underset { p } { \\underset { p } { \\underset { p } { \\underset { p } { \\underset { p } { \\underset { p } { \\underset { p } { \\underset { p } { \\underset { p } { \\underset { p } { \\underset { p } { \\underset { p } { \\underset { p } { \\underset { p } { \\underset { p } { \\underset { p } { \\underset { p } { \\underset { p } { \\underset { p } { \\underset { p } { \\underset { p } { \\underset { p } { \\underset { p } { \\underset { p } { \\underset { p } { \\underset { p } { \\underset { p } { \\underset { p } { \\underset { p } { \\underset { p } { \\underset { p } { \\underset { p } { \\underset { p } { \\underset { p } { \\underset { p } { \\underset { p } { \\underset { p } { \\underset { p } { \\underset { p } { \\underset { p } { \\underset { p } { \\underset { p } { \\underset { p } { \\underset { p } { \\underset { p } { \\underset { p } { \\underset { p } { \\underset { p } { \\underset { p } { \\underset { p } { \\underset { p } { \\underset { p } { \\underset { p } { \\underset { p } { \\underset { p } { \\underset { p } { \\underset { p } { \\underset { p } { \\underset { p } { \\underset { p } { \\underset { p } { \\underset { p } { \\underset { p } { \\underset { p } { \\underset { p } { \\underset { p } { \\underset { p } { \\underset { p } { \\underset { p } { \\underset { p } { \\underset { p } { \\underset { p } { \\underset { p } { \\underset { p } { \\underset { p } { \\underset { p } { \\underset { p } { \\underset { p } { \\underset { p } { \\underset { p } { \\underset { p } { \\underset { p } { \\underset { p } { \\underset { p } { \\underset { p } { \\underset { p } { \\underset { p } { \\underset { p } { \\underset { p } { \\underset { p } { \\underset { p } { \\underset { p } { \\underset { p } { \\underset { p } { \\underset { p } { \\underset { p } { \\underset { p } { \\underset { p } { \\underset { p } { \\underset { p } { \\underset { p } { \\underset { p } { \\underset { p } { \\underset { p } { \\underset { p } { \\underset { p } { \\underset { p } { \\underset { p } { \\underset { p } { \\underset { p } { \\underset { p } { \\underset { p } { \\underset { p } { \\underset { p } { \\underset { p } { \\underset { p } { \\underset { p } { \\underset { p } { \\underset { p } { \\underset { p } { \\underset { p } { \\underset { p } { \\underset { p } { \\underset { p } { \\underset { p } { \\underset { p } { \\underset { p } { \\underset { p } { \\underset { p } { \\underset { p } { \\underset { p } { \\underset { p } { \\underset { p } { \\underset { p } { \\underset { p } { \\underset { p } { \\underset { p } { \\underset { p } { \\underset { p } { \\underset { p } { \\underset { p } { \\underset { p } { \\underset { p } { \\underset { p } { \\underset { p } { \\underset { p } { \\underset { p } { \\underset { p } { \\underset { p } { \\underset { p } { \\underset { p } { \\underset { p } { \\underset { p } { \\underset { p } { \\underset { p } { \\underset { p } { \\underset { p } { \\underset { p } { \\underset { p } { \\underset { p } { \\underset { p } { \\underset { p } { \\underset { p } { \\underset { p } { \\underset { p } { \\underset { p } { \\underset { p } { \\underset { p } { \\underset { p } { \\underset { p } { \\underset { p } { \\underset { p } { \\underset { p } { \\underset { p } { \\underset { p } { \\underset { p } { \\underset { p } { \\underset { p } { \\underset { p } { \\underset { p } { \\underset { p } { \\underset { p } { \\underset { p } { \\underset { p } { \\underset { p } { \\underset { p } { \\underset { p } { \\underset { p } { \\underset { p } { \\underset { p } { \\underset { p } { \\underset { p } { \\underset { p } { \\underset { p } { \\underset { p } { \\underset { p } { \\underset { p } { \\underset { p } { \\underset { p } { \\underset { p } { \\underset { p } { \\underset { p } { \\underset { p } { \\underset { p } { \\underset { p } { \\underset { p } { \\underset { p } { \\underset { p } { \\underset { p } { \\underset { p } { \\underset { p } { \\underset { p } { \\underset { p } { \\underset { p } { \\underset { p } { \\underset { p } { \\underset { p } { \\underset { p } { \\underset { p } { \\underset { p } { \\underset { p } { \\underset { p } { \\underset { p } { \\underset { p } { \\underset { p } { \\underset { p } { \\underset { p } { \\underset { p } { \\underset { p } { \\underset { p } { \\underset { p } { \\underset { p } { \\underset { p } { \\underset { p } { \\underset { p } { \\underset { p } { \\underset { p } { \\underset { p } { \\underset { p } { \\underset { p } { \\underset { p } { \\underset { p } { \\underset { p } { \\underset { p } { \\underset { p } { \\underset { p } { \\underset { p } { \\underset { p } { \\underset { p } { \\underset { p } { \\underset { p } { \\underset { p } { \\underset { p } { \\underset { p } { \\underset { p } { \\underset { p } { \\underset { p } { \\underset { p } { \\underset { p } { \\underset { p } { \\underset { p } { \\underset { p } { \\underset { p } { \\underset { p } { \\underset { p } { \\underset { p } { \\underset { p } { \\underset { p } { \\underset { p } { \\underset { p } { \\underset { p } { \\underset { p } { \\underset { p } { \\unders \\cdot } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } }$$  \nThe multivariate Gaussian distribution assumes that each individual predictor follows a one-dimensional normal distribution, as in (4.16), with some correlation between each pair of predictors. Two examples of multivariate Gaussian distributions with p = 2 are shown in Figure 4.5. The height of the surface at any particular point represents the probability that both X 1 and X 2 fall in a small region around that point. In either panel, if the surface is cut along the X 1 axis or along the X 2 axis, the resulting cross-section will have the shape of a one-dimensional normal distribution. The left-hand panel of Figure 4.5 illustrates an example in which Var ( X 1 ) = Var ( X 2 ) and Cor ( X 1  X 2 ) = 0 ; this surface has a characteristic bell shape . However, the bell shape will be distorted if the predictors are correlated or have unequal variances, as is illustrated in the right-hand panel of Figure 4.5. In this situation, the base of the bell will have an elliptical, rather than circular, shape. To indicate that a p -dimensional random variable X has a multivariate Gaussian distribution, we write X ∼ N ( θ Σ ) . Here E( X ) = θ is the mean of X (a vector with p components), and Cov ( X ) = Σ is the p × p covariance matrix of X . Formally, the multivariate Gaussian density is defined as  \nIn the case of p &gt; 1 predictors, the LDA classifier assumes that the observations in the k th class are drawn from a multivariate Gaussian distribution N ( θ k  Σ ) , where θ k is a class-specific mean vector, and Σ is a covariance matrix that is common to all K classes. Plugging the density function for the k th class, f k ( X = x ) , into (4.15) and performing a little bit of algebra reveals that the Bayes classifier assigns an observation X = x multivariate Gaussian  \nFIGURE 4.6. An example with three classes. The observations from each class are drawn from a multivariate Gaussian distribution with p = 2 , with a class-specific mean vector and a common covariance matrix. Left: Ellipses that contain 95 % of the probability for each of the three classes are shown. The dashed lines are the Bayes decision boundaries. Right: 20 observations were generated from each class, and the corresponding LDA decision boundaries are indicated using solid black lines. The Bayes decision boundaries are once again shown as dashed lines.  \n<!-- image -->  \nto the class for which  \n$$\\delta _ { k } ( x ) = x ^ { T } \\Sigma ^ { - 1 } \\mu _ { k } - \\frac { 1 } { 2 } \\mu _ { k } ^ { T } \\Sigma ^ { - 1 } \\mu _ { k } + \\log \\pi _ { k }$$  \nis largest. This is the vector/matrix version of (4.18).  \nAn example is shown in the left-hand panel of Figure 4.6. Three equallysized Gaussian classes are shown with class-specific mean vectors and a common covariance matrix. The three ellipses represent regions that contain 95 % of the probability for each of the three classes. The dashed lines are the Bayes decision boundaries. In other words, they represent the set of values x for which δ k ( x ) = δ /lscript ( x ) ; i.e.  \n$$x ^ { T } \\Sigma ^ { - 1 } \\mu _ { k } - \\frac { 1 } { 2 } \\mu _ { k } ^ { T } \\Sigma ^ { - 1 } \\mu _ { k } = x ^ { T } \\Sigma ^ { - 1 } \\mu _ { l } - \\frac { 1 } { 2 } \\mu _ { l } ^ { T } \\Sigma ^ { - 1 } \\mu _ { l }$$  \n/negationslash  \nOnce again, we need to estimate the unknown parameters θ 1         θ K , π 1         π K , and Σ ; the formulas are similar to those used in the onedimensional case, given in (4.20). To assign a new observation X = x , LDA plugs these estimates into (4.24) to obtain quantities ˆ δ k ( x ) , and classifies to the class for which ˆ δ k ( x ) is largest. Note that in (4.24) δ k ( x ) is a linear function of x ; that is, the LDA decision rule depends on x only for k = l . (The log π k term from (4.24) has disappeared because each of the three classes has the same number of training observations; i.e. π k is the same for each class.) Note that there are three lines representing the Bayes decision boundaries because there are three pairs of classes among the three classes. That is, one Bayes decision boundary separates class 1 from class 2, one separates class 1 from class 3, and one separates class 2 from class 3. These three Bayes decision boundaries divide the predictor space into three regions. The Bayes classifier will classify an observation according to the region in which it is located.  \nTABLE 4.4. A confusion matrix compares the LDA predictions to the true default statuses for the 10  000 training observations in the Default data set. Elements on the diagonal of the matrix represent individuals whose default statuses were correctly predicted, while off-diagonal elements represent individuals that were misclassified. LDA made incorrect predictions for 23 individuals who did not default and for 252 individuals who did default.  \n|                |       | True default status   | True default status   | True default status   |\n|----------------|-------|-----------------------|-----------------------|-----------------------|\n|                |       | No                    | Yes                   | Total                 |\n| Predicted      | No    | 9644                  | 252                   | 9896                  |\n| default status | Yes   | 23                    | 81                    | 104                   |\n|                | Total | 9667                  | 333                   | 10000                 |  \nthrough a linear combination of its elements. As previously discussed, this is the reason for the word linear in LDA.  \nWe can perform LDA on the Default data in order to predict whether or not an individual will default on the basis of credit card balance and student status. 4 The LDA model fit to the 10  000 training samples results in a training error rate of 2   75 %. This sounds like a low error rate, but two caveats must be noted.  \nIn the right-hand panel of Figure 4.6, 20 observations drawn from each of the three classes are displayed, and the resulting LDA decision boundaries are shown as solid black lines. Overall, the LDA decision boundaries are pretty close to the Bayes decision boundaries, shown again as dashed lines. The test error rates for the Bayes and LDA classifiers are 0   0746 and 0   0770 , respectively. This indicates that LDA is performing well on this data.  \n- First of all, training error rates will usually be lower than test error rates, which are the real quantity of interest. In other words, we might expect this classifier to perform worse if we use it to predict whether or not a new set of individuals will default. The reason is that we specifically adjust the parameters of our model to do well on the training data. The higher the ratio of parameters p to number of samples n , the more we expect this overfitting to play a role. For these data we don't expect this to be a problem, since p = 2 and n = 10  000 .\n- Second, since only 3   33 % of the individuals in the training sample defaulted, a simple but useless classifier that always predicts that an individual will not default, regardless of his or her credit card balance and student status, will result in an error rate of 3   33 %. In other words, the trivial null classifier will achieve an error rate that is only a bit higher than the LDA training set error rate.  \nIn practice, a binary classifier such as this one can make two types of errors: it can incorrectly assign an individual who defaults to the no default category, or it can incorrectly assign an individual who does not default to  \n4 The careful reader will notice that student status is qualitative - thus, the normality assumption made by LDA is clearly violated in this example! However, LDA is often remarkably robust to model violations, as this example shows. Naive Bayes, discussed in Section 4.4.4, provides an alternative to LDA that does not assume normally distributed predictors.  \noverfitting null  \nthe default category. It is often of interest to determine which of these two types of errors are being made. A confusion matrix , shown for the Default data in Table 4.4, is a convenient way to display this information. The table reveals that LDA predicted that a total of 104 people would default. Of these people, 81 actually defaulted and 23 did not. Hence only 23 out of 9  667 of the individuals who did not default were incorrectly labeled. This looks like a pretty low error rate! However, of the 333 individuals who defaulted, 252 (or 75   7 %) were missed by LDA. So while the overall error rate is low, the error rate among individuals who defaulted is very high. From the perspective of a credit card company that is trying to identify high-risk individuals, an error rate of 252 glyph[triangleleft] 333 = 75   7 % among individuals who default may well be unacceptable.  \nWhy does LDA do such a poor job of classifying the customers who default? In other words, why does it have such low sensitivity? As we have seen, LDA is trying to approximate the Bayes classifier, which has the lowest total error rate out of all classifiers. That is, the Bayes classifier will yield the smallest possible total number of misclassified observations, regardless of the class from which the errors stem. Some misclassifications will result from incorrectly assigning a customer who does not default to the default class, and others will result from incorrectly assigning a customer who defaults to the non-default class. In contrast, a credit card company might particularly wish to avoid incorrectly classifying an individual who will default, whereas incorrectly classifying an individual who will not default, though still to be avoided, is less problematic. We will now see that it is possible to modify LDA in order to develop a classifier that better meets the credit card company's needs.  \nClass-specific performance is also important in medicine and biology, where the terms sensitivity and specificity characterize the performance of a classifier or screening test. In this case the sensitivity is the percentage of true defaulters that are identified; it equals 24.3 %. The specificity is the percentage of non-defaulters that are correctly identified; it equals (1 -23 glyph[triangleleft] 9667) = 99   8 %.  \nThe Bayes classifier works by assigning an observation to the class for which the posterior probability p k ( X ) is greatest. In the two-class case, this amounts to assigning an observation to the default class if  \n$$\\Pr ( \\text {default} = \\text {Yes} | X = x ) & > 0 . 5 . \\\\$$  \nThus, the Bayes classifier, and by extension LDA, uses a threshold of 50 % for the posterior probability of default in order to assign an observation to the default class. However, if we are concerned about incorrectly predicting the default status for individuals who default, then we can consider lowering this threshold. For instance, we might label any customer with a posterior probability of default above 20 % to the default class. In other words, instead of assigning an observation to the default class if (4.26) holds, we could instead assign an observation to this class if  \n$$\\Pr ( \\text {default} = \\text {Yes} | X = x ) & > 0 . 2 . \\\\$$  \nThe error rates that result from taking this approach are shown in Table 4.5. Now LDA predicts that 430 individuals will default. Of the 333 individuals who default, LDA correctly predicts all but 138 , or 41   4 %. This is a vast confusion matrix  \nsensitivity specificity  \nTABLE 4.5. A confusion matrix compares the LDA predictions to the true default statuses for the 10  000 training observations in the Default data set, using a modified threshold value that predicts default for any individuals whose posterior default probability exceeds 20 %.  \n|                |       | True default status   | True default status   | True default status   |\n|----------------|-------|-----------------------|-----------------------|-----------------------|\n|                |       | No                    | Yes                   | Total                 |\n| Predicted      | No    | 9432                  | 138                   | 9570                  |\n| default status | Yes   | 235                   | 195                   | 430                   |\n|                | Total | 9667                  | 333                   | 10000                 |  \nFIGURE 4.7. For the Default data set, error rates are shown as a function of the threshold value for the posterior probability that is used to perform the assignment. The black solid line displays the overall error rate. The blue dashed line represents the fraction of defaulting customers that are incorrectly classified, and the orange dotted line indicates the fraction of errors among the non-defaulting customers.  \n<!-- image -->  \nimprovement over the error rate of 75   7 % that resulted from using the threshold of 50 %. However, this improvement comes at a cost: now 235 individuals who do not default are incorrectly classified. As a result, the overall error rate has increased slightly to 3   73 %. But a credit card company may consider this slight increase in the total error rate to be a small price to pay for more accurate identification of individuals who do indeed default.  \nThe ROC curve is a popular graphic for simultaneously displaying the two types of errors for all possible thresholds. The name 'ROC' is historic, and comes from communications theory. It is an acronym for receiver operating characteristics . Figure 4.8 displays the ROC curve for the LDA classifier on the training data. The overall performance of a classifier, sum-  \nFigure 4.7 illustrates the trade-off that results from modifying the threshold value for the posterior probability of default. Various error rates are shown as a function of the threshold value. Using a threshold of 0   5 , as in (4.26), minimizes the overall error rate, shown as a black solid line. This is to be expected, since the Bayes classifier uses a threshold of 0   5 and is known to have the lowest overall error rate. But when a threshold of 0   5 is used, the error rate among the individuals who default is quite high (blue dashed line). As the threshold is reduced, the error rate among individuals who default decreases steadily, but the error rate among the individuals who do not default increases. How can we decide which threshold value is best? Such a decision must be based on domain knowledge , such as detailed information about the costs associated with default.  \nROC curve\nROC Curve  \nFIGURE 4.8. A ROC curve for the LDA classifier on the Default data. It traces out two types of error as we vary the threshold value for the posterior probability of default. The actual thresholds are not shown. The true positive rate is the sensitivity: the fraction of defaulters that are correctly identified, using a given threshold value. The false positive rate is 1-specificity: the fraction of non-defaulters that we classify incorrectly as defaulters, using that same threshold value. The ideal ROC curve hugs the top left corner, indicating a high true positive rate and a low false positive rate. The dotted line represents the 'no information' classifier; this is what we would expect if student status and credit card balance are not associated with probability of default.  \n<!-- image -->  \nmarized over all possible thresholds, is given by the area under the (ROC) curve (AUC). An ideal ROC curve will hug the top left corner, so the larger the AUC the better the classifier. For this data the AUC is 0   95 , which is close to the maximum of 1   0 , so would be considered very good. We expect a classifier that performs no better than chance to have an AUC of 0.5 (when evaluated on an independent test set not used in model training). ROC curves are useful for comparing different classifiers, since they take into account all possible thresholds. It turns out that the ROC curve for the logistic regression model of Section 4.3.4 fit to these data is virtually indistinguishable from this one for the LDA model, so we do not display it here.  \nAs we have seen above, varying the classifier threshold changes its true positive and false positive rate. These are also called the sensitivity and one minus the specificity of our classifier. Since there is an almost bewildering array of terms used in this context, we now give a summary. Table 4.6 shows the possible results when applying a classifier (or diagnostic test) to a population. To make the connection with the epidemiology literature, we think of '+' as the 'disease' that we are trying to detect, and ' -' as the 'non-disease' state. To make the connection to the classical hypothesis testing literature, we think of ' -' as the null hypothesis and '+' as the area under the (ROC) curve  \nsensitivity specificity  \nTABLE 4.6. Possible results when applying a classifier or diagnostic test to a population.  \n|                 |               | True class      | True class      |       |\n|-----------------|---------------|-----------------|-----------------|-------|\n|                 |               | - or Null       | + or Non-null   | Total |\n| Predicted class | - or Null     | True Neg. (TN)  | False Neg. (FN) | N ∗   |\n|                 | + or Non-null | False Pos. (FP) | True Pos. (TP)  | P ∗   |\n|                 | Total         | N               | P               |       |  \nTABLE 4.7. Important measures for classification and diagnostic testing, derived from quantities in Table 4.6.  \n| Name             | Definition                 | Synonyms                                      |\n|------------------|----------------------------|-----------------------------------------------|\n| False Pos. rate  | FP glyph[triangleleft] N   | Type I error, 1 - Specificity                 |\n| True Pos. rate   | TP glyph[triangleleft] P   | 1 - Type II error, power, sensitivity, recall |\n| Pos. Pred. value | TP glyph[triangleleft] P ∗ | Precision, 1 - false discovery proportion     |\n| Neg. Pred. value | TN glyph[triangleleft] N ∗ |                                               |  \nalternative (non-null) hypothesis. In the context of the Default data, '+' indicates an individual who defaults, and ' -' indicates one who does not.  \nTable 4.7 lists many of the popular performance measures that are used in this context. The denominators for the false positive and true positive rates are the actual population counts in each class. In contrast, the denominators for the positive predictive value and the negative predictive value are the total predicted counts for each class."", ""## 4.1 An Overview of Classification  \nClassification problems occur often, perhaps even more so than regression problems. Some examples include:  \nqualitative classification  \nclassifier logistic  \nregression linear discriminant analysis quadratic discriminant analysis naive Bayes K -nearest neighbors generalized linear models Poisson regression  \n1. A person arrives at the emergency room with a set of symptoms that could possibly be attributed to one of three medical conditions. Which of the three conditions does the individual have?\n2. An online banking service must be able to determine whether or not a transaction being performed on the site is fraudulent, on the basis of the user's IP address, past transaction history, and so forth.\n3. On the basis of DNA sequence data for a number of patients with and without a given disease, a biologist would like to figure out which DNA mutations are deleterious (disease-causing) and which are not.  \nJust as in the regression setting, in the classification setting we have a set of training observations ( x 1  y 1 )         ( x n  y n ) that we can use to build a classifier. We want our classifier to perform well not only on the training data, but also on test observations that were not used to train the classifier.  \nIt is worth noting that Figure 4.1 displays a very pronounced relationship between the predictor balance and the response default . In most real applications, the relationship between the predictor and the response will not be nearly so strong. However, for the sake of illustrating the classification procedures discussed in this chapter, we use an example in which the relationship between the predictor and the response is somewhat exaggerated.  \nIn this chapter, we will illustrate the concept of classification using the simulated Default data set. We are interested in predicting whether an individual will default on his or her credit card payment, on the basis of annual income and monthly credit card balance. The data set is displayed in Figure 4.1. In the left-hand panel of Figure 4.1, we have plotted annual income and monthly credit card balance for a subset of 10  000 individuals. The individuals who defaulted in a given month are shown in orange, and those who did not in blue. (The overall default rate is about 3 %, so we have plotted only a fraction of the individuals who did not default.) It appears that individuals who defaulted tended to have higher credit card balances than those who did not. In the center and right-hand panels of Figure 4.1, two pairs of boxplots are shown. The first shows the distribution of balance split by the binary default variable; the second is a similar plot for income . In this chapter, we learn how to build a model to predict default ( Y ) for any given value of balance ( X 1 ) and income ( X 2 ). Since Y is not quantitative, the simple linear regression model of Chapter 3 is not a good choice: we will elaborate on this further in Section 4.2."", '### 5.1.1 The Validation Set Approach  \nSuppose that we would like to estimate the test error associated with fitting a particular statistical learning method on a set of observations. The validation set approach , displayed in Figure 5.1, is a very simple strategy for this task. It involves randomly dividing the available set of observations into two parts, a training set and a validation set or hold-out set . The model is fit on the training set, and the fitted model is used to predict the responses for the observations in the validation set. The resulting validation set error rate-typically assessed using MSE in the case of a quantitative response-provides an estimate of the test error rate.  \nWeillustrate the validation set approach on the Auto data set. Recall from Chapter 3 that there appears to be a non-linear relationship between mpg and horsepower , and that a model that predicts mpg using horsepower and horsepower 2 gives better results than a model that uses only a linear term. It is natural to wonder whether a cubic or higher-order fit might provide even better results. We answer this question in Chapter 3 by looking at the p-values associated with a cubic term and higher-order polynomial terms in a linear regression. But we could also answer this question using the validation method. We randomly split the 392 observations into two validation set approach validation set hold-out set  \nFIGURE 5.1. A schematic display of the validation set approach. A set of n observations are randomly split into a training set (shown in blue, containing observations 7, 22, and 13, among others) and a validation set (shown in beige, and containing observation 91, among others). The statistical learning method is fit on the training set, and its performance is evaluated on the validation set.  \n<!-- image -->  \nsets, a training set containing 196 of the data points, and a validation set containing the remaining 196 observations. The validation set error rates that result from fitting various regression models on the training sample and evaluating their performance on the validation sample, using MSE as a measure of validation set error, are shown in the left-hand panel of Figure 5.2. The validation set MSE for the quadratic fit is considerably smaller than for the linear fit. However, the validation set MSE for the cubic fit is actually slightly larger than for the quadratic fit. This implies that including a cubic term in the regression does not lead to better prediction than simply using a quadratic term.  \nThe validation set approach is conceptually simple and is easy to implement. But it has two potential drawbacks:  \nRecall that in order to create the left-hand panel of Figure 5.2, we randomly divided the data set into two parts, a training set and a validation set. If we repeat the process of randomly splitting the sample set into two parts, we will get a somewhat different estimate for the test MSE. As an illustration, the right-hand panel of Figure 5.2 displays ten different validation set MSE curves from the Auto data set, produced using ten different random splits of the observations into training and validation sets. All ten curves indicate that the model with a quadratic term has a dramatically smaller validation set MSE than the model with only a linear term. Furthermore, all ten curves indicate that there is not much benefit in including cubic or higher-order polynomial terms in the model. But it is worth noting that each of the ten curves results in a different test MSE estimate for each of the ten regression models considered. And there is no consensus among the curves as to which model results in the smallest validation set MSE. Based on the variability among these curves, all that we can conclude with any confidence is that the linear fit is not adequate for this data.  \n1. As is shown in the right-hand panel of Figure 5.2, the validation estimate of the test error rate can be highly variable, depending on precisely which observations are included in the training set and which observations are included in the validation set.\n2. In the validation approach, only a subset of the observations-those that are included in the training set rather than in the validation set-are used to fit the model. Since statistical methods tend to perform worse when trained on fewer observations, this suggests that the  \nFIGURE 5.2. The validation set approach was used on the Auto data set in order to estimate the test error that results from predicting mpg using polynomial functions of horsepower . Left: Validation error estimates for a single split into training and validation data sets. Right: The validation method was repeated ten times, each time using a different random split of the observations into a training set and a validation set. This illustrates the variability in the estimated test MSE that results from this approach.  \n<!-- image -->  \nvalidation set error rate may tend to overestimate the test error rate for the model fit on the entire data set.  \nIn the coming subsections, we will present cross-validation , a refinement of the validation set approach that addresses these two issues.']","In the context of machine learning, training data and test data serve distinct purposes:

1. **Training Data:**
   - **Purpose:** The training data is used to fit or train the machine learning model. It is the dataset on which the model learns the underlying patterns, relationships, and features that are necessary to make predictions or classifications.
   - **Usage:** During the training phase, the model adjusts its parameters to minimize the error between the predicted outputs and the actual outputs in the training data. This process involves learning the weights or coefficients that best map the input features to the target outputs.
   - **Characteristics:** The training data typically includes both the input features and the corresponding target labels or outcomes. It is crucial that the training data is representative of the problem domain to ensure the model learns effectively.

2. **Test Data:**
   - **Purpose:** The test data is used to evaluate the performance of the trained model. It provides an unbiased assessment of how well the model generalizes to new, unseen data.
   - **Usage:** After the model has been trained, it is applied to the test data to predict the outcomes. The predicted outcomes are then compared to the actual outcomes to calculate performance metrics such as accuracy, precision, recall, or mean squared error.
   - **Characteristics:** The test data should be separate from the training data and not used during the training process. This separation helps ensure that the evaluation of the model's performance is not biased by the data it has already seen.

In summary, training data is used to build and optimize the model, while test data is used to assess the model's ability to generalize to new, unseen data.","Training data is the set of observations used to teach or train a statistical learning model. Test data, on the other hand, consists of previously unseen observations that are used to evaluate the performance and accuracy of the trained model.",1.0,,0.9266666666481335,1.0
¿Cuál es la diferencia entre un problema de regresión y uno de clasificación?,"[""### 2.1.5 Regression Versus Classification Problems  \nVariables can be characterized as either quantitative or qualitative (also known as categorical ). Quantitative variables take on numerical values. Examples include a person's age, height, or income, the value of a house, and the price of a stock. In contrast, qualitative variables take on values in one of K different classes , or categories. Examples of qualitative variables include a person's marital status (married or not), the brand of product purchased (brand A, B, or C), whether a person defaults on a debt (yes or no), or a cancer diagnosis (Acute Myelogenous Leukemia, Acute Lymphoblastic Leukemia, or No Leukemia). We tend to refer to problems with a quantitative response as regression problems, while those involving a qualitative response are often referred to as classification problems. However, the distinction is not always that crisp. Least squares linear regression (Chapter 3) is used with a quantitative response, whereas logistic regression (Chapter 4) is typically used with a qualitative (two-class, or binary ) response. Thus, despite its name, logistic regression is a classification method. But since it estimates class probabilities, it can be thought of as a regression method as well. Some statistical methods, such as K -nearest neighbors (Chapters 2 and 4) and boosting (Chapter 8), can be used in the case of either quantitative or qualitative responses.  \nWe tend to select statistical learning methods on the basis of whether the response is quantitative or qualitative; i.e. we might use linear regression when quantitative and logistic regression when qualitative. However, whether the predictors are qualitative or quantitative is generally considered less important. Most of the statistical learning methods discussed in this book can be applied regardless of the predictor variable type, provided that any qualitative predictors are properly coded before the analysis is performed. This is discussed in Chapter 3."", ""## 4.1 An Overview of Classification  \nClassification problems occur often, perhaps even more so than regression problems. Some examples include:  \nqualitative classification  \nclassifier logistic  \nregression linear discriminant analysis quadratic discriminant analysis naive Bayes K -nearest neighbors generalized linear models Poisson regression  \n1. A person arrives at the emergency room with a set of symptoms that could possibly be attributed to one of three medical conditions. Which of the three conditions does the individual have?\n2. An online banking service must be able to determine whether or not a transaction being performed on the site is fraudulent, on the basis of the user's IP address, past transaction history, and so forth.\n3. On the basis of DNA sequence data for a number of patients with and without a given disease, a biologist would like to figure out which DNA mutations are deleterious (disease-causing) and which are not.  \nJust as in the regression setting, in the classification setting we have a set of training observations ( x 1  y 1 )         ( x n  y n ) that we can use to build a classifier. We want our classifier to perform well not only on the training data, but also on test observations that were not used to train the classifier.  \nIt is worth noting that Figure 4.1 displays a very pronounced relationship between the predictor balance and the response default . In most real applications, the relationship between the predictor and the response will not be nearly so strong. However, for the sake of illustrating the classification procedures discussed in this chapter, we use an example in which the relationship between the predictor and the response is somewhat exaggerated.  \nIn this chapter, we will illustrate the concept of classification using the simulated Default data set. We are interested in predicting whether an individual will default on his or her credit card payment, on the basis of annual income and monthly credit card balance. The data set is displayed in Figure 4.1. In the left-hand panel of Figure 4.1, we have plotted annual income and monthly credit card balance for a subset of 10  000 individuals. The individuals who defaulted in a given month are shown in orange, and those who did not in blue. (The overall default rate is about 3 %, so we have plotted only a fraction of the individuals who did not default.) It appears that individuals who defaulted tended to have higher credit card balances than those who did not. In the center and right-hand panels of Figure 4.1, two pairs of boxplots are shown. The first shows the distribution of balance split by the binary default variable; the second is a similar plot for income . In this chapter, we learn how to build a model to predict default ( Y ) for any given value of balance ( X 1 ) and income ( X 2 ). Since Y is not quantitative, the simple linear regression model of Chapter 3 is not a good choice: we will elaborate on this further in Section 4.2."", '### 8.1.3 Trees Versus Linear Models  \nRegression and classification trees have a very different flavor from the more classical approaches for regression and classification presented in Chapters 3 and 4. In particular, linear regression assumes a model of the form  \n$$f ( X ) = \\beta _ { 0 } + \\sum _ { j = 1 } ^ { p } X _ { j } \\beta _ { j } ,$$  \nwhereas regression trees assume a model of the form  \n$$f ( X ) = \\sum _ { m = 1 } ^ { M } c _ { m } \\cdot 1 _ { ( X \\in R _ { m } ) } \\\\$$  \nwhere R 1         R M represent a partition of feature space, as in Figure 8.3.  \nOf course, other considerations beyond simply test error may come into play in selecting a statistical learning method; for instance, in certain settings, prediction using a tree may be preferred for the sake of interpretability and visualization.  \nWhich model is better? It depends on the problem at hand. If the relationship between the features and the response is well approximated by a linear model as in (8.8), then an approach such as linear regression will likely work well, and will outperform a method such as a regression tree that does not exploit this linear structure. If instead there is a highly nonlinear and complex relationship between the features and the response as indicated by model (8.9), then decision trees may outperform classical approaches. An illustrative example is displayed in Figure 8.7. The relative performances of tree-based and classical approaches can be assessed by estimating the test error, using either cross-validation or the validation set approach (Chapter 5).', '### 6.4.2 What Goes Wrong in High Dimensions?  \nIn order to illustrate the need for extra care and specialized techniques for regression and classification when p &gt; n , we begin by examining what can go wrong if we apply a statistical technique not intended for the highdimensional setting. For this purpose, we examine least squares regression. But the same concepts apply to logistic regression, linear discriminant analysis, and other classical statistical approaches.  \nAn example is shown in Figure 6.22 with p = 1 feature (plus an intercept) in two cases: when there are 20 observations, and when there are only two observations. When there are 20 observations, n &gt; p and the least squares regression line does not perfectly fit the data; instead, the regression line seeks to approximate the 20 observations as well as possible. On the other hand, when there are only two observations, then regardless of the values of those observations, the regression line will fit the data exactly. This is problematic because this perfect fit will almost certainly lead to overfitting of the data. In other words, though it is possible to perfectly fit the training data in the high-dimensional setting, the resulting linear model will perform extremely poorly on an independent test set, and therefore does not constitute a useful model. In fact, we can see that this happened in Figure 6.22: the least squares line obtained in the right-hand panel will perform very poorly on a test set comprised of the observations in the lefthand panel. The problem is simple: when p &gt; n or p ≈ n , a simple least squares regression line is too flexible and hence overfits the data.  \nWhen the number of features p is as large as, or larger than, the number of observations n , least squares as described in Chapter 3 cannot (or rather, should not ) be performed. The reason is simple: regardless of whether or not there truly is a relationship between the features and the response, least squares will yield a set of coefficient estimates that result in a perfect fit to the data, such that the residuals are zero.  \nFigure 6.23 further illustrates the risk of carelessly applying least squares when the number of features p is large. Data were simulated with n = 20 observations, and regression was performed with between 1 and 20 features, highdimensional  \nFIGURE 6.22. Left: Least squares regression in the low-dimensional setting. Right: Least squares regression with n = 2 observations and two parameters to be estimated (an intercept and a coefficient).  \n<!-- image -->  \nFIGURE 6.23. On a simulated example with n = 20 training observations, features that are completely unrelated to the outcome are added to the model. Left: The R 2 increases to 1 as more features are included. Center: The training set MSE decreases to 0 as more features are included. Right: The test set MSE increases as more features are included.  \n<!-- image -->  \neach of which was completely unrelated to the response. As shown in the figure, the model R 2 increases to 1 as the number of features included in the model increases, and correspondingly the training set MSE decreases to 0 as the number of features increases, even though the features are completely unrelated to the response . On the other hand, the MSE on an independent test set becomes extremely large as the number of features included in the model increases, because including the additional predictors leads to a vast increase in the variance of the coefficient estimates. Looking at the test set MSE, it is clear that the best model contains at most a few variables. However, someone who carelessly examines only the R 2 or the training set MSE might erroneously conclude that the model with the greatest number of variables is best. This indicates the importance of applying extra care when analyzing data sets with a large number of variables, and of always evaluating model performance on an independent test set.  \nFIGURE 6.24. The lasso was performed with n = 100 observations and three values of p , the number of features. Of the p features, 20 were associated with the response. The boxplots show the test MSEs that result using three different values of the tuning parameter λ in (6.7). For ease of interpretation, rather than reporting λ , the degrees of freedom are reported; for the lasso this turns out to be simply the number of estimated non-zero coefficients. When p = 20 , the lowest test MSE was obtained with the smallest amount of regularization. When p = 50 , the lowest test MSE was achieved when there is a substantial amount of regularization. When p = 2  000 the lasso performed poorly regardless of the amount of regularization, due to the fact that only 20 of the 2,000 features truly are associated with the outcome.  \n<!-- image -->  \nIn Section 6.1.3, we saw a number of approaches for adjusting the training set RSS or R 2 in order to account for the number of variables used to fit a least squares model. Unfortunately, the C p , AIC, and BIC approaches are not appropriate in the high-dimensional setting, because estimating ˆ σ 2 is problematic. (For instance, the formula for ˆ σ 2 from Chapter 3 yields an estimate ˆ σ 2 = 0 in this setting.) Similarly, problems arise in the application of adjusted R 2 in the high-dimensional setting, since one can easily obtain a model with an adjusted R 2 value of 1. Clearly, alternative approaches that are better-suited to the high-dimensional setting are required.', ""## 4.4 Generative Models for Classification  \nLogistic regression involves directly modeling Pr( Y = k ♣ X = x ) using the logistic function, given by (4.7) for the case of two response classes. In statistical jargon, we model the conditional distribution of the response Y , given the predictor(s) X . We now consider an alternative and less direct approach to estimating these probabilities. In this new approach, we model the distribution of the predictors X separately in each of the response classes (i.e. for each value of Y ). We then use Bayes' theorem to flip these around into estimates for Pr( Y = k ♣ X = x ) . When the distribution of X within each class is assumed to be normal, it turns out that the model is very similar in form to logistic regression.  \nWhy do we need another method, when we have logistic regression? There are several reasons:  \n- When there is substantial separation between the two classes, the parameter estimates for the logistic regression model are surprisingly unstable. The methods that we consider in this section do not suffer from this problem.\n- If the distribution of the predictors X is approximately normal in each of the classes and the sample size is small, then the approaches in this section may be more accurate than logistic regression.\n- The methods in this section can be naturally extended to the case of more than two response classes. (In the case of more than two response classes, we can also use multinomial logistic regression from Section 4.3.5.)  \nSuppose that we wish to classify an observation into one of K classes, where K ≥ 2 . In other words, the qualitative response variable Y can take on K possible distinct and unordered values. Let π k represent the overall or prior probability that a randomly chosen observation comes from the k th class. Let f k ( X ) ≡ Pr( X ♣ Y = k ) 1 denote the density function of X for an observation that comes from the k th class. In other words, f k ( x ) is relatively large if there is a high probability that an observation in the k th class has X ≈ x , and f k ( x ) is small if it is very unlikely that an observation in the k th class has X ≈ x . Then Bayes' theorem states that  \n1 Technically, this definition is only correct if X is a qualitative random variable. If X is quantitative, then f k ( x ) dx corresponds to the probability of X falling in a small region dx around x .  \nprior density function  \nBayes' theorem  \n$$\\Pr ( Y = k | X = x ) = \\frac { \\pi _ { k } f _ { k } ( x ) } { \\sum _ { l = 1 } ^ { K } \\pi _ { l } f _ { l } ( x ) } . \\\\ \\intertext { c l e w h o r e a r i l e n t a t i o n , w e w l l u s e t h e a b r e v i a t i o n p _ { k } ( x ) = } | X \\, - \\, x \\rangle \\colon \\text { this is the probability } \\, \\text { that } \\, \\text { an observation }$$  \nEquation 4.15 suggests that instead of directly computing the posterior probability p k ( x ) as in Section 4.3.1, we can simply plug in estimates of π k and f k ( x ) into (4.15). In general, estimating π k is easy if we have a random sample from the population: we simply compute the fraction of the training observations that belong to the k th class. However, estimating the density function f k ( x ) is much more challenging. As we will see, to estimate f k ( x ) , we will typically have to make some simplifying assumptions.  \nIn accordance with our earlier notation, we will use the abbreviation p k ( x ) = Pr( Y = k ♣ X = x ) ; this is the posterior probability that an observation X = x belongs to the k th class. That is, it is the probability that the observation belongs to the k th class, given the predictor value for that observation.  \nWe know from Chapter 2 that the Bayes classifier, which classifies an observation x to the class for which p k ( x ) is largest, has the lowest possible error rate out of all classifiers. (Of course, this is only true if all of the terms in (4.15) are correctly specified.) Therefore, if we can find a way to estimate f k ( x ) , then we can plug it into (4.15) in order to approximate the Bayes classifier.  \nIn the following sections, we discuss three classifiers that use different estimates of f k ( x ) in (4.15) to approximate the Bayes classifier: linear discriminant analysis, quadratic discriminant analysis, and naive Bayes .""]","La diferencia principal entre un problema de regresión y uno de clasificación radica en el tipo de variable de respuesta que se está prediciendo:

- **Regresión**: Se refiere a problemas donde la variable de respuesta es cuantitativa, es decir, toma valores numéricos. Ejemplos de variables cuantitativas incluyen la edad de una persona, la altura, el ingreso, el valor de una casa o el precio de una acción. En estos casos, se utilizan métodos como la regresión lineal para predecir el valor numérico de la variable de respuesta.

- **Clasificación**: Se refiere a problemas donde la variable de respuesta es cualitativa o categórica, es decir, toma valores en una de varias clases o categorías. Ejemplos de variables cualitativas incluyen el estado civil de una persona (casado o no), la marca de un producto comprado (marca A, B o C), si una persona incumple con una deuda (sí o no), o un diagnóstico de cáncer (Leucemia Mieloide Aguda, Leucemia Linfoblástica Aguda o Sin Leucemia). En estos casos, se utilizan métodos como la regresión logística para predecir la clase a la que pertenece la variable de respuesta.

En resumen, la elección entre un enfoque de regresión o de clasificación depende de si la variable de respuesta es cuantitativa o cualitativa.","Un problema de regresión se refiere a la predicción de un valor de salida cuantitativo o continuo, como el precio de una acción. Un problema de clasificación se refiere a la predicción de un valor de salida cualitativo o categórico, como si un correo electrónico es spam o no.",1.0,,0.9999999999666667,1.0
"According to the text, what is the formula for the t-statistic used in hypothesis testing for linear regression coefficients?","[""### 3.2.2 Some Important Questions  \nWhen we perform multiple linear regression, we usually are interested in answering a few important questions.  \n1. Is at least one of the predictors X 1  X 2         X p useful in predicting the response?\n2. Do all the predictors help to explain Y , or is only a subset of the predictors useful?\n3. How well does the model fit the data?\n4. Given a set of predictor values, what response value should we predict, and how accurate is our prediction?  \nWe now address each of these questions in turn.\nOne: Is There a Relationship Between the Response and Predictors?  \nRecall that in the simple linear regression setting, in order to determine whether there is a relationship between the response and the predictor we can simply check whether β 1 = 0 . In the multiple regression setting with p predictors, we need to ask whether all of the regression coefficients are zero, i.e. whether β 1 = β 2 = · · · = β p = 0 . As in the simple linear regression setting, we use a hypothesis test to answer this question. We test the null hypothesis, versus the alternative  \n$$H _ { 0 } \\colon \\beta _ { 1 } = \\beta _ { 2 } = \\dots = \\beta _ { p } = 0$$  \n$$H _ { a } \\colon a t l e a s t o n e \\beta _ { j } \\text { is non-zero.}$$  \nThis hypothesis test is performed by computing the F -statistic ,  \n$$F = \\frac { ( T S S - R S S ) / p } { R S S / ( n - p - 1 ) } , \\\\$$  \n$$E \\{ R S S / ( n - p - 1 ) \\} = \\sigma ^ { 2 }$$  \nwhere, as with simple linear regression, TSS = ∑ ( y i -¯ y ) 2 and RSS = ∑ ( y i -ˆ y i ) 2 . If the linear model assumptions are correct, one can show that and that, provided H 0 is true,  \n$$E \\{ ( T S S - R S S ) / p \\} = \\sigma ^ { 2 } .$$  \nHence, when there is no relationship between the response and predictors, one would expect the F -statistic to take on a value close to 1 . On the other hand, if H a is true, then E ¶ (TSS -RSS) glyph[triangleleft]p ♦ &gt; σ 2 , so we expect F to be greater than 1 .  \nThe F -statistic for the multiple linear regression model obtained by regressing sales onto radio , TV , and newspaper is shown in Table 3.6. In this example the F -statistic is 570 . Since this is far larger than 1 , it provides compelling evidence against the null hypothesis H 0 . In other words, the large F -statistic suggests that at least one of the advertising media must be related to sales . However, what if the F -statistic had been closer to 1 ? How large does the F -statistic need to be before we can reject H 0 and  \nF  \n-statistic  \nTABLE 3.6. More information about the least squares model for the regression of number of units sold on TV, newspaper, and radio advertising budgets in the Advertising data. Other information about this model was displayed in Table 3.4.  \n| Quantity                |   Value |\n|-------------------------|---------|\n| Residual standard error |   1.69  |\n| R 2                     |   0.897 |\n| F -statistic            | 570     |  \nconclude that there is a relationship? It turns out that the answer depends on the values of n and p . When n is large, an F -statistic that is just a little larger than 1 might still provide evidence against H 0 . In contrast, a larger F -statistic is needed to reject H 0 if n is small. When H 0 is true and the errors /epsilon1 i have a normal distribution, the F -statistic follows an F -distribution. 6 For any given value of n and p , any statistical software package can be used to compute the p -value associated with the F -statistic using this distribution. Based on this p -value, we can determine whether or not to reject H 0 . For the advertising data, the p -value associated with the F -statistic in Table 3.6 is essentially zero, so we have extremely strong evidence that at least one of the media is associated with increased sales . In (3.23) we are testing H 0 that all the coefficients are zero. Sometimes we want to test that a particular subset of q of the coefficients are zero.  \nThis corresponds to a null hypothesis  \n$$H _ { 0 } \\colon \\ \\beta _ { p - q + 1 } = \\beta _ { p - q + 2 } = \\dots = \\beta _ { p } = 0 ,$$  \nwhere for convenience we have put the variables chosen for omission at the end of the list. In this case we fit a second model that uses all the variables except those last q . Suppose that the residual sum of squares for that model is RSS 0 . Then the appropriate F -statistic is  \n$$F = \\frac { ( R S S _ { 0 } - R S S ) / q } { R S S / ( n - p - 1 ) } . \\\\$$  \nGiven these individual p -values for each variable, why do we need to look at the overall F -statistic? After all, it seems likely that if any one of the p -values for the individual variables is very small, then at least one of the predictors is related to the response . However, this logic is flawed, especially when the number of predictors p is large.  \nNotice that in Table 3.4, for each individual predictor a t -statistic and a p -value were reported. These provide information about whether each individual predictor is related to the response, after adjusting for the other predictors. It turns out that each of these is exactly equivalent 7 to the F -test that omits that single variable from the model, leaving all the others in-i.e. q =1 in (3.24). So it reports the partial effect of adding that variable to the model. For instance, as we discussed earlier, these p -values indicate that TV and radio are related to sales , but that there is no evidence that newspaper is associated with sales , when TV and radio are held fixed.  \n6 Even if the errors are not normally-distributed, the F -statistic approximately follows an F -distribution provided that the sample size n is large.  \n7 The square of each t -statistic is the corresponding F -statistic.  \nFor instance, consider an example in which p = 100 and H 0 : β 1 = β 2 = · · · = β p = 0 is true, so no variable is truly associated with the response. In this situation, about 5 % of the p -values associated with each variable (of the type shown in Table 3.4) will be below 0   05 by chance. In other words, we expect to see approximately five small p -values even in the absence of any true association between the predictors and the response. 8 In fact, it is likely that we will observe at least one p -value below 0   05 by chance! Hence, if we use the individual t -statistics and associated p -values in order to decide whether or not there is any association between the variables and the response, there is a very high chance that we will incorrectly conclude that there is a relationship. However, the F -statistic does not suffer from this problem because it adjusts for the number of predictors. Hence, if H 0 is true, there is only a 5 % chance that the F -statistic will result in a p -value below 0   05 , regardless of the number of predictors or the number of observations.  \nThe approach of using an F -statistic to test for any association between the predictors and the response works when p is relatively small, and certainly small compared to n . However, sometimes we have a very large number of variables. If p &gt; n then there are more coefficients β j to estimate than observations from which to estimate them. In this case we cannot even fit the multiple linear regression model using least squares, so the F -statistic cannot be used, and neither can most of the other concepts that we have seen so far in this chapter. When p is large, some of the approaches discussed in the next section, such as forward selection , can be used. This high-dimensional setting is discussed in greater detail in Chapter 6.\nTwo: Deciding on Important Variables  \nAs discussed in the previous section, the first step in a multiple regression analysis is to compute the F -statistic and to examine the associated p -value. If we conclude on the basis of that p -value that at least one of the predictors is related to the response, then it is natural to wonder which are the guilty ones! We could look at the individual p -values as in Table 3.4, but as discussed (and as further explored in Chapter 13), if p is large we are likely to make some false discoveries.  \nIdeally, we would like to perform variable selection by trying out a lot of different models, each containing a different subset of the predictors. For instance, if p = 2 , then we can consider four models: (1) a model containing no variables, (2) a model containing X 1 only, (3) a model containing  \nIt is possible that all of the predictors are associated with the response, but it is more often the case that the response is only associated with a subset of the predictors. The task of determining which predictors are associated with the response, in order to fit a single model involving only those predictors, is referred to as variable selection . The variable selection problem is studied extensively in Chapter 6, and so here we will provide only a brief outline of some classical approaches.  \n8 This is related to the important concept of multiple testing , which is the focus of Chapter 13.  \nhighdimensional variable selection  \nX 2 only, and (4) a model containing both X 1 and X 2 . We can then select the best model out of all of the models that we have considered. How do we determine which model is best? Various statistics can be used to judge the quality of a model. These include Mallow's C p , Akaike information criterion (AIC), Bayesian information criterion (BIC), and adjusted R 2 . These are discussed in more detail in Chapter 6. We can also determine which model is best by plotting various model outputs, such as the residuals, in order to search for patterns.  \nUnfortunately, there are a total of 2 p models that contain subsets of p variables. This means that even for moderate p , trying out every possible subset of the predictors is infeasible. For instance, we saw that if p = 2 , then there are 2 2 = 4 models to consider. But if p = 30 , then we must consider 2 30 = 1  073  741  824 models! This is not practical. Therefore, unless p is very small, we cannot consider all 2 p models, and instead we need an automated and efficient approach to choose a smaller set of models to consider. There are three classical approaches for this task:  \n- Forward selection . We begin with the null model -a model that contains an intercept but no predictors. We then fit p simple linear regressions and add to the null model the variable that results in the lowest RSS. We then add to that model the variable that results in the lowest RSS for the new two-variable model. This approach is continued until some stopping rule is satisfied.\n- Backward selection . We start with all variables in the model, and remove the variable with the largest p -value-that is, the variable that is the least statistically significant. The new ( p -1) -variable model is fit, and the variable with the largest p -value is removed. This procedure continues until a stopping rule is reached. For instance, we may stop when all remaining variables have a p -value below some threshold.\n- Mixed selection . This is a combination of forward and backward selection. We start with no variables in the model, and as with forward selection, we add the variable that provides the best fit. We continue to add variables one-by-one. Of course, as we noted with the Advertising example, the p -values for variables can become larger as new predictors are added to the model. Hence, if at any point the p -value for one of the variables in the model rises above a certain threshold, then we remove that variable from the model. We continue to perform these forward and backward steps until all variables in the model have a sufficiently low p -value, and all variables outside the model would have a large p -value if added to the model.  \nBackward selection cannot be used if p &gt; n , while forward selection can always be used. Forward selection is a greedy approach, and might include variables early that later become redundant. Mixed selection can remedy this.  \nMallow's C p Akaike information criterion Bayesian information criterion adjusted R 2  \nforward selection null model backward selection  \nmixed selection\nThree: Model Fit  \nTwo of the most common numerical measures of model fit are the RSE and R 2 , the fraction of variance explained. These quantities are computed and interpreted in the same fashion as for simple linear regression.  \nAn R 2 value close to 1 indicates that the model explains a large portion of the variance in the response variable. As an example, we saw in Table 3.6 that for the Advertising data, the model that uses all three advertising media to predict sales has an R 2 of 0   8972 . On the other hand, the model that uses only TV and radio to predict sales has an R 2 value of 0   89719 . In other words, there is a small increase in R 2 if we include newspaper advertising in the model that already contains TV and radio advertising, even though we saw earlier that the p -value for newspaper advertising in Table 3.4 is not significant. It turns out that R 2 will always increase when more variables are added to the model, even if those variables are only weakly associated with the response. This is due to the fact that adding another variable always results in a decrease in the residual sum of squares on the training data (though not necessarily the testing data). Thus, the R 2 statistic, which is also computed on the training data, must increase. The fact that adding newspaper advertising to the model containing only TV and radio advertising leads to just a tiny increase in R 2 provides additional evidence that newspaper can be dropped from the model. Essentially, newspaper provides no real improvement in the model fit to the training samples, and its inclusion will likely lead to poor results on independent test samples due to overfitting.  \nRecall that in simple regression, R 2 is the square of the correlation of the response and the variable. In multiple linear regression, it turns out that it equals Cor ( Y ˆ Y ) 2 , the square of the correlation between the response and the fitted linear model; in fact one property of the fitted linear model is that it maximizes this correlation among all possible linear models.  \nBy contrast, the model containing only TV as a predictor had an R 2 of 0   61 (Table 3.2). Adding radio to the model leads to a substantial improvement in R 2 . This implies that a model that uses TV and radio expenditures to predict sales is substantially better than one that uses only TV advertising. We could further quantify this improvement by looking at the p -value for the radio coefficient in a model that contains only TV and radio as predictors.  \n$$R S = \\sqrt { \\frac { 1 } { n - p - 1 } R S S } ,$$  \nThe model that contains only TV and radio as predictors has an RSE of 1.681, and the model that also contains newspaper as a predictor has an RSE of 1.686 (Table 3.6). In contrast, the model that contains only TV has an RSE of 3   26 (Table 3.2). This corroborates our previous conclusion that a model that uses TV and radio expenditures to predict sales is much more accurate (on the training data) than one that only uses TV spending. Furthermore, given that TV and radio expenditures are used as predictors, there is no point in also using newspaper spending as a predictor in the model. The observant reader may wonder how RSE can increase when newspaper is added to the model given that RSS must decrease. In general RSE is defined as  \nFIGURE 3.5. For the Advertising data, a linear regression fit to sales using TV and radio as predictors. From the pattern of the residuals, we can see that there is a pronounced non-linear relationship in the data. The positive residuals (those visible above the surface), tend to lie along the 45-degree line, where TV and Radio budgets are split evenly. The negative residuals (most not visible), tend to lie away from this line, where budgets are more lopsided.  \n<!-- image -->  \nwhich simplifies to (3.15) for a simple linear regression. Thus, models with more variables can have higher RSE if the decrease in RSS is small relative to the increase in p .  \nIn addition to looking at the RSE and R 2 statistics just discussed, it can be useful to plot the data. Graphical summaries can reveal problems with a model that are not visible from numerical statistics. For example, Figure 3.5 displays a three-dimensional plot of TV and radio versus sales . We see that some observations lie above and some observations lie below the least squares regression plane. In particular, the linear model seems to overestimate sales for instances in which most of the advertising money was spent exclusively on either TV or radio . It underestimates sales for instances where the budget was split between the two media. This pronounced non-linear pattern suggests a synergy or interaction effect between the advertising media, whereby combining the media together results in a bigger boost to sales than using any single medium. In Section 3.3.2, we will discuss extending the linear model to accommodate such synergistic effects through the use of interaction terms.\nFour: Predictions  \nOnce we have fit the multiple regression model, it is straightforward to apply (3.21) in order to predict the response Y on the basis of a set of values for the predictors X 1  X 2         X p . However, there are three sorts of uncertainty associated with this prediction.  \n1. The coefficient estimates ˆ β 0  ˆ β 1         ˆ β p are estimates for β 0  β 1         β p . That is, the least squares plane  \n$$\\hat { Y } = \\hat { \\beta } _ { 0 } + \\hat { \\beta } _ { 1 } X _ { 1 } + \\dots + \\hat { \\beta } _ { p } X _ { p }$$  \ninteraction  \nis only an estimate for the true population regression plane  \n$$f ( X ) = \\beta _ { 0 } + \\beta _ { 1 } X _ { 1 } + \\cdots + \\beta _ { p } X _ { p } .$$  \nThe inaccuracy in the coefficient estimates is related to the reducible error from Chapter 2. We can compute a confidence interval in order to determine how close ˆ Y will be to f ( X ) .  \n2. Of course, in practice assuming a linear model for f ( X ) is almost always an approximation of reality, so there is an additional source of potentially reducible error which we call model bias . So when we use a linear model, we are in fact estimating the best linear approximation to the true surface. However, here we will ignore this discrepancy, and operate as if the linear model were correct.\n3. Even if we knew f ( X ) -that is, even if we knew the true values for β 0  β 1         β p -the response value cannot be predicted perfectly because of the random error /epsilon1 in the model (3.20). In Chapter 2, we referred to this as the irreducible error . How much will Y vary from ˆ Y ? We use prediction intervals to answer this question. Prediction intervals are always wider than confidence intervals, because they incorporate both the error in the estimate for f ( X ) (the reducible error) and the uncertainty as to how much an individual point will differ from the population regression plane (the irreducible error).  \nWe use a confidence interval to quantify the uncertainty surrounding the average sales over a large number of cities. For example, given that $100  000 is spent on TV advertising and $20  000 is spent on radio advertising in each city, the 95 % confidence interval is [10  985  11  528] . We interpret this to mean that 95 % of intervals of this form will contain the true value of f ( X ) . 9 On the other hand, a prediction interval can be used to quantify the uncertainty surrounding sales for a particular city. Given that $100  000 is spent on TV advertising and $20  000 is spent on radio advertising in that city the 95 % prediction interval is [7  930  14  580] . We interpret this to mean that 95 % of intervals of this form will contain the true value of Y for this city. Note that both intervals are centered at 11  256 , but that the prediction interval is substantially wider than the confidence interval, reflecting the increased uncertainty about sales for a given city in comparison to the average sales over many locations.  \n9 In other words, if we collect a large number of data sets like the Advertising data set, and we construct a confidence interval for the average sales on the basis of each data set (given $100  000 in TV and $20  000 in radio advertising), then 95 % of these confidence intervals will contain the true value of average sales .  \nconfidence interval prediction interval"", ""### 3.1.2 Assessing the Accuracy of the Coefficient Estimates  \nRecall from (2.1) that we assume that the true relationship between X and Y takes the form Y = f ( X ) + /epsilon1 for some unknown function f , where /epsilon1 is a mean-zero random error term. If f is to be approximated by a linear function, then we can write this relationship as  \n$$Y = \\beta _ { 0 } + \\beta _ { 1 } X + \\epsilon .$$  \nHere β 0 is the intercept term-that is, the expected value of Y when X = 0 , and β 1 is the slope-the average increase in Y associated with a one-unit increase in X . The error term is a catch-all for what we miss with this simple model: the true relationship is probably not linear, there may be other variables that cause variation in Y , and there may be measurement error. We typically assume that the error term is independent of X .  \nFIGURE 3.2. Contour and three-dimensional plots of the RSS on the Advertising data, using sales as the response and TV as the predictor. The red dots correspond to the least squares estimates ˆ β 0 and ˆ β 1 , given by (3.4).  \n<!-- image -->  \nThe model given by (3.5) defines the population regression line , which is the best linear approximation to the true relationship between X and Y . 1 The least squares regression coefficient estimates (3.4) characterize the least squares line (3.2). The left-hand panel of Figure 3.3 displays these two lines in a simple simulated example. We created 100 random X s, and generated 100 corresponding Y s from the model  \n$$Y = 2 + 3 X + \\epsilon ,$$  \nwhere /epsilon1 was generated from a normal distribution with mean zero. The red line in the left-hand panel of Figure 3.3 displays the true relationship, f ( X ) = 2 + 3 X , while the blue line is the least squares estimate based on the observed data. The true relationship is generally not known for real data, but the least squares line can always be computed using the coefficient estimates given in (3.4). In other words, in real applications, we have access to a set of observations from which we can compute the least squares line; however, the population regression line is unobserved. In the right-hand panel of Figure 3.3 we have generated ten different data sets from the model given by (3.6) and plotted the corresponding ten least squares lines. Notice that different data sets generated from the same true model result in slightly different least squares lines, but the unobserved population regression line does not change.  \nAt first glance, the difference between the population regression line and the least squares line may seem subtle and confusing. We only have one data set, and so what does it mean that two different lines describe the relationship between the predictor and the response? Fundamentally, the concept of these two lines is a natural extension of the standard statistical approach of using information from a sample to estimate characteristics of a large population. For example, suppose that we are interested in knowing  \n1 The assumption of linearity is often a useful working model. However, despite what many textbooks might tell us, we seldom believe that the true relationship is linear.  \npopulation regression line least squares line  \nFIGURE 3.3. A simulated data set. Left: The red line represents the true relationship, f ( X ) = 2 + 3 X , which is known as the population regression line. The blue line is the least squares line; it is the least squares estimate for f ( X ) based on the observed data, shown in black. Right: The population regression line is again shown in red, and the least squares line in dark blue. In light blue, ten least squares lines are shown, each computed on the basis of a separate random set of observations. Each least squares line is different, but on average, the least squares lines are quite close to the population regression line.  \n<!-- image -->  \nthe population mean θ of some random variable Y . Unfortunately, θ is unknown, but we do have access to n observations from Y , y 1         y n , which we can use to estimate θ . A reasonable estimate is ˆ θ = ¯ y , where ¯ y = 1 n ∑ n i =1 y i is the sample mean. The sample mean and the population mean are different, but in general the sample mean will provide a good estimate of the population mean. In the same way, the unknown coefficients β 0 and β 1 in linear regression define the population regression line. We seek to estimate these unknown coefficients using ˆ β 0 and ˆ β 1 given in (3.4). These coefficient estimates define the least squares line.  \nThe analogy between linear regression and estimation of the mean of a random variable is an apt one based on the concept of bias . If we use the sample mean ˆ θ to estimate θ , this estimate is unbiased , in the sense that on average, we expect ˆ θ to equal θ . What exactly does this mean? It means that on the basis of one particular set of observations y 1         y n , ˆ θ might overestimate θ , and on the basis of another set of observations, ˆ θ might underestimate θ . But if we could average a huge number of estimates of θ obtained from a huge number of sets of observations, then this average would exactly equal θ . Hence, an unbiased estimator does not systematically over- or under-estimate the true parameter. The property of unbiasedness holds for the least squares coefficient estimates given by (3.4) as well: if we estimate β 0 and β 1 on the basis of a particular data set, then our estimates won't be exactly equal to β 0 and β 1 . But if we could average the estimates obtained over a huge number of data sets, then the average of these estimates would be spot on! In fact, we can see from the righthand panel of Figure 3.3 that the average of many least squares lines, each bias unbiased  \nestimated from a separate data set, is pretty close to the true population regression line.  \nWe continue the analogy with the estimation of the population mean θ of a random variable Y . A natural question is as follows: how accurate is the sample mean ˆ θ as an estimate of θ ? We have established that the average of ˆ θ 's over many data sets will be very close to θ , but that a single estimate ˆ θ may be a substantial underestimate or overestimate of θ . How far off will that single estimate of ˆ θ be? In general, we answer this question by computing the standard error of ˆ θ , written as SE(ˆ θ ) . We have the well-known formula  \n$$V a r ( \\hat { \\mu } ) = \\text {SE} ( \\hat { \\mu } ) ^ { 2 } = \\frac { \\sigma ^ { 2 } } { n } ,$$  \nwhere σ is the standard deviation of each of the realizations y i of Y . 2 Roughly speaking, the standard error tells us the average amount that this estimate ˆ θ differs from the actual value of θ . Equation 3.7 also tells us how this deviation shrinks with n -the more observations we have, the smaller the standard error of ˆ θ . In a similar vein, we can wonder how close ˆ β 0 and ˆ β 1 are to the true values β 0 and β 1 . To compute the standard errors associated with ˆ β 0 and ˆ β 1 , we use the following formulas:  \n$$\\intertext { s u c h s c r { E } ( \\hat { \\beta } _ { 0 } ) ^ { 2 } = \\sigma ^ { 2 } \\left [ \\frac { 1 } { n } + \\frac { \\bar { x } ^ { 2 } } { \\sum _ { i = 1 } ^ { n } ( x _ { i } - \\bar { x } ) ^ { 2 } } \\right ] , \\ \\, \\text {SE} ( \\hat { \\beta } _ { 1 } ) ^ { 2 } = \\frac { \\sigma ^ { 2 } } { \\sum _ { i = 1 } ^ { n } ( x _ { i } - \\bar { x } ) ^ { 2 } } , \\ ( 3 . 8 ) } \\\\ \\text {where } \\sigma ^ { 2 } = \\text {Var} ( \\epsilon ) . \\text { For these formulas to be strictly valid, we need to }$$  \nStandard errors can be used to compute confidence intervals . A 95% confidence interval is defined as a range of values such that with 95 % probability, the range will contain the true unknown value of the parameter. The range is defined in terms of lower and upper limits computed from the sample of data. A 95% confidence interval has the following property: if we take repeated samples and construct the confidence interval for each sample, 95% of the intervals will contain the true unknown value of the parameter. For linear regression, the 95 % confidence interval for β 1 approximately takes the form where σ 2 = Var ( /epsilon1 ) . For these formulas to be strictly valid, we need to assume that the errors /epsilon1 i for each observation have common variance σ 2 and are uncorrelated. This is clearly not true in Figure 3.1, but the formula still turns out to be a good approximation. Notice in the formula that SE( ˆ β 1 ) is smaller when the x i are more spread out; intuitively we have more leverage to estimate a slope when this is the case. We also see that SE( ˆ β 0 ) would be the same as SE(ˆ θ ) if ¯ x were zero (in which case ˆ β 0 would be equal to ¯ y ). In general, σ 2 is not known, but can be estimated from the data. This estimate of σ is known as the residual standard error , and is given by the formula RSE = √ RSS glyph[triangleleft] ( n -2) . Strictly speaking, when σ 2 is estimated from the data we should write ̂ SE( ˆ β 1 ) to indicate that an estimate has been made, but for simplicity of notation we will drop this extra 'hat'.  \n$$\\hat { \\beta } _ { 1 } \\pm 2 \\cdot \\text {SE} ( \\hat { \\beta } _ { 1 } ) .$$  \n2 This formula holds provided that the n observations are uncorrelated.  \nstandard error residual standard error  \nconfidence interval  \nThat is, there is approximately a 95 % chance that the interval  \n$$\\left [ \\hat { \\beta } _ { 1 } - 2 \\cdot \\text {SE} ( \\hat { \\beta } _ { 1 } ) , \\, \\hat { \\beta } _ { 1 } + 2 \\cdot \\text {SE} ( \\hat { \\beta } _ { 1 } ) \\right ] \\\\ \\\\ \\left [ \\hat { \\beta } _ { 1 } - 2 \\cdot \\text {SE} ( \\hat { \\beta } _ { 1 } ) , \\, \\hat { \\beta } _ { 1 } + 2 \\cdot \\text {SE} ( \\hat { \\beta } _ { 1 } ) \\right ] \\\\$$  \nwill contain the true value of β 1 . 3 Similarly, a confidence interval for β 0 approximately takes the form  \n$$\\hat { \\beta } _ { 0 } \\pm 2 \\cdot \\text {SE} ( \\hat { \\beta } _ { 0 } ) .$$  \nIn the case of the advertising data, the 95 % confidence interval for β 0 is [6   130  7   935] and the 95 % confidence interval for β 1 is [0   042  0   053] . Therefore, we can conclude that in the absence of any advertising, sales will, on average, fall somewhere between 6  130 and 7  935 units. Furthermore, for each $1  000 increase in television advertising, there will be an average increase in sales of between 42 and 53 units.  \nStandard errors can also be used to perform hypothesis tests on the coefficients. The most common hypothesis test involves testing the null hypothesis of  \n$$H _ { 0 } \\colon \\text {There is no relationship between } X \\text { and } Y$$  \nversus the alternative hypothesis  \n$$H _ { a } \\colon \\text {There is some relationship between } X \\text { and } Y .$$  \nMathematically, this corresponds to testing  \n$$H _ { 0 } \\colon \\beta _ { 1 } = 0$$  \n$$H _ { a } \\colon \\beta _ { 1 } \\neq 0 ,$$  \nsince if β 1 = 0 then the model (3.5) reduces to Y = β 0 + /epsilon1 , and X is not associated with Y . To test the null hypothesis, we need to determine whether ˆ β 1 , our estimate for β 1 , is sufficiently far from zero that we can be confident that β 1 is non-zero. How far is far enough? This of course depends on the accuracy of ˆ β 1 -that is, it depends on SE( ˆ β 1 ) . If SE( ˆ β 1 ) is small, then even relatively small values of ˆ β 1 may provide strong evidence that β 1 = 0 , and hence that there is a relationship between X and Y . In contrast, if SE( ˆ β 1 ) is large, then ˆ β 1 must be large in absolute value in order for us to reject the null hypothesis. In practice, we compute a t -statistic , given by  \n/negationslash  \n$$t = \\frac { \\hat { \\beta } _ { 1 } - 0 } { \\text {SE} ( \\hat { \\beta } _ { 1 } ) } ,$$  \n3 Approximately for several reasons. Equation 3.10 relies on the assumption that the errors are Gaussian. Also, the factor of 2 in front of the SE( ˆ β 1 ) term will vary slightly depending on the number of observations n in the linear regression. To be precise, rather than the number 2, (3.10) should contain the 97.5 % quantile of a t -distribution with n -2 degrees of freedom. Details of how to compute the 95 % confidence interval precisely in R will be provided later in this chapter.  \nversus  \n/negationslash hypothesis test  \nnull hypothesis alternative hypothesis  \nt -statistic  \nTABLE 3.1. For the Advertising data, coefficients of the least squares model for the regression of number of units sold on TV advertising budget. An increase of $1  000 in the TV advertising budget is associated with an increase in sales by around 50 units. (Recall that the sales variable is in thousands of units, and the TV variable is in thousands of dollars.)  \n|           |   Coefficient |   Std. error |   t -statistic | p -value                      |\n|-----------|---------------|--------------|----------------|-------------------------------|\n| Intercept |        7.0325 |       0.4578 |          15.36 | < 0   0001 |\n| TV        |        0.0475 |       0.0027 |          17.67 | < 0   0001 |  \nwhich measures the number of standard deviations that ˆ β 1 is away from 0 . If there really is no relationship between X and Y , then we expect that (3.14) will have a t -distribution with n -2 degrees of freedom. The t -distribution has a bell shape and for values of n greater than approximately 30 it is quite similar to the standard normal distribution. Consequently, it is a simple matter to compute the probability of observing any number equal to ♣ t ♣ or larger in absolute value, assuming β 1 = 0 . We call this probability the p -value . Roughly speaking, we interpret the p -value as follows: a small p -value indicates that it is unlikely to observe such a substantial association between the predictor and the response due to chance, in the absence of any real association between the predictor and the response. Hence, if we see a small p -value, then we can infer that there is an association between the predictor and the response. We reject the null hypothesis -that is, we declare a relationship to exist between X and Y -if the p -value is small enough. Typical p -value cutoffs for rejecting the null hypothesis are 5% or 1%, although this topic will be explored in much greater detail in Chapter 13. When n = 30 , these correspond to t -statistics (3.14) of around 2 and 2.75, respectively.  \n/negationslash  \nTable 3.1 provides details of the least squares model for the regression of number of units sold on TV advertising budget for the Advertising data. Notice that the coefficients for ˆ β 0 and ˆ β 1 are very large relative to their standard errors, so the t -statistics are also large; the probabilities of seeing such values if H 0 is true are virtually zero. Hence we can conclude that β 0 = 0 and β 1 = 0 . 4  \n/negationslash"", ""### 4.3.2 Estimating the Regression Coefficients  \nThe coefficients β 0 and β 1 in (4.2) are unknown, and must be estimated based on the available training data. In Chapter 3, we used the least squares approach to estimate the unknown linear regression coefficients. Although we could use (non-linear) least squares to fit the model (4.4), the more general method of maximum likelihood is preferred, since it has better statistical properties. The basic intuition behind using maximum likelihood odds  \nlog odds logit  \nto fit a logistic regression model is as follows: we seek estimates for β 0 and β 1 such that the predicted probability ˆ p ( x i ) of default for each individual, using (4.2), corresponds as closely as possible to the individual's observed default status. In other words, we try to find ˆ β 0 and ˆ β 1 such that plugging these estimates into the model for p ( X ) , given in (4.2), yields a number close to one for all individuals who defaulted, and a number close to zero for all individuals who did not. This intuition can be formalized using a mathematical equation called a likelihood function :  \n$$\\ell ( \\beta _ { 0 } , \\beta _ { 1 } ) = \\prod _ { i \\colon y _ { i } = 1 } p ( x _ { i } ) \\prod _ { i ^ { \\prime } \\colon y _ { i ^ { \\prime } } = 0 } ( 1 - p ( x _ { i ^ { \\prime } } ) ) .$$  \nThe estimates ˆ β 0 and ˆ β 1 are chosen to maximize this likelihood function.  \nTable 4.1 shows the coefficient estimates and related information that result from fitting a logistic regression model on the Default data in order to predict the probability of default = Yes using balance . We see that ˆ β 1 = 0   0055 ; this indicates that an increase in balance is associated with an increase in the probability of default . To be precise, a one-unit increase in balance is associated with an increase in the log odds of default by 0   0055 units.  \nMaximum likelihood is a very general approach that is used to fit many of the non-linear models that we examine throughout this book. In the linear regression setting, the least squares approach is in fact a special case of maximum likelihood. The mathematical details of maximum likelihood are beyond the scope of this book. However, in general, logistic regression and other models can be easily fit using statistical software such as R , and so we do not need to concern ourselves with the details of the maximum likelihood fitting procedure.  \nMany aspects of the logistic regression output shown in Table 4.1 are similar to the linear regression output of Chapter 3. For example, we can measure the accuracy of the coefficient estimates by computing their standard errors. The z -statistic in Table 4.1 plays the same role as the t -statistic in the linear regression output, for example in Table 3.1 on page 77. For instance, the z -statistic associated with β 1 is equal to ˆ β 1 glyph[triangleleft] SE( ˆ β 1 ) , and so a large (absolute) value of the z -statistic indicates evidence against the null hypothesis H 0 : β 1 = 0 . This null hypothesis implies that p ( X ) = e β 0 1+ e β 0 : in other words, that the probability of default does not depend on balance . Since the p -value associated with balance in Table 4.1 is tiny, we can reject H 0 . In other words, we conclude that there is indeed an association between balance and probability of default . The estimated intercept in Table 4.1 is typically not of interest; its main purpose is to adjust the average fitted probabilities to the proportion of ones in the data (in this case, the overall default rate)."", '### Applied  \n8. This question involves the use of simple linear regression on the Auto data set.\n2. (a) Use the sm.OLS() function to perform a simple linear regression with mpg as the response and horsepower as the predictor. Use the summarize() function to print the results. Comment on the output. For example:\n- i. Is there a relationship between the predictor and the response?\n- ii. How strong is the relationship between the predictor and the response?\n- iii. Is the relationship between the predictor and the response positive or negative?\n- iv. What is the predicted mpg associated with a horsepower of 98? What are the associated 95 % confidence and prediction intervals?\n7. (b) Plot the response and the predictor in a new set of axes ax . Use the ax.axline() method or the abline() function defined in the lab to display the least squares regression line.\n8. (c) Produce some of diagnostic plots of the least squares regression fit as described in the lab. Comment on any problems you see with the fit.\n9. This question involves the use of multiple linear regression on the Auto data set.\n10. (a) Produce a scatterplot matrix which includes all of the variables in the data set.\n11. (b) Compute the matrix of correlations between the variables using the DataFrame.corr() method.\n12. (c) Use the sm.OLS() function to perform a multiple linear regression with mpg as the response and all other variables except name as the predictors. Use the summarize() function to print the results. Comment on the output. For instance:\n- i. Is there a relationship between the predictors and the response? Use the anova\\_lm() function from statsmodels to answer this question.  \n<!-- image -->  \n.corr()  \n- ii. Which predictors appear to have a statistically significant relationship to the response?\n- iii. What does the coefficient for the year variable suggest?\n3. (d) Produce some of diagnostic plots of the linear regression fit as described in the lab. Comment on any problems you see with the fit. Do the residual plots suggest any unusually large outliers? Does the leverage plot identify any observations with unusually high leverage?\n4. (e) Fit some models with interactions as described in the lab. Do any interactions appear to be statistically significant?\n5. (f) Try a few different transformations of the variables, such as log( X ) , √ X , X 2 . Comment on your findings.\n10. This question should be answered using the Carseats data set.\n7. (a) Fit a multiple regression model to predict Sales using Price , Urban , and US .\n8. (b) Provide an interpretation of each coefficient in the model. Be careful-some of the variables in the model are qualitative!\n9. (c) Write out the model in equation form, being careful to handle the qualitative variables properly.\n10. (d) For which of the predictors can you reject the null hypothesis H 0 : β j = 0 ?\n11. (e) On the basis of your response to the previous question, fit a smaller model that only uses the predictors for which there is evidence of association with the outcome.\n12. (f) How well do the models in (a) and (e) fit the data?\n13. (g) Using the model from (e), obtain 95 % confidence intervals for the coefficient(s).\n14. (h) Is there evidence of outliers or high leverage observations in the model from (e)?\n11. In this problem we will investigate the t -statistic for the null hypothesis H 0 : β = 0 in simple linear regression without an intercept. To begin, we generate a predictor x and a response y as follows.\n16. (a) Perform a simple linear regression of y onto x , without an intercept. Report the coefficient estimate ˆ β , the standard error of this coefficient estimate, and the t -statistic and p -value associated with the null hypothesis H 0 : β = 0 . Comment on these results. (You can perform regression without an intercept using the keywords argument intercept=False to ModelSpec() .)  \n```\nrng = np.random.default_rng(1) x = rng.normal(size=100) y = 2 * x + rng.normal(size=100)\n```  \n- (b) Now perform a simple linear regression of x onto y without an intercept, and report the coefficient estimate, its standard error, and the corresponding t -statistic and p -values associated with the null hypothesis H 0 : β = 0 . Comment on these results.\n- (c) What is the relationship between the results obtained in (a) and (b)?\n- (d) For the regression of Y onto X without an intercept, the t -statistic for H 0 : β = 0 takes the form ˆ β glyph[triangleleft] SE( ˆ β ) , where ˆ β is given by (3.38), and where  \n$$SE ( \\hat { \\beta } ) = \\sqrt { \\frac { \\sum _ { i = 1 } ^ { n } ( y _ { i } - x _ { i } \\hat { \\beta } ) ^ { 2 } } { ( n - 1 ) \\sum _ { i ^ { \\prime } = 1 } ^ { n } x _ { i ^ { \\prime } } ^ { 2 } } } . \\\\ \\intertext { s u l a s e s h i l t y d i f f e r e n t f o r m h o s e g t h o w }$$  \n(These formulas are slightly different from those given in Sections 3.1.1 and 3.1.2, since here we are performing regression without an intercept.) Show algebraically, and confirm numerically in R , that the t -statistic can be written as  \n$$\\frac { ( \\sqrt { n - 1 } ) \\sum _ { i = 1 } ^ { n } x _ { i } y _ { i } } { \\sqrt { ( \\sum _ { i = 1 } ^ { n } x _ { i } ^ { 2 } ) ( \\sum _ { i ^ { \\prime } = 1 } ^ { n } y _ { i ^ { \\prime } } ^ { 2 } ) - ( \\sum _ { i ^ { \\prime } = 1 } ^ { n } x _ { i ^ { \\prime } } y _ { i ^ { \\prime } } ) ^ { 2 } } } .$$  \n- (e) Using the results from (d), argue that the t -statistic for the regression of y onto x is the same as the t -statistic for the regression of x onto y .\n- (f) In R , show that when regression is performed with an intercept, the t -statistic for H 0 : β 1 = 0 is the same for the regression of y onto x as it is for the regression of x onto y .\n12. This problem involves simple linear regression without an intercept.\n- (a) Recall that the coefficient estimate ˆ β for the linear regression of Y onto X without an intercept is given by (3.38). Under what circumstance is the coefficient estimate for the regression of X onto Y the same as the coefficient estimate for the regression of Y onto X ?\n- (b) Generate an example in Python with n = 100 observations in which the coefficient estimate for the regression of X onto Y is different from the coefficient estimate for the regression of Y onto X .\n- (c) Generate an example in Python with n = 100 observations in which the coefficient estimate for the regression of X onto Y is the same as the coefficient estimate for the regression of Y onto X .\n13. In this exercise you will create some simulated data and will fit simple linear regression models to it. Make sure to use the default random number generator with seed set to 1 prior to starting part (a) to ensure consistent results.  \n<!-- image -->  \n- (a) Using the normal() method of your random number generator, create a vector, x , containing 100 observations drawn from a N (0  1) distribution. This represents a feature, X .\n- (b) Using the normal() method, create a vector, eps , containing 100 observations drawn from a N (0  0   25) distribution-a normal distribution with mean zero and variance 0   25 .\n- (c) Using x and eps , generate a vector y according to the model  \n$$Y = - 1 + 0 . 5 X + \\epsilon .$$  \nWhat is the length of the vector y ? What are the values of β 0 and β 1 in this linear model?  \n- (d) Create a scatterplot displaying the relationship between x and y . Comment on what you observe.\n- (e) Fit a least squares linear model to predict y using x . Comment on the model obtained. How do ˆ β 0 and ˆ β 1 compare to β 0 and β 1 ?\n- (f) Display the least squares line on the scatterplot obtained in (d). Draw the population regression line on the plot, in a different color. Use the legend() method of the axes to create an appropriate legend.\n- (g) Now fit a polynomial regression model that predicts y using x and x 2 . Is there evidence that the quadratic term improves the model fit? Explain your answer.\n- (h) Repeat (a)-(f) after modifying the data generation process in such a way that there is less noise in the data. The model (3.39) should remain the same. You can do this by decreasing the variance of the normal distribution used to generate the error term /epsilon1 in (b). Describe your results.\n- (i) Repeat (a)-(f) after modifying the data generation process in such a way that there is more noise in the data. The model (3.39) should remain the same. You can do this by increasing the variance of the normal distribution used to generate the error term /epsilon1 in (b). Describe your results.\n- (j) What are the confidence intervals for β 0 and β 1 based on the original data set, the noisier data set, and the less noisy data set? Comment on your results.\n14. This problem focuses on the collinearity problem.\n- (a) Perform the following commands in Python :  \n```\nrng = np.random.default_rng(10) x1 = rng.uniform(0, 1, size=100) x2 = 0.5 * x1 + rng.normal(size=100) / 10 y = 2 + 2 * x1 + 0.3 * x2 + rng.normal(size=100)\n```  \nThe last line corresponds to creating a linear model in which y is a function of x1 and x2 . Write out the form of the linear model. What are the regression coefficients?  \n- (b) What is the correlation between x1 and x2 ? Create a scatterplot displaying the relationship between the variables.\n- (c) Using this data, fit a least squares regression to predict y using x1 and x2 . Describe the results obtained. What are ˆ β 0 , ˆ β 1 , and ˆ β 2 ? How do these relate to the true β 0 , β 1 , and β 2 ? Can you reject the null hypothesis H 0 : β 1 = 0 ? How about the null hypothesis H 0 : β 2 = 0 ?\n- (d) Now fit a least squares regression to predict y using only x1 . Comment on your results. Can you reject the null hypothesis H 0 : β 1 = 0 ?\n- (e) Now fit a least squares regression to predict y using only x2 . Comment on your results. Can you reject the null hypothesis H 0 : β 1 = 0 ?\n- (f) Do the results obtained in (c)-(e) contradict each other? Explain your answer.\n- (g) Suppose we obtain one additional observation, which was unfortunately mismeasured. We use the function np.concatenate() to add this additional observation to each of x1 , x2 and y .  \n```\nx1 = np.concatenate([x1, [0.1]]) x2 = np.concatenate([x2, [0.8]]) y = np.concatenate([y, [6]])\n```  \nRe-fit the linear models from (c) to (e) using this new data. What effect does this new observation have on the each of the models? In each model, is this observation an outlier? A high-leverage point? Both? Explain your answers.  \n15. This problem involves the Boston data set, which we saw in the lab for this chapter. We will now try to predict per capita crime rate using the other variables in this data set. In other words, per capita crime rate is the response, and the other variables are the predictors.\n2. (a) For each predictor, fit a simple linear regression model to predict the response. Describe your results. In which of the models is there a statistically significant association between the predictor and the response? Create some plots to back up your assertions.\n3. (b) Fit a multiple regression model to predict the response using all of the predictors. Describe your results. For which predictors can we reject the null hypothesis H 0 : β j = 0 ?\n4. (c) How do your results from (a) compare to your results from (b)? Create a plot displaying the univariate regression coefficients from (a) on the x -axis, and the multiple regression coefficients from (b) on the y -axis. That is, each predictor is displayed as a single point in the plot. Its coefficient in a simple linear regression model is shown on the x -axis, and its coefficient estimate in the multiple linear regression model is shown on the y -axis.  \nnp.concatenate()  \n- 134 3. Linear Regression\n- (d) Is there evidence of non-linear association between any of the predictors and the response? To answer this question, for each predictor X , fit a model of the form  \n$$Y = \\beta _ { 0 } + \\beta _ { 1 } X + \\beta _ { 2 } X ^ { 2 } + \\beta _ { 3 } X ^ { 3 } + \\epsilon .$$']","The formula for the t-statistic used in hypothesis testing for linear regression coefficients is given by:

$$
t = \frac{\hat{\beta}_1 - 0}{\text{SE}(\hat{\beta}_1)}
$$

This formula measures the number of standard deviations that the estimated coefficient \(\hat{\beta}_1\) is away from 0.","The t-statistic is calculated as t = (β̂₁ - 0) / SE(β̂₁), which measures how many standard deviations the coefficient estimate β̂₁ is away from 0.",1.0,,0.999999999975,1.0
What is the mathematical definition of the R² statistic as provided in the book?,"[""### 3.1.3 Assessing the Accuracy of the Model  \nOnce we have rejected the null hypothesis (3.12) in favor of the alternative hypothesis (3.13), it is natural to want to quantify the extent to which the model fits the data . The quality of a linear regression fit is typically assessed using two related quantities: the residual standard error (RSE) and the R 2 statistic.  \n4 In Table 3.1, a small p -value for the intercept indicates that we can reject the null hypothesis that β 0 = 0 , and a small p -value for TV indicates that we can reject the null hypothesis that β 1 = 0 . Rejecting the latter null hypothesis allows us to conclude that there is a relationship between TV and sales . Rejecting the former allows us to conclude that in the absence of TV expenditure, sales are non-zero.  \np -value  \nR 2  \nTABLE 3.2. For the Advertising data, more information about the least squares model for the regression of number of units sold on TV advertising budget.  \n| Quantity                |   Value |\n|-------------------------|---------|\n| Residual standard error |   3.26  |\n| R 2                     |   0.612 |\n| F -statistic            | 312.1   |  \nTable 3.2 displays the RSE, the R 2 statistic, and the F -statistic (to be described in Section 3.2.2) for the linear regression of number of units sold on TV advertising budget.\nResidual Standard Error  \nRecall from the model (3.5) that associated with each observation is an error term /epsilon1 . Due to the presence of these error terms, even if we knew the true regression line (i.e. even if β 0 and β 1 were known), we would not be able to perfectly predict Y from X . The RSE is an estimate of the standard deviation of /epsilon1 . Roughly speaking, it is the average amount that the response will deviate from the true regression line. It is computed using the formula  \n$$\\text {deactivate from the true regression line. It is computed using the formula} \\\\ R S E = \\sqrt { \\frac { 1 } { n - 2 } } R S S = \\sqrt { \\frac { 1 } { n - 2 } } \\sum _ { i = 1 } ^ { n } ( y _ { i } - \\hat { y } _ { i } ) ^ { 2 } . \\\\ \\text {that RSS was defined in Section 3.1.1, and is given by the formula}$$  \nNote that RSS was defined in Section 3.1.1, and is given by the formula  \n$$R S S = \\sum _ { i = 1 } ^ { n } ( y _ { i } - \\hat { y } _ { i } ) ^ { 2 } . \\\\$$  \nIn the case of the advertising data, we see from the linear regression output in Table 3.2 that the RSE is 3   26 . In other words, actual sales in each market deviate from the true regression line by approximately 3  260 units, on average. Another way to think about this is that even if the model were correct and the true values of the unknown coefficients β 0 and β 1 were known exactly, any prediction of sales on the basis of TV advertising would still be off by about 3  260 units on average. Of course, whether or not 3  260 units is an acceptable prediction error depends on the problem context. In the advertising data set, the mean value of sales over all markets is approximately 14  000 units, and so the percentage error is 3  260 glyph[triangleleft] 14  000 = 23 %.  \nThe RSE is considered a measure of the lack of fit of the model (3.5) to the data. If the predictions obtained using the model are very close to the true outcome values-that is, if ˆ y i ≈ y i for i = 1         n -then (3.15) will be small, and we can conclude that the model fits the data very well. On the other hand, if ˆ y i is very far from y i for one or more observations, then the RSE may be quite large, indicating that the model doesn't fit the data well.\nR 2 Statistic  \nThe RSE provides an absolute measure of lack of fit of the model (3.5) to the data. But since it is measured in the units of Y , it is not always  \nclear what constitutes a good RSE. The R 2 statistic provides an alternative measure of fit. It takes the form of a proportion -the proportion of variance explained-and so it always takes on a value between 0 and 1 , and is independent of the scale of Y .  \nTo calculate R 2 , we use the formula  \n$$R ^ { 2 } = \\frac { \\text {TSSS} - \\text {RSS} } { \\text {TSSS} } = 1 - \\frac { \\text {RSS} } { \\text {TSSS} }$$  \nwhere TSS = ∑ ( y i -¯ y ) 2 is the total sum of squares , and RSS is defined in (3.16). TSS measures the total variance in the response Y , and can be thought of as the amount of variability inherent in the response before the regression is performed. In contrast, RSS measures the amount of variability that is left unexplained after performing the regression. Hence, TSS -RSS measures the amount of variability in the response that is explained (or removed) by performing the regression, and R 2 measures the proportion of variability in Y that can be explained using X . An R 2 statistic that is close to 1 indicates that a large proportion of the variability in the response is explained by the regression. A number near 0 indicates that the regression does not explain much of the variability in the response; this might occur because the linear model is wrong, or the error variance σ 2 is high, or both. In Table 3.2, the R 2 was 0   61 , and so just under two-thirds of the variability in sales is explained by a linear regression on TV .  \nThe R 2 statistic is a measure of the linear relationship between X and Y . Recall that correlation , defined as  \nThe R 2 statistic (3.17) has an interpretational advantage over the RSE (3.15), since unlike the RSE, it always lies between 0 and 1. However, it can still be challenging to determine what is a good R 2 value, and in general, this will depend on the application. For instance, in certain problems in physics, we may know that the data truly comes from a linear model with a small residual error. In this case, we would expect to see an R 2 value that is extremely close to 1 , and a substantially smaller R 2 value might indicate a serious problem with the experiment in which the data were generated. On the other hand, in typical applications in biology, psychology, marketing, and other domains, the linear model (3.5) is at best an extremely rough approximation to the data, and residual errors due to other unmeasured factors are often very large. In this setting, we would expect only a very small proportion of the variance in the response to be explained by the predictor, and an R 2 value well below 0   1 might be more realistic!  \n$$\\text {cor} ( X , Y ) = \\frac { \\sum _ { i = 1 } ^ { n } ( x _ { i } - \\bar { x } ) ( y _ { i } - \\bar { y } ) } { \\sqrt { \\sum _ { i = 1 } ^ { n } ( x _ { i } - \\bar { x } ) ^ { 2 } } \\sqrt { \\sum _ { i = 1 } ^ { n } ( y _ { i } - \\bar { y } ) ^ { 2 } } } , \\\\ \\text {also a measure of the linear relationship between } X \\text { and } Y . 5 \\text { This sur-}$$  \nis also a measure of the linear relationship between X and Y . 5 This suggests that we might be able to use r = Cor ( XY ) instead of R 2 in order to assess the fit of the linear model. In fact, it can be shown that in the simple  \n5 We note that in fact, the right-hand side of (3.18) is the sample correlation; thus, it would be more correct to write ̂ Cor ( XY ) ; however, we omit the 'hat' for ease of notation.  \ntotal sum of squares correlation  \nSimple regression of sales on radio  \n|           |   Coefficient |   Std. error |   t -statistic | p -value                      |\n|-----------|---------------|--------------|----------------|-------------------------------|\n| Intercept |         9.312 |        0.563 |          16.54 | < 0   0001 |\n| radio     |         0.203 |        0.02  |           9.92 | < 0   0001 |\nSimple regression of sales on newspaper  \nTABLE 3.3. More simple linear regression models for the Advertising data. Coefficients of the simple linear regression model for number of units sold on Top: radio advertising budget and Bottom: newspaper advertising budget. A $ 1  000 increase in spending on radio advertising is associated with an average increase in sales by around 203 units, while the same increase in spending on newspaper advertising is associated with an average increase in sales by around 55 units. (Note that the sales variable is in thousands of units, and the radio and newspaper variables are in thousands of dollars.)  \n|           |   Coefficient |   Std. error |   t -statistic | p -value                      |\n|-----------|---------------|--------------|----------------|-------------------------------|\n| Intercept |        12.351 |        0.621 |          19.88 | < 0   0001 |\n| newspaper |         0.055 |        0.017 |           3.3  | 0   00115  |  \nlinear regression setting, R 2 = r 2 . In other words, the squared correlation and the R 2 statistic are identical. However, in the next section we will discuss the multiple linear regression problem, in which we use several predictors simultaneously to predict the response. The concept of correlation between the predictors and the response does not extend automatically to this setting, since correlation quantifies the association between a single pair of variables rather than between a larger number of variables. We will see that R 2 fills this role."", ""### 6.1.3 Choosing the Optimal Model  \nBest subset selection, forward selection, and backward selection result in the creation of a set of models, each of which contains a subset of the p  \n3 Like forward stepwise selection, backward stepwise selection performs a guided search over model space, and so effectively considers substantially more than 1 + p ( p + 1) glyph[triangleleft] 2 models.  \npredictors. To apply these methods, we need a way to determine which of these models is best . As we discussed in Section 6.1.1, the model containing all of the predictors will always have the smallest RSS and the largest R 2 , since these quantities are related to the training error. Instead, we wish to choose a model with a low test error. As is evident here, and as we show in Chapter 2, the training error can be a poor estimate of the test error. Therefore, RSS and R 2 are not suitable for selecting the best model among a collection of models with different numbers of predictors.  \nIn order to select the best model with respect to test error, we need to estimate this test error. There are two common approaches:  \n1. We can indirectly estimate test error by making an adjustment to the training error to account for the bias due to overfitting.\n2. We can directly estimate the test error, using either a validation set approach or a cross-validation approach, as discussed in Chapter 5.  \nWe consider both of these approaches below.\nC p , AIC, BIC, and Adjusted R 2  \nWe show in Chapter 2 that the training set MSE is generally an underestimate of the test MSE. (Recall that MSE = RSS glyph[triangleleft]n .) This is because when we fit a model to the training data using least squares, we specifically estimate the regression coefficients such that the training RSS (but not the test RSS) is as small as possible. In particular, the training error will decrease as more variables are included in the model, but the test error may not. Therefore, training set RSS and training set R 2 cannot be used to select from among a set of models with different numbers of variables.  \nFor a fitted least squares model containing d predictors, the C p estimate of test MSE is computed using the equation  \nHowever, a number of techniques for adjusting the training error for the model size are available. These approaches can be used to select among a set of models with different numbers of variables. We now consider four such approaches: C p , Akaike information criterion (AIC), Bayesian information criterion (BIC), and adjusted R 2 . Figure 6.2 displays C p , BIC, and adjusted R 2 for the best model of each size produced by best subset selection on the Credit data set.  \n$$C _ { p } = \\frac { 1 } { n } \\left ( R S S + 2 d \\hat { \\sigma } ^ { 2 } \\right ) , \\\\ \\intertext { t i m a t e \\, o f \\, the \\, v a r i c e \\, o f \\, the \\, e r r $ \\epsilon $ \\, associated \\, with \\, each }$$  \nwhere ˆ σ 2 is an estimate of the variance of the error /epsilon1 associated with each response measurement in (6.1). 4 Typically ˆ σ 2 is estimated using the full model containing all predictors. Essentially, the C p statistic adds a penalty of 2 d ˆ σ 2 to the training RSS in order to adjust for the fact that the training error tends to underestimate the test error. Clearly, the penalty increases as the number of predictors in the model increases; this is intended to adjust  \n4 Mallow's C p is sometimes defined as C ′ p = RSS glyph[triangleleft] ˆ σ 2 +2 d -n . This is equivalent to the definition given above in the sense that C p = 1 n ˆ σ 2 ( C ′ p + n ) , and so the model with smallest C p also has smallest C ′ p .  \n<!-- image -->  \nC p Akaike information criterion Bayesian information criterion adjusted R 2  \nFIGURE 6.2. C p , BIC, and adjusted R 2 are shown for the best models of each size for the Credit data set (the lower frontier in Figure 6.1). C p and BIC are estimates of test MSE. In the middle plot we see that the BIC estimate of test error shows an increase after four variables are selected. The other two plots are rather flat after four variables are included.  \n<!-- image -->  \nfor the corresponding decrease in training RSS. Though it is beyond the scope of this book, one can show that if ˆ σ 2 is an unbiased estimate of σ 2 in (6.2), then C p is an unbiased estimate of test MSE. As a consequence, the C p statistic tends to take on a small value for models with a low test error, so when determining which of a set of models is best, we choose the model with the lowest C p value. In Figure 6.2, C p selects the six-variable model containing the predictors income , limit , rating , cards , age and student .  \nThe AIC criterion is defined for a large class of models fit by maximum likelihood. In the case of the model (6.1) with Gaussian errors, maximum likelihood and least squares are the same thing. In this case AIC is given by  \n$$A I C = \\frac { 1 } { n } \\left ( R S S + 2 d \\hat { \\sigma } ^ { 2 } \\right ) , \\\\$$  \nBIC is derived from a Bayesian point of view, but ends up looking similar to C p (and AIC) as well. For the least squares model with d predictors, the BIC is, up to irrelevant constants, given by where, for simplicity, we have omitted irrelevant constants. 5 Hence for least squares models, C p and AIC are proportional to each other, and so only C p is displayed in Figure 6.2.  \n$$B I C = \\frac { 1 } { n } \\left ( R S S + \\log ( n ) d \\hat { \\sigma } ^ { 2 } \\right ) . \\\\ \\intertext { B I C } R I C \\text { will } \\text { end } \\text { to take } \\text { on } a \\text { small value for } a \\text { mode } \\text { with } a$$  \nLike C p , the BIC will tend to take on a small value for a model with a low test error, and so generally we select the model that has the lowest BIC value. Notice that BIC replaces the 2 d ˆ σ 2 used by C p with a log( n ) d ˆ σ 2 term, where n is the number of observations. Since log n &gt; 2 for any n &gt; 7 ,  \n5 There are two formulas for AIC for least squares regression. The formula that we provide here requires an expression for σ 2 , which we obtain using the full model containing all predictors. The second formula is appropriate when σ 2 is unknown and we do not want to explicitly estimate it; that formula has a log( RSS ) term instead of an RSS term. Detailed derivations of these two formulas are outside of the scope of this book.  \nthe BIC statistic generally places a heavier penalty on models with many variables, and hence results in the selection of smaller models than C p . In Figure 6.2, we see that this is indeed the case for the Credit data set; BIC chooses a model that contains only the four predictors income , limit , cards , and student . In this case the curves are very flat and so there does not appear to be much difference in accuracy between the four-variable and six-variable models.  \nThe adjusted R 2 statistic is another popular approach for selecting among a set of models that contain different numbers of variables. Recall from Chapter 3 that the usual R 2 is defined as 1 -RSS glyph[triangleleft] TSS, where TSS = ∑ ( y i -y ) 2 is the total sum of squares for the response. Since RSS always decreases as more variables are added to the model, the R 2 always increases as more variables are added. For a least squares model with d variables, the adjusted R 2 statistic is calculated as  \n$$\\text {Adjusted} \\, R ^ { 2 } = 1 - \\frac { \\text {RSS} / ( n - d - 1 ) } { \\text {TSS} / ( n - 1 ) } .$$  \nUnlike C p , AIC, and BIC, for which a small value indicates a model with a low test error, a large value of adjusted R 2 indicates a model with a small test error. Maximizing the adjusted R 2 is equivalent to minimizing RSS n -d -1 . While RSS always decreases as the number of variables in the model increases, RSS n -d -1 may increase or decrease, due to the presence of d in the denominator.  \nC p , AIC, and BIC all have rigorous theoretical justifications that are beyond the scope of this book. These justifications rely on asymptotic arguments (scenarios where the sample size n is very large). Despite its popularity, and even though it is quite intuitive, the adjusted R 2 is not as well motivated in statistical theory as AIC, BIC, and C p . All of these measures are simple to use and compute. Here we have presented their formulas in the case of a linear model fit using least squares; however, AIC and BIC can also be defined for more general types of models.  \nThe intuition behind the adjusted R 2 is that once all of the correct variables have been included in the model, adding additional noise variables will lead to only a very small decrease in RSS. Since adding noise variables leads to an increase in d , such variables will lead to an increase in RSS n -d -1 , and consequently a decrease in the adjusted R 2 . Therefore, in theory, the model with the largest adjusted R 2 will have only correct variables and no noise variables. Unlike the R 2 statistic, the adjusted R 2 statistic pays a price for the inclusion of unnecessary variables in the model. Figure 6.2 displays the adjusted R 2 for the Credit data set. Using this statistic results in the selection of a model that contains seven variables, adding own to the model selected by C p and AIC.\nValidation and Cross-Validation  \nAs an alternative to the approaches just discussed, we can directly estimate the test error using the validation set and cross-validation methods discussed in Chapter 5. We can compute the validation set error or the cross-validation error for each model under consideration, and then select  \nFIGURE 6.3. For the Credit data set, three quantities are displayed for the best model containing d predictors, for d ranging from 1 to 11 . The overall best model, based on each of these quantities, is shown as a blue cross. Left: Square root of BIC. Center: Validation set errors. Right: Cross-validation errors.  \n<!-- image -->  \nthe model for which the resulting estimated test error is smallest. This procedure has an advantage relative to AIC, BIC, C p , and adjusted R 2 , in that it provides a direct estimate of the test error, and makes fewer assumptions about the true underlying model. It can also be used in a wider range of model selection tasks, even in cases where it is hard to pinpoint the model degrees of freedom (e.g. the number of predictors in the model) or hard to estimate the error variance σ 2 . Note that when cross-validation is used, the sequence of models M k in Algorithms 6.1-6.3 is determined separately for each training fold, and the validation errors are averaged over all folds for each model size k . This means, for example with best-subset regression, that M k , the best subset of size k , can differ across the folds. Once the best size k is chosen, we find the best model of that size on the full data set.  \nFigure 6.3 displays, as a function of d , the BIC, validation set errors, and cross-validation errors on the Credit data, for the best d -variable model. The validation errors were calculated by randomly selecting three-quarters of the observations as the training set, and the remainder as the validation set. The cross-validation errors were computed using k = 10 folds. In this case, the validation and cross-validation methods both result in a six-variable model. However, all three approaches suggest that the four-, five-, and six-variable models are roughly equivalent in terms of their test errors.  \nIn the past, performing cross-validation was computationally prohibitive for many problems with large p and/or large n , and so AIC, BIC, C p , and adjusted R 2 were more attractive approaches for choosing among a set of models. However, nowadays with fast computers, the computations required to perform cross-validation are hardly ever an issue. Thus, crossvalidation is a very attractive approach for selecting from among a number of models under consideration.  \nIn fact, the estimated test error curves displayed in the center and righthand panels of Figure 6.3 are quite flat. While a three-variable model clearly has lower estimated test error than a two-variable model, the estimated test errors of the 3- to 11-variable models are quite similar. Furthermore, if we  \nrepeated the validation set approach using a different split of the data into a training set and a validation set, or if we repeated cross-validation using a different set of cross-validation folds, then the precise model with the lowest estimated test error would surely change. In this setting, we can select a model using the one-standard-error rule . We first calculate the standard error of the estimated test MSE for each model size, and then select the smallest model for which the estimated test error is within one standard error of the lowest point on the curve. The rationale here is that if a set of models appear to be more or less equally good, then we might as well choose the simplest model-that is, the model with the smallest number of predictors. In this case, applying the one-standard-error rule to the validation set or cross-validation approach leads to selection of the three-variable model."", ""### 3.2.2 Some Important Questions  \nWhen we perform multiple linear regression, we usually are interested in answering a few important questions.  \n1. Is at least one of the predictors X 1  X 2         X p useful in predicting the response?\n2. Do all the predictors help to explain Y , or is only a subset of the predictors useful?\n3. How well does the model fit the data?\n4. Given a set of predictor values, what response value should we predict, and how accurate is our prediction?  \nWe now address each of these questions in turn.\nOne: Is There a Relationship Between the Response and Predictors?  \nRecall that in the simple linear regression setting, in order to determine whether there is a relationship between the response and the predictor we can simply check whether β 1 = 0 . In the multiple regression setting with p predictors, we need to ask whether all of the regression coefficients are zero, i.e. whether β 1 = β 2 = · · · = β p = 0 . As in the simple linear regression setting, we use a hypothesis test to answer this question. We test the null hypothesis, versus the alternative  \n$$H _ { 0 } \\colon \\beta _ { 1 } = \\beta _ { 2 } = \\dots = \\beta _ { p } = 0$$  \n$$H _ { a } \\colon a t l e a s t o n e \\beta _ { j } \\text { is non-zero.}$$  \nThis hypothesis test is performed by computing the F -statistic ,  \n$$F = \\frac { ( T S S - R S S ) / p } { R S S / ( n - p - 1 ) } , \\\\$$  \n$$E \\{ R S S / ( n - p - 1 ) \\} = \\sigma ^ { 2 }$$  \nwhere, as with simple linear regression, TSS = ∑ ( y i -¯ y ) 2 and RSS = ∑ ( y i -ˆ y i ) 2 . If the linear model assumptions are correct, one can show that and that, provided H 0 is true,  \n$$E \\{ ( T S S - R S S ) / p \\} = \\sigma ^ { 2 } .$$  \nHence, when there is no relationship between the response and predictors, one would expect the F -statistic to take on a value close to 1 . On the other hand, if H a is true, then E ¶ (TSS -RSS) glyph[triangleleft]p ♦ &gt; σ 2 , so we expect F to be greater than 1 .  \nThe F -statistic for the multiple linear regression model obtained by regressing sales onto radio , TV , and newspaper is shown in Table 3.6. In this example the F -statistic is 570 . Since this is far larger than 1 , it provides compelling evidence against the null hypothesis H 0 . In other words, the large F -statistic suggests that at least one of the advertising media must be related to sales . However, what if the F -statistic had been closer to 1 ? How large does the F -statistic need to be before we can reject H 0 and  \nF  \n-statistic  \nTABLE 3.6. More information about the least squares model for the regression of number of units sold on TV, newspaper, and radio advertising budgets in the Advertising data. Other information about this model was displayed in Table 3.4.  \n| Quantity                |   Value |\n|-------------------------|---------|\n| Residual standard error |   1.69  |\n| R 2                     |   0.897 |\n| F -statistic            | 570     |  \nconclude that there is a relationship? It turns out that the answer depends on the values of n and p . When n is large, an F -statistic that is just a little larger than 1 might still provide evidence against H 0 . In contrast, a larger F -statistic is needed to reject H 0 if n is small. When H 0 is true and the errors /epsilon1 i have a normal distribution, the F -statistic follows an F -distribution. 6 For any given value of n and p , any statistical software package can be used to compute the p -value associated with the F -statistic using this distribution. Based on this p -value, we can determine whether or not to reject H 0 . For the advertising data, the p -value associated with the F -statistic in Table 3.6 is essentially zero, so we have extremely strong evidence that at least one of the media is associated with increased sales . In (3.23) we are testing H 0 that all the coefficients are zero. Sometimes we want to test that a particular subset of q of the coefficients are zero.  \nThis corresponds to a null hypothesis  \n$$H _ { 0 } \\colon \\ \\beta _ { p - q + 1 } = \\beta _ { p - q + 2 } = \\dots = \\beta _ { p } = 0 ,$$  \nwhere for convenience we have put the variables chosen for omission at the end of the list. In this case we fit a second model that uses all the variables except those last q . Suppose that the residual sum of squares for that model is RSS 0 . Then the appropriate F -statistic is  \n$$F = \\frac { ( R S S _ { 0 } - R S S ) / q } { R S S / ( n - p - 1 ) } . \\\\$$  \nGiven these individual p -values for each variable, why do we need to look at the overall F -statistic? After all, it seems likely that if any one of the p -values for the individual variables is very small, then at least one of the predictors is related to the response . However, this logic is flawed, especially when the number of predictors p is large.  \nNotice that in Table 3.4, for each individual predictor a t -statistic and a p -value were reported. These provide information about whether each individual predictor is related to the response, after adjusting for the other predictors. It turns out that each of these is exactly equivalent 7 to the F -test that omits that single variable from the model, leaving all the others in-i.e. q =1 in (3.24). So it reports the partial effect of adding that variable to the model. For instance, as we discussed earlier, these p -values indicate that TV and radio are related to sales , but that there is no evidence that newspaper is associated with sales , when TV and radio are held fixed.  \n6 Even if the errors are not normally-distributed, the F -statistic approximately follows an F -distribution provided that the sample size n is large.  \n7 The square of each t -statistic is the corresponding F -statistic.  \nFor instance, consider an example in which p = 100 and H 0 : β 1 = β 2 = · · · = β p = 0 is true, so no variable is truly associated with the response. In this situation, about 5 % of the p -values associated with each variable (of the type shown in Table 3.4) will be below 0   05 by chance. In other words, we expect to see approximately five small p -values even in the absence of any true association between the predictors and the response. 8 In fact, it is likely that we will observe at least one p -value below 0   05 by chance! Hence, if we use the individual t -statistics and associated p -values in order to decide whether or not there is any association between the variables and the response, there is a very high chance that we will incorrectly conclude that there is a relationship. However, the F -statistic does not suffer from this problem because it adjusts for the number of predictors. Hence, if H 0 is true, there is only a 5 % chance that the F -statistic will result in a p -value below 0   05 , regardless of the number of predictors or the number of observations.  \nThe approach of using an F -statistic to test for any association between the predictors and the response works when p is relatively small, and certainly small compared to n . However, sometimes we have a very large number of variables. If p &gt; n then there are more coefficients β j to estimate than observations from which to estimate them. In this case we cannot even fit the multiple linear regression model using least squares, so the F -statistic cannot be used, and neither can most of the other concepts that we have seen so far in this chapter. When p is large, some of the approaches discussed in the next section, such as forward selection , can be used. This high-dimensional setting is discussed in greater detail in Chapter 6.\nTwo: Deciding on Important Variables  \nAs discussed in the previous section, the first step in a multiple regression analysis is to compute the F -statistic and to examine the associated p -value. If we conclude on the basis of that p -value that at least one of the predictors is related to the response, then it is natural to wonder which are the guilty ones! We could look at the individual p -values as in Table 3.4, but as discussed (and as further explored in Chapter 13), if p is large we are likely to make some false discoveries.  \nIdeally, we would like to perform variable selection by trying out a lot of different models, each containing a different subset of the predictors. For instance, if p = 2 , then we can consider four models: (1) a model containing no variables, (2) a model containing X 1 only, (3) a model containing  \nIt is possible that all of the predictors are associated with the response, but it is more often the case that the response is only associated with a subset of the predictors. The task of determining which predictors are associated with the response, in order to fit a single model involving only those predictors, is referred to as variable selection . The variable selection problem is studied extensively in Chapter 6, and so here we will provide only a brief outline of some classical approaches.  \n8 This is related to the important concept of multiple testing , which is the focus of Chapter 13.  \nhighdimensional variable selection  \nX 2 only, and (4) a model containing both X 1 and X 2 . We can then select the best model out of all of the models that we have considered. How do we determine which model is best? Various statistics can be used to judge the quality of a model. These include Mallow's C p , Akaike information criterion (AIC), Bayesian information criterion (BIC), and adjusted R 2 . These are discussed in more detail in Chapter 6. We can also determine which model is best by plotting various model outputs, such as the residuals, in order to search for patterns.  \nUnfortunately, there are a total of 2 p models that contain subsets of p variables. This means that even for moderate p , trying out every possible subset of the predictors is infeasible. For instance, we saw that if p = 2 , then there are 2 2 = 4 models to consider. But if p = 30 , then we must consider 2 30 = 1  073  741  824 models! This is not practical. Therefore, unless p is very small, we cannot consider all 2 p models, and instead we need an automated and efficient approach to choose a smaller set of models to consider. There are three classical approaches for this task:  \n- Forward selection . We begin with the null model -a model that contains an intercept but no predictors. We then fit p simple linear regressions and add to the null model the variable that results in the lowest RSS. We then add to that model the variable that results in the lowest RSS for the new two-variable model. This approach is continued until some stopping rule is satisfied.\n- Backward selection . We start with all variables in the model, and remove the variable with the largest p -value-that is, the variable that is the least statistically significant. The new ( p -1) -variable model is fit, and the variable with the largest p -value is removed. This procedure continues until a stopping rule is reached. For instance, we may stop when all remaining variables have a p -value below some threshold.\n- Mixed selection . This is a combination of forward and backward selection. We start with no variables in the model, and as with forward selection, we add the variable that provides the best fit. We continue to add variables one-by-one. Of course, as we noted with the Advertising example, the p -values for variables can become larger as new predictors are added to the model. Hence, if at any point the p -value for one of the variables in the model rises above a certain threshold, then we remove that variable from the model. We continue to perform these forward and backward steps until all variables in the model have a sufficiently low p -value, and all variables outside the model would have a large p -value if added to the model.  \nBackward selection cannot be used if p &gt; n , while forward selection can always be used. Forward selection is a greedy approach, and might include variables early that later become redundant. Mixed selection can remedy this.  \nMallow's C p Akaike information criterion Bayesian information criterion adjusted R 2  \nforward selection null model backward selection  \nmixed selection\nThree: Model Fit  \nTwo of the most common numerical measures of model fit are the RSE and R 2 , the fraction of variance explained. These quantities are computed and interpreted in the same fashion as for simple linear regression.  \nAn R 2 value close to 1 indicates that the model explains a large portion of the variance in the response variable. As an example, we saw in Table 3.6 that for the Advertising data, the model that uses all three advertising media to predict sales has an R 2 of 0   8972 . On the other hand, the model that uses only TV and radio to predict sales has an R 2 value of 0   89719 . In other words, there is a small increase in R 2 if we include newspaper advertising in the model that already contains TV and radio advertising, even though we saw earlier that the p -value for newspaper advertising in Table 3.4 is not significant. It turns out that R 2 will always increase when more variables are added to the model, even if those variables are only weakly associated with the response. This is due to the fact that adding another variable always results in a decrease in the residual sum of squares on the training data (though not necessarily the testing data). Thus, the R 2 statistic, which is also computed on the training data, must increase. The fact that adding newspaper advertising to the model containing only TV and radio advertising leads to just a tiny increase in R 2 provides additional evidence that newspaper can be dropped from the model. Essentially, newspaper provides no real improvement in the model fit to the training samples, and its inclusion will likely lead to poor results on independent test samples due to overfitting.  \nRecall that in simple regression, R 2 is the square of the correlation of the response and the variable. In multiple linear regression, it turns out that it equals Cor ( Y ˆ Y ) 2 , the square of the correlation between the response and the fitted linear model; in fact one property of the fitted linear model is that it maximizes this correlation among all possible linear models.  \nBy contrast, the model containing only TV as a predictor had an R 2 of 0   61 (Table 3.2). Adding radio to the model leads to a substantial improvement in R 2 . This implies that a model that uses TV and radio expenditures to predict sales is substantially better than one that uses only TV advertising. We could further quantify this improvement by looking at the p -value for the radio coefficient in a model that contains only TV and radio as predictors.  \n$$R S = \\sqrt { \\frac { 1 } { n - p - 1 } R S S } ,$$  \nThe model that contains only TV and radio as predictors has an RSE of 1.681, and the model that also contains newspaper as a predictor has an RSE of 1.686 (Table 3.6). In contrast, the model that contains only TV has an RSE of 3   26 (Table 3.2). This corroborates our previous conclusion that a model that uses TV and radio expenditures to predict sales is much more accurate (on the training data) than one that only uses TV spending. Furthermore, given that TV and radio expenditures are used as predictors, there is no point in also using newspaper spending as a predictor in the model. The observant reader may wonder how RSE can increase when newspaper is added to the model given that RSS must decrease. In general RSE is defined as  \nFIGURE 3.5. For the Advertising data, a linear regression fit to sales using TV and radio as predictors. From the pattern of the residuals, we can see that there is a pronounced non-linear relationship in the data. The positive residuals (those visible above the surface), tend to lie along the 45-degree line, where TV and Radio budgets are split evenly. The negative residuals (most not visible), tend to lie away from this line, where budgets are more lopsided.  \n<!-- image -->  \nwhich simplifies to (3.15) for a simple linear regression. Thus, models with more variables can have higher RSE if the decrease in RSS is small relative to the increase in p .  \nIn addition to looking at the RSE and R 2 statistics just discussed, it can be useful to plot the data. Graphical summaries can reveal problems with a model that are not visible from numerical statistics. For example, Figure 3.5 displays a three-dimensional plot of TV and radio versus sales . We see that some observations lie above and some observations lie below the least squares regression plane. In particular, the linear model seems to overestimate sales for instances in which most of the advertising money was spent exclusively on either TV or radio . It underestimates sales for instances where the budget was split between the two media. This pronounced non-linear pattern suggests a synergy or interaction effect between the advertising media, whereby combining the media together results in a bigger boost to sales than using any single medium. In Section 3.3.2, we will discuss extending the linear model to accommodate such synergistic effects through the use of interaction terms.\nFour: Predictions  \nOnce we have fit the multiple regression model, it is straightforward to apply (3.21) in order to predict the response Y on the basis of a set of values for the predictors X 1  X 2         X p . However, there are three sorts of uncertainty associated with this prediction.  \n1. The coefficient estimates ˆ β 0  ˆ β 1         ˆ β p are estimates for β 0  β 1         β p . That is, the least squares plane  \n$$\\hat { Y } = \\hat { \\beta } _ { 0 } + \\hat { \\beta } _ { 1 } X _ { 1 } + \\dots + \\hat { \\beta } _ { p } X _ { p }$$  \ninteraction  \nis only an estimate for the true population regression plane  \n$$f ( X ) = \\beta _ { 0 } + \\beta _ { 1 } X _ { 1 } + \\cdots + \\beta _ { p } X _ { p } .$$  \nThe inaccuracy in the coefficient estimates is related to the reducible error from Chapter 2. We can compute a confidence interval in order to determine how close ˆ Y will be to f ( X ) .  \n2. Of course, in practice assuming a linear model for f ( X ) is almost always an approximation of reality, so there is an additional source of potentially reducible error which we call model bias . So when we use a linear model, we are in fact estimating the best linear approximation to the true surface. However, here we will ignore this discrepancy, and operate as if the linear model were correct.\n3. Even if we knew f ( X ) -that is, even if we knew the true values for β 0  β 1         β p -the response value cannot be predicted perfectly because of the random error /epsilon1 in the model (3.20). In Chapter 2, we referred to this as the irreducible error . How much will Y vary from ˆ Y ? We use prediction intervals to answer this question. Prediction intervals are always wider than confidence intervals, because they incorporate both the error in the estimate for f ( X ) (the reducible error) and the uncertainty as to how much an individual point will differ from the population regression plane (the irreducible error).  \nWe use a confidence interval to quantify the uncertainty surrounding the average sales over a large number of cities. For example, given that $100  000 is spent on TV advertising and $20  000 is spent on radio advertising in each city, the 95 % confidence interval is [10  985  11  528] . We interpret this to mean that 95 % of intervals of this form will contain the true value of f ( X ) . 9 On the other hand, a prediction interval can be used to quantify the uncertainty surrounding sales for a particular city. Given that $100  000 is spent on TV advertising and $20  000 is spent on radio advertising in that city the 95 % prediction interval is [7  930  14  580] . We interpret this to mean that 95 % of intervals of this form will contain the true value of Y for this city. Note that both intervals are centered at 11  256 , but that the prediction interval is substantially wider than the confidence interval, reflecting the increased uncertainty about sales for a given city in comparison to the average sales over many locations.  \n9 In other words, if we collect a large number of data sets like the Advertising data set, and we construct a confidence interval for the average sales on the basis of each data set (given $100  000 in TV and $20  000 in radio advertising), then 95 % of these confidence intervals will contain the true value of average sales .  \nconfidence interval prediction interval""]","The R² statistic is defined mathematically as:

\[ R^2 = \frac{\text{TSS} - \text{RSS}}{\text{TSS}} = 1 - \frac{\text{RSS}}{\text{TSS}} \]

where:
- TSS (Total Sum of Squares) is given by \(\sum (y_i - \bar{y})^2\), which measures the total variance in the response \(Y\).
- RSS (Residual Sum of Squares) is given by \(\sum (y_i - \hat{y}_i)^2\), which measures the amount of variability that is left unexplained after performing the regression.

The R² statistic measures the proportion of variability in \(Y\) that can be explained using \(X\). It always takes on a value between 0 and 1.","The R² statistic is defined as R² = (TSS - RSS) / TSS, which is equal to 1 - (RSS / TSS), where TSS is the total sum of squares and RSS is the residual sum of squares.",1.0,,0.9999999999666667,1.0
What is the formula for the logit transformation in logistic regression?,"[""### 4.6.1 Linear Regression on the Bikeshare Data  \nTo begin, we consider predicting bikers using linear regression. The results are shown in Table 4.10.  \nWe see, for example, that a progression of weather from clear to cloudy results in, on average, 12   89 fewer bikers per hour; however, if the weather progresses further to rain or snow, then this further results in 53   60 fewer bikers per hour. Figure 4.13 displays the coefficients associated with mnth counts  \nFIGURE 4.13. A least squares linear regression model was fit to predict bikers in the Bikeshare data set. Left: The coefficients associated with the month of the year. Bike usage is highest in the spring and fall, and lowest in the winter. Right: The coefficients associated with the hour of the day. Bike usage is highest during peak commute times, and lowest overnight.  \n<!-- image -->  \nand the coefficients associated with hr . We see that bike usage is highest in the spring and fall, and lowest during the winter months. Furthermore, bike usage is greatest around rush hour (9 AM and 6 PM), and lowest overnight. Thus, at first glance, fitting a linear regression model to the Bikeshare data set seems to provide reasonable and intuitive results.  \nFurthermore, it is reasonable to suspect that when the expected value of bikers is small, the variance of bikers should be small as well. For instance, at 2 AM during a heavy December snow storm, we expect that extremely few people will use a bike, and moreover that there should be little variance associated with the number of users during those conditions. This is borne out in the data: between 1 AM and 4 AM, in December, January, and February, when it is raining, there are 5   05 users, on average, with a standard deviation of 3   73 . By contrast, between 7 AM and 10 AM, in April, May, and June, when skies are clear, there are 243   59 users, on average, with a standard deviation of 131   7 . The mean-variance relationship is displayed in the left-hand panel of Figure 4.14. This is a major violation of the assumptions of a linear model, which state that Y = ∑ p j =1 X j β j + /epsilon1 , where /epsilon1 is a mean-zero error term with variance σ 2 that is constant , and not a function of the covariates. Therefore, the heteroscedasticity of the data calls into question the suitability of a linear regression model.  \nBut upon more careful inspection, some issues become apparent. For example, 9.6% of the fitted values in the Bikeshare data set are negative: that is, the linear regression model predicts a negative number of users during 9.6% of the hours in the data set. This calls into question our ability to perform meaningful predictions on the data, and it also raises concerns about the accuracy of the coefficient estimates, confidence intervals, and other outputs of the regression model.  \nFinally, the response bikers is integer-valued. But under a linear model, Y = β 0 + ∑ p j =1 X j β j + /epsilon1 , where /epsilon1 is a continuous-valued error term. This means that in a linear model, the response Y is necessarily continuousvalued (quantitative). Thus, the integer nature of the response bikers suggests that a linear regression model is not entirely satisfactory for this data set.  \nFIGURE 4.14. Left: On the Bikeshare dataset, the number of bikers is displayed on the y -axis, and the hour of the day is displayed on the x -axis. Jitter was applied for ease of visualization. For the most part, as the mean number of bikers increases, so does the variance in the number of bikers. A smoothing spline fit is shown in green. Right: The log of the number of bikers is now displayed on the y -axis.  \n<!-- image -->  \nSome of the problems that arise when fitting a linear regression model to the Bikeshare data can be overcome by transforming the response; for instance, we can fit the model  \n$$\\log ( Y ) = \\sum _ { j = 1 } ^ { p } X _ { j } \\beta _ { j } + \\epsilon . \\\\$$  \nTransforming the response avoids the possibility of negative predictions, and it overcomes much of the heteroscedasticity in the untransformed data, as is shown in the right-hand panel of Figure 4.14. However, it is not quite a satisfactory solution, since predictions and inference are made in terms of the log of the response, rather than the response. This leads to challenges in interpretation, e.g. 'a one-unit increase in X j is associated with an increase in the mean of the log of Y by an amount β j ' . Furthermore, a log transformation of the response cannot be applied in settings where the response can take on a value of 0 . Thus, while fitting a linear model to a transformation of the response may be an adequate approach for some count-valued data sets, it often leaves something to be desired. We will see in the next section that a Poisson regression model provides a much more natural and elegant approach for this task."", '### 4.3.1 The Logistic Model  \nHow should we model the relationship between p ( X ) = Pr( Y = 1 ♣ X ) and X ? (For convenience we are using the generic 0/1 coding for the response.) In Section 4.2 we considered using a linear regression model to represent these probabilities:  \n$$p ( X ) = \\beta _ { 0 } + \\beta _ { 1 } X .$$  \nIf we use this approach to predict default = Yes using balance , then we obtain the model shown in the left-hand panel of Figure 4.2. Here we see the problem with this approach: for balances close to zero we predict a negative probability of default; if we were to predict for very large balances, we would get values bigger than 1. These predictions are not sensible, since of course the true probability of default, regardless of credit card balance, must fall between 0 and 1 . This problem is not unique to the credit default data. Any time a straight line is fit to a binary response that is coded as 0 or 1 , in principle we can always predict p ( X ) &lt; 0 for some values of X and p ( X ) &gt; 1 for others (unless the range of X is limited).  \nTo avoid this problem, we must model p ( X ) using a function that gives outputs between 0 and 1 for all values of X . Many functions meet this description. In logistic regression, we use the logistic function ,  \n$$p ( X ) = \\frac { e ^ { \\beta _ { 0 } + \\beta _ { 1 } X } } { 1 + e ^ { \\beta _ { 0 } + \\beta _ { 1 } X } } .$$  \nTo fit the model (4.2), we use a method called maximum likelihood , which we discuss in the next section. The right-hand panel of Figure 4.2 illustrates the fit of the logistic regression model to the Default data. Notice that for logistic function  \nmaximum likelihood  \nlow balances we now predict the probability of default as close to, but never below, zero. Likewise, for high balances we predict a default probability close to, but never above, one. The logistic function will always produce an S-shaped curve of this form, and so regardless of the value of X , we will obtain a sensible prediction. We also see that the logistic model is better able to capture the range of probabilities than is the linear regression model in the left-hand plot. The average fitted probability in both cases is 0.0333 (averaged over the training data), which is the same as the overall proportion of defaulters in the data set.  \nAfter a bit of manipulation of (4.2), we find that  \n$$\\frac { p ( X ) } { 1 - p ( X ) } = e ^ { \\beta _ { 0 } + \\beta _ { 1 } X } . \\\\ [ / ] 1 = p ( X ) ] \\text { is called the } \\sigma d d _ { s } \\text { and } \\text {can take on an } \\alpha \\text { value}$$  \nBy taking the logarithm of both sides of (4.3), we arrive at  \nThe quantity p ( X ) glyph[triangleleft] [1 -p ( X )] is called the odds , and can take on any value between 0 and ∞ . Values of the odds close to 0 and ∞ indicate very low and very high probabilities of default, respectively. For example, on average 1 in 5 people with an odds of 1 glyph[triangleleft] 4 will default, since p ( X ) = 0   2 implies an odds of 0   2 1 -0   2 = 1 glyph[triangleleft] 4 . Likewise, on average nine out of every ten people with an odds of 9 will default, since p ( X ) = 0   9 implies an odds of 0   9 1 -0   9 = 9 . Odds are traditionally used instead of probabilities in horse-racing, since they relate more naturally to the correct betting strategy.  \n$$\\log \\left ( \\frac { \\ p ( X ) } { 1 - p ( X ) } \\right ) & = \\beta _ { 0 } + \\beta _ { 1 } X . \\\\ \\text {side is called the } \\log \\alpha \\, \\underset { \\ } o d d s \\, \\text { or } \\, \\log i t ; \\text { We see that the logistic }$$  \nRecall from Chapter 3 that in a linear regression model, β 1 gives the average change in Y associated with a one-unit increase in X . By contrast, in a logistic regression model, increasing X by one unit changes the log odds by β 1 (4.4). Equivalently, it multiplies the odds by e β 1 (4.3). However, because the relationship between p ( X ) and X in (4.2) is not a straight line, β 1 does not correspond to the change in p ( X ) associated with a one-unit increase in X . The amount that p ( X ) changes due to a one-unit change in X depends on the current value of X . But regardless of the value of X , if β 1 is positive then increasing X will be associated with increasing p ( X ) , and if β 1 is negative then increasing X will be associated with decreasing p ( X ) . The fact that there is not a straight-line relationship between p ( X ) and X , and the fact that the rate of change in p ( X ) per unit change in X depends on the current value of X , can also be seen by inspection of the right-hand panel of Figure 4.2.  \nThe left-hand side is called the log odds or logit . We see that the logistic regression model (4.2) has a logit that is linear in X .', '### 4.3.5 Multinomial Logistic Regression  \nWe sometimes wish to classify a response variable that has more than two classes. For example, in Section 4.2 we had three categories of medical condition in the emergency room: stroke , drug overdose , epileptic seizure . However, the logistic regression approach that we have seen in this section only allows for K = 2 classes for the response variable.  \nconfounding  \nIt turns out that it is possible to extend the two-class logistic regression approach to the setting of K &gt; 2 classes. This extension is sometimes known as multinomial logistic regression . To do this, we first select a single class to serve as the baseline ; without loss of generality, we select the K th class for this role. Then we replace the model (4.7) with the model  \n$$\\Pr ( Y = k | X = x ) = \\frac { e ^ { \\beta _ { k 0 } + \\beta _ { k 1 } x _ { 1 } + \\cdots + \\beta _ { k p } x _ { p } } } { 1 + \\sum _ { l = 1 } ^ { K - 1 } e ^ { \\beta _ { l 0 } + \\beta _ { l 1 } x _ { 1 } + \\cdots + \\beta _ { l p } x _ { p } } }$$  \nfor k = 1         K -1 , and  \n$$\\Pr ( Y = K | X = x ) & = \\frac { 1 } { 1 + \\sum _ { l = 1 } ^ { K - 1 } e ^ { \\beta _ { l 0 } + \\beta _ { l 1 } x _ { 1 } + \\cdots + \\beta _ { l p } x _ { p } } } . \\\\ \\intertext { It is not hard to show that for $k=1,\\dots, K-1$, }$$  \n$$k \\text { is not half closed in } & \\mathbb { Z } = 1 , \\dots , R = 1 , \\\\ & \\log \\left ( \\frac { \\Pr ( Y = k | X = x ) } { \\Pr ( Y = K | X = x ) } \\right ) = \\beta _ { k 0 } + \\beta _ { k 1 } x _ { 1 } + \\cdots + \\beta _ { k p } x _ { p } . \\\\$$  \nIt is not hard to show that for k = 1         K -1 ,  \nNotice that (4.12) is quite similar to (4.6). Equation 4.12 indicates that once again, the log odds between any pair of classes is linear in the features.  \nNonetheless, interpretation of the coefficients in a multinomial logistic regression model must be done with care, since it is tied to the choice of baseline. For example, if we set epileptic seizure to be the baseline, then we can interpret β stroke 0 as the log odds of stroke versus epileptic seizure , given that x 1 = · · · = x p = 0 . Furthermore, a one-unit increase in X j is associated with a β stroke j increase in the log odds of stroke over epileptic seizure . Stated another way, if X j increases by one unit, then  \nIt turns out that in (4.10)-(4.12), the decision to treat the K th class as the baseline is unimportant. For example, when classifying emergency room visits into stroke , drug overdose , and epileptic seizure , suppose that we fit two multinomial logistic regression models: one treating stroke as the baseline, another treating drug overdose as the baseline. The coefficient estimates will differ between the two fitted models due to the differing choice of baseline, but the fitted values (predictions), the log odds between any pair of classes, and the other key model outputs will remain the same.  \n$$\\frac { \\Pr ( Y = \\text {stroke} | X = x ) } { \\Pr ( Y = \\text {epileptic seeirue} | X = x ) }$$  \nincreases by e β stroke j .  \nWe now briefly present an alternative coding for multinomial logistic regression, known as the softmax coding. The softmax coding is equivalent to the coding just described in the sense that the fitted values, log odds between any pair of classes, and other key model outputs will remain the same, regardless of coding. But the softmax coding is used extensively in some areas of the machine learning literature (and will appear again in Chapter 10), so it is worth being aware of it. In the softmax coding, rather than selecting a baseline class, we treat all K classes symmetrically, and assume that for k = 1         K ,  \n$$\\Pr ( Y = k | X = x ) = \\frac { e ^ { \\beta _ { k 0 } + \\beta _ { k _ { 1 } } x _ { 1 } + \\dots + \\beta _ { k _ { p } } x _ { p } } } { \\sum _ { l = 1 } ^ { K } e ^ { \\beta _ { l 0 } + \\beta _ { l 1 } x _ { 1 } + \\dots + \\beta _ { l p } x _ { p } } } .$$  \nmultinomial logistic regression softmax  \nThus, rather than estimating coefficients for K -1 classes, we actually estimate coefficients for all K classes. It is not hard to see that as a result of (4.13), the log odds ratio between the k th and k ′ th classes equals  \n$$\\log \\left ( \\frac { \\Pr ( Y = k | X = x ) } { \\Pr ( Y = k ^ { \\prime } | X = x ) } \\right ) = ( \\beta _ { k 0 } - \\beta _ { k ^ { \\prime } 0 } ) + ( \\beta _ { k 1 } - \\beta _ { k ^ { \\prime } 1 } ) x _ { 1 } + \\cdots + ( \\beta _ { k p } - \\beta _ { k ^ { \\prime } p } ) x _ { p } .$$', ""### 4.3.2 Estimating the Regression Coefficients  \nThe coefficients β 0 and β 1 in (4.2) are unknown, and must be estimated based on the available training data. In Chapter 3, we used the least squares approach to estimate the unknown linear regression coefficients. Although we could use (non-linear) least squares to fit the model (4.4), the more general method of maximum likelihood is preferred, since it has better statistical properties. The basic intuition behind using maximum likelihood odds  \nlog odds logit  \nto fit a logistic regression model is as follows: we seek estimates for β 0 and β 1 such that the predicted probability ˆ p ( x i ) of default for each individual, using (4.2), corresponds as closely as possible to the individual's observed default status. In other words, we try to find ˆ β 0 and ˆ β 1 such that plugging these estimates into the model for p ( X ) , given in (4.2), yields a number close to one for all individuals who defaulted, and a number close to zero for all individuals who did not. This intuition can be formalized using a mathematical equation called a likelihood function :  \n$$\\ell ( \\beta _ { 0 } , \\beta _ { 1 } ) = \\prod _ { i \\colon y _ { i } = 1 } p ( x _ { i } ) \\prod _ { i ^ { \\prime } \\colon y _ { i ^ { \\prime } } = 0 } ( 1 - p ( x _ { i ^ { \\prime } } ) ) .$$  \nThe estimates ˆ β 0 and ˆ β 1 are chosen to maximize this likelihood function.  \nTable 4.1 shows the coefficient estimates and related information that result from fitting a logistic regression model on the Default data in order to predict the probability of default = Yes using balance . We see that ˆ β 1 = 0   0055 ; this indicates that an increase in balance is associated with an increase in the probability of default . To be precise, a one-unit increase in balance is associated with an increase in the log odds of default by 0   0055 units.  \nMaximum likelihood is a very general approach that is used to fit many of the non-linear models that we examine throughout this book. In the linear regression setting, the least squares approach is in fact a special case of maximum likelihood. The mathematical details of maximum likelihood are beyond the scope of this book. However, in general, logistic regression and other models can be easily fit using statistical software such as R , and so we do not need to concern ourselves with the details of the maximum likelihood fitting procedure.  \nMany aspects of the logistic regression output shown in Table 4.1 are similar to the linear regression output of Chapter 3. For example, we can measure the accuracy of the coefficient estimates by computing their standard errors. The z -statistic in Table 4.1 plays the same role as the t -statistic in the linear regression output, for example in Table 3.1 on page 77. For instance, the z -statistic associated with β 1 is equal to ˆ β 1 glyph[triangleleft] SE( ˆ β 1 ) , and so a large (absolute) value of the z -statistic indicates evidence against the null hypothesis H 0 : β 1 = 0 . This null hypothesis implies that p ( X ) = e β 0 1+ e β 0 : in other words, that the probability of default does not depend on balance . Since the p -value associated with balance in Table 4.1 is tiny, we can reject H 0 . In other words, we conclude that there is indeed an association between balance and probability of default . The estimated intercept in Table 4.1 is typically not of interest; its main purpose is to adjust the average fitted probabilities to the proportion of ones in the data (in this case, the overall default rate)."", ""## 4.8 Exercises  \n### Conceptual  \n1. Using a little bit of algebra, prove that (4.2) is equivalent to (4.3). In other words, the logistic function representation and logit representation for the logistic regression model are equivalent.\n2. It was stated in the text that classifying an observation to the class for which (4.17) is largest is equivalent to classifying an observation to the class for which (4.18) is largest. Prove that this is the case. In other words, under the assumption that the observations in the k th class are drawn from a N ( θ k  σ 2 ) distribution, the Bayes classifier assigns an observation to the class for which the discriminant function is maximized.\n3. This problem relates to the QDA model, in which the observations within each class are drawn from a normal distribution with a classspecific mean vector and a class specific covariance matrix. We consider the simple case where p = 1 ; i.e. there is only one feature.\n4. Suppose that we have classes, and that if an observation belongs  \nK to the k th class then X comes from a one-dimensional normal distribution, X ∼ N ( θ k  σ 2 k ) . Recall that the density function for the one-dimensional normal distribution is given in (4.16). Prove that in this case, the Bayes classifier is not linear. Argue that it is in fact quadratic.  \nHint: For this problem, you should follow the arguments laid out in Section 4.4.1, but without making the assumption that σ 2 1 = · · · = σ 2 K .  \n4. When the number of features p is large, there tends to be a deterioration in the performance of KNN and other local approaches that perform prediction using only observations that are near the test observation for which a prediction must be made. This phenomenon is known as the curse of dimensionality , and it ties into the fact that non-parametric approaches often perform poorly when p is large. We will now investigate this curse.  \n<!-- image -->  \ncurse of dimensionality  \n- (a) Suppose that we have a set of observations, each with measurements on p = 1 feature, X . We assume that X is uniformly (evenly) distributed on [0  1] . Associated with each observation is a response value. Suppose that we wish to predict a test observation's response using only observations that are within 10 % of the range of X closest to that test observation. For instance, in order to predict the response for a test observation with X = 0   6 , we will use observations in the range [0   55  0   65] . On average, what fraction of the available observations will we use to make the prediction?\n- (b) Now suppose that we have a set of observations, each with measurements on p = 2 features, X 1 and X 2 . We assume that ( X 1  X 2 ) are uniformly distributed on [0  1] × [0  1] . We wish to predict a test observation's response using only observations that are within 10 % of the range of X 1 and within 10 % of the range of X 2 closest to that test observation. For instance, in order to predict the response for a test observation with X 1 = 0   6 and X 2 = 0   35 , we will use observations in the range [0   55  0   65] for X 1 and in the range [0   3  0   4] for X 2 . On average, what fraction of the available observations will we use to make the prediction?\n- (c) Now suppose that we have a set of observations on p = 100 features. Again the observations are uniformly distributed on each feature, and again each feature ranges in value from 0 to 1. We wish to predict a test observation's response using observations within the 10 % of each feature's range that is closest to that test observation. What fraction of the available observations will we use to make the prediction?\n- (d) Using your answers to parts (a)-(c), argue that a drawback of KNN when p is large is that there are very few training observations 'near' any given test observation.\n- (e) Now suppose that we wish to make a prediction for a test observation by creating a p -dimensional hypercube centered around the test observation that contains, on average, 10 % of the training observations. For p = 1  2 , and 100 , what is the length of each side of the hypercube? Comment on your answer.  \nNote: A hypercube is a generalization of a cube to an arbitrary number of dimensions. When p = 1 , a hypercube is simply a line segment, when p = 2 it is a square, and when p = 100 it is a 100-dimensional cube.  \n5. We now examine the differences between LDA and QDA.\n2. (a) If the Bayes decision boundary is linear, do we expect LDA or QDA to perform better on the training set? On the test set?\n3. (b) If the Bayes decision boundary is non-linear, do we expect LDA or QDA to perform better on the training set? On the test set?  \n- (c) In general, as the sample size n increases, do we expect the test prediction accuracy of QDA relative to LDA to improve, decline, or be unchanged? Why?\n- (d) True or False: Even if the Bayes decision boundary for a given problem is linear, we will probably achieve a superior test error rate using QDA rather than LDA because QDA is flexible enough to model a linear decision boundary. Justify your answer.\n6. Suppose we collect data for a group of students in a statistics class with variables X 1 =hours studied, X 2 =undergrad GPA, and Y = receive an A. We fit a logistic regression and produce estimated coefficient, ˆ β 0 = -6  ˆ β 1 = 0   05  ˆ β 2 = 1 .\n- (a) Estimate the probability that a student who studies for 40 h and has an undergrad GPA of 3   5 gets an A in the class.\n- (b) How many hours would the student in part (a) need to study to have a 50 % chance of getting an A in the class?\n7. Suppose that we wish to predict whether a given stock will issue a dividend this year ('Yes' or 'No') based on X , last year's percent profit. We examine a large number of companies and discover that the mean value of X for companies that issued a dividend was ¯ X = 10 , while the mean for those that didn't was ¯ X = 0 . In addition, the variance of X for these two sets of companies was ˆ σ 2 = 36 . Finally, 80 % of companies issued dividends. Assuming that X follows a normal distribution, predict the probability that a company will issue a dividend this year given that its percentage profit was X = 4 last year.  \nHint: Recall that the density function for a normal random variable is f ( x ) = 1 √ 2 πσ 2 e -( x -θ ) 2 glyph[triangleleft] 2 σ 2 . You will need to use Bayes' theorem.  \n8. Suppose that we take a data set, divide it into equally-sized training and test sets, and then try out two different classification procedures. First we use logistic regression and get an error rate of 20 % on the training data and 30 % on the test data. Next we use 1-nearest neighbors (i.e. K = 1 ) and get an average error rate (averaged over both test and training data sets) of 18 %. Based on these results, which method should we prefer to use for classification of new observations? Why?\n9. This problem has to do with odds .\n3. (a) On average, what fraction of people with an odds of 0.37 of defaulting on their credit card payment will in fact default?\n4. (b) Suppose that an individual has a 16 % chance of defaulting on her credit card payment. What are the odds that she will default?  \n10. Equation 4.32 derived an expression for log ( Pr( Y = k ♣ X = x ) Pr( Y = K ♣ X = x ) ) in the setting where p &gt; 1 , so that the mean for the k th class, θ k , is a p -dimensional vector, and the shared covariance Σ is a p × p matrix. However, in the setting with p = 1 , (4.32) takes a simpler form, since the means θ 1         θ K and the variance σ 2 are scalars. In this simpler setting, repeat the calculation in (4.32), and provide expressions for a k and b kj in terms of π k , π K , θ k , θ K , and σ 2 .\n11. Work out the detailed forms of a k , b kj , and b kjl in (4.33). Your answer should involve π k , π K , θ k , θ K , Σ k , and Σ K .\n12. Suppose that you wish to classify an observation X ∈ R into apples and oranges . You fit a logistic regression model and find that  \n$$\\widehat { \\Pr } ( Y = \\text {orange} | X = x ) = \\frac { \\exp ( \\hat { \\beta } _ { 0 } + \\hat { \\beta } _ { 1 } x ) } { 1 + \\exp ( \\hat { \\beta } _ { 0 } + \\hat { \\beta } _ { 1 } x ) } . \\\\ \\text {fr} \\text { friend fits a logistic regression model to the same data using}$$  \nYour friend fits a logistic regression model to the same data using the softmax formulation in (4.13), and finds that  \n$$\\widehat { \\Pr } ( Y = \\text {orange} | X = x ) = \\\\ \\frac { \\exp ( \\hat { \\alpha } _ { \\text {orange} } 0 + \\hat { \\alpha } _ { \\text {orange} } 1 x ) } { \\exp ( \\hat { \\alpha } _ { \\text {orange} } 0 + \\hat { \\alpha } _ { \\text {orange} } 1 x ) + \\exp ( \\hat { \\alpha } _ { \\text {apple} } 0 + \\hat { \\alpha } _ { \\text {apple} } 1 x ) } .$$  \n- (a) What is the log odds of orange versus apple in your model?\n- (b) What is the log odds of orange versus apple in your friend's model?\n- (c) Suppose that in your model, ˆ β 0 = 2 and ˆ β 1 = -1 . What are the coefficient estimates in your friend's model? Be as specific as possible.\n- (d) Now suppose that you and your friend fit the same two models on a different data set. This time, your friend gets the coefficient estimates ˆ α orange 0 = 1   2 , ˆ α orange 1 = -2 , ˆ α orange 0 = 3 , ˆ α orange 1 = 0   6 . What are the coefficient estimates in your model?\n- (e) Finally, suppose you apply both models from (d) to a data set with 2,000 test observations. What fraction of the time do you expect the predicted class labels from your model to agree with those from your friend's model? Explain your answer.""]","The formula for the logit transformation in logistic regression is given by:

\[
\log \left( \frac{p(X)}{1 - p(X)} \right) = \beta_0 + \beta_1 X
\]

Here, \( p(X) \) is the probability of the event occurring, and the logit transformation is the natural logarithm of the odds of the event. The right-hand side of the equation represents a linear combination of the predictors, where \(\beta_0\) is the intercept and \(\beta_1\) is the coefficient for the predictor \(X\).",The logit transformation is given by the formula log(p(X) / (1 - p(X))) = β₀ + β₁X₁ + ... + βₚXₚ. The left-hand side is referred to as the log-odds or logit.,1.0,,0.6388888888675925,1.0
¿Cómo se calcula la estimación de la Máxima Verosimilitud (MLE) para un modelo de regresión logística?,"[""### 4.3.2 Estimating the Regression Coefficients  \nThe coefficients β 0 and β 1 in (4.2) are unknown, and must be estimated based on the available training data. In Chapter 3, we used the least squares approach to estimate the unknown linear regression coefficients. Although we could use (non-linear) least squares to fit the model (4.4), the more general method of maximum likelihood is preferred, since it has better statistical properties. The basic intuition behind using maximum likelihood odds  \nlog odds logit  \nto fit a logistic regression model is as follows: we seek estimates for β 0 and β 1 such that the predicted probability ˆ p ( x i ) of default for each individual, using (4.2), corresponds as closely as possible to the individual's observed default status. In other words, we try to find ˆ β 0 and ˆ β 1 such that plugging these estimates into the model for p ( X ) , given in (4.2), yields a number close to one for all individuals who defaulted, and a number close to zero for all individuals who did not. This intuition can be formalized using a mathematical equation called a likelihood function :  \n$$\\ell ( \\beta _ { 0 } , \\beta _ { 1 } ) = \\prod _ { i \\colon y _ { i } = 1 } p ( x _ { i } ) \\prod _ { i ^ { \\prime } \\colon y _ { i ^ { \\prime } } = 0 } ( 1 - p ( x _ { i ^ { \\prime } } ) ) .$$  \nThe estimates ˆ β 0 and ˆ β 1 are chosen to maximize this likelihood function.  \nTable 4.1 shows the coefficient estimates and related information that result from fitting a logistic regression model on the Default data in order to predict the probability of default = Yes using balance . We see that ˆ β 1 = 0   0055 ; this indicates that an increase in balance is associated with an increase in the probability of default . To be precise, a one-unit increase in balance is associated with an increase in the log odds of default by 0   0055 units.  \nMaximum likelihood is a very general approach that is used to fit many of the non-linear models that we examine throughout this book. In the linear regression setting, the least squares approach is in fact a special case of maximum likelihood. The mathematical details of maximum likelihood are beyond the scope of this book. However, in general, logistic regression and other models can be easily fit using statistical software such as R , and so we do not need to concern ourselves with the details of the maximum likelihood fitting procedure.  \nMany aspects of the logistic regression output shown in Table 4.1 are similar to the linear regression output of Chapter 3. For example, we can measure the accuracy of the coefficient estimates by computing their standard errors. The z -statistic in Table 4.1 plays the same role as the t -statistic in the linear regression output, for example in Table 3.1 on page 77. For instance, the z -statistic associated with β 1 is equal to ˆ β 1 glyph[triangleleft] SE( ˆ β 1 ) , and so a large (absolute) value of the z -statistic indicates evidence against the null hypothesis H 0 : β 1 = 0 . This null hypothesis implies that p ( X ) = e β 0 1+ e β 0 : in other words, that the probability of default does not depend on balance . Since the p -value associated with balance in Table 4.1 is tiny, we can reject H 0 . In other words, we conclude that there is indeed an association between balance and probability of default . The estimated intercept in Table 4.1 is typically not of interest; its main purpose is to adjust the average fitted probabilities to the proportion of ones in the data (in this case, the overall default rate)."", '### 4.3.5 Multinomial Logistic Regression  \nWe sometimes wish to classify a response variable that has more than two classes. For example, in Section 4.2 we had three categories of medical condition in the emergency room: stroke , drug overdose , epileptic seizure . However, the logistic regression approach that we have seen in this section only allows for K = 2 classes for the response variable.  \nconfounding  \nIt turns out that it is possible to extend the two-class logistic regression approach to the setting of K &gt; 2 classes. This extension is sometimes known as multinomial logistic regression . To do this, we first select a single class to serve as the baseline ; without loss of generality, we select the K th class for this role. Then we replace the model (4.7) with the model  \n$$\\Pr ( Y = k | X = x ) = \\frac { e ^ { \\beta _ { k 0 } + \\beta _ { k 1 } x _ { 1 } + \\cdots + \\beta _ { k p } x _ { p } } } { 1 + \\sum _ { l = 1 } ^ { K - 1 } e ^ { \\beta _ { l 0 } + \\beta _ { l 1 } x _ { 1 } + \\cdots + \\beta _ { l p } x _ { p } } }$$  \nfor k = 1         K -1 , and  \n$$\\Pr ( Y = K | X = x ) & = \\frac { 1 } { 1 + \\sum _ { l = 1 } ^ { K - 1 } e ^ { \\beta _ { l 0 } + \\beta _ { l 1 } x _ { 1 } + \\cdots + \\beta _ { l p } x _ { p } } } . \\\\ \\intertext { It is not hard to show that for $k=1,\\dots, K-1$, }$$  \n$$k \\text { is not half closed in } & \\mathbb { Z } = 1 , \\dots , R = 1 , \\\\ & \\log \\left ( \\frac { \\Pr ( Y = k | X = x ) } { \\Pr ( Y = K | X = x ) } \\right ) = \\beta _ { k 0 } + \\beta _ { k 1 } x _ { 1 } + \\cdots + \\beta _ { k p } x _ { p } . \\\\$$  \nIt is not hard to show that for k = 1         K -1 ,  \nNotice that (4.12) is quite similar to (4.6). Equation 4.12 indicates that once again, the log odds between any pair of classes is linear in the features.  \nNonetheless, interpretation of the coefficients in a multinomial logistic regression model must be done with care, since it is tied to the choice of baseline. For example, if we set epileptic seizure to be the baseline, then we can interpret β stroke 0 as the log odds of stroke versus epileptic seizure , given that x 1 = · · · = x p = 0 . Furthermore, a one-unit increase in X j is associated with a β stroke j increase in the log odds of stroke over epileptic seizure . Stated another way, if X j increases by one unit, then  \nIt turns out that in (4.10)-(4.12), the decision to treat the K th class as the baseline is unimportant. For example, when classifying emergency room visits into stroke , drug overdose , and epileptic seizure , suppose that we fit two multinomial logistic regression models: one treating stroke as the baseline, another treating drug overdose as the baseline. The coefficient estimates will differ between the two fitted models due to the differing choice of baseline, but the fitted values (predictions), the log odds between any pair of classes, and the other key model outputs will remain the same.  \n$$\\frac { \\Pr ( Y = \\text {stroke} | X = x ) } { \\Pr ( Y = \\text {epileptic seeirue} | X = x ) }$$  \nincreases by e β stroke j .  \nWe now briefly present an alternative coding for multinomial logistic regression, known as the softmax coding. The softmax coding is equivalent to the coding just described in the sense that the fitted values, log odds between any pair of classes, and other key model outputs will remain the same, regardless of coding. But the softmax coding is used extensively in some areas of the machine learning literature (and will appear again in Chapter 10), so it is worth being aware of it. In the softmax coding, rather than selecting a baseline class, we treat all K classes symmetrically, and assume that for k = 1         K ,  \n$$\\Pr ( Y = k | X = x ) = \\frac { e ^ { \\beta _ { k 0 } + \\beta _ { k _ { 1 } } x _ { 1 } + \\dots + \\beta _ { k _ { p } } x _ { p } } } { \\sum _ { l = 1 } ^ { K } e ^ { \\beta _ { l 0 } + \\beta _ { l 1 } x _ { 1 } + \\dots + \\beta _ { l p } x _ { p } } } .$$  \nmultinomial logistic regression softmax  \nThus, rather than estimating coefficients for K -1 classes, we actually estimate coefficients for all K classes. It is not hard to see that as a result of (4.13), the log odds ratio between the k th and k ′ th classes equals  \n$$\\log \\left ( \\frac { \\Pr ( Y = k | X = x ) } { \\Pr ( Y = k ^ { \\prime } | X = x ) } \\right ) = ( \\beta _ { k 0 } - \\beta _ { k ^ { \\prime } 0 } ) + ( \\beta _ { k 1 } - \\beta _ { k ^ { \\prime } 1 } ) x _ { 1 } + \\cdots + ( \\beta _ { k p } - \\beta _ { k ^ { \\prime } p } ) x _ { p } .$$', '### 4.3.1 The Logistic Model  \nHow should we model the relationship between p ( X ) = Pr( Y = 1 ♣ X ) and X ? (For convenience we are using the generic 0/1 coding for the response.) In Section 4.2 we considered using a linear regression model to represent these probabilities:  \n$$p ( X ) = \\beta _ { 0 } + \\beta _ { 1 } X .$$  \nIf we use this approach to predict default = Yes using balance , then we obtain the model shown in the left-hand panel of Figure 4.2. Here we see the problem with this approach: for balances close to zero we predict a negative probability of default; if we were to predict for very large balances, we would get values bigger than 1. These predictions are not sensible, since of course the true probability of default, regardless of credit card balance, must fall between 0 and 1 . This problem is not unique to the credit default data. Any time a straight line is fit to a binary response that is coded as 0 or 1 , in principle we can always predict p ( X ) &lt; 0 for some values of X and p ( X ) &gt; 1 for others (unless the range of X is limited).  \nTo avoid this problem, we must model p ( X ) using a function that gives outputs between 0 and 1 for all values of X . Many functions meet this description. In logistic regression, we use the logistic function ,  \n$$p ( X ) = \\frac { e ^ { \\beta _ { 0 } + \\beta _ { 1 } X } } { 1 + e ^ { \\beta _ { 0 } + \\beta _ { 1 } X } } .$$  \nTo fit the model (4.2), we use a method called maximum likelihood , which we discuss in the next section. The right-hand panel of Figure 4.2 illustrates the fit of the logistic regression model to the Default data. Notice that for logistic function  \nmaximum likelihood  \nlow balances we now predict the probability of default as close to, but never below, zero. Likewise, for high balances we predict a default probability close to, but never above, one. The logistic function will always produce an S-shaped curve of this form, and so regardless of the value of X , we will obtain a sensible prediction. We also see that the logistic model is better able to capture the range of probabilities than is the linear regression model in the left-hand plot. The average fitted probability in both cases is 0.0333 (averaged over the training data), which is the same as the overall proportion of defaulters in the data set.  \nAfter a bit of manipulation of (4.2), we find that  \n$$\\frac { p ( X ) } { 1 - p ( X ) } = e ^ { \\beta _ { 0 } + \\beta _ { 1 } X } . \\\\ [ / ] 1 = p ( X ) ] \\text { is called the } \\sigma d d _ { s } \\text { and } \\text {can take on an } \\alpha \\text { value}$$  \nBy taking the logarithm of both sides of (4.3), we arrive at  \nThe quantity p ( X ) glyph[triangleleft] [1 -p ( X )] is called the odds , and can take on any value between 0 and ∞ . Values of the odds close to 0 and ∞ indicate very low and very high probabilities of default, respectively. For example, on average 1 in 5 people with an odds of 1 glyph[triangleleft] 4 will default, since p ( X ) = 0   2 implies an odds of 0   2 1 -0   2 = 1 glyph[triangleleft] 4 . Likewise, on average nine out of every ten people with an odds of 9 will default, since p ( X ) = 0   9 implies an odds of 0   9 1 -0   9 = 9 . Odds are traditionally used instead of probabilities in horse-racing, since they relate more naturally to the correct betting strategy.  \n$$\\log \\left ( \\frac { \\ p ( X ) } { 1 - p ( X ) } \\right ) & = \\beta _ { 0 } + \\beta _ { 1 } X . \\\\ \\text {side is called the } \\log \\alpha \\, \\underset { \\ } o d d s \\, \\text { or } \\, \\log i t ; \\text { We see that the logistic }$$  \nRecall from Chapter 3 that in a linear regression model, β 1 gives the average change in Y associated with a one-unit increase in X . By contrast, in a logistic regression model, increasing X by one unit changes the log odds by β 1 (4.4). Equivalently, it multiplies the odds by e β 1 (4.3). However, because the relationship between p ( X ) and X in (4.2) is not a straight line, β 1 does not correspond to the change in p ( X ) associated with a one-unit increase in X . The amount that p ( X ) changes due to a one-unit change in X depends on the current value of X . But regardless of the value of X , if β 1 is positive then increasing X will be associated with increasing p ( X ) , and if β 1 is negative then increasing X will be associated with decreasing p ( X ) . The fact that there is not a straight-line relationship between p ( X ) and X , and the fact that the rate of change in p ( X ) per unit change in X depends on the current value of X , can also be seen by inspection of the right-hand panel of Figure 4.2.  \nThe left-hand side is called the log odds or logit . We see that the logistic regression model (4.2) has a logit that is linear in X .', ""### 5.1.5 Cross-Validation on Classification Problems  \nIn this chapter so far, we have illustrated the use of cross-validation in the regression setting where the outcome Y is quantitative, and so have used MSE to quantify test error. But cross-validation can also be a very useful approach in the classification setting when Y is qualitative. In this setting, cross-validation works just as described earlier in this chapter, except that rather than using MSE to quantify test error, we instead use the number of misclassified observations. For instance, in the classification setting, the LOOCV error rate takes the form  \n$$C V _ { ( n ) } = \\frac { 1 } { n } \\sum _ { i = 1 } ^ { n } E r r _ { i } , \\\\$$  \nwhere Err i = I ( y i = ˆ y i ) . The k -fold CV error rate and validation set error rates are defined analogously.  \n/negationslash  \nAs an example, we fit various logistic regression models on the twodimensional classification data displayed in Figure 2.13. In the top-left panel of Figure 5.7, the black solid line shows the estimated decision boundary resulting from fitting a standard logistic regression model to this data set. Since this is simulated data, we can compute the true test error rate, which takes a value of 0   201 and so is substantially larger than the Bayes  \nFIGURE 5.7. Logistic regression fits on the two-dimensional classification data displayed in Figure 2.13. The Bayes decision boundary is represented using a purple dashed line. Estimated decision boundaries from linear, quadratic, cubic and quartic (degrees 1-4) logistic regressions are displayed in black. The test error rates for the four logistic regression fits are respectively 0   201 , 0   197 , 0   160 , and 0   162 , while the Bayes error rate is 0   133 .  \n<!-- image -->  \nerror rate of 0   133 . Clearly logistic regression does not have enough flexibility to model the Bayes decision boundary in this setting. We can easily extend logistic regression to obtain a non-linear decision boundary by using polynomial functions of the predictors, as we did in the regression setting in Section 3.3.2. For example, we can fit a quadratic logistic regression model, given by  \n$$\\log \\left ( \\frac { p } { 1 - p } \\right ) = \\beta _ { 0 } + \\beta _ { 1 } X _ { 1 } + \\beta _ { 2 } X _ { 1 } ^ { 2 } + \\beta _ { 3 } X _ { 2 } + \\beta _ { 4 } X _ { 2 } ^ { 2 } . \\\\$$  \nThe top-right panel of Figure 5.7 displays the resulting decision boundary, which is now curved. However, the test error rate has improved only slightly, to 0   197 . A much larger improvement is apparent in the bottom-left panel  \nFIGURE 5.8. Test error (brown), training error (blue), and 10 -fold CV error (black) on the two-dimensional classification data displayed in Figure 5.7. Left: Logistic regression using polynomial functions of the predictors. The order of the polynomials used is displayed on the x -axis. Right: The KNN classifier with different values of K , the number of neighbors used in the KNN classifier.  \n<!-- image -->  \nof Figure 5.7, in which we have fit a logistic regression model involving cubic polynomials of the predictors. Now the test error rate has decreased to 0   160 . Going to a quartic polynomial (bottom-right) slightly increases the test error.  \nThe right-hand panel of Figure 5.8 displays the same three curves using the KNN approach for classification, as a function of the value of K (which in this context indicates the number of neighbors used in the KNN classifier, rather than the number of CV folds used). Again the training error rate declines as the method becomes more flexible, and so we see that the training error rate cannot be used to select the optimal value for K . Though the cross-validation error curve slightly underestimates the test error rate, it takes on a minimum very close to the best value for K .  \nIn practice, for real data, the Bayes decision boundary and the test error rates are unknown. So how might we decide between the four logistic regression models displayed in Figure 5.7? We can use cross-validation in order to make this decision. The left-hand panel of Figure 5.8 displays in black the 10 -fold CV error rates that result from fitting ten logistic regression models to the data, using polynomial functions of the predictors up to tenth order. The true test errors are shown in brown, and the training errors are shown in blue. As we have seen previously, the training error tends to decrease as the flexibility of the fit increases. (The figure indicates that though the training error rate doesn't quite decrease monotonically, it tends to decrease on the whole as the model complexity increases.) In contrast, the test error displays a characteristic U-shape. The 10 -fold CV error rate provides a pretty good approximation to the test error rate. While it somewhat underestimates the error rate, it reaches a minimum when fourth-order polynomials are used, which is very close to the minimum of the test curve, which occurs when third-order polynomials are used. In fact, using fourth-order polynomials would likely lead to good test set performance, as the true test error rate is approximately the same for third, fourth, fifth, and sixth-order polynomials."", ""### 11.5.2 Proportional Hazards  \nThe Proportional Hazards Assumption  \nThe proportional hazards assumption states that  \n$$h ( t | x _ { i } ) = & h _ { 0 } ( t ) \\exp \\left ( \\sum _ { j = 1 } ^ { p } x _ { i j } \\beta _ { j } \\right ) , \\\\ \\ > 0 \\ \\text {is an unspecified function, known as the baseline hazard.}$$  \nwhere h 0 ( t ) ≥ 0 is an unspecified function, known as the baseline hazard . It is the hazard function for an individual with features x i 1 = · · · = x ip = 0 . The name 'proportional hazards' arises from the fact that the hazard function for an individual with feature vector x i is some unknown function  \n9 See Exercise 9.  \n11 The notation h ( t ♣ x i ) indicates that we are now considering the hazard function for the i th observation conditional on the values of the covariates, x i .  \n10 Given the close relationship between the hazard function h ( t ) and the density function f ( t ) explored in Exercise 8, posing an assumption about the form of the hazard function is closely related to posing an assumption about the form of the density function, as was done in the previous paragraph.  \nproportional hazards assumption baseline hazard  \nFIGURE 11.4. Top: In a simple example with p = 1 and a binary covariate x i ∈ ¶ 0  1 ♦ , the log hazard and the survival function under the model (11.14) are shown (green for x i = 0 and black for x i = 1 ). Because of the proportional hazards assumption (11.14) , the log hazard functions differ by a constant, and the survival functions do not cross. Bottom: Again we have a single binary covariate x i ∈ ¶ 0  1 ♦ . However, the proportional hazards assumption (11.14) does not hold. The log hazard functions cross, as do the survival functions.  \n<!-- image -->  \nh 0 ( t ) times the factor exp ( ∑ p j =1 x ij β j ) . The quantity exp ( ∑ p j =1 x ij β j ) is called the relative risk for the feature vector x i = ( x i 1         x ip ) T , relative to that for the feature vector x i = (0         0) T .  \nAn illustration of the proportional hazards assumption (11.14) is given in Figure 11.4, in a simple setting with a single binary covariate x i ∈ ¶ 0  1 ♦ (so that p = 1 ). In the top row, the proportional hazards assumption (11.14) holds. Thus, the hazard functions of the two groups are a constant multiple of each other, so that on the log scale, the gap between them is constant. Furthermore, the survival curves never cross, and in fact the gap between the survival curves tends to (initially) increase over time. By contrast, in the bottom row, (11.14) does not hold. We see that the log hazard functions for the two groups cross, as do the survival curves.  \nWhat does it mean that the baseline hazard function h 0 ( t ) in (11.14) is unspecified? Basically, we make no assumptions about its functional form. We allow the instantaneous probability of death at time t , given that one has survived at least until time t , to take any form. This means that the hazard function is very flexible and can model a wide range of relationships between the covariates and survival time. Our only assumption is that a one-unit increase in x ij corresponds to an increase in h ( t ♣ x i ) by a factor of exp( β j ) .\nCox's Proportional Hazards Model  \nBecause the form of h 0 ( t ) in the proportional hazards assumption (11.14) is unknown, we cannot simply plug h ( t ♣ x i ) into the likelihood (11.13) and then estimate β = ( β 1         β p ) T by maximum likelihood. The magic of Cox's proportional hazards model lies in the fact that it is in fact possible to estimate β without having to specify the form of h 0 ( t ) .  \nTo accomplish this, we make use of the same 'sequential in time' logic that we used to derive the Kaplan-Meier survival curve and the log-rank test. For simplicity, assume that there are no ties among the failure, or death, times: i.e. each failure occurs at a distinct time. Assume that δ i = 1 , i.e. the i th observation is uncensored, and thus y i is its failure time. Then the hazard function for the i th observation at time y i is h ( y i ♣ x i ) = h 0 ( y i ) exp ( ∑ p j =1 x ij β j ) , and the total hazard at time y i for the at risk observations 12 is  \n$$\\sum _ { i ^ { \\prime } \\colon y _ { i ^ { \\prime } } \\geq y _ { i } } h _ { 0 } ( y _ { i } ) \\exp \\left ( \\sum _ { j = 1 } ^ { p } x _ { i ^ { \\prime } j } \\beta _ { j } \\right ) .$$  \nTherefore, the probability that the i th observation is the one to fail at time y i (as opposed to one of the other observations in the risk set) is  \n$$\\widehat { y } _ { i } \\left ( \\text {as $6$ or $6$-sized} \\delta \\text { of the $6$-neighbor classes in the $M$ set} \\right ) \\text { is } & \\quad \\\\ \\frac { h _ { 0 } ( y _ { i } ) \\exp \\left ( \\sum _ { j = 1 } ^ { p } x _ { i j } \\beta _ { j } \\right ) } { \\sum _ { i ^ { \\prime } ; y _ { i ^ { \\prime } } \\geq y _ { i } } h _ { 0 } ( y _ { i } ) \\exp \\left ( \\sum _ { j = 1 } ^ { p } x _ { i ^ { \\prime } j } \\beta _ { j } \\right ) } = \\frac { \\exp \\left ( \\sum _ { j = 1 } ^ { p } x _ { i j } \\beta _ { j } \\right ) } { \\sum _ { i ^ { \\prime } ; y _ { i ^ { \\prime } } \\geq y _ { i } } \\exp \\left ( \\sum _ { j = 1 } ^ { p } x _ { i ^ { \\prime } j } \\beta _ { j } \\right ) } . \\\\ \\text {Notice that the unspecified baseline hardware function } h _ { 0 } ( y _ { i } ) \\text { cancels out of } &$$  \nThe partial likelihood is simply the product of these probabilities over all of the uncensored observations,  \nNotice that the unspecified baseline hazard function h 0 ( y i ) cancels out of the numerator and denominator!  \n$$\\text {unless observed observations,} & & \\exp \\left ( \\sum _ { j = 1 } ^ { p } x _ { i j } \\beta _ { j } \\right ) \\\\ & & \\quad \\text {PL} ( \\beta ) = \\prod _ { i ; \\delta _ { i } = 1 } \\frac { \\exp \\left ( \\sum _ { j = 1 } ^ { p } x _ { i j } \\beta _ { j } \\right ) } { \\sum _ { i ^ { \\prime } ; y _ { i ^ { \\prime } } \\geq y _ { i } } \\exp \\left ( \\sum _ { j = 1 } ^ { p } x _ { i ^ { \\prime } j } \\beta _ { j } \\right ) } .$$  \nTo estimate β , we simply maximize the partial likelihood (11.16) with respect to β . As was the case for logistic regression in Chapter 4, no closedform solution is available, and so iterative algorithms are required.  \nCritically, the partial likelihood is valid regardless of the true value of h 0 ( t ) , making the model very flexible and robust. 13  \nIn addition to estimating β , we can also obtain other model outputs that we saw in the context of least squares regression in Chapter 3 and logistic regression in Chapter 4. For example, we can obtain p -values corresponding  \n12 Recall that the 'at risk' observations at time y i are those that are still at risk of failure, i.e. those that have not yet failed or been censored before time y i .  \n13 In general, the partial likelihood is used in settings where it is difficult to compute the full likelihood for all of the parameters. Instead, we compute a likelihood for just the parameters of primary interest: in this case, β 1         β p . It can be shown that maximizing (11.16) provides good estimates for these parameters.  \nCox's proportional hazards model partial likelihood  \nto particular null hypotheses (e.g. H 0 : β j = 0 ), as well as confidence intervals associated with the coefficients.\nConnection With The Log-Rank Test  \nSuppose we have just a single predictor ( p = 1 ), which we assume to be binary, i.e. x i ∈ ¶ 0  1 ♦ . In order to determine whether there is a difference between the survival times of the observations in the group ¶ i : x i = 0 ♦ and those in the group ¶ i : x i = 1 ♦ , we can consider taking two possible approaches:  \nApproach #1: Fit a Cox proportional hazards model, and test the null hypothesis H 0 : β = 0 . (Since p = 1 , β is a scalar.)  \nApproach #2: Perform a log-rank test to compare the two groups, as in Section 11.4.  \nWhich one should we prefer?  \nIn fact, there is a close relationship between these two approaches. In particular, when taking Approach #1, there are a number of possible ways to test H 0 . One way is known as a score test. It turns out that in the case of a single binary covariate, the score test for H 0 : β = 0 in Cox's proportional hazards model is exactly equal to the log-rank test. In other words, it does not matter whether we take Approach #1 or Approach #2!\nAdditional Details  \nThe discussion of Cox's proportional hazards model glossed over a few subtleties:  \n- There is no intercept in (11.14) nor in the equations that follow, because an intercept can be absorbed into the baseline hazard h 0 ( t ) .\n- We have assumed that there are no tied failure times. In the case of ties, the exact form of the partial likelihood (11.16) is a bit more complicated, and a number of computational approximations must be used.\n- (11.16) is known as the partial likelihood because it is not exactly a likelihood. That is, it does not correspond exactly to the probability of the data under the assumption (11.14). However, it is a very good approximation.\n- We have focused only on estimation of the coefficients β = ( β 1         β p ) T . However, at times we may also wish to estimate the baseline hazard h 0 ( t ) , for instance so that we can estimate the survival curve S ( t ♣ x ) for an individual with feature vector x . The details are beyond the scope of this book. Estimation of h 0 ( t ) is implemented in the lifelines package in Python , which we will see in Section 11.8.""]","Para calcular la estimación de la Máxima Verosimilitud (MLE) en un modelo de regresión logística, buscamos estimar los coeficientes \(\beta_0\) y \(\beta_1\) de tal manera que la probabilidad predicha \(\hat{p}(x_i)\) de un evento (por ejemplo, un incumplimiento) se acerque lo más posible al estado observado de cada individuo. Esto se formaliza mediante una función de verosimilitud, que se define como:

\[
\ell(\beta_0, \beta_1) = \prod_{i: y_i = 1} p(x_i) \prod_{i': y_{i'} = 0} (1 - p(x_{i'}))
\]

Donde \(p(x_i)\) es la probabilidad predicha de que el evento ocurra para el individuo \(i\). La estimación de MLE se obtiene eligiendo \(\hat{\beta}_0\) y \(\hat{\beta}_1\) que maximicen esta función de verosimilitud. En la práctica, esto se realiza utilizando software estadístico que implementa algoritmos iterativos para encontrar los valores de los coeficientes que maximizan la verosimilitud.","Para estimar los coeficientes β₀ y β₁ en la regresión logística, se utiliza el método de máxima verosimilitud, que maximiza la función de verosimilitud ℓ(β₀, β₁) = Π_{i: y_i=1} p(x_i) * Π_{i': y_{i'}=0} (1 - p(x_{i'})).",1.0,,0.999999999975,1.0
"In ridge regression, what is the shrinkage penalty term that is added to the Residual Sum of Squares (RSS)?","[""### 6.2.1 Ridge Regression  \nRecall from Chapter 3 that the least squares fitting procedure estimates β 0  β 1         β p using the values that minimize  \n$$R S S = \\sum _ { i = 1 } ^ { n } \\left ( y _ { i } - \\beta _ { 0 } - \\sum _ { j = 1 } ^ { p } \\beta _ { j } x _ { j } \\right ) ^ { 2 } .$$  \nRidge regression is very similar to least squares, except that the coefficients are estimated by minimizing a slightly different quantity. In particular, the ridge regression coefficient estimates ˆ β R are the values that minimize  \n$$\\sum _ { i = 1 } ^ { n } \\left ( y _ { i } - \\beta _ { 0 } - \\sum _ { j = 1 } ^ { p } \\beta _ { j } x _ { i j } \\right ) ^ { 2 } + \\lambda \\sum _ { j = 1 } ^ { p } \\beta _ { j } ^ { 2 } & = R S S + \\lambda \\sum _ { j = 1 } ^ { p } \\beta _ { j } ^ { 2 } , \\\\ \\intertext { w h e r $ \\lambda \\geq 0 $ i s a t u n i n g $ \\text { parameter} $ , $ \\text { to be determined separately} $ . }$$  \nwhere λ ≥ 0 is a tuning parameter , to be determined separately. Equation 6.5 trades off two different criteria. As with least squares, ridge regression seeks coefficient estimates that fit the data well, by making the RSS small. However, the second term, λ ∑ j β 2 j , called a shrinkage penalty , is small when β 1         β p are close to zero, and so it has the effect of shrinking the estimates of β j towards zero. The tuning parameter λ serves to control onestandarderror rule  \nridge regression tuning parameter  \nshrinkage penalty  \nFIGURE 6.4. The standardized ridge regression coefficients are displayed for the Credit data set, as a function of λ and ‖ ˆ β R λ ‖ 2 glyph[triangleleft] ‖ ˆ β ‖ 2 .  \n<!-- image -->  \nthe relative impact of these two terms on the regression coefficient estimates. When λ = 0 , the penalty term has no effect, and ridge regression will produce the least squares estimates. However, as λ →∞ , the impact of the shrinkage penalty grows, and the ridge regression coefficient estimates will approach zero. Unlike least squares, which generates only one set of coefficient estimates, ridge regression will produce a different set of coefficient estimates, ˆ β R λ , for each value of λ . Selecting a good value for λ is critical; we defer this discussion to Section 6.2.3, where we use cross-validation.\nAn Application to the Credit Data  \nNote that in (6.5), the shrinkage penalty is applied to β 1         β p , but not to the intercept β 0 . We want to shrink the estimated association of each variable with the response; however, we do not want to shrink the intercept, which is simply a measure of the mean value of the response when x i 1 = x i 2 =       = x ip = 0 . If we assume that the variables-that is, the columns of the data matrix X -have been centered to have mean zero before ridge regression is performed, then the estimated intercept will take the form ˆ β 0 = ¯ y = ∑ n i =1 y i glyph[triangleleft]n .  \nIn Figure 6.4, the ridge regression coefficient estimates for the Credit data set are displayed. In the left-hand panel, each curve corresponds to the ridge regression coefficient estimate for one of the ten variables, plotted as a function of λ . For example, the black solid line represents the ridge regression estimate for the income coefficient, as λ is varied. At the extreme left-hand side of the plot, λ is essentially zero, and so the corresponding ridge coefficient estimates are the same as the usual least squares estimates. But as λ increases, the ridge coefficient estimates shrink towards zero. When λ is extremely large, then all of the ridge coefficient estimates are basically zero; this corresponds to the null model that contains no predictors. In this plot, the income , limit , rating , and student variables are displayed in distinct colors, since these variables tend to have by far the largest coefficient estimates. While the ridge coefficient estimates tend to decrease in aggregate as λ increases, individual coefficients, such as rating and income , may occasionally increase as λ increases.  \nThe right-hand panel of Figure 6.4 displays the same ridge coefficient estimates as the left-hand panel, but instead of displaying λ on the x -axis, we now display ‖ ˆ β R λ ‖ 2 glyph[triangleleft] ‖ ˆ β ‖ 2 , where ˆ β denotes the vector of least squares coefficient estimates. The notation ‖ β ‖ 2 denotes the /lscript 2 norm (pronounced 'ell 2') of a vector, and is defined as ‖ β ‖ 2 = √ ∑ p j =1 β j 2 . It measures the distance of β from zero. As λ increases, the /lscript 2 norm of ˆ β R λ will always decrease, and so will ‖ ˆ β R λ ‖ 2 glyph[triangleleft] ‖ ˆ β ‖ 2 . The latter quantity ranges from 1 (when λ = 0 , in which case the ridge regression coefficient estimate is the same as the least squares estimate, and so their /lscript 2 norms are the same) to 0 (when λ = ∞ , in which case the ridge regression coefficient estimate is a vector of zeros, with /lscript 2 norm equal to zero). Therefore, we can think of the x -axis in the right-hand panel of Figure 6.4 as the amount that the ridge regression coefficient estimates have been shrunken towards zero; a small value indicates that they have been shrunken very close to zero.  \nThe standard least squares coefficient estimates discussed in Chapter 3 are scale equivariant : multiplying X j by a constant c simply leads to a scaling of the least squares coefficient estimates by a factor of 1 glyph[triangleleft]c . In other words, regardless of how the j th predictor is scaled, X j ˆ β j will remain the same. In contrast, the ridge regression coefficient estimates can change substantially when multiplying a given predictor by a constant. For instance, consider the income variable, which is measured in dollars. One could reasonably have measured income in thousands of dollars, which would result in a reduction in the observed values of income by a factor of 1,000. Now due to the sum of squared coefficients term in the ridge regression formulation (6.5), such a change in scale will not simply cause the ridge regression coefficient estimate for income to change by a factor of 1,000. In other words, X j ˆ β R j λ will depend not only on the value of λ , but also on the scaling of the j th predictor. In fact, the value of X j ˆ β R j λ may even depend on the scaling of the other predictors! Therefore, it is best to apply ridge regression after standardizing the predictors , using the formula  \n$$\\tilde { x } _ { i j } = \\frac { x _ { i j } } { \\sqrt { \\frac { 1 } { n } \\sum _ { i = 1 } ^ { n } ( x _ { i j } - \\bar { x } _ { j } ) ^ { 2 } } } , \\\\ \\text {are all on the same scale. In } ( 6 . 6 ) , \\, \\text { the denominator is the}$$  \nso that they are all on the same scale. In (6.6), the denominator is the estimated standard deviation of the j th predictor. Consequently, all of the standardized predictors will have a standard deviation of one. As a result the final fit will not depend on the scale on which the predictors are measured. In Figure 6.4, the y -axis displays the standardized ridge regression coefficient estimates-that is, the coefficient estimates that result from performing ridge regression using standardized predictors.\nWhy Does Ridge Regression Improve Over Least Squares?  \nRidge regression's advantage over least squares is rooted in the bias-variance trade-off . As λ increases, the flexibility of the ridge regression fit decreases, leading to decreased variance but increased bias. This is illustrated in the left-hand panel of Figure 6.5, using a simulated data set containing p = 45 predictors and n = 50 observations. The green curve in the left-hand panel  \n/lscript 2 norm scale equivariant  \nFIGURE 6.5. Squared bias (black), variance (green), and test mean squared error (purple) for the ridge regression predictions on a simulated data set, as a function of λ and ‖ ˆ β R λ ‖ 2 glyph[triangleleft] ‖ ˆ β ‖ 2 . The horizontal dashed lines indicate the minimum possible MSE. The purple crosses indicate the ridge regression models for which the MSE is smallest.  \n<!-- image -->  \nof Figure 6.5 displays the variance of the ridge regression predictions as a function of λ . At the least squares coefficient estimates, which correspond to ridge regression with λ = 0 , the variance is high but there is no bias. But as λ increases, the shrinkage of the ridge coefficient estimates leads to a substantial reduction in the variance of the predictions, at the expense of a slight increase in bias. Recall that the test mean squared error (MSE), plotted in purple, is closely related to the variance plus the squared bias. For values of λ up to about 10 , the variance decreases rapidly, with very little increase in bias, plotted in black. Consequently, the MSE drops considerably as λ increases from 0 to 10 . Beyond this point, the decrease in variance due to increasing λ slows, and the shrinkage on the coefficients causes them to be significantly underestimated, resulting in a large increase in the bias. The minimum MSE is achieved at approximately λ = 30 . Interestingly, because of its high variance, the MSE associated with the least squares fit, when λ = 0 , is almost as high as that of the null model for which all coefficient estimates are zero, when λ = ∞ . However, for an intermediate value of λ , the MSE is considerably lower.  \nIn general, in situations where the relationship between the response and the predictors is close to linear, the least squares estimates will have low bias but may have high variance. This means that a small change in the training data can cause a large change in the least squares coefficient estimates. In particular, when the number of variables p is almost as large as the number of observations n , as in the example in Figure 6.5, the least squares estimates will be extremely variable. And if p &gt; n , then the least squares estimates do not even have a unique solution, whereas ridge regression can still perform well by trading off a small increase in bias for a  \nThe right-hand panel of Figure 6.5 displays the same curves as the lefthand panel, this time plotted against the /lscript 2 norm of the ridge regression coefficient estimates divided by the /lscript 2 norm of the least squares estimates. Now as we move from left to right, the fits become more flexible, and so the bias decreases and the variance increases.  \nlarge decrease in variance. Hence, ridge regression works best in situations where the least squares estimates have high variance.  \nRidge regression also has substantial computational advantages over best subset selection, which requires searching through 2 p models. As we discussed previously, even for moderate values of p , such a search can be computationally infeasible. In contrast, for any fixed value of λ , ridge regression only fits a single model, and the model-fitting procedure can be performed quite quickly. In fact, one can show that the computations required to solve (6.5), simultaneously for all values of λ , are almost identical to those for fitting a model using least squares."", ""### 6.2.2 The Lasso  \nRidge regression does have one obvious disadvantage. Unlike best subset, forward stepwise, and backward stepwise selection, which will generally select models that involve just a subset of the variables, ridge regression will include all p predictors in the final model. The penalty λ ∑ β 2 j in (6.5) will shrink all of the coefficients towards zero, but it will not set any of them exactly to zero (unless λ = ∞ ). This may not be a problem for prediction accuracy, but it can create a challenge in model interpretation in settings in which the number of variables p is quite large. For example, in the Credit data set, it appears that the most important variables are income , limit , rating , and student . So we might wish to build a model including just these predictors. However, ridge regression will always generate a model involving all ten predictors. Increasing the value of λ will tend to reduce the magnitudes of the coefficients, but will not result in exclusion of any of the variables.  \nThe lasso is a relatively recent alternative to ridge regression that overcomes this disadvantage. The lasso coefficients, ˆ β L λ , minimize the quantity  \n$$& \\quad \\text {Conjures this disagrandage.} \\, \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\$$  \nAs with ridge regression, the lasso shrinks the coefficient estimates towards zero. However, in the case of the lasso, the /lscript 1 penalty has the effect of forcing some of the coefficient estimates to be exactly equal to zero when the tuning parameter λ is sufficiently large. Hence, much like best subset selection, the lasso performs variable selection . As a result, models generated from the lasso are generally much easier to interpret than those produced by ridge regression. We say that the lasso yields sparse models-that is, models that involve only a subset of the variables. As in ridge regression, selecting a good value of λ for the lasso is critical; we defer this discussion to Section 6.2.3, where we use cross-validation.  \nComparing (6.7) to (6.5), we see that the lasso and ridge regression have similar formulations. The only difference is that the β 2 j term in the ridge regression penalty (6.5) has been replaced by ♣ β j ♣ in the lasso penalty (6.7). In statistical parlance, the lasso uses an /lscript 1 (pronounced 'ell 1') penalty instead of an /lscript 2 penalty. The /lscript 1 norm of a coefficient vector β is given by ‖ β ‖ 1 = ∑ ♣ β j ♣ .  \nlasso sparse  \nFIGURE 6.6. The standardized lasso coefficients on the Credit data set are shown as a function of λ and ‖ ˆ β L λ ‖ 1 glyph[triangleleft] ‖ ˆ β ‖ 1 .  \n<!-- image -->  \nAs an example, consider the coefficient plots in Figure 6.6, which are generated from applying the lasso to the Credit data set. When λ = 0 , then the lasso simply gives the least squares fit, and when λ becomes sufficiently large, the lasso gives the null model in which all coefficient estimates equal zero. However, in between these two extremes, the ridge regression and lasso models are quite different from each other. Moving from left to right in the right-hand panel of Figure 6.6, we observe that at first the lasso results in a model that contains only the rating predictor. Then student and limit enter the model almost simultaneously, shortly followed by income . Eventually, the remaining variables enter the model. Hence, depending on the value of λ , the lasso can produce a model involving any number of variables. In contrast, ridge regression will always include all of the variables in the model, although the magnitude of the coefficient estimates will depend on λ .\nAnother Formulation for Ridge Regression and the Lasso  \nOne can show that the lasso and ridge regression coefficient estimates solve the problems  \n$$\\min _ { \\beta } \\left \\{ \\sum _ { i = 1 } ^ { n } \\left ( y _ { i } - \\beta _ { 0 } - \\sum _ { j = 1 } ^ { p } \\beta _ { j } x _ { i j } \\right ) ^ { 2 } \\right \\} & \\quad \\text {subject to } \\quad \\sum _ { j = 1 } ^ { p } | \\beta _ { j } | \\leq s \\\\ \\intertext { a n d }$$  \nand  \n$$\\text { and } \\\\ \\minimize _ { \\beta } \\left \\{ \\sum _ { i = 1 } ^ { n } \\left ( y _ { i } - \\beta _ { 0 } - \\sum _ { j = 1 } ^ { p } \\beta _ { j } x _ { i j } \\right ) ^ { 2 } \\right \\} \\text { subject to } \\sum _ { j = 1 } ^ { p } \\beta _ { j } ^ { 2 } \\leq s , \\\\ \\text {respectively. In other words, for every value of } \\lambda , \\text { there is some } s \\text { such that }$$  \nrespectively. In other words, for every value of λ , there is some s such that the Equations (6.7) and (6.8) will give the same lasso coefficient estimates. Similarly, for every value of λ there is a corresponding s such that Equations (6.5) and (6.9) will give the same ridge regression coefficient estimates.  \nWhen p = 2 , then (6.8) indicates that the lasso coefficient estimates have the smallest RSS out of all points that lie within the diamond defined by ♣ β 1 ♣ + ♣ β 2 ♣ ≤ s . Similarly, the ridge regression estimates have the smallest RSS out of all points that lie within the circle defined by β 2 1 + β 2 2 ≤ s .  \nThe formulations (6.8) and (6.9) reveal a close connection between the lasso, ridge regression, and best subset selection. Consider the problem  \nWe can think of (6.8) as follows. When we perform the lasso we are trying to find the set of coefficient estimates that lead to the smallest RSS, subject to the constraint that there is a budget s for how large ∑ p j =1 ♣ β j ♣ can be. When s is extremely large, then this budget is not very restrictive, and so the coefficient estimates can be large. In fact, if s is large enough that the least squares solution falls within the budget, then (6.8) will simply yield the least squares solution. In contrast, if s is small, then ∑ p j =1 ♣ β j ♣ must be small in order to avoid violating the budget. Similarly, (6.9) indicates that when we perform ridge regression, we seek a set of coefficient estimates such that the RSS is as small as possible, subject to the requirement that ∑ p j =1 β 2 j not exceed the budget s .  \n/negationslash  \nHere I ( β j = 0) is an indicator variable: it takes on a value of 1 if β j = 0 , and equals zero otherwise. Then (6.10) amounts to finding a set of coefficient estimates such that RSS is as small as possible, subject to the constraint that no more than s coefficients can be nonzero. The problem (6.10) is equivalent to best subset selection. Unfortunately, solving (6.10) is computationally infeasible when p is large, since it requires considering all ( p s ) models containing s predictors. Therefore, we can interpret ridge regression and the lasso as computationally feasible alternatives to best subset selection that replace the intractable form of the budget in (6.10) with forms that are much easier to solve. Of course, the lasso is much more closely related to best subset selection, since the lasso performs feature selection for s sufficiently small in (6.8), while ridge regression does not.  \n$$\\L a s o , \\, \\text {ride regression, and best subset selection. Consider the problem} \\\\ \\minimize & \\left \\{ \\sum _ { i = 1 } ^ { n } \\left ( y _ { i } - \\beta _ { 0 } - \\sum _ { j = 1 } ^ { p } \\beta _ { j } x _ { i j } \\right ) ^ { 2 } \\right \\} \\text { subject to } \\sum _ { j = 1 } ^ { p } I ( \\beta _ { j } \\neq 0 ) \\leq s . \\\\ \\intertext { h e r I ( \\beta _ { j } \\neq 0 ) is an indicator variable \\colon t takes on a value of 1 if \\beta _ { j } \\neq 0 , and } & \\cosh \\min \\left ( 1 - \\beta _ { 0 } \\right ) ,$$  \n/negationslash\nThe Variable Selection Property of the Lasso  \nWhy is it that the lasso, unlike ridge regression, results in coefficient estimates that are exactly equal to zero? The formulations (6.8) and (6.9) can be used to shed light on the issue. Figure 6.7 illustrates the situation. The least squares solution is marked as ˆ β , while the blue diamond and circle represent the lasso and ridge regression constraints in (6.8) and (6.9), respectively. If s is sufficiently large, then the constraint regions will contain ˆ β , and so the ridge regression and lasso estimates will be the same as the least squares estimates. (Such a large value of s corresponds to λ = 0 in (6.5) and (6.7).) However, in Figure 6.7 the least squares estimates lie outside of the diamond and the circle, and so the least squares estimates are not the same as the lasso and ridge regression estimates.  \nEach of the ellipses centered around ˆ β represents a contour : this means that all of the points on a particular ellipse have the same RSS value. As  \n/negationslash contour  \nFIGURE 6.7. Contours of the error and constraint functions for the lasso (left) and ridge regression (right) . The solid blue areas are the constraint regions, ♣ β 1 ♣ + ♣ β 2 ♣ ≤ s and β 2 1 + β 2 2 ≤ s , while the red ellipses are the contours of the RSS.  \n<!-- image -->  \nthe ellipses expand away from the least squares coefficient estimates, the RSS increases. Equations (6.8) and (6.9) indicate that the lasso and ridge regression coefficient estimates are given by the first point at which an ellipse contacts the constraint region. Since ridge regression has a circular constraint with no sharp points, this intersection will not generally occur on an axis, and so the ridge regression coefficient estimates will be exclusively non-zero. However, the lasso constraint has corners at each of the axes, and so the ellipse will often intersect the constraint region at an axis. When this occurs, one of the coefficients will equal zero. In higher dimensions, many of the coefficient estimates may equal zero simultaneously. In Figure 6.7, the intersection occurs at β 1 = 0 , and so the resulting model will only include β 2 .  \nIn Figure 6.7, we considered the simple case of p = 2 . When p = 3 , then the constraint region for ridge regression becomes a sphere, and the constraint region for the lasso becomes a polyhedron. When p &gt; 3 , the constraint for ridge regression becomes a hypersphere, and the constraint for the lasso becomes a polytope. However, the key ideas depicted in Figure 6.7 still hold. In particular, the lasso leads to feature selection when p &gt; 2 due to the sharp corners of the polyhedron or polytope.\nComparing the Lasso and Ridge Regression  \nIt is clear that the lasso has a major advantage over ridge regression, in that it produces simpler and more interpretable models that involve only a subset of the predictors. However, which method leads to better prediction accuracy? Figure 6.8 displays the variance, squared bias, and test MSE of the lasso applied to the same simulated data as in Figure 6.5. Clearly the lasso leads to qualitatively similar behavior to ridge regression, in that as λ increases, the variance decreases and the bias increases. In the right-hand  \nFIGURE 6.8. Left: Plots of squared bias (black), variance (green), and test MSE (purple) for the lasso on a simulated data set. Right: Comparison of squared bias, variance, and test MSE between lasso (solid) and ridge (dotted). Both are plotted against their R 2 on the training data, as a common form of indexing. The crosses in both plots indicate the lasso model for which the MSE is smallest.  \n<!-- image -->  \npanel of Figure 6.8, the dotted lines represent the ridge regression fits. Here we plot both against their R 2 on the training data. This is another useful way to index models, and can be used to compare models with different types of regularization, as is the case here. In this example, the lasso and ridge regression result in almost identical biases. However, the variance of ridge regression is slightly lower than the variance of the lasso. Consequently, the minimum MSE of ridge regression is slightly smaller than that of the lasso.  \nThese two examples illustrate that neither ridge regression nor the lasso will universally dominate the other. In general, one might expect the lasso to perform better in a setting where a relatively small number of predictors have substantial coefficients, and the remaining predictors have coefficients that are very small or that equal zero. Ridge regression will perform better when the response is a function of many predictors, all with coefficients of roughly equal size. However, the number of predictors that is related to the response is never known a priori for real data sets. A technique such as cross-validation can be used in order to determine which approach is better on a particular data set.  \nHowever, the data in Figure 6.8 were generated in such a way that all 45 predictors were related to the response-that is, none of the true coefficients β 1         β 45 equaled zero. The lasso implicitly assumes that a number of the coefficients truly equal zero. Consequently, it is not surprising that ridge regression outperforms the lasso in terms of prediction error in this setting. Figure 6.9 illustrates a similar situation, except that now the response is a function of only 2 out of 45 predictors. Now the lasso tends to outperform ridge regression in terms of bias, variance, and MSE.  \nAs with ridge regression, when the least squares estimates have excessively high variance, the lasso solution can yield a reduction in variance at the expense of a small increase in bias, and consequently can generate more accurate predictions. Unlike ridge regression, the lasso performs variable selection, and hence results in models that are easier to interpret.  \nFIGURE 6.9. Left: Plots of squared bias (black), variance (green), and test MSE (purple) for the lasso. The simulated data is similar to that in Figure 6.8, except that now only two predictors are related to the response. Right: Comparison of squared bias, variance, and test MSE between lasso (solid) and ridge (dotted). Both are plotted against their R 2 on the training data, as a common form of indexing. The crosses in both plots indicate the lasso model for which the MSE is smallest.  \n<!-- image -->  \nThere are very efficient algorithms for fitting both ridge and lasso models; in both cases the entire coefficient paths can be computed with about the same amount of work as a single least squares fit. We will explore this further in the lab at the end of this chapter.\nA Simple Special Case for Ridge Regression and the Lasso  \nIn order to obtain a better intuition about the behavior of ridge regression and the lasso, consider a simple special case with n = p , and X a diagonal matrix with 1 's on the diagonal and 0 's in all off-diagonal elements. To simplify the problem further, assume also that we are performing regression without an intercept. With these assumptions, the usual least squares problem simplifies to finding β 1         β p that minimize  \n$$\\sum _ { j = 1 } ^ { p } ( y _ { j } - \\beta _ { j } ) ^ { 2 } .$$  \nIn this case, the least squares solution is given by  \n$$\\hat { \\beta } _ { j } = y _ { j } .$$  \nAnd in this setting, ridge regression amounts to finding β 1         β p such that  \n$$\\sum _ { j = 1 } ^ { p } ( y _ { j } - \\beta _ { j } ) ^ { 2 } + \\lambda \\sum _ { j = 1 } ^ { p } \\beta _ { j } ^ { 2 }$$  \nis minimized, and the lasso amounts to finding the coefficients such that  \n$$\\sum _ { j = 1 } ^ { p } ( y _ { j } - \\beta _ { j } ) ^ { 2 } + \\lambda \\sum _ { j = 1 } ^ { p } | \\beta _ { j } |$$  \nFIGURE 6.10. The ridge regression and lasso coefficient estimates for a simple setting with n = p and X a diagonal matrix with 1 's on the diagonal. Left: The ridge regression coefficient estimates are shrunken proportionally towards zero, relative to the least squares estimates. Right: The lasso coefficient estimates are soft-thresholded towards zero.  \n<!-- image -->  \nis minimized. One can show that in this setting, the ridge regression estimates take the form  \n$$\\hat { \\beta } _ { j } ^ { R } = y _ { j } / ( 1 + \\lambda ) ,$$  \nand the lasso estimates take the form  \n$$\\ s o \\text { estimates take the form} \\\\ \\hat { \\beta } _ { j } ^ { L } = \\begin{cases} y _ { j } - \\lambda / 2 & \\text {if } y _ { j } > \\lambda / 2 ; \\\\ y _ { j } + \\lambda / 2 & \\text {if } y _ { j } < - \\lambda / 2 ; \\end{cases} \\\\ \\hat { 0 } & \\text {if } | y _ { j } | \\leq \\lambda / 2 . \\\\ \\intertext { 1 . 0 \\, d i s l a y s } \\text {the situation. We can see that ridge regression and } \\\\ \\text {form two very different types of shrinkage. In ridge regression}$$  \nIn the case of a more general data matrix X , the story is a little more complicated than what is depicted in Figure 6.10, but the main ideas still hold approximately: ridge regression more or less shrinks every dimension of the data by the same proportion, whereas the lasso more or less shrinks all coefficients toward zero by a similar amount, and sufficiently small coefficients are shrunken all the way to zero.  \nFigure 6.10 displays the situation. We can see that ridge regression and the lasso perform two very different types of shrinkage. In ridge regression, each least squares coefficient estimate is shrunken by the same proportion. In contrast, the lasso shrinks each least squares coefficient towards zero by a constant amount, λ glyph[triangleleft] 2 ; the least squares coefficients that are less than λ glyph[triangleleft] 2 in absolute value are shrunken entirely to zero. The type of shrinkage performed by the lasso in this simple setting (6.15) is known as softthresholding . The fact that some lasso coefficients are shrunken entirely to zero explains why the lasso performs feature selection.\nBayesian Interpretation of Ridge Regression and the Lasso  \nWe now show that one can view ridge regression and the lasso through a Bayesian lens. A Bayesian viewpoint for regression assumes that the coefficient vector β has some prior distribution, say p ( β ) , where β = ( β 0  β 1         β p ) T . The likelihood of the data can be written as f ( Y ♣ X β ) , softthresholding  \n<!-- image -->  \nFIGURE 6.11. Left: Ridge regression is the posterior mode for β under a Gaussian prior. Right: The lasso is the posterior mode for β under a double-exponential prior.  \n<!-- image -->  \nwhere X = ( X 1         X p ) . Multiplying the prior distribution by the likelihood gives us (up to a proportionality constant) the posterior distribution , which takes the form  \n$$p ( \\beta | X , Y ) \\, \\infty \\, f ( Y | X , \\beta ) p ( \\beta | X ) = f ( Y | X , \\beta ) p ( \\beta ) ,$$  \nwhere the proportionality above follows from Bayes' theorem, and the equality above follows from the assumption that X is fixed.  \nWe assume the usual linear model,  \n$$Y = \\beta _ { 0 } + X _ { 1 } \\beta _ { 1 } + \\dots + X _ { p } \\beta _ { p } + \\epsilon ,$$  \nand suppose that the errors are independent and drawn from a normal distribution. Furthermore, assume that p ( β ) = ∏ p j =1 g ( β j ) , for some density function g . It turns out that ridge regression and the lasso follow naturally from two special cases of g :  \n- If g is a Gaussian distribution with mean zero and standard deviation a function of λ , then it follows that the posterior mode for β -that is, the most likely value for β , given the data-is given by the ridge regression solution. (In fact, the ridge regression solution is also the posterior mean.)\n- If g is a double-exponential (Laplace) distribution with mean zero and scale parameter a function of λ , then it follows that the posterior mode for β is the lasso solution. (However, the lasso solution is not the posterior mean, and in fact, the posterior mean does not yield a sparse coefficient vector.)  \nThe Gaussian and double-exponential priors are displayed in Figure 6.11. Therefore, from a Bayesian viewpoint, ridge regression and the lasso follow directly from assuming the usual linear model with normal errors, together with a simple prior distribution for β . Notice that the lasso prior is steeply peaked at zero, while the Gaussian is flatter and fatter at zero. Hence, the lasso expects a priori that many of the coefficients are (exactly) zero, while ridge assumes the coefficients are randomly distributed about zero.  \nposterior distribution posterior mode  \nFIGURE 6.12. Left: Cross-validation errors that result from applying ridge regression to the Credit data set with various values of λ . Right: The coefficient estimates as a function of λ . The vertical dashed lines indicate the value of λ selected by cross-validation.  \n<!-- image -->"", '### 3.1.1 Estimating the Coefficients  \nIn practice, β 0 and β 1 are unknown. So before we can use (3.1) to make predictions, we must use data to estimate the coefficients. Let  \n$$( x _ { 1 } , y _ { 1 } ) , \\, ( x _ { 2 } , y _ { 2 } ) , \\dots , \\, ( x _ { n } , y _ { n } )$$  \nrepresent n observation pairs, each of which consists of a measurement of X and a measurement of Y . In the Advertising example, this data set consists of the TV advertising budget and product sales in n = 200 different markets. (Recall that the data are displayed in Figure 2.1.) Our goal is to obtain coefficient estimates ˆ β 0 and ˆ β 1 such that the linear model (3.1) fits the available data well-that is, so that y i ≈ ˆ β 0 + ˆ β 1 x i for i = 1         n . In other words, we want to find an intercept ˆ β 0 and a slope ˆ β 1 such that the resulting line is as close as possible to the n = 200 data points. There are a number of ways of measuring closeness . However, by far the most common approach involves minimizing the least squares criterion, and we take that approach in this chapter. Alternative approaches will be considered in Chapter 6.  \nLet ˆ y i = ˆ β 0 + ˆ β 1 x i be the prediction for Y based on the i th value of X . Then e i = y i -ˆ y i represents the i th residual -this is the difference between the i th observed response value and the i th response value that is predicted by our linear model. We define the residual sum of squares (RSS) as  \n$$R S S = e _ { 1 } ^ { 2 } + e _ { 2 } ^ { 2 } + \\dots + e _ { n } ^ { 2 } ,$$  \nor equivalently as  \n$$R S S = ( y _ { 1 } - \\hat { \\beta } _ { 0 } - \\hat { \\beta } _ { 1 } x _ { 1 } ) ^ { 2 } + ( y _ { 2 } - \\hat { \\beta } _ { 0 } - \\hat { \\beta } _ { 1 } x _ { 2 } ) ^ { 2 } + \\dots + ( y _ { n } - \\hat { \\beta } _ { 0 } - \\hat { \\beta } _ { 1 } x _ { n } ) ^ { 2 } .$$  \nThe least squares approach chooses ˆ β 0 and ˆ β 1 to minimize the RSS. Using some calculus, one can show that the minimizers are  \n$$1 \\text { can show that the minimizers are } \\\\ \\hat { \\beta } _ { 1 } = \\frac { \\sum _ { i = 1 } ^ { n } ( x _ { i } - \\bar { x } ) ( y _ { i } - \\bar { y } ) } { \\sum _ { i = 1 } ^ { n } ( x _ { i } - \\bar { x } ) ^ { 2 } } , \\\\ \\hat { \\beta } _ { 0 } = \\bar { y } - \\hat { \\beta } _ { 1 } \\bar { x } ,$$  \nintercept slope coefficient parameter least squares  \nresidual residual sum of squares  \nFIGURE 3.1. For the Advertising data, the least squares fit for the regression of sales onto TV is shown. The fit is found by minimizing the residual sum of squares. Each grey line segment represents a residual. In this case a linear fit captures the essence of the relationship, although it overestimates the trend in the left of the plot.  \n<!-- image -->  \nwhere ¯ y ≡ 1 n ∑ n i =1 y i and ¯ x ≡ 1 n ∑ n i =1 x i are the sample means. In other words, (3.4) defines the least squares coefficient estimates for simple linear regression.  \nFigure 3.1 displays the simple linear regression fit to the Advertising data, where ˆ β 0 = 7   03 and ˆ β 1 = 0   0475 . In other words, according to this approximation, an additional $1  000 spent on TV advertising is associated with selling approximately 47   5 additional units of the product. In Figure 3.2, we have computed RSS for a number of values of β 0 and β 1 , using the advertising data with sales as the response and TV as the predictor. In each plot, the red dot represents the pair of least squares estimates ( ˆ β 0  ˆ β 1 ) given by (3.4). These values clearly minimize the RSS.', ""## 11.6 Shrinkage for the Cox Model  \nIn this section, we illustrate that the shrinkage methods of Section 6.2 can be applied to the survival data setting. In particular, motivated by the 'loss+penalty' formulation of Section 6.2, we consider minimizing a penalized version of the negative log partial likelihood in (11.16),  \n$$\\text {penalized version of the negative log pair that innerkinned} \\, & \\, ( 1 . 1 6 ) , \\\\ & \\, - \\log \\left ( \\prod _ { i \\colon \\delta _ { i } = 1 } \\frac { \\exp \\left ( \\sum _ { j = 1 } ^ { p } x _ { j } \\beta _ { j } \\right ) } { \\sum _ { i ^ { \\prime } \\colon y _ { i ^ { \\prime } } \\geq y _ { i } } \\exp \\left ( \\sum _ { j = 1 } ^ { p } x _ { i ^ { \\prime } j } \\beta _ { j } \\right ) } \\right ) + \\lambda P ( \\beta ) , \\\\ \\intertext { w i t h r e s p e c t o } \\text {with respect to } \\beta = ( \\beta _ { 1 } , \\dots , \\beta _ { p } ) ^ { T } . \\text { We might take } P ( \\beta ) = \\sum _ { j = 1 } ^ { p } \\beta _ { j } ^ { 2 } , \\text { which}$$  \nIn (11.17), λ is a non-negative tuning parameter; typically we will minimize it over a range of values of λ . When λ = 0 , then minimizing (11.17) is equivalent to simply maximizing the usual Cox partial likelihood (11.16). However, when λ &gt; 0 , then minimizing (11.17) yields a shrunken version of the coefficient estimates. When λ is large, then using a ridge penalty will give small coefficients that are not exactly equal to zero. By contrast, for a with respect to β = ( β 1         β p ) T . We might take P ( β ) = ∑ p j =1 β 2 j , which corresponds to a ridge penalty, or P ( β ) = ∑ p j =1 ♣ β j ♣ , which corresponds to a lasso penalty.  \nFIGURE 11.6. For the Publication data, we display survival curves for time until publication, stratified by whether or not the study produced a positive result, after adjusting for all other covariates.  \n<!-- image -->  \nsufficiently large value of λ , using a lasso penalty will give some coefficients that are exactly equal to zero.  \nNow, how do we apply this model to the test set? This brings up an important conceptual point: in essence, there is no simple way to compare predicted survival times and true survival times on the test set. The first problem is that some of the observations are censored, and so the true survival times for those observations are unobserved. The second issue arises from the fact that in the Cox model, rather than predicting a single survival time given a covariate vector x , we instead estimate an entire survival curve, S ( t ♣ x ) , as a function of t .  \nWe now apply the lasso-penalized Cox model to the Publication data, described in Section 11.5.4. We first randomly split the 244 trials into equallysized training and test sets. The cross-validation results from the training set are shown in Figure 11.7. The 'partial likelihood deviance', shown on the y -axis, is twice the cross-validated negative log partial likelihood; it plays the role of the cross-validation error. 15 Note the 'U-shape' of the partial likelihood deviance: just as we saw in previous chapters, the crossvalidation error is minimized for an intermediate level of model complexity. Specifically, this occurs when just two predictors, budget and impact , have non-zero estimated coefficients.  \nTherefore, to assess the model fit, we must take a different approach, which involves stratifying the observations using the coefficient estimates. In particular, for each test observation, we compute the 'risk' score  \n$$\\ b u d g e t _ { i } \\cdot \\hat { \\beta } _ { b u d g e t } + \\text {impact} _ { i } \\cdot \\hat { \\beta } _ { \\text {impact} } , \\\\$$  \nwhere ˆ β budget and ˆ β impact are the coefficient estimates for these two features from the training set. We then use these risk scores to categorize the observations based on their 'risk'. For instance, the high risk group consists of the observations for which budget i · ˆ β budget + impact i · ˆ β impact is largest; by  \n15 Cross-validation for the Cox model is more involved than for linear or logistic regression, because the objective function is not a sum over the observations.  \nFIGURE 11.7. For the Publication data described in Section 11.5.4, cross-validation results for the lasso-penalized Cox model are shown. The y -axis displays the partial likelihood deviance, which plays the role of the cross-validation error. The x -axis displays the /lscript 1 norm (that is, the sum of the absolute values) of the coefficients of the lasso-penalized Cox model with tuning parameter λ , divided by the /lscript 1 norm of the coefficients of the unpenalized Cox model. The dashed line indicates the minimum cross-validation error.  \n<!-- image -->  \n(11.14), we see that these are the observations for which the instantaneous probability of being published at any moment in time is largest. In other words, the high risk group consists of the trials that are likely to be published sooner. On the Publication data, we stratify the observations into tertiles of low, medium, and high risk. The resulting survival curves for each of the three strata are displayed in Figure 11.8. We see that there is clear separation between the three strata, and that the strata are correctly ordered in terms of low, medium, and high risk of publication.""]","In ridge regression, the shrinkage penalty term that is added to the Residual Sum of Squares (RSS) is \( \lambda \sum_{j=1}^{p} \beta_{j}^{2} \). This term is used to shrink the coefficient estimates \(\beta_j\) towards zero, where \(\lambda \geq 0\) is a tuning parameter that controls the impact of the penalty.","The shrinkage penalty in ridge regression is λ times the sum of the squares of the coefficients, represented as λΣ(β_j²).",1.0,,0.9166666666361111,1.0
How does the penalty term in LASSO regression differ from that in ridge regression?,"[""### 6.2.2 The Lasso  \nRidge regression does have one obvious disadvantage. Unlike best subset, forward stepwise, and backward stepwise selection, which will generally select models that involve just a subset of the variables, ridge regression will include all p predictors in the final model. The penalty λ ∑ β 2 j in (6.5) will shrink all of the coefficients towards zero, but it will not set any of them exactly to zero (unless λ = ∞ ). This may not be a problem for prediction accuracy, but it can create a challenge in model interpretation in settings in which the number of variables p is quite large. For example, in the Credit data set, it appears that the most important variables are income , limit , rating , and student . So we might wish to build a model including just these predictors. However, ridge regression will always generate a model involving all ten predictors. Increasing the value of λ will tend to reduce the magnitudes of the coefficients, but will not result in exclusion of any of the variables.  \nThe lasso is a relatively recent alternative to ridge regression that overcomes this disadvantage. The lasso coefficients, ˆ β L λ , minimize the quantity  \n$$& \\quad \\text {Conjures this disagrandage.} \\, \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\$$  \nAs with ridge regression, the lasso shrinks the coefficient estimates towards zero. However, in the case of the lasso, the /lscript 1 penalty has the effect of forcing some of the coefficient estimates to be exactly equal to zero when the tuning parameter λ is sufficiently large. Hence, much like best subset selection, the lasso performs variable selection . As a result, models generated from the lasso are generally much easier to interpret than those produced by ridge regression. We say that the lasso yields sparse models-that is, models that involve only a subset of the variables. As in ridge regression, selecting a good value of λ for the lasso is critical; we defer this discussion to Section 6.2.3, where we use cross-validation.  \nComparing (6.7) to (6.5), we see that the lasso and ridge regression have similar formulations. The only difference is that the β 2 j term in the ridge regression penalty (6.5) has been replaced by ♣ β j ♣ in the lasso penalty (6.7). In statistical parlance, the lasso uses an /lscript 1 (pronounced 'ell 1') penalty instead of an /lscript 2 penalty. The /lscript 1 norm of a coefficient vector β is given by ‖ β ‖ 1 = ∑ ♣ β j ♣ .  \nlasso sparse  \nFIGURE 6.6. The standardized lasso coefficients on the Credit data set are shown as a function of λ and ‖ ˆ β L λ ‖ 1 glyph[triangleleft] ‖ ˆ β ‖ 1 .  \n<!-- image -->  \nAs an example, consider the coefficient plots in Figure 6.6, which are generated from applying the lasso to the Credit data set. When λ = 0 , then the lasso simply gives the least squares fit, and when λ becomes sufficiently large, the lasso gives the null model in which all coefficient estimates equal zero. However, in between these two extremes, the ridge regression and lasso models are quite different from each other. Moving from left to right in the right-hand panel of Figure 6.6, we observe that at first the lasso results in a model that contains only the rating predictor. Then student and limit enter the model almost simultaneously, shortly followed by income . Eventually, the remaining variables enter the model. Hence, depending on the value of λ , the lasso can produce a model involving any number of variables. In contrast, ridge regression will always include all of the variables in the model, although the magnitude of the coefficient estimates will depend on λ .\nAnother Formulation for Ridge Regression and the Lasso  \nOne can show that the lasso and ridge regression coefficient estimates solve the problems  \n$$\\min _ { \\beta } \\left \\{ \\sum _ { i = 1 } ^ { n } \\left ( y _ { i } - \\beta _ { 0 } - \\sum _ { j = 1 } ^ { p } \\beta _ { j } x _ { i j } \\right ) ^ { 2 } \\right \\} & \\quad \\text {subject to } \\quad \\sum _ { j = 1 } ^ { p } | \\beta _ { j } | \\leq s \\\\ \\intertext { a n d }$$  \nand  \n$$\\text { and } \\\\ \\minimize _ { \\beta } \\left \\{ \\sum _ { i = 1 } ^ { n } \\left ( y _ { i } - \\beta _ { 0 } - \\sum _ { j = 1 } ^ { p } \\beta _ { j } x _ { i j } \\right ) ^ { 2 } \\right \\} \\text { subject to } \\sum _ { j = 1 } ^ { p } \\beta _ { j } ^ { 2 } \\leq s , \\\\ \\text {respectively. In other words, for every value of } \\lambda , \\text { there is some } s \\text { such that }$$  \nrespectively. In other words, for every value of λ , there is some s such that the Equations (6.7) and (6.8) will give the same lasso coefficient estimates. Similarly, for every value of λ there is a corresponding s such that Equations (6.5) and (6.9) will give the same ridge regression coefficient estimates.  \nWhen p = 2 , then (6.8) indicates that the lasso coefficient estimates have the smallest RSS out of all points that lie within the diamond defined by ♣ β 1 ♣ + ♣ β 2 ♣ ≤ s . Similarly, the ridge regression estimates have the smallest RSS out of all points that lie within the circle defined by β 2 1 + β 2 2 ≤ s .  \nThe formulations (6.8) and (6.9) reveal a close connection between the lasso, ridge regression, and best subset selection. Consider the problem  \nWe can think of (6.8) as follows. When we perform the lasso we are trying to find the set of coefficient estimates that lead to the smallest RSS, subject to the constraint that there is a budget s for how large ∑ p j =1 ♣ β j ♣ can be. When s is extremely large, then this budget is not very restrictive, and so the coefficient estimates can be large. In fact, if s is large enough that the least squares solution falls within the budget, then (6.8) will simply yield the least squares solution. In contrast, if s is small, then ∑ p j =1 ♣ β j ♣ must be small in order to avoid violating the budget. Similarly, (6.9) indicates that when we perform ridge regression, we seek a set of coefficient estimates such that the RSS is as small as possible, subject to the requirement that ∑ p j =1 β 2 j not exceed the budget s .  \n/negationslash  \nHere I ( β j = 0) is an indicator variable: it takes on a value of 1 if β j = 0 , and equals zero otherwise. Then (6.10) amounts to finding a set of coefficient estimates such that RSS is as small as possible, subject to the constraint that no more than s coefficients can be nonzero. The problem (6.10) is equivalent to best subset selection. Unfortunately, solving (6.10) is computationally infeasible when p is large, since it requires considering all ( p s ) models containing s predictors. Therefore, we can interpret ridge regression and the lasso as computationally feasible alternatives to best subset selection that replace the intractable form of the budget in (6.10) with forms that are much easier to solve. Of course, the lasso is much more closely related to best subset selection, since the lasso performs feature selection for s sufficiently small in (6.8), while ridge regression does not.  \n$$\\L a s o , \\, \\text {ride regression, and best subset selection. Consider the problem} \\\\ \\minimize & \\left \\{ \\sum _ { i = 1 } ^ { n } \\left ( y _ { i } - \\beta _ { 0 } - \\sum _ { j = 1 } ^ { p } \\beta _ { j } x _ { i j } \\right ) ^ { 2 } \\right \\} \\text { subject to } \\sum _ { j = 1 } ^ { p } I ( \\beta _ { j } \\neq 0 ) \\leq s . \\\\ \\intertext { h e r I ( \\beta _ { j } \\neq 0 ) is an indicator variable \\colon t takes on a value of 1 if \\beta _ { j } \\neq 0 , and } & \\cosh \\min \\left ( 1 - \\beta _ { 0 } \\right ) ,$$  \n/negationslash\nThe Variable Selection Property of the Lasso  \nWhy is it that the lasso, unlike ridge regression, results in coefficient estimates that are exactly equal to zero? The formulations (6.8) and (6.9) can be used to shed light on the issue. Figure 6.7 illustrates the situation. The least squares solution is marked as ˆ β , while the blue diamond and circle represent the lasso and ridge regression constraints in (6.8) and (6.9), respectively. If s is sufficiently large, then the constraint regions will contain ˆ β , and so the ridge regression and lasso estimates will be the same as the least squares estimates. (Such a large value of s corresponds to λ = 0 in (6.5) and (6.7).) However, in Figure 6.7 the least squares estimates lie outside of the diamond and the circle, and so the least squares estimates are not the same as the lasso and ridge regression estimates.  \nEach of the ellipses centered around ˆ β represents a contour : this means that all of the points on a particular ellipse have the same RSS value. As  \n/negationslash contour  \nFIGURE 6.7. Contours of the error and constraint functions for the lasso (left) and ridge regression (right) . The solid blue areas are the constraint regions, ♣ β 1 ♣ + ♣ β 2 ♣ ≤ s and β 2 1 + β 2 2 ≤ s , while the red ellipses are the contours of the RSS.  \n<!-- image -->  \nthe ellipses expand away from the least squares coefficient estimates, the RSS increases. Equations (6.8) and (6.9) indicate that the lasso and ridge regression coefficient estimates are given by the first point at which an ellipse contacts the constraint region. Since ridge regression has a circular constraint with no sharp points, this intersection will not generally occur on an axis, and so the ridge regression coefficient estimates will be exclusively non-zero. However, the lasso constraint has corners at each of the axes, and so the ellipse will often intersect the constraint region at an axis. When this occurs, one of the coefficients will equal zero. In higher dimensions, many of the coefficient estimates may equal zero simultaneously. In Figure 6.7, the intersection occurs at β 1 = 0 , and so the resulting model will only include β 2 .  \nIn Figure 6.7, we considered the simple case of p = 2 . When p = 3 , then the constraint region for ridge regression becomes a sphere, and the constraint region for the lasso becomes a polyhedron. When p &gt; 3 , the constraint for ridge regression becomes a hypersphere, and the constraint for the lasso becomes a polytope. However, the key ideas depicted in Figure 6.7 still hold. In particular, the lasso leads to feature selection when p &gt; 2 due to the sharp corners of the polyhedron or polytope.\nComparing the Lasso and Ridge Regression  \nIt is clear that the lasso has a major advantage over ridge regression, in that it produces simpler and more interpretable models that involve only a subset of the predictors. However, which method leads to better prediction accuracy? Figure 6.8 displays the variance, squared bias, and test MSE of the lasso applied to the same simulated data as in Figure 6.5. Clearly the lasso leads to qualitatively similar behavior to ridge regression, in that as λ increases, the variance decreases and the bias increases. In the right-hand  \nFIGURE 6.8. Left: Plots of squared bias (black), variance (green), and test MSE (purple) for the lasso on a simulated data set. Right: Comparison of squared bias, variance, and test MSE between lasso (solid) and ridge (dotted). Both are plotted against their R 2 on the training data, as a common form of indexing. The crosses in both plots indicate the lasso model for which the MSE is smallest.  \n<!-- image -->  \npanel of Figure 6.8, the dotted lines represent the ridge regression fits. Here we plot both against their R 2 on the training data. This is another useful way to index models, and can be used to compare models with different types of regularization, as is the case here. In this example, the lasso and ridge regression result in almost identical biases. However, the variance of ridge regression is slightly lower than the variance of the lasso. Consequently, the minimum MSE of ridge regression is slightly smaller than that of the lasso.  \nThese two examples illustrate that neither ridge regression nor the lasso will universally dominate the other. In general, one might expect the lasso to perform better in a setting where a relatively small number of predictors have substantial coefficients, and the remaining predictors have coefficients that are very small or that equal zero. Ridge regression will perform better when the response is a function of many predictors, all with coefficients of roughly equal size. However, the number of predictors that is related to the response is never known a priori for real data sets. A technique such as cross-validation can be used in order to determine which approach is better on a particular data set.  \nHowever, the data in Figure 6.8 were generated in such a way that all 45 predictors were related to the response-that is, none of the true coefficients β 1         β 45 equaled zero. The lasso implicitly assumes that a number of the coefficients truly equal zero. Consequently, it is not surprising that ridge regression outperforms the lasso in terms of prediction error in this setting. Figure 6.9 illustrates a similar situation, except that now the response is a function of only 2 out of 45 predictors. Now the lasso tends to outperform ridge regression in terms of bias, variance, and MSE.  \nAs with ridge regression, when the least squares estimates have excessively high variance, the lasso solution can yield a reduction in variance at the expense of a small increase in bias, and consequently can generate more accurate predictions. Unlike ridge regression, the lasso performs variable selection, and hence results in models that are easier to interpret.  \nFIGURE 6.9. Left: Plots of squared bias (black), variance (green), and test MSE (purple) for the lasso. The simulated data is similar to that in Figure 6.8, except that now only two predictors are related to the response. Right: Comparison of squared bias, variance, and test MSE between lasso (solid) and ridge (dotted). Both are plotted against their R 2 on the training data, as a common form of indexing. The crosses in both plots indicate the lasso model for which the MSE is smallest.  \n<!-- image -->  \nThere are very efficient algorithms for fitting both ridge and lasso models; in both cases the entire coefficient paths can be computed with about the same amount of work as a single least squares fit. We will explore this further in the lab at the end of this chapter.\nA Simple Special Case for Ridge Regression and the Lasso  \nIn order to obtain a better intuition about the behavior of ridge regression and the lasso, consider a simple special case with n = p , and X a diagonal matrix with 1 's on the diagonal and 0 's in all off-diagonal elements. To simplify the problem further, assume also that we are performing regression without an intercept. With these assumptions, the usual least squares problem simplifies to finding β 1         β p that minimize  \n$$\\sum _ { j = 1 } ^ { p } ( y _ { j } - \\beta _ { j } ) ^ { 2 } .$$  \nIn this case, the least squares solution is given by  \n$$\\hat { \\beta } _ { j } = y _ { j } .$$  \nAnd in this setting, ridge regression amounts to finding β 1         β p such that  \n$$\\sum _ { j = 1 } ^ { p } ( y _ { j } - \\beta _ { j } ) ^ { 2 } + \\lambda \\sum _ { j = 1 } ^ { p } \\beta _ { j } ^ { 2 }$$  \nis minimized, and the lasso amounts to finding the coefficients such that  \n$$\\sum _ { j = 1 } ^ { p } ( y _ { j } - \\beta _ { j } ) ^ { 2 } + \\lambda \\sum _ { j = 1 } ^ { p } | \\beta _ { j } |$$  \nFIGURE 6.10. The ridge regression and lasso coefficient estimates for a simple setting with n = p and X a diagonal matrix with 1 's on the diagonal. Left: The ridge regression coefficient estimates are shrunken proportionally towards zero, relative to the least squares estimates. Right: The lasso coefficient estimates are soft-thresholded towards zero.  \n<!-- image -->  \nis minimized. One can show that in this setting, the ridge regression estimates take the form  \n$$\\hat { \\beta } _ { j } ^ { R } = y _ { j } / ( 1 + \\lambda ) ,$$  \nand the lasso estimates take the form  \n$$\\ s o \\text { estimates take the form} \\\\ \\hat { \\beta } _ { j } ^ { L } = \\begin{cases} y _ { j } - \\lambda / 2 & \\text {if } y _ { j } > \\lambda / 2 ; \\\\ y _ { j } + \\lambda / 2 & \\text {if } y _ { j } < - \\lambda / 2 ; \\end{cases} \\\\ \\hat { 0 } & \\text {if } | y _ { j } | \\leq \\lambda / 2 . \\\\ \\intertext { 1 . 0 \\, d i s l a y s } \\text {the situation. We can see that ridge regression and } \\\\ \\text {form two very different types of shrinkage. In ridge regression}$$  \nIn the case of a more general data matrix X , the story is a little more complicated than what is depicted in Figure 6.10, but the main ideas still hold approximately: ridge regression more or less shrinks every dimension of the data by the same proportion, whereas the lasso more or less shrinks all coefficients toward zero by a similar amount, and sufficiently small coefficients are shrunken all the way to zero.  \nFigure 6.10 displays the situation. We can see that ridge regression and the lasso perform two very different types of shrinkage. In ridge regression, each least squares coefficient estimate is shrunken by the same proportion. In contrast, the lasso shrinks each least squares coefficient towards zero by a constant amount, λ glyph[triangleleft] 2 ; the least squares coefficients that are less than λ glyph[triangleleft] 2 in absolute value are shrunken entirely to zero. The type of shrinkage performed by the lasso in this simple setting (6.15) is known as softthresholding . The fact that some lasso coefficients are shrunken entirely to zero explains why the lasso performs feature selection.\nBayesian Interpretation of Ridge Regression and the Lasso  \nWe now show that one can view ridge regression and the lasso through a Bayesian lens. A Bayesian viewpoint for regression assumes that the coefficient vector β has some prior distribution, say p ( β ) , where β = ( β 0  β 1         β p ) T . The likelihood of the data can be written as f ( Y ♣ X β ) , softthresholding  \n<!-- image -->  \nFIGURE 6.11. Left: Ridge regression is the posterior mode for β under a Gaussian prior. Right: The lasso is the posterior mode for β under a double-exponential prior.  \n<!-- image -->  \nwhere X = ( X 1         X p ) . Multiplying the prior distribution by the likelihood gives us (up to a proportionality constant) the posterior distribution , which takes the form  \n$$p ( \\beta | X , Y ) \\, \\infty \\, f ( Y | X , \\beta ) p ( \\beta | X ) = f ( Y | X , \\beta ) p ( \\beta ) ,$$  \nwhere the proportionality above follows from Bayes' theorem, and the equality above follows from the assumption that X is fixed.  \nWe assume the usual linear model,  \n$$Y = \\beta _ { 0 } + X _ { 1 } \\beta _ { 1 } + \\dots + X _ { p } \\beta _ { p } + \\epsilon ,$$  \nand suppose that the errors are independent and drawn from a normal distribution. Furthermore, assume that p ( β ) = ∏ p j =1 g ( β j ) , for some density function g . It turns out that ridge regression and the lasso follow naturally from two special cases of g :  \n- If g is a Gaussian distribution with mean zero and standard deviation a function of λ , then it follows that the posterior mode for β -that is, the most likely value for β , given the data-is given by the ridge regression solution. (In fact, the ridge regression solution is also the posterior mean.)\n- If g is a double-exponential (Laplace) distribution with mean zero and scale parameter a function of λ , then it follows that the posterior mode for β is the lasso solution. (However, the lasso solution is not the posterior mean, and in fact, the posterior mean does not yield a sparse coefficient vector.)  \nThe Gaussian and double-exponential priors are displayed in Figure 6.11. Therefore, from a Bayesian viewpoint, ridge regression and the lasso follow directly from assuming the usual linear model with normal errors, together with a simple prior distribution for β . Notice that the lasso prior is steeply peaked at zero, while the Gaussian is flatter and fatter at zero. Hence, the lasso expects a priori that many of the coefficients are (exactly) zero, while ridge assumes the coefficients are randomly distributed about zero.  \nposterior distribution posterior mode  \nFIGURE 6.12. Left: Cross-validation errors that result from applying ridge regression to the Credit data set with various values of λ . Right: The coefficient estimates as a function of λ . The vertical dashed lines indicate the value of λ selected by cross-validation.  \n<!-- image -->"", ""## 11.6 Shrinkage for the Cox Model  \nIn this section, we illustrate that the shrinkage methods of Section 6.2 can be applied to the survival data setting. In particular, motivated by the 'loss+penalty' formulation of Section 6.2, we consider minimizing a penalized version of the negative log partial likelihood in (11.16),  \n$$\\text {penalized version of the negative log pair that innerkinned} \\, & \\, ( 1 . 1 6 ) , \\\\ & \\, - \\log \\left ( \\prod _ { i \\colon \\delta _ { i } = 1 } \\frac { \\exp \\left ( \\sum _ { j = 1 } ^ { p } x _ { j } \\beta _ { j } \\right ) } { \\sum _ { i ^ { \\prime } \\colon y _ { i ^ { \\prime } } \\geq y _ { i } } \\exp \\left ( \\sum _ { j = 1 } ^ { p } x _ { i ^ { \\prime } j } \\beta _ { j } \\right ) } \\right ) + \\lambda P ( \\beta ) , \\\\ \\intertext { w i t h r e s p e c t o } \\text {with respect to } \\beta = ( \\beta _ { 1 } , \\dots , \\beta _ { p } ) ^ { T } . \\text { We might take } P ( \\beta ) = \\sum _ { j = 1 } ^ { p } \\beta _ { j } ^ { 2 } , \\text { which}$$  \nIn (11.17), λ is a non-negative tuning parameter; typically we will minimize it over a range of values of λ . When λ = 0 , then minimizing (11.17) is equivalent to simply maximizing the usual Cox partial likelihood (11.16). However, when λ &gt; 0 , then minimizing (11.17) yields a shrunken version of the coefficient estimates. When λ is large, then using a ridge penalty will give small coefficients that are not exactly equal to zero. By contrast, for a with respect to β = ( β 1         β p ) T . We might take P ( β ) = ∑ p j =1 β 2 j , which corresponds to a ridge penalty, or P ( β ) = ∑ p j =1 ♣ β j ♣ , which corresponds to a lasso penalty.  \nFIGURE 11.6. For the Publication data, we display survival curves for time until publication, stratified by whether or not the study produced a positive result, after adjusting for all other covariates.  \n<!-- image -->  \nsufficiently large value of λ , using a lasso penalty will give some coefficients that are exactly equal to zero.  \nNow, how do we apply this model to the test set? This brings up an important conceptual point: in essence, there is no simple way to compare predicted survival times and true survival times on the test set. The first problem is that some of the observations are censored, and so the true survival times for those observations are unobserved. The second issue arises from the fact that in the Cox model, rather than predicting a single survival time given a covariate vector x , we instead estimate an entire survival curve, S ( t ♣ x ) , as a function of t .  \nWe now apply the lasso-penalized Cox model to the Publication data, described in Section 11.5.4. We first randomly split the 244 trials into equallysized training and test sets. The cross-validation results from the training set are shown in Figure 11.7. The 'partial likelihood deviance', shown on the y -axis, is twice the cross-validated negative log partial likelihood; it plays the role of the cross-validation error. 15 Note the 'U-shape' of the partial likelihood deviance: just as we saw in previous chapters, the crossvalidation error is minimized for an intermediate level of model complexity. Specifically, this occurs when just two predictors, budget and impact , have non-zero estimated coefficients.  \nTherefore, to assess the model fit, we must take a different approach, which involves stratifying the observations using the coefficient estimates. In particular, for each test observation, we compute the 'risk' score  \n$$\\ b u d g e t _ { i } \\cdot \\hat { \\beta } _ { b u d g e t } + \\text {impact} _ { i } \\cdot \\hat { \\beta } _ { \\text {impact} } , \\\\$$  \nwhere ˆ β budget and ˆ β impact are the coefficient estimates for these two features from the training set. We then use these risk scores to categorize the observations based on their 'risk'. For instance, the high risk group consists of the observations for which budget i · ˆ β budget + impact i · ˆ β impact is largest; by  \n15 Cross-validation for the Cox model is more involved than for linear or logistic regression, because the objective function is not a sum over the observations.  \nFIGURE 11.7. For the Publication data described in Section 11.5.4, cross-validation results for the lasso-penalized Cox model are shown. The y -axis displays the partial likelihood deviance, which plays the role of the cross-validation error. The x -axis displays the /lscript 1 norm (that is, the sum of the absolute values) of the coefficients of the lasso-penalized Cox model with tuning parameter λ , divided by the /lscript 1 norm of the coefficients of the unpenalized Cox model. The dashed line indicates the minimum cross-validation error.  \n<!-- image -->  \n(11.14), we see that these are the observations for which the instantaneous probability of being published at any moment in time is largest. In other words, the high risk group consists of the trials that are likely to be published sooner. On the Publication data, we stratify the observations into tertiles of low, medium, and high risk. The resulting survival curves for each of the three strata are displayed in Figure 11.8. We see that there is clear separation between the three strata, and that the strata are correctly ordered in terms of low, medium, and high risk of publication.""]","In LASSO regression, the penalty term is based on the L1 norm, which is the sum of the absolute values of the coefficients. This is expressed as:

\[
\sum_{j=1}^{p} |\beta_j|
\]

In contrast, ridge regression uses the L2 norm for its penalty term, which is the sum of the squares of the coefficients:

\[
\sum_{j=1}^{p} \beta_j^2
\]

The key difference is that the L1 penalty in LASSO can force some coefficients to be exactly zero, leading to a sparse model with variable selection. On the other hand, the L2 penalty in ridge regression shrinks the coefficients towards zero but does not set any of them exactly to zero, resulting in a model that includes all predictors.","The LASSO regression uses an L1 penalty, which is λ times the sum of the absolute values of the coefficients (λΣ|β_j|), whereas ridge regression uses an L2 penalty (λΣβ_j²).",1.0,,0.99999999995,1.0
What is the formula for the K-Nearest Neighbors (KNN) regression predictor?,"[""## 3.5 Comparison of Linear Regression with K -Nearest Neighbors  \nAs discussed in Chapter 2, linear regression is an example of a parametric approach because it assumes a linear functional form for f ( X ) . Parametric methods have several advantages. They are often easy to fit, because one need estimate only a small number of coefficients. In the case of linear regression, the coefficients have simple interpretations, and tests of statistical significance can be easily performed. But parametric methods do have a disadvantage: by construction, they make strong assumptions about the form of f ( X ) . If the specified functional form is far from the truth, and prediction accuracy is our goal, then the parametric method will perform poorly. For instance, if we assume a linear relationship between X and Y but the true relationship is far from linear, then the resulting model will provide a poor fit to the data, and any conclusions drawn from it will be suspect.  \nIn contrast, non-parametric methods do not explicitly assume a parametric form for f ( X ) , and thereby provide an alternative and more flexible approach for performing regression. We discuss various non-parametric methods in this book. Here we consider one of the simplest and best-known non-parametric methods, K -nearest neighbors regression (KNN regression). The KNN regression method is closely related to the KNN classifier discussed in Chapter 2. Given a value for K and a prediction point x 0 , KNN regression first identifies the K training observations that are closest to x 0 , represented by N 0 . It then estimates f ( x 0 ) using the average of all the training responses in N 0 . In other words,  \n$$\\hat { f } ( x _ { 0 } ) = \\frac { 1 } { K } \\sum _ { x _ { i } \\in \\mathcal { N } _ { 0 } } y _ { i } .$$  \nFigure 3.16 illustrates two KNN fits on a data set with p = 2 predictors. The fit with K = 1 is shown in the left-hand panel, while the right-hand panel corresponds to K = 9 . We see that when K = 1 , the KNN fit perfectly interpolates the training observations, and consequently takes the form of a step function. When K = 9 , the KNN fit still is a step function, but averaging over nine observations results in much smaller regions of constant prediction, and consequently a smoother fit. In general, the optimal value for K will depend on the bias-variance tradeoff , which we introduced in Chapter 2. A small value for K provides the most flexible fit, which will have low bias but high variance. This variance is due to the fact that the prediction in a given region is entirely dependent on just one observation.  \nK -nearest neighbors regression  \nFIGURE 3.16. Plots of ˆ f ( X ) using KNN regression on a two-dimensional data set with 64 observations (orange dots). Left: K = 1 results in a rough step function fit. Right: K = 9 produces a much smoother fit.  \n<!-- image -->  \nIn contrast, larger values of K provide a smoother and less variable fit; the prediction in a region is an average of several points, and so changing one observation has a smaller effect. However, the smoothing may cause bias by masking some of the structure in f ( X ) . In Chapter 5, we introduce several approaches for estimating test error rates. These methods can be used to identify the optimal value of K in KNN regression.  \nIn practice, the true relationship between X and Y is rarely exactly linear. Figure 3.19 examines the relative performances of least squares regression and KNN under increasing levels of non-linearity in the relationship between X and Y . In the top row, the true relationship is nearly linear. In this case we see that the test MSE for linear regression is still superior  \nIn what setting will a parametric approach such as least squares linear regression outperform a non-parametric approach such as KNN regression? The answer is simple: the parametric approach will outperform the nonparametric approach if the parametric form that has been selected is close to the true form of f . Figure 3.17 provides an example with data generated from a one-dimensional linear regression model. The black solid lines represent f ( X ) , while the blue curves correspond to the KNN fits using K = 1 and K = 9 . In this case, the K = 1 predictions are far too variable, while the smoother K = 9 fit is much closer to f ( X ) . However, since the true relationship is linear, it is hard for a non-parametric approach to compete with linear regression: a non-parametric approach incurs a cost in variance that is not offset by a reduction in bias. The blue dashed line in the lefthand panel of Figure 3.18 represents the linear regression fit to the same data. It is almost perfect. The right-hand panel of Figure 3.18 reveals that linear regression outperforms KNN for this data. The green solid line, plotted as a function of 1 glyph[triangleleft]K , represents the test set mean squared error (MSE) for KNN. The KNN errors are well above the black dashed line, which is the test MSE for linear regression. When the value of K is large, then KNN performs only a little worse than least squares regression in terms of MSE. It performs far worse when K is small.  \n<!-- image -->  \nx  \nx  \nFIGURE 3.17. Plots of ˆ f ( X ) using KNN regression on a one-dimensional data set with 50 observations. The true relationship is given by the black solid line. Left: The blue curve corresponds to K = 1 and interpolates (i.e. passes directly through) the training data. Right: The blue curve corresponds to K = 9 , and represents a smoother fit.  \n<!-- image -->  \nFIGURE 3.18. The same data set shown in Figure 3.17 is investigated further. Left: The blue dashed line is the least squares fit to the data. Since f ( X ) is in fact linear (displayed as the black line), the least squares regression line provides a very good estimate of f ( X ) . Right: The dashed horizontal line represents the least squares test set MSE, while the green solid line corresponds to the MSE for KNN as a function of 1 glyph[triangleleft]K (on the log scale). Linear regression achieves a lower test MSE than does KNN regression, since f ( X ) is in fact linear. For KNN regression, the best results occur with a very large value of K , corresponding to a small value of 1 glyph[triangleleft]K .  \nFIGURE 3.19. Top Left: In a setting with a slightly non-linear relationship between X and Y (solid black line), the KNN fits with K = 1 (blue) and K = 9 (red) are displayed. Top Right: For the slightly non-linear data, the test set MSE for least squares regression (horizontal black) and KNN with various values of 1 glyph[triangleleft]K (green) are displayed. Bottom Left and Bottom Right: As in the top panel, but with a strongly non-linear relationship between X and Y .  \n<!-- image -->  \nto that of KNN for low values of K . However, for K ≥ 4 , KNN outperforms linear regression. The second row illustrates a more substantial deviation from linearity. In this situation, KNN substantially outperforms linear regression for all values of K . Note that as the extent of non-linearity increases, there is little change in the test set MSE for the non-parametric KNN method, but there is a large increase in the test set MSE of linear regression.  \nFigures 3.18 and 3.19 display situations in which KNN performs slightly worse than linear regression when the relationship is linear, but much better than linear regression for nonlinear situations. In a real life situation in which the true relationship is unknown, one might suspect that KNN should be favored over linear regression because it will at worst be slightly inferior to linear regression if the true relationship is linear, and may give substantially better results if the true relationship is non-linear. But in reality, even when the true relationship is highly non-linear, KNN may still provide inferior results to linear regression. In particular, both Figures 3.18  \nFIGURE 3.20. Test MSE for linear regression (black dashed lines) and KNN (green curves) as the number of variables p increases. The true function is nonlinear in the first variable, as in the lower panel in Figure 3.19, and does not depend on the additional variables. The performance of linear regression deteriorates slowly in the presence of these additional noise variables, whereas KNN's performance degrades much more quickly as p increases.  \n<!-- image -->  \nand 3.19 illustrate settings with p = 1 predictor. But in higher dimensions, KNN often performs worse than linear regression.  \nEven when the dimension is small, we might prefer linear regression to KNN from an interpretability standpoint. If the test MSE of KNN is only slightly lower than that of linear regression, we might be willing to forego a little bit of prediction accuracy for the sake of a simple model that can be described in terms of just a few coefficients, and for which p -values are available.  \nFigure 3.20 considers the same strongly non-linear situation as in the second row of Figure 3.19, except that we have added additional noise predictors that are not associated with the response. When p = 1 or p = 2 , KNN outperforms linear regression. But for p = 3 the results are mixed, and for p ≥ 4 linear regression is superior to KNN. In fact, the increase in dimension has only caused a small deterioration in the linear regression test set MSE, but it has caused more than a ten-fold increase in the MSE for KNN. This decrease in performance as the dimension increases is a common problem for KNN, and results from the fact that in higher dimensions there is effectively a reduction in sample size. In this data set there are 50 training observations; when p = 1 , this provides enough information to accurately estimate f ( X ) . However, spreading 50 observations over p = 20 dimensions results in a phenomenon in which a given observation has no nearby neighbors -this is the so-called curse of dimensionality . That is, the K observations that are nearest to a given test observation x 0 may be very far away from x 0 in p -dimensional space when p is large, leading to a very poor prediction of f ( x 0 ) and hence a poor KNN fit. As a general rule, parametric methods will tend to outperform non-parametric approaches when there is a small number of observations per predictor.  \ncurse of dimensionality"", ""### 2.2.3 The Classification Setting  \nThus far, our discussion of model accuracy has been focused on the regression setting. But many of the concepts that we have encountered, such as the bias-variance trade-off, transfer over to the classification setting with only some modifications due to the fact that y i is no longer quantitative. Suppose that we seek to estimate f on the basis of training observations ¶ ( x 1  y 1 )         ( x n  y n ) ♦ , where now y 1         y n are qualitative. The most common approach for quantifying the accuracy of our estimate ˆ f is the training error rate , the proportion of mistakes that are made if we apply bias-variance trade-off  \nerror rate  \nour estimate ˆ f to the training observations:  \n/negationslash  \n$$\\frac { 1 } { n } \\sum _ { i = 1 } ^ { n } I ( y _ { i } \\neq \\hat { y } _ { i } ) .$$  \n/negationslash  \n/negationslash  \n/negationslash  \nEquation 2.8 is referred to as the training error rate because it is computed based on the data that was used to train our classifier. As in the regression setting, we are most interested in the error rates that result from applying our classifier to test observations that were not used in training. The test error rate associated with a set of test observations of the form ( x 0  y 0 ) is given by  \nHere ˆ y i is the predicted class label for the i th observation using ˆ f . And I ( y i = ˆ y i ) is an indicator variable that equals 1 if y i = ˆ y i and zero if y i = ˆ y i . If I ( y i = ˆ y i ) = 0 then the i th observation was classified correctly by our classification method; otherwise it was misclassified. Hence Equation 2.8 computes the fraction of incorrect classifications.  \n$$\\ A v e \\left ( I ( y _ { 0 } \\neq \\hat { y } _ { 0 } ) \\right ) ,$$  \nwhere ˆ y 0 is the predicted class label that results from applying the classifier to the test observation with predictor x 0 . A good classifier is one for which the test error (2.9) is smallest.\nThe Bayes Classifier  \nIt is possible to show (though the proof is outside of the scope of this book) that the test error rate given in (2.9) is minimized, on average, by a very simple classifier that assigns each observation to the most likely class, given its predictor values . In other words, we should simply assign a test observation with predictor vector x 0 to the class j for which  \n$$\\Pr ( Y = j | X = x _ { 0 } )$$  \nis largest. Note that (2.10) is a conditional probability : it is the probability that Y = j , given the observed predictor vector x 0 . This very simple classifier is called the Bayes classifier . In a two-class problem where there are only two possible response values, say class 1 or class 2 , the Bayes classifier corresponds to predicting class one if Pr( Y = 1 ♣ X = x 0 ) &gt; 0   5 , and class two otherwise.  \nFigure 2.13 provides an example using a simulated data set in a twodimensional space consisting of predictors X 1 and X 2 . The orange and blue circles correspond to training observations that belong to two different classes. For each value of X 1 and X 2 , there is a different probability of the response being orange or blue. Since this is simulated data, we know how the data were generated and we can calculate the conditional probabilities for each value of X 1 and X 2 . The orange shaded region reflects the set of points for which Pr( Y = orange ♣ X ) is greater than 50 % , while the blue shaded region indicates the set of points for which the probability is below 50 % . The purple dashed line represents the points where the probability is exactly 50 % . This is called the Bayes decision boundary . The Bayes classifier's prediction is determined by the Bayes decision boundary; an observation that falls on the orange side of the boundary will be assigned  \n/negationslash indicator variable  \ntraining error test error  \nconditional probability Bayes classifier  \nBayes decision boundary  \nFIGURE 2.13. A simulated data set consisting of 100 observations in each of two groups, indicated in blue and in orange. The purple dashed line represents the Bayes decision boundary. The orange background grid indicates the region in which a test observation will be assigned to the orange class, and the blue background grid indicates the region in which a test observation will be assigned to the blue class.  \n<!-- image -->  \nto the orange class, and similarly an observation on the blue side of the boundary will be assigned to the blue class.  \nThe Bayes classifier produces the lowest possible test error rate, called the Bayes error rate . Since the Bayes classifier will always choose the class for which (2.10) is largest, the error rate will be 1 -max j Pr( Y = j ♣ X = x 0 ) at X = x 0 . In general, the overall Bayes error rate is given by  \n$$1 - E \\left ( \\max _ { j } \\Pr ( Y = j | X ) \\right ) ,$$  \nwhere the expectation averages the probability over all possible values of X . For our simulated data, the Bayes error rate is 0   133 . It is greater than zero, because the classes overlap in the true population, which implies that max j Pr( Y = j ♣ X = x 0 ) &lt; 1 for some values of x 0 . The Bayes error rate is analogous to the irreducible error, discussed earlier.\nK -Nearest Neighbors  \nIn theory we would always like to predict qualitative responses using the Bayes classifier. But for real data, we do not know the conditional distribution of Y given X , and so computing the Bayes classifier is impossible. Therefore, the Bayes classifier serves as an unattainable gold standard against which to compare other methods. Many approaches attempt to estimate the conditional distribution of Y given X , and then classify a given observation to the class with highest estimated probability. One such method is the K -nearest neighbors (KNN) classifier. Given a positive in-  \nBayes error rate  \nK -nearest neighbors  \nteger K and a test observation x 0 , the KNN classifier first identifies the K points in the training data that are closest to x 0 , represented by N 0 . It then estimates the conditional probability for class j as the fraction of points in N 0 whose response values equal j :  \n$$\\Pr ( Y = j | X = x _ { 0 } ) = \\frac { 1 } { K } \\sum _ { i \\in \\mathcal { N } _ { ( 0 ) } } I ( y _ { i } = j ) .$$  \nFinally, KNN classifies the test observation x 0 to the class with the largest probability from (2.12).  \nDespite the fact that it is a very simple approach, KNN can often produce classifiers that are surprisingly close to the optimal Bayes classifier. Figure 2.15 displays the KNN decision boundary, using K = 10 , when applied to the larger simulated data set from Figure 2.13. Notice that even though the true distribution is not known by the KNN classifier, the KNN decision boundary is very close to that of the Bayes classifier. The test error rate using KNN is 0   1363 , which is close to the Bayes error rate of 0   1304 .  \nFigure 2.14 provides an illustrative example of the KNN approach. In the left-hand panel, we have plotted a small training data set consisting of six blue and six orange observations. Our goal is to make a prediction for the point labeled by the black cross. Suppose that we choose K = 3 . Then KNN will first identify the three observations that are closest to the cross. This neighborhood is shown as a circle. It consists of two blue points and one orange point, resulting in estimated probabilities of 2 glyph[triangleleft] 3 for the blue class and 1 glyph[triangleleft] 3 for the orange class. Hence KNN will predict that the black cross belongs to the blue class. In the right-hand panel of Figure 2.14 we have applied the KNN approach with K = 3 at all of the possible values for X 1 and X 2 , and have drawn in the corresponding KNN decision boundary.  \nThe choice of K has a drastic effect on the KNN classifier obtained. Figure 2.16 displays two KNN fits to the simulated data from Figure 2.13, using K = 1 and K = 100 . When K = 1 , the decision boundary is overly flexible and finds patterns in the data that don't correspond to the Bayes decision boundary. This corresponds to a classifier that has low bias but very high variance. As K grows, the method becomes less flexible and produces a decision boundary that is close to linear. This corresponds to a low-variance but high-bias classifier. On this simulated data set, neither K = 1 nor K = 100 give good predictions: they have test error rates of 0   1695 and 0   1925 , respectively.  \nJust as in the regression setting, there is not a strong relationship between the training error rate and the test error rate. With K = 1 , the KNN training error rate is 0 , but the test error rate may be quite high. In general, as we use more flexible classification methods, the training error rate will decline but the test error rate may not. In Figure 2.17, we have plotted the KNN test and training errors as a function of 1 glyph[triangleleft]K . As 1 glyph[triangleleft]K increases, the method becomes more flexible. As in the regression setting, the training error rate consistently declines as the flexibility increases. However, the test error exhibits a characteristic U-shape, declining at first (with a minimum at approximately K = 10 ) before increasing again when the method becomes excessively flexible and overfits.  \nFIGURE 2.14. The KNN approach, using K = 3 , is illustrated in a simple situation with six blue observations and six orange observations. Left: a test observation at which a predicted class label is desired is shown as a black cross. The three closest points to the test observation are identified, and it is predicted that the test observation belongs to the most commonly-occurring class, in this case blue. Right: The KNN decision boundary for this example is shown in black. The blue grid indicates the region in which a test observation will be assigned to the blue class, and the orange grid indicates the region in which it will be assigned to the orange class.  \n<!-- image -->  \nKNN: K=10  \nFIGURE 2.15. The black curve indicates the KNN decision boundary on the data from Figure 2.13, using K = 10 . The Bayes decision boundary is shown as a purple dashed line. The KNN and Bayes decision boundaries are very similar.  \n<!-- image -->  \nIn both the regression and classification settings, choosing the correct level of flexibility is critical to the success of any statistical learning method. The bias-variance tradeoff, and the resulting U-shape in the test error, can make this a difficult task. In Chapter 5, we return to this topic and discuss  \nFIGURE 2.16. A comparison of the KNN decision boundaries (solid black curves) obtained using K = 1 and K = 100 on the data from Figure 2.13. With K = 1 , the decision boundary is overly flexible, while with K = 100 it is not sufficiently flexible. The Bayes decision boundary is shown as a purple dashed line.  \n<!-- image -->  \nFIGURE 2.17. The KNN training error rate (blue, 200 observations) and test error rate (orange, 5,000 observations) on the data from Figure 2.13, as the level of flexibility (assessed using 1 glyph[triangleleft]K on the log scale) increases, or equivalently as the number of neighbors K decreases. The black dashed line indicates the Bayes error rate. The jumpiness of the curves is due to the small size of the training data set.  \n<!-- image -->  \nvarious methods for estimating test error rates and thereby choosing the optimal level of flexibility for a given statistical learning method."", '## 7.6 Local Regression  \nLocal regression is a different approach for fitting flexible non-linear functions, which involves computing the fit at a target point x 0 using only the nearby training observations. Figure 7.9 illustrates the idea on some simulated data, with one target point near 0   4 , and another near the boundary at 0   05 . In this figure the blue line represents the function f ( x ) from which the data were generated, and the light orange line corresponds to the local regression estimate ˆ f ( x ) . Local regression is described in Algorithm 7.1.  \nIn order to perform local regression, there are a number of choices to be made, such as how to define the weighting function K , and whether to fit a linear, constant, or quadratic regression in Step 3. (Equation 7.14 corresponds to a linear regression.) While all of these choices make some difference, the most important choice is the span s , which is the proportion of points used to compute the local regression at x 0 , as defined in Step 1 above. The span plays a role like that of the tuning parameter λ in smooth-  \nNote that in Step 3 of Algorithm 7.1, the weights K i 0 will differ for each value of x 0 . In other words, in order to obtain the local regression fit at a new point, we need to fit a new weighted least squares regression model by minimizing (7.14) for a new set of weights. Local regression is sometimes referred to as a memory-based procedure, because like nearest-neighbors, we need all the training data each time we wish to compute a prediction. We will avoid getting into the technical details of local regression here-there are books written on the topic.  \nlocal regression\nLocal Regression  \nFIGURE 7.9. Local regression illustrated on some simulated data, where the blue curve represents f ( x ) from which the data were generated, and the light orange curve corresponds to the local regression estimate ˆ f ( x ) . The orange colored points are local to the target point x 0 , represented by the orange vertical line. The yellow bell-shape superimposed on the plot indicates weights assigned to each point, decreasing to zero with distance from the target point. The fit ˆ f ( x 0 ) at x 0 is obtained by fitting a weighted linear regression (orange line segment), and using the fitted value at x 0 (orange solid dot) as the estimate ˆ f ( x 0 ) .  \n<!-- image -->  \ning splines: it controls the flexibility of the non-linear fit. The smaller the value of s , the more local and wiggly will be our fit; alternatively, a very large value of s will lead to a global fit to the data using all of the training observations. We can again use cross-validation to choose s , or we can specify it directly. Figure 7.10 displays local linear regression fits on the Wage data, using two values of s : 0   7 and 0   2 . As expected, the fit obtained using s = 0   7 is smoother than that obtained using s = 0   2 .  \nThe idea of local regression can be generalized in many different ways. In a setting with multiple features X 1  X 2         X p , one very useful generalization involves fitting a multiple linear regression model that is global in some variables, but local in another, such as time. Such varying coefficient models are a useful way of adapting a model to the most recently gathered data. Local regression also generalizes very naturally when we want to fit models that are local in a pair of variables X 1 and X 2 , rather than one. We can simply use two-dimensional neighborhoods, and fit bivariate linear regression models using the observations that are near each target point in two-dimensional space. Theoretically the same approach can be implemented in higher dimensions, using linear regressions fit to p -dimensional neighborhoods. However, local regression can perform poorly if p is much larger than about 3 or 4 because there will generally be very few training observations close to x 0 . Nearest-neighbors regression, discussed in Chapter 3, suffers from a similar problem in high dimensions.  \nvarying coefficient model\nAlgorithm 7.1 Local Regression At X = x 0  \n1. Gather the fraction s = kglyph[triangleleft]n of training points whose x i are closest to x 0 .\n2. Assign a weight K i 0 = K ( x i  x 0 ) to each point in this neighborhood, so that the point furthest from x 0 has weight zero, and the closest has the highest weight. All but these k nearest neighbors get weight zero.\n3. Fit a weighted least squares regression of the y i on the x i using the aforementioned weights, by finding ˆ β 0 and ˆ β 1 that minimize  \n$$\\sum _ { i = 1 } ^ { n } K _ { i 0 } ( y _ { i } - \\beta _ { 0 } - \\beta _ { 1 } x _ { i } ) ^ { 2 } .$$  \n4. The fitted value at x 0 is given by ˆ f ( x 0 ) = ˆ β 0 + ˆ β 1 x 0 .\nLocal Linear Regression  \nFIGURE 7.10. Local linear fits to the Wage data. The span specifies the fraction of the data used to compute the fit at each target point.  \n<!-- image -->']","The formula for the K-Nearest Neighbors (KNN) regression predictor is given by:

\[
\hat{f}(x_0) = \frac{1}{K} \sum_{x_i \in \mathcal{N}_0} y_i
\]

Here, \( \hat{f}(x_0) \) is the predicted value at the prediction point \( x_0 \), \( K \) is the number of nearest neighbors, \( \mathcal{N}_0 \) represents the set of the \( K \) training observations closest to \( x_0 \), and \( y_i \) are the response values of these nearest neighbors. The prediction is made by averaging the response values of the \( K \) nearest neighbors.","The KNN regression predictor estimates f(x₀) by averaging the training responses in the neighborhood N₀, using the formula f̂(x₀) = (1/K) * Σ_{xᵢ ∈ N₀} yᵢ.",1.0,,0.9999999999666667,1.0
¿Cuál es la fórmula para la regresión polinómica de grado d?,"[""## 7.1 Polynomial Regression  \nHistorically, the standard way to extend linear regression to settings in which the relationship between the predictors and the response is nonlinear has been to replace the standard linear model  \n$$y _ { i } = \\beta _ { 0 } + \\beta _ { 1 } x _ { i } + \\epsilon _ { i }$$  \nwith a polynomial function  \n$$y _ { i } = \\beta _ { 0 } + \\beta _ { 1 } x _ { i } + \\beta _ { 2 } x _ { i } ^ { 2 } + \\beta _ { 3 } x _ { i } ^ { 3 } + \\cdots + \\beta _ { d } x _ { i } ^ { d } + \\epsilon _ { i } ,$$  \nwhere /epsilon1 i is the error term. This approach is known as polynomial regression , and in fact we saw an example of this method in Section 3.3.2. For large enough degree d , a polynomial regression allows us to produce an extremely non-linear curve. Notice that the coefficients in (7.1) can be easily estimated using least squares linear regression because this is just a standard linear model with predictors x i  x 2 i  x 3 i         x d i . Generally speaking, it is unusual to use d greater than 3 or 4 because for large values of d , the polynomial curve can become overly flexible and can take on some very strange shapes. This is especially true near the boundary of the X variable.  \nThe left-hand panel in Figure 7.1 is a plot of wage against age for the Wage data set, which contains income and demographic information for males who reside in the central Atlantic region of the United States. We see the results of fitting a degree-4 polynomial using least squares (solid blue curve). Even though this is a linear regression model like any other, the individual coefficients are not of particular interest. Instead, we look at the entire fitted function across a grid of 63 values for age from 18 to 80 in order to understand the relationship between age and wage .  \npolynomial regression\nDegree-4 Polynomial  \nFIGURE 7.1. The Wage data. Left: The solid blue curve is a degree-4 polynomial of wage (in thousands of dollars) as a function of age , fit by least squares. The dashed curves indicate an estimated 95 % confidence interval. Right: We model the binary event wage&gt;250 using logistic regression, again with a degree-4 polynomial. The fitted posterior probability of wage exceeding $250  000 is shown in blue, along with an estimated 95 % confidence interval.  \n<!-- image -->  \nIn Figure 7.1, a pair of dashed curves accompanies the fit; these are ( 2 × ) standard error curves. Let's see how these arise. Suppose we have computed the fit at a particular value of age , x 0 :  \n$$\\hat { f } ( x _ { 0 } ) = \\hat { \\beta } _ { 0 } + \\hat { \\beta } _ { 1 } x _ { 0 } + \\hat { \\beta } _ { 2 } x _ { 0 } ^ { 2 } + \\hat { \\beta } _ { 3 } x _ { 0 } ^ { 3 } + \\hat { \\beta } _ { 4 } x _ { 0 } ^ { 4 } .$$  \nWhat is the variance of the fit, i.e. Var ˆ f ( x 0 ) ? Least squares returns variance estimates for each of the fitted coefficients ˆ β j , as well as the covariances between pairs of coefficient estimates. We can use these to compute the estimated variance of ˆ f ( x 0 ) . 1 The estimated pointwise standard error of ˆ f ( x 0 ) is the square-root of this variance. This computation is repeated at each reference point x 0 , and we plot the fitted curve, as well as twice the standard error on either side of the fitted curve. We plot twice the standard error because, for normally distributed error terms, this quantity corresponds to an approximate 95 % confidence interval.  \nIt seems like the wages in Figure 7.1 are from two distinct populations: there appears to be a high earners group earning more than $250  000 per annum, as well as a low earners group. We can treat wage as a binary variable by splitting it into these two groups. Logistic regression can then be used to predict this binary response, using polynomial functions of age  \n1 If ˆ C is the 5 × 5 covariance matrix of the ˆ β j , and if /lscript T 0 = (1  x 0  x 2 0  x 3 0  x 4 0 ) , then Var [ ˆ f ( x 0 )] = /lscript T 0 ˆ C /lscript 0 .  \nas predictors. In other words, we fit the model  \n$$\\Pr ( y _ { i } > 2 5 0 ) | x _ { i } ) = \\frac { \\exp ( \\beta _ { 0 } + \\beta _ { 1 } x _ { i } + \\beta _ { 2 } x _ { i } ^ { 2 } + \\dots + \\beta _ { d } x _ { i } ^ { d } ) } { 1 + \\exp ( \\beta _ { 0 } + \\beta _ { 1 } x _ { i } + \\beta _ { 2 } x _ { i } ^ { 2 } + \\dots + \\beta _ { d } x _ { i } ^ { d } ) } .$$  \nThe result is shown in the right-hand panel of Figure 7.1. The gray marks on the top and bottom of the panel indicate the ages of the high earners and the low earners. The solid blue curve indicates the fitted probabilities of being a high earner, as a function of age . The estimated 95 % confidence interval is shown as well. We see that here the confidence intervals are fairly wide, especially on the right-hand side. Although the sample size for this data set is substantial ( n = 3  000 ), there are only 79 high earners, which results in a high variance in the estimated coefficients and consequently wide confidence intervals."", ""## 10.8 Interpolation and Double Descent  \nThroughout this book, we have repeatedly discussed the bias-variance tradeoff, first presented in Section 2.2.2. This trade-off indicates that statistical learning methods tend to perform the best, in terms of test-set error, for an intermediate level of model complexity. In particular, if we plot 'flexibility' on the x -axis and error on the y -axis, then we generally expect to see that test error has a U-shape, whereas training error decreases monotonically. Two 'typical' examples of this behavior can be seen in the right-hand panel of Figure 2.9 on page 29, and in Figure 2.17 on page 39. One implication of the bias-variance trade-off is that it is generally not a good idea to interpolate the training data - that is, to get zero training error - since that will often result in very high test error.  \nWe now describe the set-up that resulted in Figure 10.20. We simulated n = 20 observations from the model  \nHowever, it turns out that in certain specific settings it can be possible for a statistical learning method that interpolates the training data to perform well - or at least, better than a slightly less complex model that does not quite interpolate the data. This phenomenon is known as double descent , and is displayed in Figure 10.20. 'Double descent' gets its name from the fact that the test error has a U-shape before the interpolation threshold is reached, and then it descends again (for a while, at least) as an increasingly flexible model is fit.  \n$$Y = \\sin ( X ) + \\epsilon ,$$  \nwhere X ∼ U [ -5  5] (uniform distribution), and /epsilon1 ∼ N (0  σ 2 ) with σ = 0   3 . We then fit a natural spline to the data, as described in Section 7.4, with d interpolate  \nFIGURE 10.21. Fitted functions ˆ f d ( X ) (orange), true function f ( X ) (black) and the observed 20 training data points. A different value of d (degrees of freedom) is used in each panel. For d ≥ 20 the orange curves all interpolate the training points, and hence the training error is zero.  \n<!-- image -->  \ndegrees of freedom. 22 Recall from Section 7.4 that fitting a natural spline with d degrees of freedom amounts to fitting a least-squares regression of the response onto a set of d basis functions. The upper-left panel of Figure 10.21 shows the data, the true function f ( X ) , and ˆ f 8 ( X ) , the fitted natural spline with d = 8 degrees of freedom.  \nWe now continue to fit natural splines to the data, with increasing values of d . For d &gt; 20 , the least squares regression of Y onto d basis functions is not unique: there are an infinite number of least squares coefficient estimates that achieve zero error. To select among them, we choose the one with the smallest sum of squared coefficients, ∑ d j =1 ˆ β 2 j . This is known as the minimum-norm solution.  \nNext, we fit a natural spline with d = 20 degrees of freedom. Since n = 20 , this means that n = d , and we have zero training error; in other words, we have interpolated the training data! We can see from the top-right panel of Figure 10.21 that ˆ f 20 ( X ) makes wild excursions, and hence the test error will be large.  \nThe two lower panels of Figure 10.21 show the minimum-norm natural spline fits with d = 42 and d = 80 degrees of freedom. Incredibly, ˆ f 42 ( X ) is quite a bit less less wild than ˆ f 20 ( X ) , even though it makes use of more degrees of freedom . And ˆ f 80 ( X ) is not much different. How can this be? Essentially, ˆ f 20 ( X ) is very wild because there is just a single way to interpolate n = 20 observations using d = 20 basis functions, and that single way results in a somewhat extreme fitted function. By contrast, there are an  \n22 This implies the choice of d knots, here chosen at d equi-probability quantiles of the training data. When d &gt; n , the quantiles are found by interpolation.  \ninfinite number of ways to interpolate n = 20 observations using d = 42 or d = 80 basis functions, and the smoothest of them - that is, the minimum norm solution - is much less wild than ˆ f 20 ( X ) !  \nIn Figures 10.20 and 10.21, we have illustrated the double descent phenomenon in a simple one-dimensional setting using natural splines. However, it turns out that the same phenomenon can arise for deep learning. Basically, when we fit neural networks with a huge number of parameters, we are sometimes able to get good results with zero training error. This is particularly true in problems with high signal-to-noise ratio, such as natural image recognition and language translation, for example. This is because the techniques used to fit neural networks, including stochastic gradient descent, naturally lend themselves to selecting a 'smooth' interpolating model that has good test-set performance on these kinds of problems.  \nIn Figure 10.20, we display the training error and test error associated with ˆ f d ( X ) , for a range of values of the degrees of freedom d . We see that the training error drops to zero once d = 20 and beyond; i.e. once the interpolation threshold is reached. By contrast, the test error shows a U -shape for d ≤ 20 , grows extremely large around d = 20 , and then shows a second region of descent for d &gt; 20 . For this example the signal-to-noise ratio Var( f ( X )) glyph[triangleleft] σ 2 - is 5   9 , which is quite high (the data points are close to the true curve). So an estimate that interpolates the data and does not wander too far inbetween the observed data points will likely do well.  \nSome points are worth emphasizing:  \n- The double-descent phenomenon does not contradict the bias-variance trade-off, as presented in Section 2.2.2. Rather, the double-descent curve seen in the right-hand side of Figure 10.20 is a consequence of the fact that the x -axis displays the number of spline basis functions used, which does not properly capture the true 'flexibility' of models that interpolate the training data. Stated another way, in this example, the minimum-norm natural spline with d = 42 has lower variance than the natural spline with d = 20 .\n- Most of the statistical learning methods seen in this book do not exhibit double descent. For instance, regularization approaches typically do not interpolate the training data, and thus double descent does not occur. This is not a drawback of regularized methods: they can give great results without interpolating the data !\n- In particular, in the examples here, if we had fit the natural splines using ridge regression with an appropriately-chosen penalty rather than least squares, then we would not have seen double descent, and in fact would have obtained better test error results.\n- In Chapter 9, we saw that maximal margin classifiers and SVMs that have zero training error nonetheless often achieve very good test error. This is in part because those methods seek smooth minimum norm solutions. This is similar to the fact that the minimum-norm natural spline can give good results with zero training error.\n- The double-descent phenomenon has been used by the machine learning community to explain the successful practice of using an over-  \nparametrized neural network (many layers, and many hidden units), and then fitting all the way to zero training error. However, fitting to zero error is not always optimal, and whether it is advisable depends on the signal-to-noise ratio. For instance, we may use ridge regularization to avoid overfitting a neural network, as in (10.31). In this case, provided that we use an appropriate choice for the tuning parameter λ , we will never interpolate the training data, and thus will not see the double descent phenomenon. Nonetheless we can get very good test-set performance, likely much better than we would have achieved had we interpolated the training data. Early stopping during stochastic gradient descent can also serve as a form of regularization that prevents us from interpolating the training data, while still getting very good results on test data.  \nTo summarize: though double descent can sometimes occur in neural networks, we typically do not want to rely on this behavior. Moreover, it is important to remember that the bias-variance trade-off always holds (though it is possible that test error as a function of flexibility may not exhibit a U-shape, depending on how we have parametrized the notion of 'flexibility' on the x -axis)."", '## 7.9 Exercises  \n### Conceptual  \n1. It was mentioned in this chapter that a cubic regression spline with one knot at ξ can be obtained using a basis of the form x , x 2 , x 3 , ( x -ξ ) 3 + , where ( x -ξ ) 3 + = ( x -ξ ) 3 if x &gt; ξ and equals 0 otherwise. We will now show that a function of the form  \n$$f ( x ) = \\beta _ { 0 } + \\beta _ { 1 } x + \\beta _ { 2 } x ^ { 2 } + \\beta _ { 3 } x ^ { 3 } + \\beta _ { 4 } ( x - \\xi ) _ { + } ^ { 3 }$$  \nis indeed a cubic regression spline, regardless of the values of β 0  β 1  β 2  β 3  β 4 .  \n- (a) Find a cubic polynomial  \n$$f _ { 1 } ( x ) = a _ { 1 } + b _ { 1 } x + c _ { 1 } x ^ { 2 } + d _ { 1 } x ^ { 3 }$$  \nsuch that f ( x ) = f 1 ( x ) for all x ≤ ξ . Express a 1  b 1  c 1  d 1 in terms of β 0  β 1  β 2  β 3  β 4 .  \n- (b) Find a cubic polynomial  \n$$f _ { 2 } ( x ) = a _ { 2 } + b _ { 2 } x + c _ { 2 } x ^ { 2 } + d _ { 2 } x ^ { 3 }$$  \nsuch that f ( x ) = f 2 ( x ) for all x &gt; ξ . Express a 2  b 2  c 2  d 2 in terms of β 0  β 1  β 2  β 3  β 4 . We have now established that f ( x ) is a piecewise polynomial.  \n- (c) Show that f 1 ( ξ ) = f 2 ( ξ ) . That is, f ( x ) is continuous at ξ .\n- (d) Show that f ′ 1 ( ξ ) = f ′ 2 ( ξ ) . That is, f ′ ( x ) is continuous at ξ .\n- (e) Show that f ′′ 1 ( ξ ) = f ′′ 2 ( ξ ) . That is, f ′′ ( x ) is continuous at ξ .  \nTherefore, f ( x ) is indeed a cubic spline.  \nHint: Parts (d) and (e) of this problem require knowledge of singlevariable calculus. As a reminder, given a cubic polynomial  \n$$f _ { 1 } ( x ) = a _ { 1 } + b _ { 1 } x + c _ { 1 } x ^ { 2 } + d _ { 1 } x ^ { 3 } ,$$  \nthe first derivative takes the form  \n$$f _ { 1 } ^ { \\prime } ( x ) = b _ { 1 } + 2 c _ { 1 } x + 3 d _ { 1 } x ^ { 2 }$$  \n<!-- image -->  \nand the second derivative takes the form  \n$$f _ { 1 } ^ { \\prime \\prime } ( x ) = 2 c _ { 1 } + 6 d _ { 1 } x .$$  \n2. Suppose that a curve ˆ g is computed to smoothly fit a set of n points using the following formula:  \n$$\\begin{array} { r l } { \\text {using the following formula:} } \\\\ { \\hat { g } = \\arg \\min _ { g } \\left ( \\sum _ { i = 1 } ^ { n } ( y _ { i } - g ( x _ { i } ) ) ^ { 2 } + \\lambda \\int \\left [ g ^ { ( m ) } ( x ) \\right ] ^ { 2 } d x \\right ) , } \\\\ { \\quad } \\\\ { \\quad } \\\\ { ( m ) } \\\\ { \\hat { g } = \\arg \\min _ { g } \\left ( \\sum _ { i = 1 } ^ { n } ( y _ { i } - g ( x _ { i } ) ) ^ { 2 } + \\lambda \\int \\left [ g ^ { ( m ) } ( x ) \\right ] ^ { 2 } d x \\right ) , } \\\\ { \\quad } \\\\ { \\quad } \\\\ { ( x ) } \\\\ { \\quad } \\end{array}$$  \nwhere g ( m ) represents the m th derivative of g (and g (0) = g ). Provide example sketches of ˆ g in each of the following scenarios.  \n- (a) λ = ∞  m = 0 .\n- (c) λ = ∞  m = 2 .\n- (b) λ = ∞  m = 1 .\n- (d) λ = ∞  m = 3 .\n- (e) λ = 0  m = 3 .\n3. Suppose we fit a curve with basis functions b 1 ( X ) = X , b 2 ( X ) = ( X -1) 2 I ( X ≥ 1) . (Note that I ( X ≥ 1) equals 1 for X ≥ 1 and 0 otherwise.) We fit the linear regression model  \n$$Y = \\beta _ { 0 } + \\beta _ { 1 } b _ { 1 } ( X ) + \\beta _ { 2 } b _ { 2 } ( X ) + \\epsilon ,$$  \nand obtain coefficient estimates ˆ β 0 = 1  ˆ β 1 = 1  ˆ β 2 = -2 . Sketch the estimated curve between X = -2 and X = 2 . Note the intercepts, slopes, and other relevant information.  \n4. Suppose we fit a curve with basis functions b 1 ( X ) = I (0 ≤ X ≤ 2) -( X -1) I (1 ≤ X ≤ 2) , b 2 ( X ) = ( X -3) I (3 ≤ X ≤ 4)+ I (4 &lt; X ≤ 5) . We fit the linear regression model  \n$$Y = \\beta _ { 0 } + \\beta _ { 1 } b _ { 1 } ( X ) + \\beta _ { 2 } b _ { 2 } ( X ) + \\epsilon ,$$  \nand obtain coefficient estimates ˆ β 0 = 1  ˆ β 1 = 1  ˆ β 2 = 3 . Sketch the estimated curve between X = -2 and X = 6 . Note the intercepts, slopes, and other relevant information.  \n5. Consider two curves, ˆ g 1 and ˆ g 2 , defined by  \n$$\\text {. Consider two curves, } \\hat { g } _ { 1 } \\text { and } \\hat { g } _ { 2 } , \\text { defined by} \\\\ \\hat { g } _ { 1 } = \\arg \\min _ { g } \\left ( \\sum _ { i = 1 } ^ { n } ( y _ { i } - g ( x _ { i } ) ) ^ { 2 } + \\lambda \\int \\left [ g ^ { ( 3 ) } ( x ) \\right ] \\right ] ^ { 2 } d x \\right ) , \\\\ \\hat { g } _ { 2 } = \\arg \\min _ { g } \\left ( \\sum _ { i = 1 } ^ { n } ( y _ { i } - g ( x _ { i } ) ) ^ { 2 } + \\lambda \\int \\left [ g ^ { ( 4 ) } ( x ) \\right ] ^ { 2 } d x \\right ) , \\\\ \\text {where } g ^ { ( m ) } \\, \\text { represents the mth derivative of } g .$$  \nwhere g ( m ) represents the m th derivative of g .  \n- (a) As λ →∞ , will ˆ g 1 or ˆ g 2 have the smaller training RSS?\n- (c) For λ = 0 , will ˆ g 1 or ˆ g 2 have the smaller training and test RSS?\n- (b) As λ →∞ , will ˆ g 1 or ˆ g 2 have the smaller test RSS?', '### 7.4.1 Piecewise Polynomials  \nInstead of fitting a high-degree polynomial over the entire range of X , piecewise polynomial regression involves fitting separate low-degree polynomials over different regions of X . For example, a piecewise cubic polynomial works by fitting a cubic regression model of the form  \n$$y _ { i } = \\beta _ { 0 } + \\beta _ { 1 } x _ { i } + \\beta _ { 2 } x _ { i } ^ { 2 } + \\beta _ { 3 } x _ { i } ^ { 3 } + \\epsilon _ { i } ,$$  \nwhere the coefficients β 0 , β 1 , β 2 , and β 3 differ in different parts of the range of X . The points where the coefficients change are called knots .  \nFor example, a piecewise cubic with no knots is just a standard cubic polynomial, as in (7.1) with d = 3 . A piecewise cubic polynomial with a single knot at a point c takes the form  \n$$y _ { i } = \\begin{cases} \\beta _ { 0 1 } + \\beta _ { 1 1 } x _ { i } + \\beta _ { 2 1 } x _ { i } ^ { 2 } + \\beta _ { 3 1 } x _ { i } ^ { 3 } + \\epsilon _ { i } & \\text {if } x _ { i } < c \\\\ \\beta _ { 0 2 } + \\beta _ { 1 2 } x _ { i } + \\beta _ { 2 2 } x _ { i } ^ { 2 } + \\beta _ { 3 2 } x _ { i } ^ { 3 } + \\epsilon _ { i } & \\text {if } x _ { i } \\geq c . \\end{cases}$$  \nIn other words, we fit two different polynomial functions to the data, one on the subset of the observations with x i &lt; c , and one on the subset of the observations with x i ≥ c . The first polynomial function has coefficients regression spline  \npiecewise polynomial regression knot  \nFIGURE 7.3. Various piecewise polynomials are fit to a subset of the Wage data, with a knot at age=50 . Top Left: The cubic polynomials are unconstrained. Top Right: The cubic polynomials are constrained to be continuous at age=50 . Bottom Left: The cubic polynomials are constrained to be continuous, and to have continuous first and second derivatives. Bottom Right: A linear spline is shown, which is constrained to be continuous.  \n<!-- image -->  \nβ 01  β 11  β 21  and β 31 , and the second has coefficients β 02  β 12  β 22  and β 32 . Each of these polynomial functions can be fit using least squares applied to simple functions of the original predictor.  \nThe top left panel of Figure 7.3 shows a piecewise cubic polynomial fit to a subset of the Wage data, with a single knot at age=50 . We immediately see a problem: the function is discontinuous and looks ridiculous! Since each polynomial has four parameters, we are using a total of eight degrees of freedom in fitting this piecewise polynomial model.  \nUsing more knots leads to a more flexible piecewise polynomial. In general, if we place K different knots throughout the range of X , then we will end up fitting K +1 different cubic polynomials. Note that we do not need to use a cubic polynomial. For example, we can instead fit piecewise linear functions. In fact, our piecewise constant functions of Section 7.2 are piecewise polynomials of degree 0!']","La fórmula para la regresión polinómica de grado \(d\) es:

\[ 
y_i = \beta_0 + \beta_1 x_i + \beta_2 x_i^2 + \beta_3 x_i^3 + \cdots + \beta_d x_i^d + \epsilon_i 
\]

donde \(y_i\) es la variable de respuesta, \(x_i\) es el predictor, \(\beta_0, \beta_1, \ldots, \beta_d\) son los coeficientes del modelo, y \(\epsilon_i\) es el término de error.",La fórmula para la regresión polinómica es yᵢ = β₀ + β₁xᵢ + β₂xᵢ² + ... + βₔxᵢᵈ + εᵢ.,1.0,,0.7499999999625,1.0
What is the objective function that a smoothing spline aims to minimize?,"[""### 7.5.1 An Overview of Smoothing Splines  \nIn fitting a smooth curve to a set of data, what we really want to do is find some function, say g ( x ) , that fits the observed data well: that is, we want RSS = ∑ n i =1 ( y i -g ( x i )) 2 to be small. However, there is a problem with this approach. If we don't put any constraints on g ( x i ) , then we can always make RSS zero simply by choosing g such that it interpolates all of the y i . Such a function would woefully overfit the data-it would be far too flexible. What we really want is a function g that makes RSS small, but that is also smooth .  \nHow might we ensure that g is smooth? There are a number of ways to do this. A natural approach is to find the function g that minimizes  \n$$\\sum _ { i = 1 } ^ { n } ( y _ { i } - g ( x _ { i } ) ) ^ { 2 } + \\lambda \\int g ^ { \\prime \\prime } ( t ) ^ { 2 } d t \\\\$$  \nwhere λ is a nonnegative tuning parameter . The function g that minimizes (7.11) is known as a smoothing spline .  \nWhat does (7.11) mean? Equation 7.11 takes the 'Loss+Penalty' formulation that we encounter in the context of ridge regression and the lasso in Chapter 6. The term ∑ n i =1 ( y i -g ( x i )) 2 is a loss function that encourages g to fit the data well, and the term λ ∫ g ′′ ( t ) 2 dt is a penalty term that penalizes the variability in g . The notation g ′′ ( t ) indicates the second derivative of the function g . The first derivative g ′ ( t ) measures the slope smoothing spline  \nloss function  \nof a function at t , and the second derivative corresponds to the amount by which the slope is changing. Hence, broadly speaking, the second derivative of a function is a measure of its roughness : it is large in absolute value if g ( t ) is very wiggly near t , and it is close to zero otherwise. (The second derivative of a straight line is zero; note that a line is perfectly smooth.) The ∫ notation is an integral , which we can think of as a summation over the range of t . In other words, ∫ g ′′ ( t ) 2 dt is simply a measure of the total change in the function g ′ ( t ) , over its entire range. If g is very smooth, then g ′ ( t ) will be close to constant and ∫ g ′′ ( t ) 2 dt will take on a small value. Conversely, if g is jumpy and variable then g ′ ( t ) will vary significantly and ∫ g ′′ ( t ) 2 dt will take on a large value. Therefore, in (7.11), λ ∫ g ′′ ( t ) 2 dt encourages g to be smooth. The larger the value of λ , the smoother g will be.  \nThe function g ( x ) that minimizes (7.11) can be shown to have some special properties: it is a piecewise cubic polynomial with knots at the unique values of x 1         x n , and continuous first and second derivatives at each knot. Furthermore, it is linear in the region outside of the extreme knots. In other words, the function g ( x ) that minimizes (7.11) is a natural cubic spline with knots at x 1         x n ! However, it is not the same natural cubic spline that one would get if one applied the basis function approach described in Section 7.4.3 with knots at x 1         x n -rather, it is a shrunken version of such a natural cubic spline, where the value of the tuning parameter λ in (7.11) controls the level of shrinkage.  \nWhen λ = 0 , then the penalty term in (7.11) has no effect, and so the function g will be very jumpy and will exactly interpolate the training observations. When λ → ∞ , g will be perfectly smooth-it will just be a straight line that passes as closely as possible to the training points. In fact, in this case, g will be the linear least squares line, since the loss function in (7.11) amounts to minimizing the residual sum of squares. For an intermediate value of λ , g will approximate the training observations but will be somewhat smooth. We see that λ controls the bias-variance trade-off of the smoothing spline."", ""### 2.2.1 Measuring the Quality of Fit  \nIn order to evaluate the performance of a statistical learning method on a given data set, we need some way to measure how well its predictions actually match the observed data. That is, we need to quantify the extent to which the predicted response value for a given observation is close to the true response value for that observation. In the regression setting, the most commonly-used measure is the mean squared error (MSE), given by  \n$$M S E = \\frac { 1 } { n } \\sum _ { i = 1 } ^ { n } ( y _ { i } - \\hat { f } ( x _ { i } ) ) ^ { 2 } , \\quad \\ \\ ( 2 . 5 ) ^ { \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\$$  \nwhere ˆ f ( x i ) is the prediction that ˆ f gives for the i th observation. The MSE will be small if the predicted responses are very close to the true responses, and will be large if for some of the observations, the predicted and true responses differ substantially.  \nTo state it more mathematically, suppose that we fit our statistical learning method on our training observations ¶ ( x 1  y 1 )  ( x 2  y 2 )         ( x n  y n ) ♦ , and we obtain the estimate ˆ f . We can then compute ˆ f ( x 1 )  ˆ f ( x 2 )         ˆ f ( x n ) .  \nThe MSE in (2.5) is computed using the training data that was used to fit the model, and so should more accurately be referred to as the training MSE . But in general, we do not really care how well the method works on the training data. Rather, we are interested in the accuracy of the predictions that we obtain when we apply our method to previously unseen test data . Why is this what we care about? Suppose that we are interested in developing an algorithm to predict a stock's price based on previous stock returns. We can train the method using stock returns from the past 6 months. But we don't really care how well our method predicts last week's stock price. We instead care about how well it will predict tomorrow's price or next month's price. On a similar note, suppose that we have clinical measurements (e.g. weight, blood pressure, height, age, family history of disease) for a number of patients, as well as information about whether each patient has diabetes. We can use these patients to train a statistical learning method to predict risk of diabetes based on clinical measurements. In practice, we want this method to accurately predict diabetes risk for future patients based on their clinical measurements. We are not very interested in whether or not the method accurately predicts diabetes risk for patients used to train the model, since we already know which of those patients have diabetes.  \nmean squared error training MSE  \ntest data  \nFIGURE 2.9. Left: Data simulated from f , shown in black. Three estimates of f are shown: the linear regression line (orange curve), and two smoothing spline fits (blue and green curves). Right: Training MSE (grey curve), test MSE (red curve), and minimum possible test MSE over all methods (dashed line). Squares represent the training and test MSEs for the three fits shown in the left-hand panel.  \n<!-- image -->  \nIf these are approximately equal to y 1  y 2         y n , then the training MSE given by (2.5) is small. However, we are really not interested in whether ˆ f ( x i ) ≈ y i ; instead, we want to know whether ˆ f ( x 0 ) is approximately equal to y 0 , where ( x 0  y 0 ) is a previously unseen test observation not used to train the statistical learning method . We want to choose the method that gives the lowest test MSE , as opposed to the lowest training MSE. In other words, if we had a large number of test observations, we could compute  \n$$\\ A v e ( y _ { 0 } - \\hat { f } ( x _ { 0 } ) ) ^ { 2 } , \\\\ \\quad \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\$$  \nHow can we go about trying to select a method that minimizes the test MSE? In some settings, we may have a test data set available-that is, we may have access to a set of observations that were not used to train the statistical learning method. We can then simply evaluate (2.6) on the test observations, and select the learning method for which the test MSE is smallest. But what if no test observations are available? In that case, one might imagine simply selecting a statistical learning method that minimizes the training MSE (2.5). This seems like it might be a sensible approach, since the training MSE and the test MSE appear to be closely related. Unfortunately, there is a fundamental problem with this strategy: there is no guarantee that the method with the lowest training MSE will also have the lowest test MSE. Roughly speaking, the problem is that many statistical methods specifically estimate coefficients so as to minimize the training set MSE. For these methods, the training set MSE can be quite small, but the test MSE is often much larger.  \nthe average squared prediction error for these test observations ( x 0  y 0 ) . We'd like to select the model for which this quantity is as small as possible.  \nFigure 2.9 illustrates this phenomenon on a simple example. In the lefthand panel of Figure 2.9, we have generated observations from (2.1) with test MSE  \nthe true f given by the black curve. The orange, blue and green curves illustrate three possible estimates for f obtained using methods with increasing levels of flexibility. The orange line is the linear regression fit, which is relatively inflexible. The blue and green curves were produced using smoothing splines , discussed in Chapter 7, with different levels of smoothness. It is clear that as the level of flexibility increases, the curves fit the observed data more closely. The green curve is the most flexible and matches the data very well; however, we observe that it fits the true f (shown in black) poorly because it is too wiggly. By adjusting the level of flexibility of the smoothing spline fit, we can produce many different fits to this data.  \nIn this example, we know the true function f , and so we can also compute the test MSE over a very large test set, as a function of flexibility. (Of course, in general f is unknown, so this will not be possible.) The test MSE is displayed using the red curve in the right-hand panel of Figure 2.9. As with the training MSE, the test MSE initially declines as the level of flexibility increases. However, at some point the test MSE levels off and then starts to increase again. Consequently, the orange and green curves both have high test MSE. The blue curve minimizes the test MSE, which should not be surprising given that visually it appears to estimate f the best in the left-hand panel of Figure 2.9. The horizontal dashed line indicates Var ( /epsilon1 ) , the irreducible error in (2.3), which corresponds to the lowest achievable test MSE among all possible methods. Hence, the smoothing spline represented by the blue curve is close to optimal.  \nWe now move on to the right-hand panel of Figure 2.9. The grey curve displays the average training MSE as a function of flexibility, or more formally the degrees of freedom , for a number of smoothing splines. The degrees of freedom is a quantity that summarizes the flexibility of a curve; it is discussed more fully in Chapter 7. The orange, blue and green squares indicate the MSEs associated with the corresponding curves in the lefthand panel. A more restricted and hence smoother curve has fewer degrees of freedom than a wiggly curve-note that in Figure 2.9, linear regression is at the most restrictive end, with two degrees of freedom. The training MSE declines monotonically as flexibility increases. In this example the true f is non-linear, and so the orange linear fit is not flexible enough to estimate f well. The green curve has the lowest training MSE of all three methods, since it corresponds to the most flexible of the three curves fit in the left-hand panel.  \nIn the right-hand panel of Figure 2.9, as the flexibility of the statistical learning method increases, we observe a monotone decrease in the training MSE and a U-shape in the test MSE. This is a fundamental property of statistical learning that holds regardless of the particular data set at hand and regardless of the statistical method being used. As model flexibility increases, the training MSE will decrease, but the test MSE may not. When a given method yields a small training MSE but a large test MSE, we are said to be overfitting the data. This happens because our statistical learning procedure is working too hard to find patterns in the training data, and may be picking up some patterns that are just caused by random chance rather than by true properties of the unknown function f . When we overfit the training data, the test MSE will be very large because the supposed smoothing spline  \ndegrees of freedom  \nFIGURE 2.10. Details are as in Figure 2.9, using a different true f that is much closer to linear. In this setting, linear regression provides a very good fit to the data.  \n<!-- image -->  \npatterns that the method found in the training data simply don't exist in the test data. Note that regardless of whether or not overfitting has occurred, we almost always expect the training MSE to be smaller than the test MSE because most statistical learning methods either directly or indirectly seek to minimize the training MSE. Overfitting refers specifically to the case in which a less flexible model would have yielded a smaller test MSE.  \nIn practice, one can usually compute the training MSE with relative ease, but estimating the test MSE is considerably more difficult because usually no test data are available. As the previous three examples illustrate, the flexibility level corresponding to the model with the minimal test MSE can vary considerably among data sets. Throughout this book, we discuss a variety of approaches that can be used in practice to estimate this minimum point. One important method is cross-validation (Chapter 5), which is a method for estimating the test MSE using the training data.  \nFigure 2.10 provides another example in which the true f is approximately linear. Again we observe that the training MSE decreases monotonically as the model flexibility increases, and that there is a U-shape in the test MSE. However, because the truth is close to linear, the test MSE only decreases slightly before increasing again, so that the orange least squares fit is substantially better than the highly flexible green curve. Finally, Figure 2.11 displays an example in which f is highly non-linear. The training and test MSE curves still exhibit the same general patterns, but now there is a rapid decrease in both curves before the test MSE starts to increase slowly."", '## 10.7 Fitting a Neural Network  \nFitting neural networks is somewhat complex, and we give a brief overview here. The ideas generalize to much more complex networks. Readers who find this material challenging can safely skip it. Fortunately, as we see in the lab at the end of this chapter, good software is available to fit neural network models in a relatively automated way, without worrying about the technical details of the model-fitting procedure.  \nWe start with the simple network depicted in Figure 10.1 in Section 10.1. In model (10.1) the parameters are β = ( β 0  β 1         β K ) , as well as each of the w k = ( w k 0  w k 1         w kp )  k = 1         K  Given observations ( x i  y i )  i = 1         n we could fit the model by solving a nonlinear least squares problem  \n$$\\min \\min i { \\frac { 1 } { \\{ w _ { k } \\} _ { 1 } ^ { K } , \\beta } } \\frac { 1 } { 2 } \\sum _ { i = 1 } ^ { n } ( y _ { i } - f ( x _ { i } ) ) ^ { 2 } ,$$  \n$$f ( x _ { i } ) = \\beta _ { 0 } + \\sum _ { k = 1 } ^ { K } \\beta _ { k } g \\left ( w _ { k 0 } + \\sum _ { j = 1 } ^ { p } w _ { k j } x _ { i j } \\right ) . \\\\$$  \nwhere  \nThe objective in (10.23) looks simple enough, but because of the nested arrangement of the parameters and the symmetry of the hidden units, it is not straightforward to minimize. The problem is nonconvex in the parameters, and hence there are multiple solutions. As an example, Figure 10.17 shows a simple nonconvex function of a single variable θ ; there are two solutions: one is a local minimum and the other is a global minimum . Furthermore, (10.1) is the very simplest of neural networks; in this chapter we have presented much more complex ones where these problems are compounded. To overcome some of these issues and to protect from overfitting, two general strategies are employed when fitting neural networks.  \n- Slow Learning: the model is fit in a somewhat slow iterative fashion, using gradient descent . The fitting process is then stopped when overfitting is detected.\n- Regularization: penalties are imposed on the parameters, usually lasso or ridge as discussed in Section 6.2.  \nSuppose we represent all the parameters in one long vector θ . Then we can rewrite the objective in (10.23) as  \n$$R ( \\theta ) = \\frac { 1 } { 2 } \\sum _ { i = 1 } ^ { n } ( y _ { i } - f _ { \\theta } ( x _ { i } ) ) ^ { 2 } ,$$  \n<!-- image -->  \nlocal minimum global minimum gradient descent  \nFIGURE 10.17. Illustration of gradient descent for one-dimensional θ . The objective function R ( θ ) is not convex, and has two minima, one at θ = -0   46 (local), the other at θ = 1   02 (global). Starting at some value θ 0 (typically randomly chosen), each step in θ moves downhill - against the gradient - until it cannot go down any further. Here gradient descent reached the global minimum in 7 steps.  \n<!-- image -->  \nwhere we make explicit the dependence of f on the parameters. The idea of gradient descent is very simple.  \n1. Start with a guess θ 0 for all the parameters in θ , and set t = 0 .\n2. Iterate until the objective (10.25) fails to decrease:\n3. (a) Find a vector δ that reflects a small change in θ , such that θ t +1 = θ t + δ reduces the objective; i.e. such that R ( θ t +1 ) &lt; R ( θ t ) .\n4. (b) Set t ← t +1 .  \nOne can visualize (Figure 10.17) standing in a mountainous terrain, and the goal is to get to the bottom through a series of steps. As long as each step goes downhill, we must eventually get to the bottom. In this case we were lucky, because with our starting guess θ 0 we end up at the global minimum. In general we can hope to end up at a (good) local minimum.', '### 7.5.2 Choosing the Smoothing Parameter λ  \nWe have seen that a smoothing spline is simply a natural cubic spline with knots at every unique value of x i . It might seem that a smoothing spline will have far too many degrees of freedom, since a knot at each data point allows a great deal of flexibility. But the tuning parameter λ controls the roughness of the smoothing spline, and hence the effective degrees of freedom . It is possible to show that as λ increases from 0 to ∞ , the effective degrees of freedom, which we write df λ , decrease from n to 2 .  \nIn the context of smoothing splines, why do we discuss effective degrees of freedom instead of degrees of freedom? Usually degrees of freedom refer to the number of free parameters, such as the number of coefficients fit in a polynomial or cubic spline. Although a smoothing spline has n parameters and hence n nominal degrees of freedom, these n parameters are heavily constrained or shrunk down. Hence df λ is a measure of the flexibility of the smoothing spline-the higher it is, the more flexible (and the lower-bias but higher-variance) the smoothing spline. The definition of effective degrees of effective degrees of freedom  \nfreedom is somewhat technical. We can write  \n$$\\hat { g } _ { \\lambda } = S _ { \\lambda } y ,$$  \nwhere ˆ g λ is the solution to (7.11) for a particular choice of λ -that is, it is an n -vector containing the fitted values of the smoothing spline at the training points x 1         x n . Equation 7.12 indicates that the vector of fitted values when applying a smoothing spline to the data can be written as a n × n matrix S λ (for which there is a formula) times the response vector y . Then the effective degrees of freedom is defined to be  \n$$d f _ { \\lambda } = \\sum _ { i = 1 } ^ { n } \\{ S _ { \\lambda } \\} _ { i i } , \\\\$$  \nthe sum of the diagonal elements of the matrix S λ .  \nIn fitting a smoothing spline, we do not need to select the number or location of the knots-there will be a knot at each training observation, x 1         x n . Instead, we have another problem: we need to choose the value of λ . It should come as no surprise that one possible solution to this problem is cross-validation. In other words, we can find the value of λ that makes the cross-validated RSS as small as possible. It turns out that the leaveone-out cross-validation error (LOOCV) can be computed very efficiently for smoothing splines, with essentially the same cost as computing a single fit, using the following formula:  \n$$R S _ { c v } ( \\lambda ) = \\sum _ { i = 1 } ^ { n } ( y _ { i } - \\hat { g } _ { \\lambda } ^ { ( - i ) } ( x _ { i } ) ) ^ { 2 } = \\sum _ { i = 1 } ^ { n } \\left [ \\frac { y _ { i } - \\hat { g } _ { \\lambda } ( x _ { i } ) } { 1 - \\{ S _ { \\lambda } \\} _ { i i } } \\right ] ^ { 2 } .$$  \nThe notation ˆ g ( -i ) λ ( x i ) indicates the fitted value for this smoothing spline evaluated at x i , where the fit uses all of the training observations except for the i th observation ( x i  y i ) . In contrast, ˆ g λ ( x i ) indicates the smoothing spline function fit to all of the training observations and evaluated at x i . This remarkable formula says that we can compute each of these leaveone-out fits using only ˆ g λ , the original fit to all of the data! 5 We have a very similar formula (5.2) on page 205 in Chapter 5 for least squares linear regression. Using (5.2), we can very quickly perform LOOCV for the regression splines discussed earlier in this chapter, as well as for least squares regression using arbitrary basis functions.  \nFigure 7.8 shows the results from fitting a smoothing spline to the Wage data. The red curve indicates the fit obtained from pre-specifying that we would like a smoothing spline with 16 effective degrees of freedom. The blue curve is the smoothing spline obtained when λ is chosen using LOOCV; in this case, the value of λ chosen results in 6   8 effective degrees of freedom (computed using (7.13)). For this data, there is little discernible difference between the two smoothing splines, beyond the fact that the one with 16 degrees of freedom seems slightly wigglier. Since there is little difference between the two fits, the smoothing spline fit with 6   8 degrees of freedom  \n5 The exact formulas for computing ˆ g ( x i ) and S λ are very technical; however, efficient algorithms are available for computing these quantities.\nSmoothing Spline  \nFIGURE 7.8. Smoothing spline fits to the Wage data. The red curve results from specifying 16 effective degrees of freedom. For the blue curve, λ was found automatically by leave-one-out cross-validation, which resulted in 6   8 effective degrees of freedom.  \n<!-- image -->  \nis preferable, since in general simpler models are better unless the data provides evidence in support of a more complex model.']","A smoothing spline aims to minimize the following objective function:

$$
\sum _ { i = 1 } ^ { n } ( y _ { i } - g ( x _ { i } ) ) ^ { 2 } + \lambda \int g ^ { \prime \prime } ( t ) ^ { 2 } d t
$$

Here, the first term, $\sum _ { i = 1 } ^ { n } ( y _ { i } - g ( x _ { i } ) ) ^ { 2 }$, is a loss function that encourages the function $g$ to fit the data well. The second term, $\lambda \int g ^ { \prime \prime } ( t ) ^ { 2 } d t$, is a penalty term that penalizes the variability in $g$, where $\lambda$ is a nonnegative tuning parameter. The function $g$ that minimizes this objective is known as a smoothing spline.","A smoothing spline minimizes the expression Σ(yᵢ - g(xᵢ))² + λ∫g''(t)²dt, where the first term is a loss function and the second is a penalty term that penalizes the variability in g.",1.0,0.9999985988839466,0.9166666666361111,1.0
How is the effective degrees of freedom (dfλ) for a smoothing spline defined?,"['### 7.5.2 Choosing the Smoothing Parameter λ  \nWe have seen that a smoothing spline is simply a natural cubic spline with knots at every unique value of x i . It might seem that a smoothing spline will have far too many degrees of freedom, since a knot at each data point allows a great deal of flexibility. But the tuning parameter λ controls the roughness of the smoothing spline, and hence the effective degrees of freedom . It is possible to show that as λ increases from 0 to ∞ , the effective degrees of freedom, which we write df λ , decrease from n to 2 .  \nIn the context of smoothing splines, why do we discuss effective degrees of freedom instead of degrees of freedom? Usually degrees of freedom refer to the number of free parameters, such as the number of coefficients fit in a polynomial or cubic spline. Although a smoothing spline has n parameters and hence n nominal degrees of freedom, these n parameters are heavily constrained or shrunk down. Hence df λ is a measure of the flexibility of the smoothing spline-the higher it is, the more flexible (and the lower-bias but higher-variance) the smoothing spline. The definition of effective degrees of effective degrees of freedom  \nfreedom is somewhat technical. We can write  \n$$\\hat { g } _ { \\lambda } = S _ { \\lambda } y ,$$  \nwhere ˆ g λ is the solution to (7.11) for a particular choice of λ -that is, it is an n -vector containing the fitted values of the smoothing spline at the training points x 1         x n . Equation 7.12 indicates that the vector of fitted values when applying a smoothing spline to the data can be written as a n × n matrix S λ (for which there is a formula) times the response vector y . Then the effective degrees of freedom is defined to be  \n$$d f _ { \\lambda } = \\sum _ { i = 1 } ^ { n } \\{ S _ { \\lambda } \\} _ { i i } , \\\\$$  \nthe sum of the diagonal elements of the matrix S λ .  \nIn fitting a smoothing spline, we do not need to select the number or location of the knots-there will be a knot at each training observation, x 1         x n . Instead, we have another problem: we need to choose the value of λ . It should come as no surprise that one possible solution to this problem is cross-validation. In other words, we can find the value of λ that makes the cross-validated RSS as small as possible. It turns out that the leaveone-out cross-validation error (LOOCV) can be computed very efficiently for smoothing splines, with essentially the same cost as computing a single fit, using the following formula:  \n$$R S _ { c v } ( \\lambda ) = \\sum _ { i = 1 } ^ { n } ( y _ { i } - \\hat { g } _ { \\lambda } ^ { ( - i ) } ( x _ { i } ) ) ^ { 2 } = \\sum _ { i = 1 } ^ { n } \\left [ \\frac { y _ { i } - \\hat { g } _ { \\lambda } ( x _ { i } ) } { 1 - \\{ S _ { \\lambda } \\} _ { i i } } \\right ] ^ { 2 } .$$  \nThe notation ˆ g ( -i ) λ ( x i ) indicates the fitted value for this smoothing spline evaluated at x i , where the fit uses all of the training observations except for the i th observation ( x i  y i ) . In contrast, ˆ g λ ( x i ) indicates the smoothing spline function fit to all of the training observations and evaluated at x i . This remarkable formula says that we can compute each of these leaveone-out fits using only ˆ g λ , the original fit to all of the data! 5 We have a very similar formula (5.2) on page 205 in Chapter 5 for least squares linear regression. Using (5.2), we can very quickly perform LOOCV for the regression splines discussed earlier in this chapter, as well as for least squares regression using arbitrary basis functions.  \nFigure 7.8 shows the results from fitting a smoothing spline to the Wage data. The red curve indicates the fit obtained from pre-specifying that we would like a smoothing spline with 16 effective degrees of freedom. The blue curve is the smoothing spline obtained when λ is chosen using LOOCV; in this case, the value of λ chosen results in 6   8 effective degrees of freedom (computed using (7.13)). For this data, there is little discernible difference between the two smoothing splines, beyond the fact that the one with 16 degrees of freedom seems slightly wigglier. Since there is little difference between the two fits, the smoothing spline fit with 6   8 degrees of freedom  \n5 The exact formulas for computing ˆ g ( x i ) and S λ are very technical; however, efficient algorithms are available for computing these quantities.\nSmoothing Spline  \nFIGURE 7.8. Smoothing spline fits to the Wage data. The red curve results from specifying 16 effective degrees of freedom. For the blue curve, λ was found automatically by leave-one-out cross-validation, which resulted in 6   8 effective degrees of freedom.  \n<!-- image -->  \nis preferable, since in general simpler models are better unless the data provides evidence in support of a more complex model.', '### 7.4.4 Choosing the Number and Locations of the Knots  \nWhen we fit a spline, where should we place the knots? The regression spline is most flexible in regions that contain a lot of knots, because in those regions the polynomial coefficients can change rapidly. Hence, one natural spline\nNatural Cubic Spline  \nFIGURE 7.5. A natural cubic spline function with four degrees of freedom is fit to the Wage data. Left: A spline is fit to wage (in thousands of dollars) as a function of age . Right: Logistic regression is used to model the binary event wage&gt;250 as a function of age . The fitted posterior probability of wage exceeding $250  000 is shown. The dashed lines denote the knot locations.  \n<!-- image -->  \noption is to place more knots in places where we feel the function might vary most rapidly, and to place fewer knots where it seems more stable. While this option can work well, in practice it is common to place knots in a uniform fashion. One way to do this is to specify the desired degrees of freedom, and then have the software automatically place the corresponding number of knots at uniform quantiles of the data.  \nHow many knots should we use, or equivalently how many degrees of freedom should our spline contain? One option is to try out different numbers of knots and see which produces the best looking curve. A somewhat more objective approach is to use cross-validation, as discussed in Chapters 5 and 6. With this method, we remove a portion of the data (say 10 %), fit a spline with a certain number of knots to the remaining data, and then use the spline to make predictions for the held-out portion. We repeat this process multiple times until each observation has been left out once, and  \nFigure 7.5 shows an example on the Wage data. As in Figure 7.4, we have fit a natural cubic spline with three knots, except this time the knot locations were chosen automatically as the 25th, 50th, and 75th percentiles of age . This was specified by requesting four degrees of freedom. The argument by which four degrees of freedom leads to three interior knots is somewhat technical. 4  \n4 There are actually five knots, including the two boundary knots. A cubic spline with five knots has nine degrees of freedom. But natural cubic splines have two additional natural constraints at each boundary to enforce linearity, resulting in 9 -4 = 5 degrees of freedom. Since this includes a constant, which is absorbed in the intercept, we count it as four degrees of freedom.  \nFIGURE 7.6. Ten-fold cross-validated mean squared errors for selecting the degrees of freedom when fitting splines to the Wage data. The response is wage and the predictor age . Left: A natural cubic spline. Right: A cubic spline.  \n<!-- image -->  \nthen compute the overall cross-validated RSS . This procedure can be repeated for different numbers of knots K . Then the value of K giving the smallest RSS is chosen.  \nIn Section 7.7 we fit additive spline models simultaneously on several variables at a time. This could potentially require the selection of degrees of freedom for each variable. In cases like this we typically adopt a more pragmatic approach and set the degrees of freedom to a fixed number, say four, for all terms.  \nFigure 7.6 shows ten-fold cross-validated mean squared errors for splines with various degrees of freedom fit to the Wage data. The left-hand panel corresponds to a natural cubic spline and the right-hand panel to a cubic spline. The two methods produce almost identical results, with clear evidence that a one-degree fit (a linear regression) is not adequate. Both curves flatten out quickly, and it seems that three degrees of freedom for the natural spline and four degrees of freedom for the cubic spline are quite adequate.', ""### 2.2.1 Measuring the Quality of Fit  \nIn order to evaluate the performance of a statistical learning method on a given data set, we need some way to measure how well its predictions actually match the observed data. That is, we need to quantify the extent to which the predicted response value for a given observation is close to the true response value for that observation. In the regression setting, the most commonly-used measure is the mean squared error (MSE), given by  \n$$M S E = \\frac { 1 } { n } \\sum _ { i = 1 } ^ { n } ( y _ { i } - \\hat { f } ( x _ { i } ) ) ^ { 2 } , \\quad \\ \\ ( 2 . 5 ) ^ { \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\$$  \nwhere ˆ f ( x i ) is the prediction that ˆ f gives for the i th observation. The MSE will be small if the predicted responses are very close to the true responses, and will be large if for some of the observations, the predicted and true responses differ substantially.  \nTo state it more mathematically, suppose that we fit our statistical learning method on our training observations ¶ ( x 1  y 1 )  ( x 2  y 2 )         ( x n  y n ) ♦ , and we obtain the estimate ˆ f . We can then compute ˆ f ( x 1 )  ˆ f ( x 2 )         ˆ f ( x n ) .  \nThe MSE in (2.5) is computed using the training data that was used to fit the model, and so should more accurately be referred to as the training MSE . But in general, we do not really care how well the method works on the training data. Rather, we are interested in the accuracy of the predictions that we obtain when we apply our method to previously unseen test data . Why is this what we care about? Suppose that we are interested in developing an algorithm to predict a stock's price based on previous stock returns. We can train the method using stock returns from the past 6 months. But we don't really care how well our method predicts last week's stock price. We instead care about how well it will predict tomorrow's price or next month's price. On a similar note, suppose that we have clinical measurements (e.g. weight, blood pressure, height, age, family history of disease) for a number of patients, as well as information about whether each patient has diabetes. We can use these patients to train a statistical learning method to predict risk of diabetes based on clinical measurements. In practice, we want this method to accurately predict diabetes risk for future patients based on their clinical measurements. We are not very interested in whether or not the method accurately predicts diabetes risk for patients used to train the model, since we already know which of those patients have diabetes.  \nmean squared error training MSE  \ntest data  \nFIGURE 2.9. Left: Data simulated from f , shown in black. Three estimates of f are shown: the linear regression line (orange curve), and two smoothing spline fits (blue and green curves). Right: Training MSE (grey curve), test MSE (red curve), and minimum possible test MSE over all methods (dashed line). Squares represent the training and test MSEs for the three fits shown in the left-hand panel.  \n<!-- image -->  \nIf these are approximately equal to y 1  y 2         y n , then the training MSE given by (2.5) is small. However, we are really not interested in whether ˆ f ( x i ) ≈ y i ; instead, we want to know whether ˆ f ( x 0 ) is approximately equal to y 0 , where ( x 0  y 0 ) is a previously unseen test observation not used to train the statistical learning method . We want to choose the method that gives the lowest test MSE , as opposed to the lowest training MSE. In other words, if we had a large number of test observations, we could compute  \n$$\\ A v e ( y _ { 0 } - \\hat { f } ( x _ { 0 } ) ) ^ { 2 } , \\\\ \\quad \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\$$  \nHow can we go about trying to select a method that minimizes the test MSE? In some settings, we may have a test data set available-that is, we may have access to a set of observations that were not used to train the statistical learning method. We can then simply evaluate (2.6) on the test observations, and select the learning method for which the test MSE is smallest. But what if no test observations are available? In that case, one might imagine simply selecting a statistical learning method that minimizes the training MSE (2.5). This seems like it might be a sensible approach, since the training MSE and the test MSE appear to be closely related. Unfortunately, there is a fundamental problem with this strategy: there is no guarantee that the method with the lowest training MSE will also have the lowest test MSE. Roughly speaking, the problem is that many statistical methods specifically estimate coefficients so as to minimize the training set MSE. For these methods, the training set MSE can be quite small, but the test MSE is often much larger.  \nthe average squared prediction error for these test observations ( x 0  y 0 ) . We'd like to select the model for which this quantity is as small as possible.  \nFigure 2.9 illustrates this phenomenon on a simple example. In the lefthand panel of Figure 2.9, we have generated observations from (2.1) with test MSE  \nthe true f given by the black curve. The orange, blue and green curves illustrate three possible estimates for f obtained using methods with increasing levels of flexibility. The orange line is the linear regression fit, which is relatively inflexible. The blue and green curves were produced using smoothing splines , discussed in Chapter 7, with different levels of smoothness. It is clear that as the level of flexibility increases, the curves fit the observed data more closely. The green curve is the most flexible and matches the data very well; however, we observe that it fits the true f (shown in black) poorly because it is too wiggly. By adjusting the level of flexibility of the smoothing spline fit, we can produce many different fits to this data.  \nIn this example, we know the true function f , and so we can also compute the test MSE over a very large test set, as a function of flexibility. (Of course, in general f is unknown, so this will not be possible.) The test MSE is displayed using the red curve in the right-hand panel of Figure 2.9. As with the training MSE, the test MSE initially declines as the level of flexibility increases. However, at some point the test MSE levels off and then starts to increase again. Consequently, the orange and green curves both have high test MSE. The blue curve minimizes the test MSE, which should not be surprising given that visually it appears to estimate f the best in the left-hand panel of Figure 2.9. The horizontal dashed line indicates Var ( /epsilon1 ) , the irreducible error in (2.3), which corresponds to the lowest achievable test MSE among all possible methods. Hence, the smoothing spline represented by the blue curve is close to optimal.  \nWe now move on to the right-hand panel of Figure 2.9. The grey curve displays the average training MSE as a function of flexibility, or more formally the degrees of freedom , for a number of smoothing splines. The degrees of freedom is a quantity that summarizes the flexibility of a curve; it is discussed more fully in Chapter 7. The orange, blue and green squares indicate the MSEs associated with the corresponding curves in the lefthand panel. A more restricted and hence smoother curve has fewer degrees of freedom than a wiggly curve-note that in Figure 2.9, linear regression is at the most restrictive end, with two degrees of freedom. The training MSE declines monotonically as flexibility increases. In this example the true f is non-linear, and so the orange linear fit is not flexible enough to estimate f well. The green curve has the lowest training MSE of all three methods, since it corresponds to the most flexible of the three curves fit in the left-hand panel.  \nIn the right-hand panel of Figure 2.9, as the flexibility of the statistical learning method increases, we observe a monotone decrease in the training MSE and a U-shape in the test MSE. This is a fundamental property of statistical learning that holds regardless of the particular data set at hand and regardless of the statistical method being used. As model flexibility increases, the training MSE will decrease, but the test MSE may not. When a given method yields a small training MSE but a large test MSE, we are said to be overfitting the data. This happens because our statistical learning procedure is working too hard to find patterns in the training data, and may be picking up some patterns that are just caused by random chance rather than by true properties of the unknown function f . When we overfit the training data, the test MSE will be very large because the supposed smoothing spline  \ndegrees of freedom  \nFIGURE 2.10. Details are as in Figure 2.9, using a different true f that is much closer to linear. In this setting, linear regression provides a very good fit to the data.  \n<!-- image -->  \npatterns that the method found in the training data simply don't exist in the test data. Note that regardless of whether or not overfitting has occurred, we almost always expect the training MSE to be smaller than the test MSE because most statistical learning methods either directly or indirectly seek to minimize the training MSE. Overfitting refers specifically to the case in which a less flexible model would have yielded a smaller test MSE.  \nIn practice, one can usually compute the training MSE with relative ease, but estimating the test MSE is considerably more difficult because usually no test data are available. As the previous three examples illustrate, the flexibility level corresponding to the model with the minimal test MSE can vary considerably among data sets. Throughout this book, we discuss a variety of approaches that can be used in practice to estimate this minimum point. One important method is cross-validation (Chapter 5), which is a method for estimating the test MSE using the training data.  \nFigure 2.10 provides another example in which the true f is approximately linear. Again we observe that the training MSE decreases monotonically as the model flexibility increases, and that there is a U-shape in the test MSE. However, because the truth is close to linear, the test MSE only decreases slightly before increasing again, so that the orange least squares fit is substantially better than the highly flexible green curve. Finally, Figure 2.11 displays an example in which f is highly non-linear. The training and test MSE curves still exhibit the same general patterns, but now there is a rapid decrease in both curves before the test MSE starts to increase slowly."", ""### 7.8.3 Smoothing Splines and GAMs  \nA smoothing spline is a special case of a GAM with squared-error loss and a single feature. To fit GAMs in Python we will use the pygam package which can be installed via pip install pygam . The estimator LinearGAM() uses squared-error loss. The GAM is specified by associating each column of a model matrix with a particular smoothing operation: s for smoothing spline; l for linear, and f for factor or categorical variables. The argument 0 passed to s below indicates that this smoother will apply to the first column of a feature matrix. Below, we pass it a matrix with a single column: X\\_age . The argument lam is the penalty parameter λ as discussed in Section 7.5.2.  \n```\nIn[23]: X_age = np.asarray(age).reshape((-1,1)) gam = LinearGAM(s_gam(0, lam=0.6)) gam.fit(X_age, y)\n```  \nNatural Spline()  \npygam  \nLinearGAM()  \n```\nOut[23]: LinearGAM(callbacks=[Deviance(), Diffs()], fit_intercept=True, max_iter=100, scale=None, terms=s(0) + intercept, tol=0.0001, verbose=False)\n```  \nThe pygam library generally expects a matrix of features so we reshape age to be a matrix (a two-dimensional array) instead of a vector (i.e. a onedimensional array). The -1 in the call to the reshape() method tells numpy to impute the size of that dimension based on the remaining entries of the shape tuple.  \nLet's investigate how the fit changes with the smoothing parameter lam . The function np.logspace() is similar to np.linspace() but spaces points evenly on the log-scale. Below we vary lam from 10 -2 to 10 6 .  \n```\nIn[24]: fig, ax = subplots(figsize=(8,8)) ax.scatter(age, y, facecolor='gray', alpha=0.5) for lam in np.logspace(-2, 6, 5): gam = LinearGAM(s_gam(0, lam=lam)).fit(X_age, y) ax.plot(age_grid, gam.predict(age_grid), label='{:.1e}'.format(lam), linewidth=3) ax.set_xlabel('Age', fontsize=20) ax.set_ylabel('Wage', fontsize=20); ax.legend(title='$\\lambda$');\n```  \nThe pygam package can perform a search for an optimal smoothing parameter.  \n```\nIn[25]: gam_opt = gam.gridsearch(X_age, y) ax.plot(age_grid, gam_opt.predict(age_grid), label='Grid search', linewidth=4) ax.legend() fig\n```  \nAlternatively, we can fix the degrees of freedom of the smoothing spline using a function included in the ISLP.pygam package. Below we find a value of λ that gives us roughly four degrees of freedom. We note here that these degrees of freedom include the unpenalized intercept and linear term of the smoothing spline, hence there are at least two degrees of freedom.  \n```\nIn[26]: age_term = gam.terms[0] lam_4 = approx_lam(X_age, age_term, 4) age_term.lam = lam_4 degrees_of_freedom(X_age, age_term)\n```\nOut[26]: 4.000000100004728  \nLet's vary the degrees of freedom in a similar plot to above. We choose the degrees of freedom as the desired degrees of freedom plus one to account for the fact that these smoothing splines always have an intercept term. Hence, a value of one for df is just a linear fit.  \n```\nIn[27]: fig, ax = subplots(figsize=(8,8)) ax.scatter(X_age, y,\n```  \nnp.logspace()  \n```\nfacecolor='gray', alpha=0.3) for df in [1,3,4,8,15]: lam = approx_lam(X_age, age_term, df+1) age_term.lam = lam gam.fit(X_age, y) ax.plot(age_grid, gam.predict(age_grid), label='{:d}'.format(df), linewidth=4) ax.set_xlabel('Age', fontsize=20) ax.set_ylabel('Wage', fontsize=20); ax.legend(title='Degrees of freedom');\n```\nAdditive Models with Several Terms  \nThe strength of generalized additive models lies in their ability to fit multivariate regression models with more flexibility than linear models. We demonstrate two approaches: the first in a more manual fashion using natural splines and piecewise constant functions, and the second using the pygam package and smoothing splines.  \nWe will build the model matrix in a more manual fashion here, since we wish to access the pieces separately when constructing partial dependence plots.  \nWe now fit a GAM by hand to predict wage using natural spline functions of year and age , treating education as a qualitative predictor, as in (7.16). Since this is just a big linear regression model using an appropriate choice of basis functions, we can simply do this using the sm.OLS() function.  \n```\nIn[28]: ns_age = NaturalSpline(df=4).fit(age) ns_year = NaturalSpline(df=5).fit(Wage['year']) Xs = [ns_age.transform(age), ns_year.transform(Wage['year']), pd.get_dummies(Wage['education']).values] X_bh = np.hstack(Xs) gam_bh = sm.OLS(y, X_bh).fit()\n```  \nHere the function NaturalSpline() is the workhorse supporting the ns() helper function. We chose to use all columns of the indicator matrix for the categorical variable education , making an intercept redundant. Finally, we stacked the three component matrices horizontally to form the model matrix X\\_bh .  \nWe now show how to construct partial dependence plots for each of the terms in our rudimentary GAM. We can do this by hand, given grids for age and year . We simply predict with new X matrices, fixing all but one of the features at a time.  \n```\nIn[29]: age_grid = np.linspace(age.min(), age.max(), 100) X_age_bh = X_bh.copy()[:100] X_age_bh[:] = X_bh[:].mean(0)[None,:] X_age_bh[:,:4] = ns_age.transform(age_grid) preds = gam_bh.get_prediction(X_age_bh) bounds_age = preds.conf_int(alpha=0.05)\n```  \n```\npartial_age = preds.predicted_mean center = partial_age.mean() partial_age -= center bounds_age -= center fig, ax = subplots(figsize=(8,8)) ax.plot(age_grid, partial_age, 'b', linewidth=3) ax.plot(age_grid, bounds_age[:,0], 'r--', linewidth=3) ax.plot(age_grid, bounds_age[:,1], 'r--', linewidth=3) ax.set_xlabel('Age') ax.set_ylabel('Effect on wage') ax.set_title('Partial dependence of age on wage', fontsize=20);\n```  \nLet's explain in some detail what we did above. The idea is to create a new prediction matrix, where all but the columns belonging to age are constant (and set to their training-data means). The four columns for age are filled in with the natural spline basis evaluated at the 100 values in age\\_grid .  \n1. We made a grid of length 100 in age , and created a matrix X\\_age\\_bh with 100 rows and the same number of columns as X\\_bh .\n2. We replaced every row of this matrix with the column means of the original.\n3. We then replace just the first four columns representing age with the natural spline basis computed at the values in age\\_grid .  \nThe remaining steps should by now be familiar.  \nWe also look at the effect of year on wage ; the process is the same.  \n```\nIn[30]: year_grid = np.linspace(2003, 2009, 100) year_grid = np.linspace(Wage['year'].min(), Wage['year'].max(), 100) X_year_bh = X_bh.copy()[:100] X_year_bh[:] = X_bh[:].mean(0)[None,:] X_year_bh[:,4:9] = ns_year.transform(year_grid) preds = gam_bh.get_prediction(X_year_bh) bounds_year = preds.conf_int(alpha=0.05) partial_year = preds.predicted_mean center = partial_year.mean() partial_year -= center bounds_year -= center fig, ax = subplots(figsize=(8,8)) ax.plot(year_grid, partial_year, 'b', linewidth=3) ax.plot(year_grid, bounds_year[:,0], 'r--', linewidth=3) ax.plot(year_grid, bounds_year[:,1], 'r--', linewidth=3) ax.set_xlabel('Year') ax.set_ylabel('Effect on wage') ax.set_title('Partial dependence of year on wage', fontsize=20);\n```  \nWe now fit the model (7.16) using smoothing splines rather than natural splines. All of the terms in (7.16) are fit simultaneously, taking each other into account to explain the response. The pygam package only works with matrices, so we must convert the categorical series education to its array representation, which can be found with the cat.codes attribute of education . As year only has 7 unique values, we use only seven basis functions for it.  \n```\nIn[31]: gam_full = LinearGAM(s_gam(0) + s_gam(1, n_splines=7) + f_gam(2, lam=0)) Xgam = np.column_stack([age, Wage['year'], Wage['education'].cat.codes]) gam_full = gam_full.fit(Xgam, y)\n```  \nThe two s\\_gam() terms result in smoothing spline fits, and use a default value for λ ( lam=0.6 ), which is somewhat arbitrary. For the categorical term education , specified using a f\\_gam() term, we specify lam=0 to avoid any shrinkage. We produce the partial dependence plot in age to see the effect of these choices.  \nThe values for the plot are generated by the pygam package. We provide a plot\\_gam() function for partial-dependence plots in ISLP.pygam , which makes this job easier than in our last example with natural splines.  \n```\nIn[32]: fig, ax = subplots(figsize=(8,8)) plot_gam(gam_full, 0, ax=ax) ax.set_xlabel('Age') ax.set_ylabel('Effect on wage') ax.set_title('Partial dependence of age on wage -default lam=0.6', fontsize=20);\n```  \nWe see that the function is somewhat wiggly. It is more natural to specify the df than a value for lam . We refit a GAM using four degrees of freedom each for age and year . Recall that the addition of one below takes into account the intercept of the smoothing spline.  \n```\nIn[33]: age_term = gam_full.terms[0] age_term.lam = approx_lam(Xgam, age_term, df=4+1) year_term = gam_full.terms[1] year_term.lam = approx_lam(Xgam, year_term, df=4+1) gam_full = gam_full.fit(Xgam, y)\n```  \nNote that updating age\\_term.lam above updates it in gam\\_full.terms[0] as well! Likewise for year\\_term.lam .  \nRepeating the plot for age , we see that it is much smoother. We also produce the plot for year .  \n```\nIn[34]: fig, ax = subplots(figsize=(8,8)) plot_gam(gam_full, 1, ax=ax) ax.set_xlabel('Year') ax.set_ylabel('Effect on wage') ax.set_title('Partial dependence of year on wage', fontsize=20)\n```  \nFinally we plot education , which is categorical. The partial dependence plot is different, and more suitable for the set of fitted constants for each level of this variable.  \n```\nIn[35]: fig, ax = subplots(figsize=(8, 8)) ax = plot_gam(gam_full, 2) ax.set_xlabel('Education') ax.set_ylabel('Effect on wage')\n```  \nplot\\_gam()  \n```\nax.set_title('Partial dependence of wage on education', fontsize=20); ax.set_xticklabels(Wage['education'].cat.categories, fontsize=8);\n```\nANOVA Tests for Additive Models  \nIn all of our models, the function of year looks rather linear. We can perform a series of ANOVA tests in order to determine which of these three models is best: a GAM that excludes year ( M 1 ), a GAM that uses a linear function of year ( M 2 ), or a GAM that uses a spline function of year ( M 3 ).  \n```\nIn[36]: gam_0 = LinearGAM(age_term + f_gam(2, lam=0)) gam_0.fit(Xgam, y) gam_linear = LinearGAM(age_term + l_gam(1, lam=0) + f_gam(2, lam=0)) gam_linear.fit(Xgam, y)\n```  \n```\nOut[36]: LinearGAM(callbacks=[Deviance(), Diffs()], fit_intercept=True, max_iter=100, scale=None, terms=s(0) + l(1) + f(2) + intercept, tol=0.0001, verbose=False)\n```  \nNotice our use of age\\_term in the expressions above. We do this because earlier we set the value for lam in this term to achieve four degrees of freedom.  \nTo directly assess the effect of year we run an ANOVA on the three models fit above.  \n```\nIn[37]: anova_gam(gam_0, gam_linear, gam_full) Out[37]: deviance df deviance_diff df_diff F pvalue 0 3714362.366 2991.004 NaN NaN NaN NaN 1 3696745.823 2990.005 17616.543 0.999 14.265 0.002 2 3693142.930 2987.007 3602.894 2.998 0.972 0.436\n```  \nWefind that there is compelling evidence that a GAM with a linear function in year is better than a GAM that does not include year at all ( p -value= 0.002). However, there is no evidence that a non-linear function of year is needed ( p -value=0.435). In other words, based on the results of this ANOVA, M 2 is preferred.  \nWe can repeat the same process for age as well. We see there is very clear evidence that a non-linear term is required for age .  \n```\nIn[38]: gam_0 = LinearGAM(year_term + f_gam(2, lam=0)) gam_linear = LinearGAM(l_gam(0, lam=0) + year_term + f_gam(2, lam=0)) gam_0.fit(Xgam, y) gam_linear.fit(Xgam, y) anova_gam(gam_0, gam_linear, gam_full)\n```  \n```\nOut[38]: deviance df deviance_diff df_diff F pvalue 0 3975443.045 2991.001 NaN NaN NaN NaN 1 3850246.908 2990.001 125196.137 1.000 101.270 0.000 2 3693142.930 2987.007 157103.978 2.993 42.448 0.000\n```  \nThere is a (verbose) summary() method for the GAM fit. (We do not reproduce it here.)  \n```\nIn[39]: gam_full.summary()\n```  \nWe can make predictions from gam objects, just like from lm objects, using the predict() method for the class gam . Here we make predictions on the training set.  \n```\nIn[40]: Yhat = gam_full.predict(Xgam)\n```  \nIn order to fit a logistic regression GAM, we use LogisticGAM() from LogisticGAM()  \npygam .  \n```\nIn[41]: gam_logit = LogisticGAM(age_term + l_gam(1, lam=0) + f_gam(2, lam=0)) gam_logit.fit(Xgam, high_earn)\n```  \n```\nOut[41]: LogisticGAM(callbacks=[Deviance(), Diffs(), Accuracy()],\n```  \n```\nfit_intercept=True, max_iter=100, terms=s(0) + l(1) + f(2) + intercept, tol=0.0001, verbose=False)\n```  \n```\nIn[42]: fig, ax = subplots(figsize=(8, 8)) ax = plot_gam(gam_logit, 2) ax.set_xlabel('Education') ax.set_ylabel('Effect on wage') ax.set_title('Partial dependence of wage on education', fontsize=20); ax.set_xticklabels(Wage['education'].cat.categories, fontsize=8);\n```  \nThe model seems to be very flat, with especially high error bars for the first category. Let's look at the data a bit more closely.  \n```\nIn[43]: pd.crosstab(Wage['high_earn'], Wage['education'])\n```  \nWe see that there are no high earners in the first category of education, meaning that the model will have a hard time fitting. We will fit a logistic regression GAM excluding all observations falling into this category. This provides more sensible results.  \nTo do so, we could subset the model matrix, though this will not remove the column from Xgam . While we can deduce which column corresponds to this feature, for reproducibility's sake we reform the model matrix on this smaller subset.  \n```\nIn[44]: only_hs = Wage['education'] == '1. < HS Grad' Wage_ = Wage.loc[ ∼ only_hs] Xgam_ = np.column_stack([Wage_['age'], Wage_['year'], Wage_['education'].cat.codes-1]) high_earn_ = Wage_['high_earn']\n```  \nIn the second-to-last line above, we subtract one from the codes of the category, due to a bug in pygam . It just relabels the education values and hence has no effect on the fit.  \nWe now fit the model.  \n```\nIn[45]: gam_logit_ = LogisticGAM(age_term + year_term + f_gam(2, lam=0)) gam_logit_.fit(Xgam_, high_earn_)\n```  \n```\nOut[45]: LogisticGAM(callbacks=[Deviance(), Diffs(), Accuracy()], fit_intercept=True, max_iter=100, terms=s(0) + s(1) + f(2) + intercept, tol=0.0001, verbose=False)\n```  \nLet's look at the effect of education , year and age on high earner status now that we've removed those observations.  \n```\nIn[46]: fig, ax = subplots(figsize=(8, 8)) ax = plot_gam(gam_logit_, 2) ax.set_xlabel('Education') ax.set_ylabel('Effect on wage') ax.set_title('Partial dependence of high earner status on education ', fontsize=20); ax.set_xticklabels(Wage['education'].cat.categories[1:], fontsize=8);\n```  \n```\nIn[47]: fig, ax = subplots(figsize=(8, 8)) ax = plot_gam(gam_logit_, 1) ax.set_xlabel('Year') ax.set_ylabel('Effect on wage') ax.set_title('Partial dependence of high earner status on year', fontsize=20);\n```  \n```\nIn[48]: fig, ax = subplots(figsize=(8, 8)) ax = plot_gam(gam_logit_, 0) ax.set_xlabel('Age') ax.set_ylabel('Effect on wage') ax.set_title('Partial dependence of high earner status on age', fontsize=20);\n```"", ""## 10.8 Interpolation and Double Descent  \nThroughout this book, we have repeatedly discussed the bias-variance tradeoff, first presented in Section 2.2.2. This trade-off indicates that statistical learning methods tend to perform the best, in terms of test-set error, for an intermediate level of model complexity. In particular, if we plot 'flexibility' on the x -axis and error on the y -axis, then we generally expect to see that test error has a U-shape, whereas training error decreases monotonically. Two 'typical' examples of this behavior can be seen in the right-hand panel of Figure 2.9 on page 29, and in Figure 2.17 on page 39. One implication of the bias-variance trade-off is that it is generally not a good idea to interpolate the training data - that is, to get zero training error - since that will often result in very high test error.  \nWe now describe the set-up that resulted in Figure 10.20. We simulated n = 20 observations from the model  \nHowever, it turns out that in certain specific settings it can be possible for a statistical learning method that interpolates the training data to perform well - or at least, better than a slightly less complex model that does not quite interpolate the data. This phenomenon is known as double descent , and is displayed in Figure 10.20. 'Double descent' gets its name from the fact that the test error has a U-shape before the interpolation threshold is reached, and then it descends again (for a while, at least) as an increasingly flexible model is fit.  \n$$Y = \\sin ( X ) + \\epsilon ,$$  \nwhere X ∼ U [ -5  5] (uniform distribution), and /epsilon1 ∼ N (0  σ 2 ) with σ = 0   3 . We then fit a natural spline to the data, as described in Section 7.4, with d interpolate  \nFIGURE 10.21. Fitted functions ˆ f d ( X ) (orange), true function f ( X ) (black) and the observed 20 training data points. A different value of d (degrees of freedom) is used in each panel. For d ≥ 20 the orange curves all interpolate the training points, and hence the training error is zero.  \n<!-- image -->  \ndegrees of freedom. 22 Recall from Section 7.4 that fitting a natural spline with d degrees of freedom amounts to fitting a least-squares regression of the response onto a set of d basis functions. The upper-left panel of Figure 10.21 shows the data, the true function f ( X ) , and ˆ f 8 ( X ) , the fitted natural spline with d = 8 degrees of freedom.  \nWe now continue to fit natural splines to the data, with increasing values of d . For d &gt; 20 , the least squares regression of Y onto d basis functions is not unique: there are an infinite number of least squares coefficient estimates that achieve zero error. To select among them, we choose the one with the smallest sum of squared coefficients, ∑ d j =1 ˆ β 2 j . This is known as the minimum-norm solution.  \nNext, we fit a natural spline with d = 20 degrees of freedom. Since n = 20 , this means that n = d , and we have zero training error; in other words, we have interpolated the training data! We can see from the top-right panel of Figure 10.21 that ˆ f 20 ( X ) makes wild excursions, and hence the test error will be large.  \nThe two lower panels of Figure 10.21 show the minimum-norm natural spline fits with d = 42 and d = 80 degrees of freedom. Incredibly, ˆ f 42 ( X ) is quite a bit less less wild than ˆ f 20 ( X ) , even though it makes use of more degrees of freedom . And ˆ f 80 ( X ) is not much different. How can this be? Essentially, ˆ f 20 ( X ) is very wild because there is just a single way to interpolate n = 20 observations using d = 20 basis functions, and that single way results in a somewhat extreme fitted function. By contrast, there are an  \n22 This implies the choice of d knots, here chosen at d equi-probability quantiles of the training data. When d &gt; n , the quantiles are found by interpolation.  \ninfinite number of ways to interpolate n = 20 observations using d = 42 or d = 80 basis functions, and the smoothest of them - that is, the minimum norm solution - is much less wild than ˆ f 20 ( X ) !  \nIn Figures 10.20 and 10.21, we have illustrated the double descent phenomenon in a simple one-dimensional setting using natural splines. However, it turns out that the same phenomenon can arise for deep learning. Basically, when we fit neural networks with a huge number of parameters, we are sometimes able to get good results with zero training error. This is particularly true in problems with high signal-to-noise ratio, such as natural image recognition and language translation, for example. This is because the techniques used to fit neural networks, including stochastic gradient descent, naturally lend themselves to selecting a 'smooth' interpolating model that has good test-set performance on these kinds of problems.  \nIn Figure 10.20, we display the training error and test error associated with ˆ f d ( X ) , for a range of values of the degrees of freedom d . We see that the training error drops to zero once d = 20 and beyond; i.e. once the interpolation threshold is reached. By contrast, the test error shows a U -shape for d ≤ 20 , grows extremely large around d = 20 , and then shows a second region of descent for d &gt; 20 . For this example the signal-to-noise ratio Var( f ( X )) glyph[triangleleft] σ 2 - is 5   9 , which is quite high (the data points are close to the true curve). So an estimate that interpolates the data and does not wander too far inbetween the observed data points will likely do well.  \nSome points are worth emphasizing:  \n- The double-descent phenomenon does not contradict the bias-variance trade-off, as presented in Section 2.2.2. Rather, the double-descent curve seen in the right-hand side of Figure 10.20 is a consequence of the fact that the x -axis displays the number of spline basis functions used, which does not properly capture the true 'flexibility' of models that interpolate the training data. Stated another way, in this example, the minimum-norm natural spline with d = 42 has lower variance than the natural spline with d = 20 .\n- Most of the statistical learning methods seen in this book do not exhibit double descent. For instance, regularization approaches typically do not interpolate the training data, and thus double descent does not occur. This is not a drawback of regularized methods: they can give great results without interpolating the data !\n- In particular, in the examples here, if we had fit the natural splines using ridge regression with an appropriately-chosen penalty rather than least squares, then we would not have seen double descent, and in fact would have obtained better test error results.\n- In Chapter 9, we saw that maximal margin classifiers and SVMs that have zero training error nonetheless often achieve very good test error. This is in part because those methods seek smooth minimum norm solutions. This is similar to the fact that the minimum-norm natural spline can give good results with zero training error.\n- The double-descent phenomenon has been used by the machine learning community to explain the successful practice of using an over-  \nparametrized neural network (many layers, and many hidden units), and then fitting all the way to zero training error. However, fitting to zero error is not always optimal, and whether it is advisable depends on the signal-to-noise ratio. For instance, we may use ridge regularization to avoid overfitting a neural network, as in (10.31). In this case, provided that we use an appropriate choice for the tuning parameter λ , we will never interpolate the training data, and thus will not see the double descent phenomenon. Nonetheless we can get very good test-set performance, likely much better than we would have achieved had we interpolated the training data. Early stopping during stochastic gradient descent can also serve as a form of regularization that prevents us from interpolating the training data, while still getting very good results on test data.  \nTo summarize: though double descent can sometimes occur in neural networks, we typically do not want to rely on this behavior. Moreover, it is important to remember that the bias-variance trade-off always holds (though it is possible that test error as a function of flexibility may not exhibit a U-shape, depending on how we have parametrized the notion of 'flexibility' on the x -axis).""]","The effective degrees of freedom (\(df_\lambda\)) for a smoothing spline is defined as the sum of the diagonal elements of the matrix \(S_\lambda\). Mathematically, it is expressed as:

\[ df_\lambda = \sum_{i=1}^{n} \{ S_\lambda \}_{ii} \]

Here, \(S_\lambda\) is an \(n \times n\) matrix that relates to the fitted values of the smoothing spline at the training points, and \(\{ S_\lambda \}_{ii}\) represents the diagonal elements of this matrix. The effective degrees of freedom is a measure of the flexibility of the smoothing spline, with higher values indicating more flexibility.","The effective degrees of freedom for a smoothing spline is defined as dfλ = Σ_{i=1 to n} {S_λ}_{ii}, where S_λ is the matrix that transforms the response vector y into the vector of fitted values ĝλ.",1.0,,0.99999999998,1.0
What is the formula for a polynomial kernel of degree d in a Support Vector Machine?,"['### 9.3.2 The Support Vector Machine  \nThe support vector machine (SVM) is an extension of the support vector classifier that results from enlarging the feature space in a specific way, using kernels . We will now discuss this extension, the details of which are somewhat complex and beyond the scope of this book. However, the main idea is described in Section 9.3.1: we may want to enlarge our feature space in order to accommodate a non-linear boundary between the classes. The kernel approach that we describe here is simply an efficient computational approach for enacting this idea.  \nWe have not discussed exactly how the support vector classifier is computed because the details become somewhat technical. However, it turns out that the solution to the support vector classifier problem (9.12)-(9.15) involves only the inner products of the observations (as opposed to the observations themselves). The inner product of two r -vectors a and b is defined as 〈 a b 〉 = ∑ r i =1 a i b i . Thus the inner product of two observations x i , x i ′ is given by  \nIt can be shown that  \n$$\\langle x _ { i } , x _ { i ^ { \\prime } } \\rangle = \\sum _ { j = 1 } ^ { p } x _ { i j } x _ { i ^ { \\prime } j } .$$  \n- The linear support vector classifier can be represented as  \n$$f ( x ) = \\beta _ { 0 } + \\sum _ { i = 1 } ^ { n } \\alpha _ { i } \\langle x , x _ { i } \\rangle ,$$  \nsupport vector machine kernel  \nwhere there are n parameters α i  i = 1         n , one per training observation.  \n- To estimate the parameters α 1         α n and β 0 , all we need are the ( n 2 ) inner products 〈 x i  x i ′ 〉 between all pairs of training observations. (The notation ( n 2 ) means n ( n -1) glyph[triangleleft] 2 , and gives the number of pairs among a set of n items.)  \nNotice that in (9.18), in order to evaluate the function f ( x ) , we need to compute the inner product between the new point x and each of the training points x i . However, it turns out that α i is nonzero only for the support vectors in the solution-that is, if a training observation is not a support vector, then its α i equals zero. So if S is the collection of indices of these support points, we can rewrite any solution function of the form (9.18) as  \n$$f ( x ) = \\beta _ { 0 } + \\sum _ { i \\in \\mathcal { S } } \\alpha _ { i } \\langle x , x _ { i } \\rangle ,$$  \nwhich typically involves far fewer terms than in (9.18). 2  \nNow suppose that every time the inner product (9.17) appears in the representation (9.18), or in a calculation of the solution for the support vector classifier, we replace it with a generalization of the inner product of the form  \nTo summarize, in representing the linear classifier f ( x ) , and in computing its coefficients, all we need are inner products.  \n$$K ( x _ { i } , x _ { i ^ { \\prime } } ) ,$$  \nwhere K is some function that we will refer to as a kernel . A kernel is a function that quantifies the similarity of two observations. For instance, we could simply take  \n$$K ( x _ { i } , x _ { i ^ { \\prime } } ) = \\sum _ { j = 1 } ^ { p } x _ { i j } x _ { i ^ { \\prime } j } , \\\\ \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad$$  \nwhich would just give us back the support vector classifier. Equation 9.21 is known as a linear kernel because the support vector classifier is linear in the features; the linear kernel essentially quantifies the similarity of a pair of observations using Pearson (standard) correlation. But one could instead choose another form for (9.20). For instance, one could replace every instance of ∑ p j =1 x ij x i ′ j with the quantity  \n$$K ( x _ { i } , x _ { i ^ { \\prime } } ) = ( 1 + \\sum _ { j = 1 } ^ { p } x _ { i j } x _ { i j ^ { \\prime } j } ) ^ { d } .$$  \nThis is known as a polynomial kernel of degree d , where d is a positive integer. Using such a kernel with d &gt; 1 , instead of the standard linear kernel (9.21), in the support vector classifier algorithm leads to a much more flexible decision boundary. It essentially amounts to fitting a support vector  \n2 By expanding each of the inner products in (9.19), it is easy to see that f ( x ) is a linear function of the coordinates of x . Doing so also establishes the correspondence between the α i and the original parameters β j .  \nkernel polynomial kernel  \nFIGURE 9.9. Left: An SVM with a polynomial kernel of degree 3 is applied to the non-linear data from Figure 9.8, resulting in a far more appropriate decision rule. Right: An SVM with a radial kernel is applied. In this example, either kernel is capable of capturing the decision boundary.  \n<!-- image -->  \nclassifier in a higher-dimensional space involving polynomials of degree d , rather than in the original feature space. When the support vector classifier is combined with a non-linear kernel such as (9.22), the resulting classifier is known as a support vector machine. Note that in this case the (non-linear) function has the form  \n$$f ( x ) = \\beta _ { 0 } + \\sum _ { i \\in \\mathcal { S } } \\alpha _ { i } K ( x , x _ { i } ) . \\\\$$  \nThe left-hand panel of Figure 9.9 shows an example of an SVM with a polynomial kernel applied to the non-linear data from Figure 9.8. The fit is a substantial improvement over the linear support vector classifier. When d = 1 , then the SVM reduces to the support vector classifier seen earlier in this chapter.  \nThe polynomial kernel shown in (9.22) is one example of a possible non-linear kernel, but alternatives abound. Another popular choice is the radial kernel , which takes the form  \n$$K ( x _ { i } , x _ { i ^ { \\prime } } ) = \\exp ( - \\gamma \\sum _ { j = 1 } ^ { p } ( x _ { i j } - x _ { i ^ { \\prime } j } ) ^ { 2 } ) .$$  \nIn (9.24), γ is a positive constant. The right-hand panel of Figure 9.9 shows an example of an SVM with a radial kernel on this non-linear data; it also does a good job in separating the two classes.  \nHow does the radial kernel (9.24) actually work? If a given test observation x ∗ = ( x ∗ 1         x ∗ p ) T is far from a training observation x i in terms of Euclidean distance, then ∑ p j =1 ( x ∗ j -x ij ) 2 will be large, and so K ( x ∗  x i ) = exp( -γ ∑ p j =1 ( x ∗ j -x ij ) 2 ) will be tiny. This means that in (9.23), x i will play virtually no role in f ( x ∗ ) . Recall that the predicted class label for the test observation x ∗ is based on the sign of f ( x ∗ ) . In other words, training observations that are far from x ∗ will play essentially no role in the predicted class label for x ∗ . This means that the radial kernel has very local radial kernel  \nFIGURE 9.10. ROC curves for the Heart data training set. Left: The support vector classifier and LDA are compared. Right: The support vector classifier is compared to an SVM using a radial basis kernel with γ = 10 -3 , 10 -2 , and 10 -1 .  \n<!-- image -->  \nbehavior, in the sense that only nearby training observations have an effect on the class label of a test observation.  \nWhat is the advantage of using a kernel rather than simply enlarging the feature space using functions of the original features, as in (9.16)? One advantage is computational, and it amounts to the fact that using kernels, one need only compute K ( x i  x ′ i ) for all ( n 2 ) distinct pairs i i ′ . This can be done without explicitly working in the enlarged feature space. This is important because in many applications of SVMs, the enlarged feature space is so large that computations are intractable. For some kernels, such as the radial kernel (9.24), the feature space is implicit and infinite-dimensional, so we could never do the computations there anyway!', ""## 7.1 Polynomial Regression  \nHistorically, the standard way to extend linear regression to settings in which the relationship between the predictors and the response is nonlinear has been to replace the standard linear model  \n$$y _ { i } = \\beta _ { 0 } + \\beta _ { 1 } x _ { i } + \\epsilon _ { i }$$  \nwith a polynomial function  \n$$y _ { i } = \\beta _ { 0 } + \\beta _ { 1 } x _ { i } + \\beta _ { 2 } x _ { i } ^ { 2 } + \\beta _ { 3 } x _ { i } ^ { 3 } + \\cdots + \\beta _ { d } x _ { i } ^ { d } + \\epsilon _ { i } ,$$  \nwhere /epsilon1 i is the error term. This approach is known as polynomial regression , and in fact we saw an example of this method in Section 3.3.2. For large enough degree d , a polynomial regression allows us to produce an extremely non-linear curve. Notice that the coefficients in (7.1) can be easily estimated using least squares linear regression because this is just a standard linear model with predictors x i  x 2 i  x 3 i         x d i . Generally speaking, it is unusual to use d greater than 3 or 4 because for large values of d , the polynomial curve can become overly flexible and can take on some very strange shapes. This is especially true near the boundary of the X variable.  \nThe left-hand panel in Figure 7.1 is a plot of wage against age for the Wage data set, which contains income and demographic information for males who reside in the central Atlantic region of the United States. We see the results of fitting a degree-4 polynomial using least squares (solid blue curve). Even though this is a linear regression model like any other, the individual coefficients are not of particular interest. Instead, we look at the entire fitted function across a grid of 63 values for age from 18 to 80 in order to understand the relationship between age and wage .  \npolynomial regression\nDegree-4 Polynomial  \nFIGURE 7.1. The Wage data. Left: The solid blue curve is a degree-4 polynomial of wage (in thousands of dollars) as a function of age , fit by least squares. The dashed curves indicate an estimated 95 % confidence interval. Right: We model the binary event wage&gt;250 using logistic regression, again with a degree-4 polynomial. The fitted posterior probability of wage exceeding $250  000 is shown in blue, along with an estimated 95 % confidence interval.  \n<!-- image -->  \nIn Figure 7.1, a pair of dashed curves accompanies the fit; these are ( 2 × ) standard error curves. Let's see how these arise. Suppose we have computed the fit at a particular value of age , x 0 :  \n$$\\hat { f } ( x _ { 0 } ) = \\hat { \\beta } _ { 0 } + \\hat { \\beta } _ { 1 } x _ { 0 } + \\hat { \\beta } _ { 2 } x _ { 0 } ^ { 2 } + \\hat { \\beta } _ { 3 } x _ { 0 } ^ { 3 } + \\hat { \\beta } _ { 4 } x _ { 0 } ^ { 4 } .$$  \nWhat is the variance of the fit, i.e. Var ˆ f ( x 0 ) ? Least squares returns variance estimates for each of the fitted coefficients ˆ β j , as well as the covariances between pairs of coefficient estimates. We can use these to compute the estimated variance of ˆ f ( x 0 ) . 1 The estimated pointwise standard error of ˆ f ( x 0 ) is the square-root of this variance. This computation is repeated at each reference point x 0 , and we plot the fitted curve, as well as twice the standard error on either side of the fitted curve. We plot twice the standard error because, for normally distributed error terms, this quantity corresponds to an approximate 95 % confidence interval.  \nIt seems like the wages in Figure 7.1 are from two distinct populations: there appears to be a high earners group earning more than $250  000 per annum, as well as a low earners group. We can treat wage as a binary variable by splitting it into these two groups. Logistic regression can then be used to predict this binary response, using polynomial functions of age  \n1 If ˆ C is the 5 × 5 covariance matrix of the ˆ β j , and if /lscript T 0 = (1  x 0  x 2 0  x 3 0  x 4 0 ) , then Var [ ˆ f ( x 0 )] = /lscript T 0 ˆ C /lscript 0 .  \nas predictors. In other words, we fit the model  \n$$\\Pr ( y _ { i } > 2 5 0 ) | x _ { i } ) = \\frac { \\exp ( \\beta _ { 0 } + \\beta _ { 1 } x _ { i } + \\beta _ { 2 } x _ { i } ^ { 2 } + \\dots + \\beta _ { d } x _ { i } ^ { d } ) } { 1 + \\exp ( \\beta _ { 0 } + \\beta _ { 1 } x _ { i } + \\beta _ { 2 } x _ { i } ^ { 2 } + \\dots + \\beta _ { d } x _ { i } ^ { d } ) } .$$  \nThe result is shown in the right-hand panel of Figure 7.1. The gray marks on the top and bottom of the panel indicate the ages of the high earners and the low earners. The solid blue curve indicates the fitted probabilities of being a high earner, as a function of age . The estimated 95 % confidence interval is shown as well. We see that here the confidence intervals are fairly wide, especially on the right-hand side. Although the sample size for this data set is substantial ( n = 3  000 ), there are only 79 high earners, which results in a high variance in the estimated coefficients and consequently wide confidence intervals."", ""## 9.5 Relationship to Logistic Regression  \nWhen SVMs were first introduced in the mid-1990s, they made quite a splash in the statistical and machine learning communities. This was due in part to their good performance, good marketing, and also to the fact that the underlying approach seemed both novel and mysterious. The idea of finding a hyperplane that separates the data as well as possible, while allowing some violations to this separation, seemed distinctly different from classical approaches for classification, such as logistic regression and linear discriminant analysis. Moreover, the idea of using a kernel to expand the feature space in order to accommodate non-linear class boundaries appeared to be a unique and valuable characteristic.  \n$$x ) = \\beta _ { 0 } + \\beta _ { 1 } x _ { 1 } + \\dots + \\beta _ { p } x _ { p } \\text { as } \\\\ \\min _ { \\beta _ { 0 } , \\beta _ { 1 } , \\dots , \\beta _ { p } } \\left \\{ \\sum _ { i = 1 } ^ { n } \\max \\left [ 0 , 1 - y _ { i } f ( x _ { i } ) \\right ] + \\lambda \\sum _ { j = 1 } ^ { p } \\beta _ { j } ^ { 2 } \\right \\} , \\\\ \\intertext { s o r e } \\text { are } \\lambda \\text { is a nonnegative tuning parameter. } \\text { When } \\lambda \\text { is large then } \\beta _ { 1 } , \\dots , \\beta _ { p } \\\\ \\text {small } \\text { more volatile } \\text { to the margin are } \\text {tolerated } \\text { and } a \\text { low-variance }$$  \nHowever, since that time, deep connections between SVMs and other more classical statistical methods have emerged. It turns out that one can rewrite the criterion (9.12)-(9.15) for fitting the support vector classifier f ( X ) = β 0 + β 1 X 1 + · · · + β p X p as where λ is a nonnegative tuning parameter. When λ is large then β 1         β p are small, more violations to the margin are tolerated, and a low-variance but high-bias classifier will result. When λ is small then few violations to the margin will occur; this amounts to a high-variance but low-bias  \none-versusone one-versusall one-versusrest  \n<!-- image -->  \nclassifier. Thus, a small value of λ in (9.25) amounts to a small value of C in (9.15). Note that the λ ∑ p j =1 β 2 j term in (9.25) is the ridge penalty term from Section 6.2.1, and plays a similar role in controlling the bias-variance trade-off for the support vector classifier.  \nNow (9.25) takes the 'Loss + Penalty' form that we have seen repeatedly throughout this book:  \n$$\\min _ { \\beta _ { 0 } , \\beta _ { 1 } , \\dots , \\beta _ { p } } \\{ L ( X , y , \\beta ) + \\lambda P ( \\beta ) \\} \\, .$$  \nIn (9.26), L ( X  y  β ) is some loss function quantifying the extent to which the model, parametrized by β , fits the data ( X  y ) , and P ( β ) is a penalty function on the parameter vector β whose effect is controlled by a nonnegative tuning parameter λ . For instance, ridge regression and the lasso both take this form with  \n$$L ( X , y , \\beta ) \\equiv \\sum _ { i = 1 } ^ { n } \\left ( y _ { i } - \\beta _ { 0 } - \\sum _ { j = 1 } ^ { p } x _ { j } \\beta _ { j } \\right ) ^ { 2 } \\\\ P ( \\beta ) = \\sum _ { i = 1 } ^ { p } \\, , \\beta _ { i } ^ { 2 } \\text { for ridge regression and } P ( \\beta ) = \\sum _ { i = 1 } ^ { p }$$  \nand with P ( β ) = ∑ p j =1 β 2 j for ridge regression and P ( β ) = ∑ p j =1 ♣ β j ♣ for the lasso. In the case of (9.25) the loss function instead takes the form  \n$$L ( X , y , \\beta ) = \\sum _ { i = 1 } ^ { n } \\max \\left [ 0 , 1 - y _ { i } ( \\beta _ { 0 } + \\beta _ { 1 } x _ { i 1 } + \\cdots + \\beta _ { p } x _ { i p } ) \\right ] . \\\\$$  \nThis is known as hinge loss , and is depicted in Figure 9.12. However, it turns out that the hinge loss function is closely related to the loss function used in logistic regression, also shown in Figure 9.12.  \nWhen the support vector classifier and SVM were first introduced, it was thought that the tuning parameter C in (9.15) was an unimportant 'nuisance' parameter that could be set to some default value, like 1. However, the 'Loss + Penalty' formulation (9.25) for the support vector classifier indicates that this is not the case. The choice of tuning parameter is very important and determines the extent to which the model underfits or overfits the data, as illustrated, for example, in Figure 9.7.  \nAn interesting characteristic of the support vector classifier is that only support vectors play a role in the classifier obtained; observations on the correct side of the margin do not affect it. This is due to the fact that the loss function shown in Figure 9.12 is exactly zero for observations for which y i ( β 0 + β 1 x i 1 + · · · + β p x ip ) ≥ 1 ; these correspond to observations that are on the correct side of the margin. 3 In contrast, the loss function for logistic regression shown in Figure 9.12 is not exactly zero anywhere. But it is very small for observations that are far from the decision boundary. Due to the similarities between their loss functions, logistic regression and the support vector classifier often give very similar results. When the classes are well separated, SVMs tend to behave better than logistic regression; in more overlapping regimes, logistic regression is often preferred.  \n3 With this hinge-loss + penalty representation, the margin corresponds to the value one, and the width of the margin is determined by ∑ β 2 j .  \nhinge loss  \nFIGURE 9.12. The SVM and logistic regression loss functions are compared, as a function of y i ( β 0 + β 1 x i 1 + · · · + β p x ip ) . When y i ( β 0 + β 1 x i 1 + · · · + β p x ip ) is greater than 1, then the SVM loss is zero, since this corresponds to an observation that is on the correct side of the margin. Overall, the two loss functions have quite similar behavior.  \n<!-- image -->  \nWe have established that the support vector classifier is closely related to logistic regression and other preexisting statistical methods. Is the SVM unique in its use of kernels to enlarge the feature space to accommodate non-linear class boundaries? The answer to this question is 'no'. We could just as well perform logistic regression or many of the other classification methods seen in this book using non-linear kernels; this is closely related to some of the non-linear approaches seen in Chapter 7. However, for historical reasons, the use of non-linear kernels is much more widespread in the context of SVMs than in the context of logistic regression or other methods.  \nThough we have not addressed it here, there is in fact an extension of the SVM for regression (i.e. for a quantitative rather than a qualitative response), called support vector regression . In Chapter 3, we saw that least squares regression seeks coefficients β 0  β 1         β p such that the sum of squared residuals is as small as possible. (Recall from Chapter 3 that residuals are defined as y i -β 0 -β 1 x i 1 -· · · -β p x ip .) Support vector regression instead seeks coefficients that minimize a different type of loss, where only residuals larger in absolute value than some positive constant contribute to the loss function. This is an extension of the margin used in support vector classifiers to the regression setting.  \nsupport vector regression""]","The formula for a polynomial kernel of degree \( d \) in a Support Vector Machine is given by:

\[ 
K(x_i, x_{i'}) = \left( 1 + \sum_{j=1}^{p} x_{ij} x_{i'j} \right)^d 
\]

This kernel allows the SVM to fit a more flexible decision boundary by effectively mapping the input features into a higher-dimensional space involving polynomials of degree \( d \).","A polynomial kernel of degree d is defined by the formula K(x_i, x_i') = (1 + Σ_{j=1 to p} x_ij * x_i'j)^d.",1.0,,0.9999999999,1.0
¿Qué es la función de pérdida de bisagra (hinge loss) utilizada en los clasificadores de vectores de soporte?,"[""## 9.5 Relationship to Logistic Regression  \nWhen SVMs were first introduced in the mid-1990s, they made quite a splash in the statistical and machine learning communities. This was due in part to their good performance, good marketing, and also to the fact that the underlying approach seemed both novel and mysterious. The idea of finding a hyperplane that separates the data as well as possible, while allowing some violations to this separation, seemed distinctly different from classical approaches for classification, such as logistic regression and linear discriminant analysis. Moreover, the idea of using a kernel to expand the feature space in order to accommodate non-linear class boundaries appeared to be a unique and valuable characteristic.  \n$$x ) = \\beta _ { 0 } + \\beta _ { 1 } x _ { 1 } + \\dots + \\beta _ { p } x _ { p } \\text { as } \\\\ \\min _ { \\beta _ { 0 } , \\beta _ { 1 } , \\dots , \\beta _ { p } } \\left \\{ \\sum _ { i = 1 } ^ { n } \\max \\left [ 0 , 1 - y _ { i } f ( x _ { i } ) \\right ] + \\lambda \\sum _ { j = 1 } ^ { p } \\beta _ { j } ^ { 2 } \\right \\} , \\\\ \\intertext { s o r e } \\text { are } \\lambda \\text { is a nonnegative tuning parameter. } \\text { When } \\lambda \\text { is large then } \\beta _ { 1 } , \\dots , \\beta _ { p } \\\\ \\text {small } \\text { more volatile } \\text { to the margin are } \\text {tolerated } \\text { and } a \\text { low-variance }$$  \nHowever, since that time, deep connections between SVMs and other more classical statistical methods have emerged. It turns out that one can rewrite the criterion (9.12)-(9.15) for fitting the support vector classifier f ( X ) = β 0 + β 1 X 1 + · · · + β p X p as where λ is a nonnegative tuning parameter. When λ is large then β 1         β p are small, more violations to the margin are tolerated, and a low-variance but high-bias classifier will result. When λ is small then few violations to the margin will occur; this amounts to a high-variance but low-bias  \none-versusone one-versusall one-versusrest  \n<!-- image -->  \nclassifier. Thus, a small value of λ in (9.25) amounts to a small value of C in (9.15). Note that the λ ∑ p j =1 β 2 j term in (9.25) is the ridge penalty term from Section 6.2.1, and plays a similar role in controlling the bias-variance trade-off for the support vector classifier.  \nNow (9.25) takes the 'Loss + Penalty' form that we have seen repeatedly throughout this book:  \n$$\\min _ { \\beta _ { 0 } , \\beta _ { 1 } , \\dots , \\beta _ { p } } \\{ L ( X , y , \\beta ) + \\lambda P ( \\beta ) \\} \\, .$$  \nIn (9.26), L ( X  y  β ) is some loss function quantifying the extent to which the model, parametrized by β , fits the data ( X  y ) , and P ( β ) is a penalty function on the parameter vector β whose effect is controlled by a nonnegative tuning parameter λ . For instance, ridge regression and the lasso both take this form with  \n$$L ( X , y , \\beta ) \\equiv \\sum _ { i = 1 } ^ { n } \\left ( y _ { i } - \\beta _ { 0 } - \\sum _ { j = 1 } ^ { p } x _ { j } \\beta _ { j } \\right ) ^ { 2 } \\\\ P ( \\beta ) = \\sum _ { i = 1 } ^ { p } \\, , \\beta _ { i } ^ { 2 } \\text { for ridge regression and } P ( \\beta ) = \\sum _ { i = 1 } ^ { p }$$  \nand with P ( β ) = ∑ p j =1 β 2 j for ridge regression and P ( β ) = ∑ p j =1 ♣ β j ♣ for the lasso. In the case of (9.25) the loss function instead takes the form  \n$$L ( X , y , \\beta ) = \\sum _ { i = 1 } ^ { n } \\max \\left [ 0 , 1 - y _ { i } ( \\beta _ { 0 } + \\beta _ { 1 } x _ { i 1 } + \\cdots + \\beta _ { p } x _ { i p } ) \\right ] . \\\\$$  \nThis is known as hinge loss , and is depicted in Figure 9.12. However, it turns out that the hinge loss function is closely related to the loss function used in logistic regression, also shown in Figure 9.12.  \nWhen the support vector classifier and SVM were first introduced, it was thought that the tuning parameter C in (9.15) was an unimportant 'nuisance' parameter that could be set to some default value, like 1. However, the 'Loss + Penalty' formulation (9.25) for the support vector classifier indicates that this is not the case. The choice of tuning parameter is very important and determines the extent to which the model underfits or overfits the data, as illustrated, for example, in Figure 9.7.  \nAn interesting characteristic of the support vector classifier is that only support vectors play a role in the classifier obtained; observations on the correct side of the margin do not affect it. This is due to the fact that the loss function shown in Figure 9.12 is exactly zero for observations for which y i ( β 0 + β 1 x i 1 + · · · + β p x ip ) ≥ 1 ; these correspond to observations that are on the correct side of the margin. 3 In contrast, the loss function for logistic regression shown in Figure 9.12 is not exactly zero anywhere. But it is very small for observations that are far from the decision boundary. Due to the similarities between their loss functions, logistic regression and the support vector classifier often give very similar results. When the classes are well separated, SVMs tend to behave better than logistic regression; in more overlapping regimes, logistic regression is often preferred.  \n3 With this hinge-loss + penalty representation, the margin corresponds to the value one, and the width of the margin is determined by ∑ β 2 j .  \nhinge loss  \nFIGURE 9.12. The SVM and logistic regression loss functions are compared, as a function of y i ( β 0 + β 1 x i 1 + · · · + β p x ip ) . When y i ( β 0 + β 1 x i 1 + · · · + β p x ip ) is greater than 1, then the SVM loss is zero, since this corresponds to an observation that is on the correct side of the margin. Overall, the two loss functions have quite similar behavior.  \n<!-- image -->  \nWe have established that the support vector classifier is closely related to logistic regression and other preexisting statistical methods. Is the SVM unique in its use of kernels to enlarge the feature space to accommodate non-linear class boundaries? The answer to this question is 'no'. We could just as well perform logistic regression or many of the other classification methods seen in this book using non-linear kernels; this is closely related to some of the non-linear approaches seen in Chapter 7. However, for historical reasons, the use of non-linear kernels is much more widespread in the context of SVMs than in the context of logistic regression or other methods.  \nThough we have not addressed it here, there is in fact an extension of the SVM for regression (i.e. for a quantitative rather than a qualitative response), called support vector regression . In Chapter 3, we saw that least squares regression seeks coefficients β 0  β 1         β p such that the sum of squared residuals is as small as possible. (Recall from Chapter 3 that residuals are defined as y i -β 0 -β 1 x i 1 -· · · -β p x ip .) Support vector regression instead seeks coefficients that minimize a different type of loss, where only residuals larger in absolute value than some positive constant contribute to the loss function. This is an extension of the margin used in support vector classifiers to the regression setting.  \nsupport vector regression"", ""## 10.2 Multilayer Neural Networks  \nModern neural networks typically have more than one hidden layer, and often many units per layer. In theory a single hidden layer with a large number of units has the ability to approximate most functions. However, the learning task of discovering a good solution is made much easier with multiple layers each of modest size.  \nWe will illustrate a large dense network on the famous and publicly available MNIST handwritten digit dataset. 1 Figure 10.3 shows examples of these digits. The idea is to build a model to classify the images into their correct digit class 0-9. Every image has p = 28 × 28 = 784 pixels, each of which is an eight-bit grayscale value between 0 and 255 representing  \n1 See LeCun, Cortes, and Burges (2010) 'The MNIST database of handwritten digits', available at http://yann.lecun.com/exdb/mnist .  \nFIGURE 10.3. Examples of handwritten digits from the MNIST corpus. Each grayscale image has 28 × 28 pixels, each of which is an eight-bit number (0-255) which represents how dark that pixel is. The first 3, 5, and 8 are enlarged to show their 784 individual pixel values.  \n<!-- image -->  \nthe relative amount of the written digit in that tiny square. 2 These pixels are stored in the input vector X (in, say, column order). The output is the class label, represented by a vector Y = ( Y 0  Y 1         Y 9 ) of 10 dummy variables, with a one in the position corresponding to the label, and zeros elsewhere. In the machine learning community, this is known as one-hot encoding . There are 60,000 training images, and 10,000 test images.  \nFigure 10.4 shows a multilayer network architecture that works well for solving the digit-classification task. It differs from Figure 10.1 in several ways:  \nOn a historical note, digit recognition problems were the catalyst that accelerated the development of neural network technology in the late 1980s at AT&amp;T Bell Laboratories and elsewhere. Pattern recognition tasks of this kind are relatively simple for humans. Our visual system occupies a large fraction of our brains, and good recognition is an evolutionary force for survival. These tasks are not so simple for machines, and it has taken more than 30 years to refine the neural-network architectures to match human performance.  \n- It has two hidden layers L 1 (256 units) and L 2 (128 units) rather than one. Later we will see a network with seven hidden layers.\n- It has ten output variables, rather than one. In this case the ten variables really represent a single qualitative variable and so are quite dependent. (We have indexed them by the digit class 0-9 rather than 1-10, for clarity.) More generally, in multi-task learning one can predict different responses simultaneously with a single network; they all have a say in the formation of the hidden layers.\n- The loss function used for training the network is tailored for the multiclass classification task.  \n2 In the analog-to-digital conversion process, only part of the written numeral may fall in the square representing a particular pixel.  \none-hot encoding multi-task learning  \nFIGURE 10.4. Neural network diagram with two hidden layers and multiple outputs, suitable for the MNIST handwritten-digit problem. The input layer has p = 784 units, the two hidden layers K 1 = 256 and K 2 = 128 units respectively, and the output layer 10 units. Along with intercepts (referred to as biases in the deep-learning community) this network has 235,146 parameters (referred to as weights).  \n<!-- image -->  \nThe first hidden layer is as in (10.2), with  \n$$\\begin{array} { r l r } { A _ { k } ^ { ( 1 ) } } & { = } & { h _ { k } ^ { ( 1 ) } ( X ) } \\\\ & { = } & { g ( w _ { k 0 } ^ { ( 1 ) } + \\sum _ { j = 1 } ^ { p } w _ { k j } ^ { ( 1 ) } X _ { j } ) } \\\\ { \\dots } & { K _ { 1 } \\colon T h e s e d h i n d e h i v e t r e a t i v a t i o n s \\ A _ { k } ^ { ( 1 ) } \\ o f } \\end{array}$$  \nfor k = 1         K 1 . The second hidden layer treats the activations A (1) k of the first hidden layer as inputs and computes new activations  \n$$\\begin{array} { r l r } { A _ { \\ell } ^ { ( 2 ) } } & { = } & { h _ { \\ell } ^ { ( 2 ) } ( X ) } \\\\ & { = } & { g ( w _ { \\ell 0 } ^ { ( 2 ) } + \\sum _ { k = 1 } ^ { K _ { 1 } } w _ { k } ^ { ( 2 ) } A _ { k } ^ { ( 1 ) } ) } \\\\ { \\dots , K _ { 2 } . \\, \\text {Notice that each of the activations in the second layer} } \\end{array}$$  \nfor /lscript = 1         K 2   Notice that each of the activations in the second layer A (2) /lscript = h (2) /lscript ( X ) is a function of the input vector X . This is the case because while they are explicitly a function of the activations A (1) k from layer L 1 , these in turn are functions of X . This would also be the case with more hidden layers. Thus, through a chain of transformations, the network is able to build up fairly complex transformations of X that ultimately feed into the output layer as features.  \nWe have introduced additional superscript notation such as h (2) /lscript ( X ) and w (2) /lscript j in (10.10) and (10.11) to indicate to which layer the activations and weights (coefficients) belong, in this case layer 2. The notation W 1 in Fig-  \nure 10.4 represents the entire matrix of weights that feed from the input layer to the first hidden layer L 1 . This matrix will have 785 × 256 = 200  960 elements; there are 785 rather than 784 because we must account for the intercept or bias term. 3  \nEach element A (1) k feeds to the second hidden layer L 2 via the matrix of weights W 2 of dimension 257 × 128 = 32  896 .  \nWe now get to the output layer, where we now have ten responses rather than one. The first step is to compute ten different linear models similar to our single model (10.1),  \n$$\\begin{array} { r l } { \\mathrm e } { \\mathrm e } { \\mathrm d } { \\mathrm m } & { Z _ { m } \\ = \\ \\beta _ { m 0 } + \\sum _ { \\ell = 1 } ^ { K _ { 2 } } \\beta _ { m \\ell } h _ { \\ell } ^ { ( 2 ) } ( X ) } \\\\ & { = \\ \\beta _ { m 0 } + \\sum _ { \\ell = 1 } ^ { K _ { 2 } } \\beta _ { m \\ell } A _ { \\ell } ^ { ( 2 ) } , } \\\\ { 1 , 1 , \\dots , 9 . } \\, \\mathrm T h i r x { \\mathrm B } { \\mathrm s o r s a l l } & { 1 2 9 \\times 1 0 = 1 , 2 9 0 \\, o f \\, \\mathrm s e } \\end{array}$$  \nIf these were all separate quantitative responses, we would simply set each f m ( X ) = Z m and be done. However, we would like our estimates to represent class probabilities f m ( X ) = Pr( Y = m ♣ X ) , just like in multinomial logistic regression in Section 4.3.5. So we use the special softmax activation function (see (4.13) on page 145), for m = 0  1         9 . The matrix B stores all 129 × 10 = 1  290 of these weights.  \n$$f _ { m } ( X ) = & \\Pr ( Y = m | X ) = \\frac { e ^ { Z _ { m } } } { \\sum _ { \\ell = 0 } ^ { 9 } e ^ { Z _ { \\ell } } } , \\\\ 0 , 1 , \\dots , 9 \\text {  This ensures that the 10 numbers behave like proba-} \\\\ \\text {non-negative and sum to one} \\ \\text {Even though the goal is to build}$$  \nTo train this network, since the response is qualitative, we look for coefficient estimates that minimize the negative multinomial log-likelihood for m = 0  1         9 . This ensures that the 10 numbers behave like probabilities (non-negative and sum to one). Even though the goal is to build a classifier, our model actually estimates a probability for each of the 10 classes. The classifier then assigns the image to the class with the highest probability.  \n$$- \\sum _ { i = 1 } ^ { n } \\sum _ { m = 0 } ^ { 9 } y _ { i m } \\log ( f _ { m } ( x _ { i } ) ) , \\\\ \\intertext { t h o w } \\intertext { a n d } \\intertext { s u n t w }$$  \nTable 10.1 compares the test performance of the neural network with two simple models presented in Chapter 4 that make use of linear decision boundaries: multinomial logistic regression and linear discriminant analysis. The improvement of neural networks over both of these linear methods is dramatic: the network with dropout regularization achieves a test error rate below 2% on the 10  000 test images. (We describe dropout regularization in Section 10.7.3.) In Section 10.9.2 of the lab, we present the code for fitting this model, which runs in just over two minutes on a laptop computer.  \nalso known as the cross-entropy . This is a generalization of the criterion (4.5) for two-class logistic regression. Details on how to minimize this objective are given in Section 10.7. If the response were quantitative, we would instead minimize squared-error loss as in (10.9).  \n3 The use of 'weights' for coefficients and 'bias' for the intercepts w k 0 in (10.2) is popular in the machine learning community; this use of bias is not to be confused with the 'bias-variance' usage elsewhere in this book.  \nbias softmax  \ncrossentropy  \nTABLE 10.1. Test error rate on the MNIST data, for neural networks with two forms of regularization, as well as multinomial logistic regression and linear discriminant analysis. In this example, the extra complexity of the neural network leads to a marked improvement in test error.  \n| Method                                  | Test Error                 |\n|-----------------------------------------|----------------------------|\n| Neural Network + Ridge Regularization   | 2   3%  |\n| Neural Network + Dropout Regularization | 1   8%  |\n| Multinomial Logistic Regression         | 7   2%  |\n| Linear Discriminant Analysis            | 12   7% |  \nFIGURE 10.5. A sample of images from the CIFAR100 database: a collection of natural images from everyday life, with 100 different classes represented.  \n<!-- image -->  \nAdding the number of coefficients in W 1 , W 2 and B , we get 235  146 in all, more than 33 times the number 785 × 9 = 7  065 needed for multinomial logistic regression. Recall that there are 60  000 images in the training set. While this might seem like a large training set, there are almost four times as many coefficients in the neural network model as there are observations in the training set! To avoid overfitting, some regularization is needed. In this example, we used two forms of regularization: ridge regularization, which is similar to ridge regression from Chapter 6, and dropout regularization. We discuss both forms of regularization in Section 10.7."", ""### 9.2.2 Details of the Support Vector Classifier  \nThe support vector classifier classifies a test observation depending on which side of a hyperplane it lies. The hyperplane is chosen to correctly separate most of the training observations into the two classes, but may support vector classifier soft margin classifier  \nFIGURE 9.6. Left: A support vector classifier was fit to a small data set. The hyperplane is shown as a solid line and the margins are shown as dashed lines. Purple observations: Observations 3  4  5 , and 6 are on the correct side of the margin, observation 2 is on the margin, and observation 1 is on the wrong side of the margin. Blue observations: Observations 7 and 10 are on the correct side of the margin, observation 9 is on the margin, and observation 8 is on the wrong side of the margin. No observations are on the wrong side of the hyperplane. Right: Same as left panel with two additional points, 11 and 12 . These two observations are on the wrong side of the hyperplane and the wrong side of the margin.  \n<!-- image -->  \nmisclassify a few observations. It is the solution to the optimization problem  \n$$\\max _ { \\beta _ { 0 } , \\beta _ { 1 } , \\dots , \\beta _ { p } , \\epsilon _ { 1 } , \\dots , \\epsilon _ { n } , M } \\ M$$  \n$$& \\quad ( \\pi ^ { 2 } , \\pi ^ { 2 } , \\pi ^ { 2 } ) ^ { p } , \\\\ & \\quad \\text {subject to } \\sum _ { j = 1 } ^ { p } \\beta _ { j } ^ { 2 } = 1 , \\\\ & \\quad \\\\ & \\quad \\pi ( \\beta _ { j } , \\pi _ { 0 } , \\beta _ { r } \\pi _ { 0 } , \\pi _ { 1 } \\pi _ { 0 } , \\dots , \\pi _ { k } ) > M ( 1 , \\pi _ { 0 } )$$  \n$$\\epsilon _ { i } \\geq 0 , \\ \\sum _ { i = 1 } ^ { n } \\epsilon _ { i } \\leq C ,$$  \n$$y _ { i } ( \\beta _ { 0 } + \\beta _ { 1 } x _ { i 1 } + \\beta _ { 2 } x _ { i 2 } + \\cdots + \\beta _ { p } x _ { i p } ) \\geq M ( 1 - \\epsilon _ { i } ) ,$$  \nwhere C is a nonnegative tuning parameter. As in (9.11), M is the width of the margin; we seek to make this quantity as large as possible. In (9.14), /epsilon1 1         /epsilon1 n are slack variables that allow individual observations to be on the wrong side of the margin or the hyperplane; we will explain them in greater detail momentarily. Once we have solved (9.12)-(9.15), we classify a test observation x ∗ as before, by simply determining on which side of the hyperplane it lies. That is, we classify the test observation based on the sign of f ( x ∗ ) = β 0 + β 1 x ∗ 1 + · · · + β p x ∗ p .  \nThe problem (9.12)-(9.15) seems complex, but insight into its behavior can be made through a series of simple observations presented below. First of all, the slack variable /epsilon1 i tells us where the i th observation is located, relative to the hyperplane and relative to the margin. If /epsilon1 i = 0 then the i th observation is on the correct side of the margin, as we saw in Section 9.1.4. If /epsilon1 i &gt; 0 then the i th observation is on the wrong side of the margin, and we say that the i th observation has violated the margin. If /epsilon1 i &gt; 1 then it is on the wrong side of the hyperplane.  \nslack variable  \nWe now consider the role of the tuning parameter C . In (9.15), C bounds the sum of the /epsilon1 i 's, and so it determines the number and severity of the violations to the margin (and to the hyperplane) that we will tolerate. We can think of C as a budget for the amount that the margin can be violated by the n observations. If C = 0 then there is no budget for violations to the margin, and it must be the case that /epsilon1 1 = · · · = /epsilon1 n = 0 , in which case (9.12)-(9.15) simply amounts to the maximal margin hyperplane optimization problem (9.9)-(9.11). (Of course, a maximal margin hyperplane exists only if the two classes are separable.) For C &gt; 0 no more than C observations can be on the wrong side of the hyperplane, because if an observation is on the wrong side of the hyperplane then /epsilon1 i &gt; 1 , and (9.15) requires that ∑ n i =1 /epsilon1 i ≤ C . As the budget C increases, we become more tolerant of violations to the margin, and so the margin will widen. Conversely, as C decreases, we become less tolerant of violations to the margin and so the margin narrows. An example is shown in Figure 9.7.  \nThe optimization problem (9.12)-(9.15) has a very interesting property: it turns out that only observations that either lie on the margin or that violate the margin will affect the hyperplane, and hence the classifier obtained. In other words, an observation that lies strictly on the correct side of the margin does not affect the support vector classifier! Changing the position of that observation would not change the classifier at all, provided that its position remains on the correct side of the margin. Observations that lie directly on the margin, or on the wrong side of the margin for their class, are known as support vectors . These observations do affect the support vector classifier.  \nIn practice, C is treated as a tuning parameter that is generally chosen via cross-validation. As with the tuning parameters that we have seen throughout this book, C controls the bias-variance trade-off of the statistical learning technique. When C is small, we seek narrow margins that are rarely violated; this amounts to a classifier that is highly fit to the data, which may have low bias but high variance. On the other hand, when C is larger, the margin is wider and we allow more violations to it; this amounts to fitting the data less hard and obtaining a classifier that is potentially more biased but may have lower variance.  \nThe fact that only support vectors affect the classifier is in line with our previous assertion that C controls the bias-variance trade-off of the support vector classifier. When the tuning parameter C is large, then the margin is wide, many observations violate the margin, and so there are many support vectors. In this case, many observations are involved in determining the hyperplane. The top left panel in Figure 9.7 illustrates this setting: this classifier has low variance (since many observations are support vectors) but potentially high bias. In contrast, if C is small, then there will be fewer support vectors and hence the resulting classifier will have low bias but high variance. The bottom right panel in Figure 9.7 illustrates this setting, with only eight support vectors.  \nThe fact that the support vector classifier's decision rule is based only on a potentially small subset of the training observations (the support vectors) means that it is quite robust to the behavior of observations that are far away from the hyperplane. This property is distinct from some of  \nFIGURE 9.7. A support vector classifier was fit using four different values of the tuning parameter C in (9.12)-(9.15). The largest value of C was used in the top left panel, and smaller values were used in the top right, bottom left, and bottom right panels. When C is large, then there is a high tolerance for observations being on the wrong side of the margin, and so the margin will be large. As C decreases, the tolerance for observations being on the wrong side of the margin decreases, and the margin narrows.  \n<!-- image -->  \nthe other classification methods that we have seen in preceding chapters, such as linear discriminant analysis. Recall that the LDA classification rule depends on the mean of all of the observations within each class, as well as the within-class covariance matrix computed using all of the observations. In contrast, logistic regression, unlike LDA, has very low sensitivity to observations far from the decision boundary. In fact we will see in Section 9.5 that the support vector classifier and logistic regression are closely related."", ""### 7.5.1 An Overview of Smoothing Splines  \nIn fitting a smooth curve to a set of data, what we really want to do is find some function, say g ( x ) , that fits the observed data well: that is, we want RSS = ∑ n i =1 ( y i -g ( x i )) 2 to be small. However, there is a problem with this approach. If we don't put any constraints on g ( x i ) , then we can always make RSS zero simply by choosing g such that it interpolates all of the y i . Such a function would woefully overfit the data-it would be far too flexible. What we really want is a function g that makes RSS small, but that is also smooth .  \nHow might we ensure that g is smooth? There are a number of ways to do this. A natural approach is to find the function g that minimizes  \n$$\\sum _ { i = 1 } ^ { n } ( y _ { i } - g ( x _ { i } ) ) ^ { 2 } + \\lambda \\int g ^ { \\prime \\prime } ( t ) ^ { 2 } d t \\\\$$  \nwhere λ is a nonnegative tuning parameter . The function g that minimizes (7.11) is known as a smoothing spline .  \nWhat does (7.11) mean? Equation 7.11 takes the 'Loss+Penalty' formulation that we encounter in the context of ridge regression and the lasso in Chapter 6. The term ∑ n i =1 ( y i -g ( x i )) 2 is a loss function that encourages g to fit the data well, and the term λ ∫ g ′′ ( t ) 2 dt is a penalty term that penalizes the variability in g . The notation g ′′ ( t ) indicates the second derivative of the function g . The first derivative g ′ ( t ) measures the slope smoothing spline  \nloss function  \nof a function at t , and the second derivative corresponds to the amount by which the slope is changing. Hence, broadly speaking, the second derivative of a function is a measure of its roughness : it is large in absolute value if g ( t ) is very wiggly near t , and it is close to zero otherwise. (The second derivative of a straight line is zero; note that a line is perfectly smooth.) The ∫ notation is an integral , which we can think of as a summation over the range of t . In other words, ∫ g ′′ ( t ) 2 dt is simply a measure of the total change in the function g ′ ( t ) , over its entire range. If g is very smooth, then g ′ ( t ) will be close to constant and ∫ g ′′ ( t ) 2 dt will take on a small value. Conversely, if g is jumpy and variable then g ′ ( t ) will vary significantly and ∫ g ′′ ( t ) 2 dt will take on a large value. Therefore, in (7.11), λ ∫ g ′′ ( t ) 2 dt encourages g to be smooth. The larger the value of λ , the smoother g will be.  \nThe function g ( x ) that minimizes (7.11) can be shown to have some special properties: it is a piecewise cubic polynomial with knots at the unique values of x 1         x n , and continuous first and second derivatives at each knot. Furthermore, it is linear in the region outside of the extreme knots. In other words, the function g ( x ) that minimizes (7.11) is a natural cubic spline with knots at x 1         x n ! However, it is not the same natural cubic spline that one would get if one applied the basis function approach described in Section 7.4.3 with knots at x 1         x n -rather, it is a shrunken version of such a natural cubic spline, where the value of the tuning parameter λ in (7.11) controls the level of shrinkage.  \nWhen λ = 0 , then the penalty term in (7.11) has no effect, and so the function g will be very jumpy and will exactly interpolate the training observations. When λ → ∞ , g will be perfectly smooth-it will just be a straight line that passes as closely as possible to the training points. In fact, in this case, g will be the linear least squares line, since the loss function in (7.11) amounts to minimizing the residual sum of squares. For an intermediate value of λ , g will approximate the training observations but will be somewhat smooth. We see that λ controls the bias-variance trade-off of the smoothing spline.""]","La función de pérdida de bisagra, o *hinge loss*, es una función de pérdida utilizada en los clasificadores de vectores de soporte (SVM, por sus siglas en inglés). Esta función cuantifica el grado en que un modelo, parametrizado por un vector de coeficientes \(\beta\), se ajusta a los datos. La función de pérdida de bisagra se define como:

\[ 
L(X, y, \beta) = \sum_{i=1}^{n} \max \left[ 0, 1 - y_i (\beta_0 + \beta_1 x_{i1} + \cdots + \beta_p x_{ip}) \right].
\]

En esta fórmula, \(y_i\) representa la etiqueta de clase de la observación \(i\), y \((\beta_0 + \beta_1 x_{i1} + \cdots + \beta_p x_{ip})\) es la predicción del modelo para esa observación. La función de pérdida de bisagra es cero cuando la predicción está en el lado correcto del margen (es decir, cuando \(y_i (\beta_0 + \beta_1 x_{i1} + \cdots + \beta_p x_{ip}) \geq 1\)), y aumenta linealmente cuando la predicción está en el lado incorrecto del margen.

Esta función de pérdida es fundamental para los SVM porque permite que el modelo tolere algunas violaciones al margen, lo que ayuda a controlar el equilibrio entre el sesgo y la varianza del clasificador.","La función de pérdida de bisagra se define como max[0, 1 - yᵢ(β₀ + β₁xᵢ₁ + ... + βₚxᵢₚ)]. Esta función es cero para las observaciones que están en el lado correcto del margen, lo que significa que solo los vectores de soporte afectan al clasificador.",1.0,0.8768194563065558,0.8333333332916666,1.0
What are the two main steps involved in Principal Component Regression (PCR)?,"['### 6.3.1 Principal Components Regression  \nPrincipal components analysis (PCA) is a popular approach for deriving  \na low-dimensional set of features from a large set of variables. PCA is discussed in greater detail as a tool for unsupervised learning in Chapter 12. Here we describe its use as a dimension reduction technique for regression.\nAn Overview of Principal Components Analysis  \nPCA is a technique for reducing the dimension of an n × p data matrix X . The first principal component direction of the data is that along which the observations vary the most . For instance, consider Figure 6.14, which shows population size ( pop ) in tens of thousands of people, and ad spending for a particular company ( ad ) in thousands of dollars, for 100 cities. 6 The green solid line represents the first principal component direction of the data. We can see by eye that this is the direction along which there is the greatest variability in the data. That is, if we projected the 100 observations onto this line (as shown in the left-hand panel of Figure 6.15), then the resulting projected observations would have the largest possible variance; projecting the observations onto any other line would yield projected observations with lower variance. Projecting a point onto a line simply involves finding the location on the line which is closest to the point.  \nThe first principal component is displayed graphically in Figure 6.14, but how can it be summarized mathematically? It is given by the formula  \n$$Z _ { 1 } = 0 . 8 3 9 \\times ( \\text {pop} - \\overline { \\text {pop} } ) + 0 . 5 4 4 \\times ( \\text {ad} - \\overline { \\text {ad} } ) .$$  \nHere φ 11 = 0   839 and φ 21 = 0   544 are the principal component loadings, which define the direction referred to above. In (6.19), pop indicates the mean of all pop values in this data set, and ad indicates the mean of all advertising spending. The idea is that out of every possible linear combination of pop and ad such that φ 2 11 + φ 2 21 = 1 , this particular linear combination yields the highest variance: i.e. this is the linear combination for which Var( φ 11 × ( pop -pop ) + φ 21 × ( ad -ad )) is maximized. It is necessary to consider only linear combinations of the form φ 2 11 + φ 2 21 = 1 , since otherwise we could increase φ 11 and φ 21 arbitrarily in order to blow up the variance. In (6.19), the two loadings are both positive and have similar size, and so Z 1 is almost an average of the two variables.  \nSince n = 100 , pop and ad are vectors of length 100 , and so is Z 1 in (6.19). For instance,  \n$$z _ { i 1 } = 0 . 8 3 9 \\times ( \\text {pop} _ { i } - \\overline { \\text {pop} } ) + 0 . 5 4 4 \\times ( \\text {ad} _ { i } - \\overline { \\text {ad} } ) .$$  \nThe values of z 11         z n 1 are known as the principal component scores , and can be seen in the right-hand panel of Figure 6.15.  \nThere is also another interpretation of PCA: the first principal component vector defines the line that is as close as possible to the data. For instance, in Figure 6.14, the first principal component line minimizes the sum of the squared perpendicular distances between each point and the line. These distances are plotted as dashed line segments in the left-hand  \n6 This dataset is distinct from the Advertising data discussed in Chapter 3.  \nFIGURE 6.15. A subset of the advertising data. The mean pop and ad budgets are indicated with a blue circle. Left: The first principal component direction is shown in green. It is the dimension along which the data vary the most, and it also defines the line that is closest to all n of the observations. The distances from each observation to the principal component are represented using the black dashed line segments. The blue dot represents ( pop  ad ) . Right: The left-hand panel has been rotated so that the first principal component direction coincides with the x-axis.  \n<!-- image -->  \npanel of Figure 6.15, in which the crosses represent the projection of each point onto the first principal component line. The first principal component has been chosen so that the projected observations are as close as possible to the original observations.  \nWe can think of the values of the principal component Z 1 as singlenumber summaries of the joint pop and ad budgets for each location. In this example, if z i 1 = 0   839 × ( pop i -pop ) + 0   544 × ( ad i -ad ) &lt; 0 , then this indicates a city with below-average population size and belowaverage ad spending. A positive score suggests the opposite. How well can a single number represent both pop and ad ? In this case, Figure 6.14 indicates that pop and ad have approximately a linear relationship, and so we might expect that a single-number summary will work well. Figure 6.16 displays z i 1 versus both pop and ad . 7 The plots show a strong relationship between the first principal component and the two features. In other words, the first principal component appears to capture most of the information contained in the pop and ad predictors.  \nIn the right-hand panel of Figure 6.15, the left-hand panel has been rotated so that the first principal component direction coincides with the x -axis. It is possible to show that the first principal component score for the i th observation, given in (6.20), is the distance in the x -direction of the i th cross from zero. So for example, the point in the bottom-left corner of the left-hand panel of Figure 6.15 has a large negative principal component score, z i 1 = -26   1 , while the point in the top-right corner has a large positive score, z i 1 = 18   7 . These scores can be computed directly using (6.20).  \nSo far we have concentrated on the first principal component. In general, one can construct up to p distinct principal components. The second  \n7 The principal components were calculated after first standardizing both pop and ad , a common approach. Hence, the x-axes on Figures 6.15 and 6.16 are not on the same scale.  \nFIGURE 6.16. Plots of the first principal component scores z i 1 versus pop and ad . The relationships are strong.  \n<!-- image -->  \nprincipal component Z 2 is a linear combination of the variables that is uncorrelated with Z 1 , and has largest variance subject to this constraint. The second principal component direction is illustrated as a dashed blue line in Figure 6.14. It turns out that the zero correlation condition of Z 1 with Z 2 is equivalent to the condition that the direction must be perpendicular , or orthogonal , to the first principal component direction. The second principal component is given by the formula  \n$$Z _ { 2 } = 0 . 5 4 4 \\times ( \\text {pop} - \\overline { \\text {pop} } ) - 0 . 8 3 9 \\times ( \\text {ad} - \\overline { \\text {ad} } ) .$$  \nSince the advertising data has two predictors, the first two principal components contain all of the information that is in pop and ad . However, by construction, the first component will contain the most information. Consider, for example, the much larger variability of z i 1 (the x -axis) versus z i 2 (the y -axis) in the right-hand panel of Figure 6.15. The fact that the second principal component scores are much closer to zero indicates that this component captures far less information. As another illustration, Figure 6.17 displays z i 2 versus pop and ad . There is little relationship between the second principal component and these two predictors, again suggesting that in this case, one only needs the first principal component in order to accurately represent the pop and ad budgets.  \nWith two-dimensional data, such as in our advertising example, we can construct at most two principal components. However, if we had other predictors, such as population age, income level, education, and so forth, then additional components could be constructed. They would successively maximize variance, subject to the constraint of being uncorrelated with the preceding components.\nThe Principal Components Regression Approach  \nThe principal components regression (PCR) approach involves constructing the first M principal components, Z 1         Z M , and then using these components as the predictors in a linear regression model that is fit using least squares. The key idea is that often a small number of principal components suffice to explain most of the variability in the data, as well as the relationship with the response. In other words, we assume that the perpendicular orthogonal  \nprincipal components regression  \nFIGURE 6.17. Plots of the second principal component scores z i 2 versus pop and ad . The relationships are weak.  \n<!-- image -->  \nFIGURE 6.18. PCR was applied to two simulated data sets. In each panel, the horizontal dashed line represents the irreducible error. Left: Simulated data from Figure 6.8. Right: Simulated data from Figure 6.9.  \n<!-- image -->  \ndirections in which X 1         X p show the most variation are the directions that are associated with Y . While this assumption is not guaranteed to be true, it often turns out to be a reasonable enough approximation to give good results.  \nFigure 6.18 displays the PCR fits on the simulated data sets from Figures 6.8 and 6.9. Recall that both data sets were generated using n = 50 observations and p = 45 predictors. However, while the response in the first data set was a function of all the predictors, the response in the second data set was generated using only two of the predictors. The curves are plotted as a function of M , the number of principal components used as predictors in the regression model. As more principal components are used  \nIf the assumption underlying PCR holds, then fitting a least squares model to Z 1         Z M will lead to better results than fitting a least squares model to X 1         X p , since most or all of the information in the data that relates to the response is contained in Z 1         Z M , and by estimating only M /lessmuch p coefficients we can mitigate overfitting. In the advertising data, the first principal component explains most of the variance in both pop and ad , so a principal component regression that uses this single variable to predict some response of interest, such as sales , will likely perform quite well.  \nFIGURE 6.19. PCR, ridge regression, and the lasso were applied to a simulated data set in which the first five principal components of X contain all the information about the response Y . In each panel, the irreducible error Var( /epsilon1 ) is shown as a horizontal dashed line. Left: Results for PCR. Right: Results for lasso (solid) and ridge regression (dotted). The x -axis displays the shrinkage factor of the coefficient estimates, defined as the /lscript 2 norm of the shrunken coefficient estimates divided by the /lscript 2 norm of the least squares estimate.  \n<!-- image -->  \nin the regression model, the bias decreases, but the variance increases. This results in a typical U-shape for the mean squared error. When M = p = 45 , then PCR amounts simply to a least squares fit using all of the original predictors. The figure indicates that performing PCR with an appropriate choice of M can result in a substantial improvement over least squares, especially in the left-hand panel. However, by examining the ridge regression and lasso results in Figures 6.5, 6.8, and 6.9, we see that PCR does not perform as well as the two shrinkage methods in this example.  \nWe note that even though PCR provides a simple way to perform regression using M &lt; p predictors, it is not a feature selection method. This is because each of the M principal components used in the regression is a linear combination of all p of the original features. For instance, in (6.19), Z 1 was a linear combination of both pop and ad . Therefore, while PCR often performs quite well in many practical settings, it does not result in the  \nThe relatively worse performance of PCR in Figure 6.18 is a consequence of the fact that the data were generated in such a way that many principal components are required in order to adequately model the response. In contrast, PCR will tend to do well in cases when the first few principal components are sufficient to capture most of the variation in the predictors as well as the relationship with the response. The left-hand panel of Figure 6.19 illustrates the results from another simulated data set designed to be more favorable to PCR. Here the response was generated in such a way that it depends exclusively on the first five principal components. Now the bias drops to zero rapidly as M , the number of principal components used in PCR, increases. The mean squared error displays a clear minimum at M = 5 . The right-hand panel of Figure 6.19 displays the results on these data using ridge regression and the lasso. All three methods offer a significant improvement over least squares. However, PCR and ridge regression slightly outperform the lasso.  \nFIGURE 6.20. Left: PCR standardized coefficient estimates on the Credit data set for different values of M . Right: The ten-fold cross-validation MSE obtained using PCR, as a function of M .  \n<!-- image -->  \ndevelopment of a model that relies upon a small set of the original features. In this sense, PCR is more closely related to ridge regression than to the lasso. In fact, one can show that PCR and ridge regression are very closely related. One can even think of ridge regression as a continuous version of PCR! 8  \nWhen performing PCR, we generally recommend standardizing each predictor, using (6.6), prior to generating the principal components. This standardization ensures that all variables are on the same scale. In the absence of standardization, the high-variance variables will tend to play a larger role in the principal components obtained, and the scale on which the variables are measured will ultimately have an effect on the final PCR model. However, if the variables are all measured in the same units (say, kilograms, or inches), then one might choose not to standardize them.  \nIn PCR, the number of principal components, M , is typically chosen by cross-validation. The results of applying PCR to the Credit data set are shown in Figure 6.20; the right-hand panel displays the cross-validation errors obtained, as a function of M . On these data, the lowest cross-validation error occurs when there are M = 10 components; this corresponds to almost no dimension reduction at all, since PCR with M = 11 is equivalent to simply performing least squares.', '## 12.2 Principal Components Analysis  \nPrincipal components are discussed in Section 6.3.1 in the context of principal components regression. When faced with a large set of correlated variables, principal components allow us to summarize this set with a smaller number of representative variables that collectively explain most of the variability in the original set. The principal component directions are presented in Section 6.3.1 as directions in feature space along which the original data are highly variable . These directions also define lines and subspaces that are as close as possible to the data cloud. To perform principal components regression, we simply use principal components as predictors in a regression model in place of the original larger set of variables.  \nPrincipal components analysis (PCA) refers to the process by which principal components are computed, and the subsequent use of these components in understanding the data. PCA is an unsupervised approach, since it involves only a set of features X 1  X 2         X p , and no associated response Y . Apart from producing derived variables for use in supervised learning problems, PCA also serves as a tool for data visualization (visualization of exploratory data analysis  \nprincipal components analysis  \nthe observations or visualization of the variables). It can also be used as a tool for data imputation - that is, for filling in missing values in a data matrix.  \nWe now discuss PCA in greater detail, focusing on the use of PCA as a tool for unsupervised data exploration, in keeping with the topic of this chapter.', '### 6.3.2 Partial Least Squares  \nThe PCR approach that we just described involves identifying linear combinations, or directions , that best represent the predictors X 1         X p . These directions are identified in an unsupervised way, since the response Y is not used to help determine the principal component directions. That is, the response does not supervise the identification of the principal components. Consequently, PCR suffers from a drawback: there is no guarantee  \n8 More details can be found in Section 3.5 of The Elements of Statistical Learning by Hastie, Tibshirani, and Friedman.  \nFIGURE 6.21. For the advertising data, the first PLS direction (solid line) and first PCR direction (dotted line) are shown.  \n<!-- image -->  \nthat the directions that best explain the predictors will also be the best directions to use for predicting the response. Unsupervised methods are discussed further in Chapter 12.  \nWe now describe how the first PLS direction is computed. After standardizing the p predictors, PLS computes the first direction Z 1 by setting each φ j 1 in (6.16) equal to the coefficient from the simple linear regression of Y onto X j . One can show that this coefficient is proportional to the correlation between Y and X j . Hence, in computing Z 1 = ∑ p j =1 φ j 1 X j , PLS places the highest weight on the variables that are most strongly related to the response.  \nWe now present partial least squares (PLS), a supervised alternative to PCR. Like PCR, PLS is a dimension reduction method, which first identifies a new set of features Z 1         Z M that are linear combinations of the original features, and then fits a linear model via least squares using these M new features. But unlike PCR, PLS identifies these new features in a supervised way-that is, it makes use of the response Y in order to identify new features that not only approximate the old features well, but also that are related to the response . Roughly speaking, the PLS approach attempts to find directions that help explain both the response and the predictors.  \nFigure 6.21 displays an example of PLS on a synthetic dataset with Sales in each of 100 regions as the response, and two predictors; Population Size and Advertising Spending. The solid green line indicates the first PLS direction, while the dotted line shows the first principal component direction. PLS has chosen a direction that has less change in the ad dimension per unit change in the pop dimension, relative to PCA. This suggests that pop is more highly correlated with the response than is ad . The PLS direction does not fit the predictors as closely as does PCA, but it does a better job explaining the response.  \nTo identify the second PLS direction we first adjust each of the variables for Z 1 , by regressing each variable on Z 1 and taking residuals . These residuals can be interpreted as the remaining information that has not been explained by the first PLS direction. We then compute Z 2 using this or- partial least squares  \nthogonalized data in exactly the same fashion as Z 1 was computed based on the original data. This iterative approach can be repeated M times to identify multiple PLS components Z 1         Z M . Finally, at the end of this procedure, we use least squares to fit a linear model to predict Y using Z 1         Z M in exactly the same fashion as for PCR.  \nPLS is popular in the field of chemometrics, where many variables arise from digitized spectrometry signals. In practice it often performs no better than ridge regression or PCR. While the supervised dimension reduction of PLS can reduce bias, it also has the potential to increase variance, so that the overall benefit of PLS relative to PCR is a wash.  \nAs with PCR, the number M of partial least squares directions used in PLS is a tuning parameter that is typically chosen by cross-validation. We generally standardize the predictors and response before performing PLS.']","Principal Component Regression (PCR) involves two main steps:

1. **Constructing Principal Components**: The first step is to perform Principal Components Analysis (PCA) on the set of predictors \(X_1, X_2, \ldots, X_p\). This involves identifying the principal components, which are linear combinations of the original predictors that capture the most variance in the data. Typically, a smaller number of principal components \(Z_1, Z_2, \ldots, Z_M\) are selected, where \(M < p\).

2. **Regression on Principal Components**: The second step is to use these selected principal components as predictors in a linear regression model. The response variable \(Y\) is regressed on the principal components \(Z_1, Z_2, \ldots, Z_M\) using least squares. This step aims to explain the variability in the response variable using the reduced set of predictors.

These steps allow PCR to reduce the dimensionality of the data and potentially improve the regression model by focusing on the directions of greatest variance in the predictors.","The two steps are: first, constructing the first M principal components, Z₁, ..., Z_M, which are linear combinations of the original features. Second, fitting a linear regression model using these M components as predictors.",0.9090909090909091,,0.9999999999666667,1.0
What is the formula for the Proportion of Variance Explained (PVE) by the m-th principal component?,"['### 12.2.3 The Proportion of Variance Explained  \nIn Figure 12.2, we performed PCA on a three-dimensional data set (lefthand panel) and projected the data onto the first two principal component loading vectors in order to obtain a two-dimensional view of the data (i.e. the principal component score vectors; right-hand panel). We see that this two-dimensional representation of the three-dimensional data does successfully capture the major pattern in the data: the orange, green, and cyan observations that are near each other in three-dimensional space remain nearby in the two-dimensional representation. Similarly, we have seen on the USArrests data set that we can summarize the 50 observations and 4 variables using just the first two principal component score vectors and the first two principal component loading vectors.  \nWe can now ask a natural question: how much of the information in a given data set is lost by projecting the observations onto the first few principal components? That is, how much of the variance in the data is not contained in the first few principal components? More generally, we are interested in knowing the proportion of variance explained (PVE) by each  \n4 Technically, the solution to (12.6) is not unique. Thus, it is more precise to state that any solution to (12.6) can be easily transformed to yield the principal components.  \nprincipal component. The total variance present in a data set (assuming that the variables have been centered to have mean zero) is defined as  \n$$\\sum _ { j = 1 } ^ { p } \\text {Var} ( X _ { j } ) = \\sum _ { j = 1 } ^ { p } \\frac { 1 } { n } \\sum _ { i = 1 } ^ { n } x _ { i j } ^ { 2 } ,$$  \nand the variance explained by the m th principal component is  \n$$\\frac { 1 } { n } \\sum _ { i = 1 } ^ { n } z _ { i m } ^ { 2 } = \\frac { 1 } { n } \\sum _ { i = 1 } ^ { n } \\left ( \\sum _ { j = 1 } ^ { p } \\phi _ { j m } x _ { i j } \\right ) ^ { 2 } \\, . \\\\ \\intertext { t h e P V e f o r $ t h e m t h r i n c i p l a n c o m p e n t $ i s given by }$$  \nTherefore, the PVE of the m th principal component is given by  \n$$\\frac { \\sum _ { i = 1 } ^ { n } z _ { i m } ^ { 2 } } { \\sum _ { j = 1 } ^ { p } \\sum _ { i = 1 } ^ { m } x _ { i j } ^ { 2 } } = \\frac { \\sum _ { i = 1 } ^ { n } \\left ( \\sum _ { j = 1 } ^ { p } \\phi _ { j m } x _ { i j } \\right ) ^ { 2 } } { \\sum _ { j = 1 } ^ { p } \\sum _ { i = 1 } ^ { n } x _ { i j } ^ { 2 } } .$$  \nIn Section 12.2.2, we showed that the first M principal component loading and score vectors can be interpreted as the best M -dimensional approximation to the data, in terms of residual sum of squares. It turns out that the variance of the data can be decomposed into the variance of the first M principal components plus the mean squared error of this M -dimensional approximation, as follows:  \nThe PVE of each principal component is a positive quantity. In order to compute the cumulative PVE of the first M principal components, we can simply sum (12.10) over each of the first M PVEs. In total, there are min( n -1  p ) principal components, and their PVEs sum to one.  \n$$\\begin{array} { r l } & { \\text {approximation} , \\, a s \\, i n o w s . } \\\\ & { \\underbrace { \\sum _ { j = 1 } ^ { p } \\frac { 1 } { n } \\sum _ { i = 1 } ^ { n } x _ { i j } ^ { 2 } = \\sum _ { m = 1 } ^ { M } \\frac { 1 } { n } \\sum _ { i = 1 } ^ { n } z _ { i m } ^ { 2 } } _ { V a r . o f i t a r { M P C S } } \\underbrace { + \\frac { 1 } { n } \\sum _ { j = 1 } ^ { p } \\sum _ { i = 1 } ^ { n } \\left ( x _ { i j } - \\sum _ { m = 1 } ^ { M } z _ { i m } \\phi _ { j m } \\right ) } _ { M S E E } \\quad ( 1 2 . 1 1 ) } \\\\ & { \\underbrace { T h e r e t h e r e m s i n c h o d i s o n i t i o n a r e d i s u c h e s s e d i n v e r s . } _ { ( 1 2 . 8 ) , ( 1 2 . 9 ) , a n d } \\quad ( 1 2 . 1 8 ) , \\, ( 1 2 . 9 ) , \\, a n d } \\end{array}$$  \nMoreover, we can use (12.11) to see that the PVE defined in (12.10) equals  \nThe three terms in this decomposition are discussed in (12.8), (12.9), and (12.7), respectively. Since the first term is fixed, we see that by maximizing the variance of the first M principal components, we minimize the mean squared error of the M -dimensional approximation, and vice versa. This explains why principal components can be equivalently viewed as minimizing the approximation error (as in Section 12.2.2) or maximizing the variance (as in Section 12.2.1).  \n$$1 - \\frac { \\sum _ { j = 1 } ^ { p } \\sum _ { i = 1 } ^ { n } \\left ( x _ { i j } - \\sum _ { m = 1 } ^ { M } z _ { i m } \\phi _ { j m } \\right ) ^ { 2 } } { \\sum _ { j = 1 } ^ { p } \\sum _ { i = 1 } ^ { n } x _ { i j } ^ { 2 } } = 1 - \\frac { R S S } { T S S } , \\\\ \\intertext { h e r s t s r e p r e s e n t s the t o tal sum of s q u a r e d e l e m e t s of X , a n d R S S } \\intertext { p e r s u s s e r d e c t i o n s u s s e r f o r s u s s e r d e c t i o n a n d r a p r o w i m a t i o n }$$  \nwhere TSS represents the total sum of squared elements of X , and RSS represents the residual sum of squares of the M -dimensional approximation given by the principal components. Recalling the definition of R 2 from (3.17), this means that we can interpret the PVE as the R 2 of the approximation for X given by the first M principal components.  \nFIGURE 12.3. Left: a scree plot depicting the proportion of variance explained by each of the four principal components in the USArrests data. Right: the cumulative proportion of variance explained by the four principal components in the USArrests data.  \n<!-- image -->  \nIn the USArrests data, the first principal component explains 62.0 % of the variance in the data, and the next principal component explains 24.7 % of the variance. Together, the first two principal components explain almost 87 % of the variance in the data, and the last two principal components explain only 13 % of the variance. This means that Figure 12.1 provides a pretty accurate summary of the data using just two dimensions. The PVE of each principal component, as well as the cumulative PVE, is shown in Figure 12.3. The left-hand panel is known as a scree plot , and will be discussed later in this chapter.', ""### 12.2.1 What Are Principal Components?  \nSuppose that we wish to visualize n observations with measurements on a set of p features, X 1  X 2         X p , as part of an exploratory data analysis. We could do this by examining two-dimensional scatterplots of the data, each of which contains the n observations' measurements on two of the features. However, there are ( p 2 ) = p ( p -1) glyph[triangleleft] 2 such scatterplots; for example, with p = 10 there are 45 plots! If p is large, then it will certainly not be possible to look at all of them; moreover, most likely none of them will be informative since they each contain just a small fraction of the total information present in the data set. Clearly, a better method is required to visualize the n observations when p is large. In particular, we would like to find a low-dimensional representation of the data that captures as much of the information as possible. For instance, if we can obtain a two-dimensional representation of the data that captures most of the information, then we can plot the observations in this low-dimensional space.  \nThe first principal component of a set of features X 1  X 2         X p is the normalized linear combination of the features  \nPCA provides a tool to do just this. It finds a low-dimensional representation of a data set that contains as much as possible of the variation. The idea is that each of the n observations lives in p -dimensional space, but not all of these dimensions are equally interesting. PCA seeks a small number of dimensions that are as interesting as possible, where the concept of interesting is measured by the amount that the observations vary along each dimension. Each of the dimensions found by PCA is a linear combination of the p features. We now explain the manner in which these dimensions, or principal components , are found.  \n$$Z _ { 1 } = \\phi _ { 1 1 } X _ { 1 } + \\phi _ { 2 1 } X _ { 2 } + \\dots + \\phi _ { p 1 } X _ { p } \\\\$$  \nGiven an n × p data set X , how do we compute the first principal component? Since we are only interested in variance, we assume that each of the variables in X has been centered to have mean zero (that is, the column means of X are zero). We then look for the linear combination of the sample feature values of the form that has the largest variance. By normalized , we mean that ∑ p j =1 φ 2 j 1 = 1 . We refer to the elements φ 11         φ p 1 as the loadings of the first principal component; together, the loadings make up the principal component loading vector, φ 1 = ( φ 11 φ 21       φ p 1 ) T . We constrain the loadings so that their sum of squares is equal to one, since otherwise setting these elements to be arbitrarily large in absolute value could result in an arbitrarily large variance.  \n$$z _ { i 1 } = \\phi _ { 1 1 } x _ { i 1 } + \\phi _ { 2 1 } x _ { i 2 } + \\dots + \\phi _ { p 1 } x _ { i p }$$  \nloading  \nthat has largest sample variance, subject to the constraint that ∑ p j =1 φ 2 j 1 =1 . In other words, the first principal component loading vector solves the optimization problem  \n$$\\text {timization problem} \\\\ \\maximize _ { \\phi _ { 1 } , \\dots , \\phi _ { 1 } } \\left \\{ \\frac { 1 } { n } \\sum _ { i = 1 } ^ { n } \\left ( \\sum _ { j = 1 } ^ { p } \\phi _ { j } x _ { i j } \\right ) ^ { 2 } \\right \\} \\text { subject to } \\sum _ { j = 1 } ^ { p } \\phi _ { j } ^ { 2 } = 1 . \\quad ( 1 2 . 3 ) \\\\ \\\\ \\text {From } ( 1 2 . 2 ) \\text { we can write the objective in } ( 1 2 . 3 ) \\text { as } \\frac { 1 } { n } \\sum _ { i = 1 } ^ { n } z _ { i } ^ { 2 } . \\text { Since } \\\\ \\frac { 1 } { n } \\sum _ { i } ^ { n } | x _ { i } | \\equiv 0 , \\text { the average of the z$_{1}$} , \\dots , z _ { n } | 1 \\rangle \\text { will be zero as } \\text {well. Hence}$$  \nThere is a nice geometric interpretation of the first principal component. The loading vector φ 1 with elements φ 11  φ 21         φ p 1 defines a direction in feature space along which the data vary the most. If we project the n data points x 1         x n onto this direction, the projected values are the principal component scores z 11         z n 1 themselves. For instance, Figure 6.14 on page 254 displays the first principal component loading vector (green solid line) on an advertising data set. In these data, there are only two features, and so the observations as well as the first principal component loading vector can be easily displayed. As can be seen from (6.19), in that data set φ 11 = 0   839 and φ 21 = 0   544 .  \nFrom (12.2) we can write the objective in (12.3) as 1 n ∑ n i =1 z 2 i 1 . Since 1 n ∑ n i =1 x ij = 0 , the average of the z 11         z n 1 will be zero as well. Hence the objective that we are maximizing in (12.3) is just the sample variance of the n values of z i 1 . We refer to z 11         z n 1 as the scores of the first principal component. Problem (12.3) can be solved via an eigen decomposition , a standard technique in linear algebra, but the details are outside of the scope of this book. 1  \nAfter the first principal component Z 1 of the features has been determined, we can find the second principal component Z 2 . The second principal component is the linear combination of X 1         X p that has maximal variance out of all linear combinations that are uncorrelated with Z 1 . The second principal component scores z 12  z 22         z n 2 take the form  \n$$z _ { i 2 } = \\phi _ { 1 2 } x _ { i 1 } + \\phi _ { 2 2 } x _ { i 2 } + \\dots + \\phi _ { p 2 } x _ { i p } ,$$  \nwhere φ 2 is the second principal component loading vector, with elements φ 12  φ 22         φ p 2 . It turns out that constraining Z 2 to be uncorrelated with Z 1 is equivalent to constraining the direction φ 2 to be orthogonal (perpendicular) to the direction φ 1 . In the example in Figure 6.14, the observations lie in two-dimensional space (since p = 2 ), and so once we have found φ 1 , there is only one possibility for φ 2 , which is shown as a blue dashed line. (From Section 6.3.1, we know that φ 12 = 0   544 and φ 22 = -0   839 .) But in a larger data set with p &gt; 2 variables, there are multiple distinct principal components, and they are defined in a similar manner. To find φ 2 , we solve a problem similar to (12.3) with φ 2 replacing φ 1 , and with the additional constraint that φ 2 is orthogonal to φ 1 . 2  \n1 As an alternative to the eigen decomposition, a related technique called the singular value decomposition can be used. This will be explored in the lab at the end of this chapter.  \n2 On a technical note, the principal component directions φ 1 , φ 2 , φ 3        are given by the ordered sequence of eigenvectors of the matrix X T X , and the variances of the components are the eigenvalues. There are at most min( n -1  p ) principal components.  \nscore eigen decomposition  \nFIGURE 12.1. The first two principal components for the USArrests data. The blue state names represent the scores for the first two principal components. The orange arrows indicate the first two principal component loading vectors (with axes on the top and right). For example, the loading for Rape on the first component is 0   54 , and its loading on the second principal component 0   17 (the word Rape is centered at the point (0   54  0   17) ). This figure is known as a biplot, because it displays both the principal component scores and the principal component loadings.  \n<!-- image -->  \nOnce we have computed the principal components, we can plot them against each other in order to produce low-dimensional views of the data. For instance, we can plot the score vector Z 1 against Z 2 , Z 1 against Z 3 , Z 2 against Z 3 , and so forth. Geometrically, this amounts to projecting the original data down onto the subspace spanned by φ 1 , φ 2 , and φ 3 , and plotting the projected points.  \nWe illustrate the use of PCA on the USArrests data set. For each of the 50 states in the United States, the data set contains the number of arrests per 100  000 residents for each of three crimes: Assault , Murder , and Rape . We also record UrbanPop (the percent of the population in each state living in urban areas). The principal component score vectors have length n = 50 , and the principal component loading vectors have length p = 4 . PCA was performed after standardizing each variable to have mean zero and standard  \nTABLE 12.1. The principal component loading vectors, φ 1 and φ 2 , for the USArrests data. These are also displayed in Figure 12.1.  \n|          |      PC1 | PC2         |\n|----------|----------|-------------|\n| Murder   | 0.535899 | - 0.4181809 |\n| Assault  | 0.583184 | - 0.1879856 |\n| UrbanPop | 0.278191 | 0.8728062   |\n| Rape     | 0.543432 | 0.1673186   |  \ndeviation one. Figure 12.1 plots the first two principal components of these data. The figure represents both the principal component scores and the loading vectors in a single biplot display. The loadings are also given in Table 12.2.1.  \nWe can examine differences between the states via the two principal component score vectors shown in Figure 12.1. Our discussion of the loading vectors suggests that states with large positive scores on the first component, such as California, Nevada and Florida, have high crime rates, while states like North Dakota, with negative scores on the first component, have low crime rates. California also has a high score on the second component, indicating a high level of urbanization, while the opposite is true for states like Mississippi. States close to zero on both components, such as Indiana, have approximately average levels of both crime and urbanization.  \nIn Figure 12.1, we see that the first loading vector places approximately equal weight on Assault , Murder , and Rape , but with much less weight on UrbanPop . Hence this component roughly corresponds to a measure of overall rates of serious crimes. The second loading vector places most of its weight on UrbanPop and much less weight on the other three features. Hence, this component roughly corresponds to the level of urbanization of the state. Overall, we see that the crime-related variables ( Murder , Assault , and Rape ) are located close to each other, and that the UrbanPop variable is far from the other three. This indicates that the crime-related variables are correlated with each other-states with high murder rates tend to have high assault and rape rates-and that the UrbanPop variable is less correlated with the other three.""]","The formula for the Proportion of Variance Explained (PVE) by the m-th principal component is given by:

\[
\frac{\sum_{i=1}^{n} z_{im}^{2}}{\sum_{j=1}^{p} \sum_{i=1}^{n} x_{ij}^{2}} = \frac{\sum_{i=1}^{n} \left( \sum_{j=1}^{p} \phi_{jm} x_{ij} \right)^{2}}{\sum_{j=1}^{p} \sum_{i=1}^{n} x_{ij}^{2}}
\]

This formula calculates the proportion of the total variance in the data that is explained by the m-th principal component.","The PVE for the m-th principal component is given by the formula (Σ_{i=1 to n} z_{im}²) / (Σ_{j=1 to p} Σ_{i=1 to n} x_{ij}²), where z_{im} are the scores of the m-th principal component and x_{ij} are the centered data values.",0.75,,0.99999999995,1.0
What is the optimization problem that defines K-means clustering?,"['### 12.4.1 K -Means Clustering  \nK -means clustering is a simple and elegant approach for partitioning a data set into K distinct, non-overlapping clusters. To perform K -means clustering, we must first specify the desired number of clusters K ; then the K -means algorithm will assign each observation to exactly one of the K clusters. Figure 12.7 shows the results obtained from performing K -means clustering on a simulated example consisting of 150 observations in two dimensions, using three different values of K .  \nThe K -means clustering procedure results from a simple and intuitive mathematical problem. We begin by defining some notation. Let C 1         C K denote sets containing the indices of the observations in each cluster. These sets satisfy two properties:  \n1. C 1 ∪ C 2 ∪ · · · ∪ C K = ¶ 1         n ♦ . In other words, each observation belongs to at least one of the K clusters.  \n/negationslash  \n2. C k ∩ C k ′ = ∅ for all k = k ′ . In other words, the clusters are nonoverlapping: no observation belongs to more than one cluster.  \nK -means clustering hierarchical clustering dendrogram  \nFIGURE 12.7. A simulated data set with 150 observations in two-dimensional space. Panels show the results of applying K -means clustering with different values of K , the number of clusters. The color of each observation indicates the cluster to which it was assigned using the K -means clustering algorithm. Note that there is no ordering of the clusters, so the cluster coloring is arbitrary. These cluster labels were not used in clustering; instead, they are the outputs of the clustering procedure.  \n<!-- image -->  \nFor instance, if the i th observation is in the k th cluster, then i ∈ C k . The idea behind K -means clustering is that a good clustering is one for which the within-cluster variation is as small as possible. The within-cluster variation for cluster C k is a measure W ( C k ) of the amount by which the observations within a cluster differ from each other. Hence we want to solve the problem  \n$$\\min _ { C _ { 1 } , \\dots , C _ { K } } \\left \\{ \\sum _ { k = 1 } ^ { K } W ( C _ { k } ) \\right \\} .$$  \nIn words, this formula says that we want to partition the observations into K clusters such that the total within-cluster variation, summed over all K clusters, is as small as possible.  \nSolving (12.15) seems like a reasonable idea, but in order to make it actionable we need to define the within-cluster variation. There are many possible ways to define this concept, but by far the most common choice involves squared Euclidean distance . That is, we define  \n$$W ( C _ { k } ) = \\frac { 1 } { | C _ { k } | } \\sum _ { i , i ^ { \\prime } \\in C _ { k } } \\sum _ { j = 1 } ^ { p } ( x _ { i j } - x _ { i ^ { \\prime } j } ) ^ { 2 } ,$$  \nwhere ♣ C k ♣ denotes the number of observations in the k th cluster. In other words, the within-cluster variation for the k th cluster is the sum of all of the pairwise squared Euclidean distances between the observations in the k th cluster, divided by the total number of observations in the k th cluster. Combining (12.15) and (12.16) gives the optimization problem that defines  \nK -means clustering,  \n$$\\min _ { C _ { 1 } , \\dots , C _ { K } } \\left \\{ \\sum _ { k = 1 } ^ { K } \\frac { 1 } { | C _ { k } | } \\sum _ { i , i ^ { \\prime } \\in C _ { k } } \\sum _ { j = 1 } ^ { p } ( x _ { i j } - x _ { i ^ { \\prime } j } ) ^ { 2 } \\right \\} .$$  \nNow, we would like to find an algorithm to solve (12.17)-that is, a method to partition the observations into K clusters such that the objective of (12.17) is minimized. This is in fact a very difficult problem to solve precisely, since there are almost K n ways to partition n observations into K clusters. This is a huge number unless K and n are tiny! Fortunately, a very simple algorithm can be shown to provide a local optimum-a pretty good solution -to the K -means optimization problem (12.17). This approach is laid out in Algorithm 12.2.\nAlgorithm 12.2 K -Means Clustering  \n1. Randomly assign a number, from 1 to K , to each of the observations. These serve as initial cluster assignments for the observations.\n2. Iterate until the cluster assignments stop changing:\n3. (a) For each of the K clusters, compute the cluster centroid . The k th cluster centroid is the vector of the p feature means for the observations in the k th cluster.\n4. (b) Assign each observation to the cluster whose centroid is closest (where closest is defined using Euclidean distance).  \nAlgorithm 12.2 is guaranteed to decrease the value of the objective (12.17) at each step. To understand why, the following identity is illuminating:  \n$$\\frac { 1 } { | C _ { k } | } \\sum _ { i , i ^ { \\prime } \\in C _ { k } } \\sum _ { j = 1 } ^ { p } ( x _ { i j } - x _ { i ^ { \\prime } j } ) ^ { 2 } = 2 \\sum _ { i \\in C _ { k } } \\sum _ { j = 1 } ^ { p } ( x _ { i j } - \\bar { x } _ { k j } ) ^ { 2 } ,$$  \nwhere ¯ x kj = 1 ♣ C k ♣ ∑ i ∈ C k x ij is the mean for feature j in cluster C k . In Step 2(a) the cluster means for each feature are the constants that minimize the sum-of-squared deviations, and in Step 2(b), reallocating the observations can only improve (12.18). This means that as the algorithm is run, the clustering obtained will continually improve until the result no longer changes; the objective of (12.17) will never increase. When the result no longer changes, a local optimum has been reached. Figure 12.8 shows the progression of the algorithm on the toy example from Figure 12.7. K -means clustering derives its name from the fact that in Step 2(a), the cluster centroids are computed as the mean of the observations assigned to each cluster.  \nBecause the K -means algorithm finds a local rather than a global optimum, the results obtained will depend on the initial (random) cluster assignment of each observation in Step 1 of Algorithm 12.2. For this reason, it is important to run the algorithm multiple times from different random  \nFIGURE 12.8. The progress of the K-means algorithm on the example of Figure 12.7 with K =3. Top left: the observations are shown. Top center: in Step 1 of the algorithm, each observation is randomly assigned to a cluster. Top right: in Step 2(a), the cluster centroids are computed. These are shown as large colored disks. Initially the centroids are almost completely overlapping because the initial cluster assignments were chosen at random. Bottom left: in Step 2(b), each observation is assigned to the nearest centroid. Bottom center: Step 2(a) is once again performed, leading to new cluster centroids. Bottom right: the results obtained after ten iterations.  \n<!-- image -->  \ninitial configurations. Then one selects the best solution, i.e. that for which the objective (12.17) is smallest. Figure 12.9 shows the local optima obtained by running K -means clustering six times using six different initial cluster assignments, using the toy data from Figure 12.7. In this case, the best clustering is the one with an objective value of 235.8.  \nAs we have seen, to perform K -means clustering, we must decide how many clusters we expect in the data. The problem of selecting K is far from simple. This issue, along with other practical considerations that arise in performing K -means clustering, is addressed in Section 12.4.3.  \nFIGURE 12.9. K -means clustering performed six times on the data from Figure 12.7 with K = 3 , each time with a different random assignment of the observations in Step 1 of the K -means algorithm. Above each plot is the value of the objective (12.17). Three different local optima were obtained, one of which resulted in a smaller value of the objective and provides better separation between the clusters. Those labeled in red all achieved the same best solution, with an objective value of 235.8.  \n<!-- image -->']","The optimization problem that defines K-means clustering is to partition the observations into \( K \) clusters such that the total within-cluster variation, summed over all \( K \) clusters, is as small as possible. Mathematically, this is expressed as:

\[
\min _ { C _ { 1 } , \dots , C _ { K } } \left \{ \sum _ { k = 1 } ^ { K } \frac { 1 } { | C _ { k } | } \sum _ { i , i ^ { \prime } \in C _ { k } } \sum _ { j = 1 } ^ { p } ( x _ { i j } - x _ { i ^ { \prime } j } ) ^ { 2 } \right \}
\]

Here, \( | C _ { k } | \) denotes the number of observations in the \( k \)-th cluster, and the goal is to minimize the sum of all pairwise squared Euclidean distances between the observations within each cluster.","K-means clustering aims to solve the problem: minimize Σ_{k=1 to K} (1/|C_k|) * Σ_{i, i' ∈ C_k} Σ_{j=1 to p} (x_ij - x_i'j)², which seeks to partition the observations into K clusters such that the total within-cluster variation is as small as possible.",0.8,,0.9999999999,1.0
¿Cómo se define la distancia basada en la correlación entre dos observaciones?,"['### Applied  \n7. In this chapter, we mentioned the use of correlation-based distance and Euclidean distance as dissimilarity measures for hierarchical clustering. It turns out that these two measures are almost equivalent: if each observation has been centered to have mean zero and standard deviation one, and if we let r ij denote the correlation between the i th and j th observations, then the quantity 1 -r ij is proportional to the squared Euclidean distance between the i th and j th observations.  \nOn the USArrests data, show that this proportionality holds.  \nHint: The Euclidean distance can be calculated using the pairwise\\_distances() function from the sklearn.metrics module, and correlations can be calculated using the np.corrcoef() function.  \n8. In Section 12.2.3, a formula for calculating PVE was given in Equation 12.10. We also saw that the PVE can be obtained using the explained\\_variance\\_ratio\\_ attribute of a fitted PCA() estimator.  \nOn the USArrests data, calculate PVE in two ways:  \n- (a) Using the explained\\_variance\\_ratio\\_ output of the fitted PCA() estimator, as was done in Section 12.2.3.\n- (b) By applying Equation 12.10 directly. The loadings are stored as the components\\_ attribute of the fitted PCA() estimator. Use those loadings in Equation 12.10 to obtain the PVE.  \nThese two approaches should give the same results.  \nHint: You will only obtain the same results in (a) and (b) if the same data is used in both cases. For instance, if in (a) you performed PCA() using centered and scaled variables, then you must center and scale the variables before applying Equation 12.10 in (b).  \n9. Consider the USArrests data. We will now perform hierarchical clustering on the states.\n2. (a) Using hierarchical clustering with complete linkage and Euclidean distance, cluster the states.\n3. (b) Cut the dendrogram at a height that results in three distinct clusters. Which states belong to which clusters?  \npairwise\\_ distances()  \n- (c) Hierarchically cluster the states using complete linkage and Euclidean distance, after scaling the variables to have standard deviation one .\n- (d) What effect does scaling the variables have on the hierarchical clustering obtained? In your opinion, should the variables be scaled before the inter-observation dissimilarities are computed? Provide a justification for your answer.\n10. In this problem, you will generate simulated data, and then perform PCA and K -means clustering on the data.\n- (a) Generate a simulated data set with 20 observations in each of  \ngenerate data. One example is the normal() method of random() function in numpy ; the uniform() method is another class so that there are three distinct classes.  \n- three classes (i.e. 60 observations total), and 50 variables. Hint: There are a number of functions in Python that you can use to the option. Be sure to add a mean shift to the observations in each\n- (b) Perform PCA on the 60 observations and plot the first two principal component score vectors. Use a different color to indicate the observations in each of the three classes. If the three classes appear separated in this plot, then continue on to part (c). If not, then return to part (a) and modify the simulation so that there is greater separation between the three classes. Do not continue to part (c) until the three classes show at least some separation in the first two principal component score vectors.\n- (c) Perform K -means clustering of the observations with K = 3 . How well do the clusters that you obtained in K -means clustering compare to the true class labels?\n- Hint: You can use the pd.crosstab() function in Python to compare the true class labels to the class labels obtained by clustering. Be careful how you interpret the results: K -means clustering will arbitrarily number the clusters, so you cannot simply check whether the true class labels and clustering labels are the same.\n- (d) Perform K -means clustering with K = 2 . Describe your results.\n- (e) Now perform K -means clustering with K = 4 , and describe your results.\n- (f) Now perform K -means clustering with K = 3 on the first two principal component score vectors, rather than on the raw data. That is, perform K -means clustering on the 60 × 2 matrix of which the first column is the first principal component score vector, and the second column is the second principal component score vector. Comment on the results.\n- (g) Using the StandardScaler() estimator, perform K -means clustering with K = 3 on the data after scaling each variable to have standard deviation one . How do these results compare to those obtained in (b)? Explain.  \n11. Write a Python function to perform matrix completion as in Algorithm 12.1, and as outlined in Section 12.5.2. In each iteration, the function should keep track of the relative error, as well as the iteration count. Iterations should continue until the relative error is small enough or until some maximum number of iterations is reached (set a default value for this maximum number). Furthermore, there should be an option to print out the progress in each iteration.  \nTest your function on the Boston data. First, standardize the features to have mean zero and standard deviation one using the StandardScaler() function. Run an experiment where you randomly leave out an increasing (and nested) number of observations from 5% to 30%, in steps of 5%. Apply Algorithm 12.1 with M = 1  2         8 . Display the approximation error as a function of the fraction of observations that are missing, and the value of M , averaged over 10 repetitions of the experiment.  \n12. In Section 12.5.2, Algorithm 12.1 was implemented using the svd() function from the np.linalg module. However, given the connection between the svd() function and the PCA() estimator highlighted in the lab, we could have instead implemented the algorithm using PCA() .  \nWrite a function to implement Algorithm 12.1 that makes use of PCA() rather than svd() .  \n13. On the book website, www.statlearning.com , there is a gene expression data set ( Ch12Ex13.csv ) that consists of 40 tissue samples with measurements on 1,000 genes. The first 20 samples are from healthy patients, while the second 20 are from a diseased group.\n2. (a) Load in the data using pd.read\\_csv() . You will need to select header = None .\n3. (b) Apply hierarchical clustering to the samples using correlationbased distance, and plot the dendrogram. Do the genes separate the samples into the two groups? Do your results depend on the type of linkage used?\n4. (c) Your collaborator wants to know which genes differ the most across the two groups. Suggest a way to answer this question, and apply it here.', '### 12.4.2 Hierarchical Clustering  \nOne potential disadvantage of K -means clustering is that it requires us to pre-specify the number of clusters K . Hierarchical clustering is an alternative approach which does not require that we commit to a particular choice of K . Hierarchical clustering has an added advantage over K -means clustering in that it results in an attractive tree-based representation of the observations, called a dendrogram .  \nIn this section, we describe bottom-up or agglomerative clustering. This is the most common type of hierarchical clustering, and refers to the fact that a dendrogram (generally depicted as an upside-down tree; see Figure 12.11) is built starting from the leaves and combining clusters up to the trunk. We will begin with a discussion of how to interpret a dendrogram bottom-up agglomerative  \nFIGURE 12.10. Forty-five observations generated in two-dimensional space. In reality there are three distinct classes, shown in separate colors. However, we will treat these class labels as unknown and will seek to cluster the observations in order to discover the classes from the data.  \n<!-- image -->  \nand then discuss how hierarchical clustering is actually performed-that is, how the dendrogram is built.\nInterpreting a Dendrogram  \nWe begin with the simulated data set shown in Figure 12.10, consisting of 45 observations in two-dimensional space. The data were generated from a three-class model; the true class labels for each observation are shown in distinct colors. However, suppose that the data were observed without the class labels, and that we wanted to perform hierarchical clustering of the data. Hierarchical clustering (with complete linkage, to be discussed later) yields the result shown in the left-hand panel of Figure 12.11. How can we interpret this dendrogram?  \nThis highlights a very important point in interpreting dendrograms that is often misunderstood. Consider the left-hand panel of Figure 12.12, which shows a simple dendrogram obtained from hierarchically clustering nine  \nIn the left-hand panel of Figure 12.11, each leaf of the dendrogram represents one of the 45 observations in Figure 12.10. However, as we move up the tree, some leaves begin to fuse into branches. These correspond to observations that are similar to each other. As we move higher up the tree, branches themselves fuse, either with leaves or other branches. The earlier (lower in the tree) fusions occur, the more similar the groups of observations are to each other. On the other hand, observations that fuse later (near the top of the tree) can be quite different. In fact, this statement can be made precise: for any two observations, we can look for the point in the tree where branches containing those two observations are first fused. The height of this fusion, as measured on the vertical axis, indicates how different the two observations are. Thus, observations that fuse at the very bottom of the tree are quite similar to each other, whereas observations that fuse close to the top of the tree will tend to be quite different.  \nFIGURE 12.11. Left: dendrogram obtained from hierarchically clustering the data from Figure 12.10 with complete linkage and Euclidean distance. Center: the dendrogram from the left-hand panel, cut at a height of nine (indicated by the dashed line). This cut results in two distinct clusters, shown in different colors. Right: the dendrogram from the left-hand panel, now cut at a height of five. This cut results in three distinct clusters, shown in different colors. Note that the colors were not used in clustering, but are simply used for display purposes in this figure.  \n<!-- image -->  \nobservations. One can see that observations 5 and 7 are quite similar to each other, since they fuse at the lowest point on the dendrogram. Observations 1 and 6 are also quite similar to each other. However, it is tempting but incorrect to conclude from the figure that observations 9 and 2 are quite similar to each other on the basis that they are located near each other on the dendrogram. In fact, based on the information contained in the dendrogram, observation 9 is no more similar to observation 2 than it is to observations 8  5  and 7 . (This can be seen from the right-hand panel of Figure 12.12, in which the raw data are displayed.) To put it mathematically, there are 2 n -1 possible reorderings of the dendrogram, where n is the number of leaves. This is because at each of the n -1 points where fusions occur, the positions of the two fused branches could be swapped without affecting the meaning of the dendrogram. Therefore, we cannot draw conclusions about the similarity of two observations based on their proximity along the horizontal axis . Rather, we draw conclusions about the similarity of two observations based on the location on the vertical axis where branches containing those two observations first are fused.  \nNow that we understand how to interpret the left-hand panel of Figure 12.11, we can move on to the issue of identifying clusters on the basis of a dendrogram. In order to do this, we make a horizontal cut across the dendrogram, as shown in the center and right-hand panels of Figure 12.11. The distinct sets of observations beneath the cut can be interpreted as clusters. In the center panel of Figure 12.11, cutting the dendrogram at a height of nine results in two clusters, shown in distinct colors. In the right-hand panel, cutting the dendrogram at a height of five results in three clusters. Further cuts can be made as one descends the dendrogram in order to obtain any number of clusters, between 1 (corresponding to no cut) and n  \nFIGURE 12.12. An illustration of how to properly interpret a dendrogram with nine observations in two-dimensional space. Left: a dendrogram generated using Euclidean distance and complete linkage. Observations 5 and 7 are quite similar to each other, as are observations 1 and 6 . However, observation 9 is no more similar to observation 2 than it is to observations 8  5  and 7 , even though observations 9 and 2 are close together in terms of horizontal distance. This is because observations 2  8  5  and 7 all fuse with observation 9 at the same height, approximately 1   8 . Right: the raw data used to generate the dendrogram can be used to confirm that indeed, observation 9 is no more similar to observation 2 than it is to observations 8  5  and 7 .  \n<!-- image -->  \n(corresponding to a cut at height 0, so that each observation is in its own cluster). In other words, the height of the cut to the dendrogram serves the same role as the K in K -means clustering: it controls the number of clusters obtained.  \nThe term hierarchical refers to the fact that clusters obtained by cutting the dendrogram at a given height are necessarily nested within the clusters obtained by cutting the dendrogram at any greater height. However, on an arbitrary data set, this assumption of hierarchical structure might be unrealistic. For instance, suppose that our observations correspond to a group of men and women, evenly split among Americans, Japanese, and French. We can imagine a scenario in which the best division into two groups might split these people by gender, and the best division into three groups might split them by nationality. In this case, the true clusters are not nested, in the sense that the best division into three groups does not result from taking the best division into two groups and splitting up one of those groups. Consequently, this situation could not be well-represented by hierarchical clustering. Due to situations such as this one, hierarchical clustering can sometimes yield worse (i.e. less accurate) results than K -means clustering for a given number of clusters.  \nFigure 12.11 therefore highlights a very attractive aspect of hierarchical clustering: one single dendrogram can be used to obtain any number of clusters. In practice, people often look at the dendrogram and select by eye a sensible number of clusters, based on the heights of the fusion and the number of clusters desired. In the case of Figure 12.11, one might choose to select either two or three clusters. However, often the choice of where to cut the dendrogram is not so clear.\nAlgorithm 12.3 Hierarchical Clustering  \n1. Begin with n observations and a measure (such as Euclidean distance) of all the ( n 2 ) = n ( n -1) glyph[triangleleft] 2 pairwise dissimilarities. Treat each observation as its own cluster.\n2. For i = n n -1         2 :\n3. (a) Examine all pairwise inter-cluster dissimilarities among the i clusters and identify the pair of clusters that are least dissimilar (that is, most similar). Fuse these two clusters. The dissimilarity between these two clusters indicates the height in the dendrogram at which the fusion should be placed.\n4. (b) Compute the new pairwise inter-cluster dissimilarities among the i -1 remaining clusters.\nThe Hierarchical Clustering Algorithm  \nThe hierarchical clustering dendrogram is obtained via an extremely simple algorithm. We begin by defining some sort of dissimilarity measure between each pair of observations. Most often, Euclidean distance is used; we will discuss the choice of dissimilarity measure later in this chapter. The algorithm proceeds iteratively. Starting out at the bottom of the dendrogram, each of the n observations is treated as its own cluster. The two clusters that are most similar to each other are then fused so that there now are n -1 clusters. Next the two clusters that are most similar to each other are fused again, so that there now are n -2 clusters. The algorithm proceeds in this fashion until all of the observations belong to one single cluster, and the dendrogram is complete. Figure 12.13 depicts the first few steps of the algorithm, for the data from Figure 12.12. To summarize, the hierarchical clustering algorithm is given in Algorithm 12.3.  \nThis algorithm seems simple enough, but one issue has not been addressed. Consider the bottom right panel in Figure 12.13. How did we determine that the cluster ¶ 5  7 ♦ should be fused with the cluster ¶ 8 ♦ ? We have a concept of the dissimilarity between pairs of observations, but how do we define the dissimilarity between two clusters if one or both of the clusters contains multiple observations? The concept of dissimilarity between a pair of observations needs to be extended to a pair of groups of observations . This extension is achieved by developing the notion of linkage , which defines the dissimilarity between two groups of observations. The four most common types of linkagecomplete , average , single , and centroid -are briefly described in Table 12.3. Average, complete, and single linkage are most popular among statisticians. Average and complete linkage are generally preferred over single linkage, as they tend to yield more balanced dendrograms. Centroid linkage is often used in genomics, but suffers from a major drawback in that an inversion can occur, whereby two clusters are fused at a height below either of the individual clusters in the dendrogram. This can lead to difficulties in visualization as well as in interpretation of the dendrogram. The dissimilarities computed in Step 2(b)  \nlinkage inversion  \nTABLE 12.3. A summary of the four most commonly-used types of linkage in hierarchical clustering.  \n| Linkage   | Description                                                                                                                                                                                                                                                                                                          |\n|-----------|----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|\n| Complete  | Maximal intercluster dissimilarity. Compute all pairwise dissimilarities between the observations in cluster A and the observations in cluster B, and record the largest of these dis- similarities.                                                                                                                 |\n| Single    | Minimal intercluster dissimilarity. Compute all pairwise dis- similarities between the observations in cluster A and the observations in cluster B, and record the smallest of these dissimilarities. Single linkage can result in extended, trailing clusters in which single observations are fused one-at-a-time. |\n| Average   | Mean intercluster dissimilarity. Compute all pairwise dis- similarities between the observations in cluster A and the observations in cluster B, and record the average of these dissimilarities.                                                                                                                    |\n| Centroid  | Dissimilarity between the centroid for cluster A (a mean vector of length p ) and the centroid for cluster B. Centroid linkage can result in undesirable inversions .                                                                                                                                                |  \nof the hierarchical clustering algorithm will depend on the type of linkage used, as well as on the choice of dissimilarity measure. Hence, the resulting dendrogram typically depends quite strongly on the type of linkage used, as is shown in Figure 12.14.\nChoice of Dissimilarity Measure  \nThus far, the examples in this chapter have used Euclidean distance as the dissimilarity measure. But sometimes other dissimilarity measures might be preferred. For example, correlation-based distance considers two observations to be similar if their features are highly correlated, even though the observed values may be far apart in terms of Euclidean distance. This is an unusual use of correlation, which is normally computed between variables; here it is computed between the observation profiles for each pair of observations. Figure 12.15 illustrates the difference between Euclidean and correlation-based distance. Correlation-based distance focuses on the shapes of observation profiles rather than their magnitudes.  \nFor instance, consider an online retailer interested in clustering shoppers based on their past shopping histories. The goal is to identify subgroups of similar shoppers, so that shoppers within each subgroup can be shown items and advertisements that are particularly likely to interest them. Suppose the data takes the form of a matrix where the rows are the shoppers and the columns are the items available for purchase; the elements of the data matrix indicate the number of times a given shopper has purchased a  \nThe choice of dissimilarity measure is very important, as it has a strong effect on the resulting dendrogram. In general, careful attention should be paid to the type of data being clustered and the scientific question at hand. These considerations should determine what type of dissimilarity measure is used for hierarchical clustering.  \nFIGURE 12.13. An illustration of the first few steps of the hierarchical clustering algorithm, using the data from Figure 12.12, with complete linkage and Euclidean distance. Top Left: initially, there are nine distinct clusters, ¶ 1 ♦  ¶ 2 ♦         ¶ 9 ♦ . Top Right: the two clusters that are closest together, ¶ 5 ♦ and ¶ 7 ♦ , are fused into a single cluster. Bottom Left: the two clusters that are closest together, ¶ 6 ♦ and ¶ 1 ♦ , are fused into a single cluster. Bottom Right: the two clusters that are closest together using complete linkage , ¶ 8 ♦ and the cluster ¶ 5  7 ♦ , are fused into a single cluster.  \n<!-- image -->  \ngiven item (i.e. a 0 if the shopper has never purchased this item, a 1 if the shopper has purchased it once, etc.) What type of dissimilarity measure should be used to cluster the shoppers? If Euclidean distance is used, then shoppers who have bought very few items overall (i.e. infrequent users of the online shopping site) will be clustered together. This may not be desirable. On the other hand, if correlation-based distance is used, then shoppers with similar preferences (e.g. shoppers who have bought items A and B but never items C or D) will be clustered together, even if some shoppers with these preferences are higher-volume shoppers than others. Therefore, for this application, correlation-based distance may be a better choice.  \nIn addition to carefully selecting the dissimilarity measure used, one must also consider whether or not the variables should be scaled to have standard deviation one before the dissimilarity between the observations is computed. To illustrate this point, we continue with the online shopping ex-  \nFIGURE 12.14. Average, complete, and single linkage applied to an example data set. Average and complete linkage tend to yield more balanced clusters.  \n<!-- image -->  \nample just described. Some items may be purchased more frequently than others; for instance, a shopper might buy ten pairs of socks a year, but a computer very rarely. High-frequency purchases like socks therefore tend to have a much larger effect on the inter-shopper dissimilarities, and hence on the clustering ultimately obtained, than rare purchases like computers. This may not be desirable. If the variables are scaled to have standard deviation one before the inter-observation dissimilarities are computed, then each variable will in effect be given equal importance in the hierarchical clustering performed. We might also want to scale the variables to have standard deviation one if they are measured on different scales; otherwise, the choice of units (e.g. centimeters versus kilometers) for a particular variable will greatly affect the dissimilarity measure obtained. It should come as no surprise that whether or not it is a good decision to scale the variables before computing the dissimilarity measure depends on the application at hand. An example is shown in Figure 12.16. We note that the issue of whether or not to scale the variables before performing clustering applies to K -means clustering as well.', ""## 4.8 Exercises  \n### Conceptual  \n1. Using a little bit of algebra, prove that (4.2) is equivalent to (4.3). In other words, the logistic function representation and logit representation for the logistic regression model are equivalent.\n2. It was stated in the text that classifying an observation to the class for which (4.17) is largest is equivalent to classifying an observation to the class for which (4.18) is largest. Prove that this is the case. In other words, under the assumption that the observations in the k th class are drawn from a N ( θ k  σ 2 ) distribution, the Bayes classifier assigns an observation to the class for which the discriminant function is maximized.\n3. This problem relates to the QDA model, in which the observations within each class are drawn from a normal distribution with a classspecific mean vector and a class specific covariance matrix. We consider the simple case where p = 1 ; i.e. there is only one feature.\n4. Suppose that we have classes, and that if an observation belongs  \nK to the k th class then X comes from a one-dimensional normal distribution, X ∼ N ( θ k  σ 2 k ) . Recall that the density function for the one-dimensional normal distribution is given in (4.16). Prove that in this case, the Bayes classifier is not linear. Argue that it is in fact quadratic.  \nHint: For this problem, you should follow the arguments laid out in Section 4.4.1, but without making the assumption that σ 2 1 = · · · = σ 2 K .  \n4. When the number of features p is large, there tends to be a deterioration in the performance of KNN and other local approaches that perform prediction using only observations that are near the test observation for which a prediction must be made. This phenomenon is known as the curse of dimensionality , and it ties into the fact that non-parametric approaches often perform poorly when p is large. We will now investigate this curse.  \n<!-- image -->  \ncurse of dimensionality  \n- (a) Suppose that we have a set of observations, each with measurements on p = 1 feature, X . We assume that X is uniformly (evenly) distributed on [0  1] . Associated with each observation is a response value. Suppose that we wish to predict a test observation's response using only observations that are within 10 % of the range of X closest to that test observation. For instance, in order to predict the response for a test observation with X = 0   6 , we will use observations in the range [0   55  0   65] . On average, what fraction of the available observations will we use to make the prediction?\n- (b) Now suppose that we have a set of observations, each with measurements on p = 2 features, X 1 and X 2 . We assume that ( X 1  X 2 ) are uniformly distributed on [0  1] × [0  1] . We wish to predict a test observation's response using only observations that are within 10 % of the range of X 1 and within 10 % of the range of X 2 closest to that test observation. For instance, in order to predict the response for a test observation with X 1 = 0   6 and X 2 = 0   35 , we will use observations in the range [0   55  0   65] for X 1 and in the range [0   3  0   4] for X 2 . On average, what fraction of the available observations will we use to make the prediction?\n- (c) Now suppose that we have a set of observations on p = 100 features. Again the observations are uniformly distributed on each feature, and again each feature ranges in value from 0 to 1. We wish to predict a test observation's response using observations within the 10 % of each feature's range that is closest to that test observation. What fraction of the available observations will we use to make the prediction?\n- (d) Using your answers to parts (a)-(c), argue that a drawback of KNN when p is large is that there are very few training observations 'near' any given test observation.\n- (e) Now suppose that we wish to make a prediction for a test observation by creating a p -dimensional hypercube centered around the test observation that contains, on average, 10 % of the training observations. For p = 1  2 , and 100 , what is the length of each side of the hypercube? Comment on your answer.  \nNote: A hypercube is a generalization of a cube to an arbitrary number of dimensions. When p = 1 , a hypercube is simply a line segment, when p = 2 it is a square, and when p = 100 it is a 100-dimensional cube.  \n5. We now examine the differences between LDA and QDA.\n2. (a) If the Bayes decision boundary is linear, do we expect LDA or QDA to perform better on the training set? On the test set?\n3. (b) If the Bayes decision boundary is non-linear, do we expect LDA or QDA to perform better on the training set? On the test set?  \n- (c) In general, as the sample size n increases, do we expect the test prediction accuracy of QDA relative to LDA to improve, decline, or be unchanged? Why?\n- (d) True or False: Even if the Bayes decision boundary for a given problem is linear, we will probably achieve a superior test error rate using QDA rather than LDA because QDA is flexible enough to model a linear decision boundary. Justify your answer.\n6. Suppose we collect data for a group of students in a statistics class with variables X 1 =hours studied, X 2 =undergrad GPA, and Y = receive an A. We fit a logistic regression and produce estimated coefficient, ˆ β 0 = -6  ˆ β 1 = 0   05  ˆ β 2 = 1 .\n- (a) Estimate the probability that a student who studies for 40 h and has an undergrad GPA of 3   5 gets an A in the class.\n- (b) How many hours would the student in part (a) need to study to have a 50 % chance of getting an A in the class?\n7. Suppose that we wish to predict whether a given stock will issue a dividend this year ('Yes' or 'No') based on X , last year's percent profit. We examine a large number of companies and discover that the mean value of X for companies that issued a dividend was ¯ X = 10 , while the mean for those that didn't was ¯ X = 0 . In addition, the variance of X for these two sets of companies was ˆ σ 2 = 36 . Finally, 80 % of companies issued dividends. Assuming that X follows a normal distribution, predict the probability that a company will issue a dividend this year given that its percentage profit was X = 4 last year.  \nHint: Recall that the density function for a normal random variable is f ( x ) = 1 √ 2 πσ 2 e -( x -θ ) 2 glyph[triangleleft] 2 σ 2 . You will need to use Bayes' theorem.  \n8. Suppose that we take a data set, divide it into equally-sized training and test sets, and then try out two different classification procedures. First we use logistic regression and get an error rate of 20 % on the training data and 30 % on the test data. Next we use 1-nearest neighbors (i.e. K = 1 ) and get an average error rate (averaged over both test and training data sets) of 18 %. Based on these results, which method should we prefer to use for classification of new observations? Why?\n9. This problem has to do with odds .\n3. (a) On average, what fraction of people with an odds of 0.37 of defaulting on their credit card payment will in fact default?\n4. (b) Suppose that an individual has a 16 % chance of defaulting on her credit card payment. What are the odds that she will default?  \n10. Equation 4.32 derived an expression for log ( Pr( Y = k ♣ X = x ) Pr( Y = K ♣ X = x ) ) in the setting where p &gt; 1 , so that the mean for the k th class, θ k , is a p -dimensional vector, and the shared covariance Σ is a p × p matrix. However, in the setting with p = 1 , (4.32) takes a simpler form, since the means θ 1         θ K and the variance σ 2 are scalars. In this simpler setting, repeat the calculation in (4.32), and provide expressions for a k and b kj in terms of π k , π K , θ k , θ K , and σ 2 .\n11. Work out the detailed forms of a k , b kj , and b kjl in (4.33). Your answer should involve π k , π K , θ k , θ K , Σ k , and Σ K .\n12. Suppose that you wish to classify an observation X ∈ R into apples and oranges . You fit a logistic regression model and find that  \n$$\\widehat { \\Pr } ( Y = \\text {orange} | X = x ) = \\frac { \\exp ( \\hat { \\beta } _ { 0 } + \\hat { \\beta } _ { 1 } x ) } { 1 + \\exp ( \\hat { \\beta } _ { 0 } + \\hat { \\beta } _ { 1 } x ) } . \\\\ \\text {fr} \\text { friend fits a logistic regression model to the same data using}$$  \nYour friend fits a logistic regression model to the same data using the softmax formulation in (4.13), and finds that  \n$$\\widehat { \\Pr } ( Y = \\text {orange} | X = x ) = \\\\ \\frac { \\exp ( \\hat { \\alpha } _ { \\text {orange} } 0 + \\hat { \\alpha } _ { \\text {orange} } 1 x ) } { \\exp ( \\hat { \\alpha } _ { \\text {orange} } 0 + \\hat { \\alpha } _ { \\text {orange} } 1 x ) + \\exp ( \\hat { \\alpha } _ { \\text {apple} } 0 + \\hat { \\alpha } _ { \\text {apple} } 1 x ) } .$$  \n- (a) What is the log odds of orange versus apple in your model?\n- (b) What is the log odds of orange versus apple in your friend's model?\n- (c) Suppose that in your model, ˆ β 0 = 2 and ˆ β 1 = -1 . What are the coefficient estimates in your friend's model? Be as specific as possible.\n- (d) Now suppose that you and your friend fit the same two models on a different data set. This time, your friend gets the coefficient estimates ˆ α orange 0 = 1   2 , ˆ α orange 1 = -2 , ˆ α orange 0 = 3 , ˆ α orange 1 = 0   6 . What are the coefficient estimates in your model?\n- (e) Finally, suppose you apply both models from (d) to a data set with 2,000 test observations. What fraction of the time do you expect the predicted class labels from your model to agree with those from your friend's model? Explain your answer."", '### 12.4.1 K -Means Clustering  \nK -means clustering is a simple and elegant approach for partitioning a data set into K distinct, non-overlapping clusters. To perform K -means clustering, we must first specify the desired number of clusters K ; then the K -means algorithm will assign each observation to exactly one of the K clusters. Figure 12.7 shows the results obtained from performing K -means clustering on a simulated example consisting of 150 observations in two dimensions, using three different values of K .  \nThe K -means clustering procedure results from a simple and intuitive mathematical problem. We begin by defining some notation. Let C 1         C K denote sets containing the indices of the observations in each cluster. These sets satisfy two properties:  \n1. C 1 ∪ C 2 ∪ · · · ∪ C K = ¶ 1         n ♦ . In other words, each observation belongs to at least one of the K clusters.  \n/negationslash  \n2. C k ∩ C k ′ = ∅ for all k = k ′ . In other words, the clusters are nonoverlapping: no observation belongs to more than one cluster.  \nK -means clustering hierarchical clustering dendrogram  \nFIGURE 12.7. A simulated data set with 150 observations in two-dimensional space. Panels show the results of applying K -means clustering with different values of K , the number of clusters. The color of each observation indicates the cluster to which it was assigned using the K -means clustering algorithm. Note that there is no ordering of the clusters, so the cluster coloring is arbitrary. These cluster labels were not used in clustering; instead, they are the outputs of the clustering procedure.  \n<!-- image -->  \nFor instance, if the i th observation is in the k th cluster, then i ∈ C k . The idea behind K -means clustering is that a good clustering is one for which the within-cluster variation is as small as possible. The within-cluster variation for cluster C k is a measure W ( C k ) of the amount by which the observations within a cluster differ from each other. Hence we want to solve the problem  \n$$\\min _ { C _ { 1 } , \\dots , C _ { K } } \\left \\{ \\sum _ { k = 1 } ^ { K } W ( C _ { k } ) \\right \\} .$$  \nIn words, this formula says that we want to partition the observations into K clusters such that the total within-cluster variation, summed over all K clusters, is as small as possible.  \nSolving (12.15) seems like a reasonable idea, but in order to make it actionable we need to define the within-cluster variation. There are many possible ways to define this concept, but by far the most common choice involves squared Euclidean distance . That is, we define  \n$$W ( C _ { k } ) = \\frac { 1 } { | C _ { k } | } \\sum _ { i , i ^ { \\prime } \\in C _ { k } } \\sum _ { j = 1 } ^ { p } ( x _ { i j } - x _ { i ^ { \\prime } j } ) ^ { 2 } ,$$  \nwhere ♣ C k ♣ denotes the number of observations in the k th cluster. In other words, the within-cluster variation for the k th cluster is the sum of all of the pairwise squared Euclidean distances between the observations in the k th cluster, divided by the total number of observations in the k th cluster. Combining (12.15) and (12.16) gives the optimization problem that defines  \nK -means clustering,  \n$$\\min _ { C _ { 1 } , \\dots , C _ { K } } \\left \\{ \\sum _ { k = 1 } ^ { K } \\frac { 1 } { | C _ { k } | } \\sum _ { i , i ^ { \\prime } \\in C _ { k } } \\sum _ { j = 1 } ^ { p } ( x _ { i j } - x _ { i ^ { \\prime } j } ) ^ { 2 } \\right \\} .$$  \nNow, we would like to find an algorithm to solve (12.17)-that is, a method to partition the observations into K clusters such that the objective of (12.17) is minimized. This is in fact a very difficult problem to solve precisely, since there are almost K n ways to partition n observations into K clusters. This is a huge number unless K and n are tiny! Fortunately, a very simple algorithm can be shown to provide a local optimum-a pretty good solution -to the K -means optimization problem (12.17). This approach is laid out in Algorithm 12.2.\nAlgorithm 12.2 K -Means Clustering  \n1. Randomly assign a number, from 1 to K , to each of the observations. These serve as initial cluster assignments for the observations.\n2. Iterate until the cluster assignments stop changing:\n3. (a) For each of the K clusters, compute the cluster centroid . The k th cluster centroid is the vector of the p feature means for the observations in the k th cluster.\n4. (b) Assign each observation to the cluster whose centroid is closest (where closest is defined using Euclidean distance).  \nAlgorithm 12.2 is guaranteed to decrease the value of the objective (12.17) at each step. To understand why, the following identity is illuminating:  \n$$\\frac { 1 } { | C _ { k } | } \\sum _ { i , i ^ { \\prime } \\in C _ { k } } \\sum _ { j = 1 } ^ { p } ( x _ { i j } - x _ { i ^ { \\prime } j } ) ^ { 2 } = 2 \\sum _ { i \\in C _ { k } } \\sum _ { j = 1 } ^ { p } ( x _ { i j } - \\bar { x } _ { k j } ) ^ { 2 } ,$$  \nwhere ¯ x kj = 1 ♣ C k ♣ ∑ i ∈ C k x ij is the mean for feature j in cluster C k . In Step 2(a) the cluster means for each feature are the constants that minimize the sum-of-squared deviations, and in Step 2(b), reallocating the observations can only improve (12.18). This means that as the algorithm is run, the clustering obtained will continually improve until the result no longer changes; the objective of (12.17) will never increase. When the result no longer changes, a local optimum has been reached. Figure 12.8 shows the progression of the algorithm on the toy example from Figure 12.7. K -means clustering derives its name from the fact that in Step 2(a), the cluster centroids are computed as the mean of the observations assigned to each cluster.  \nBecause the K -means algorithm finds a local rather than a global optimum, the results obtained will depend on the initial (random) cluster assignment of each observation in Step 1 of Algorithm 12.2. For this reason, it is important to run the algorithm multiple times from different random  \nFIGURE 12.8. The progress of the K-means algorithm on the example of Figure 12.7 with K =3. Top left: the observations are shown. Top center: in Step 1 of the algorithm, each observation is randomly assigned to a cluster. Top right: in Step 2(a), the cluster centroids are computed. These are shown as large colored disks. Initially the centroids are almost completely overlapping because the initial cluster assignments were chosen at random. Bottom left: in Step 2(b), each observation is assigned to the nearest centroid. Bottom center: Step 2(a) is once again performed, leading to new cluster centroids. Bottom right: the results obtained after ten iterations.  \n<!-- image -->  \ninitial configurations. Then one selects the best solution, i.e. that for which the objective (12.17) is smallest. Figure 12.9 shows the local optima obtained by running K -means clustering six times using six different initial cluster assignments, using the toy data from Figure 12.7. In this case, the best clustering is the one with an objective value of 235.8.  \nAs we have seen, to perform K -means clustering, we must decide how many clusters we expect in the data. The problem of selecting K is far from simple. This issue, along with other practical considerations that arise in performing K -means clustering, is addressed in Section 12.4.3.  \nFIGURE 12.9. K -means clustering performed six times on the data from Figure 12.7 with K = 3 , each time with a different random assignment of the observations in Step 1 of the K -means algorithm. Above each plot is the value of the objective (12.17). Three different local optima were obtained, one of which resulted in a smaller value of the objective and provides better separation between the clusters. Those labeled in red all achieved the same best solution, with an objective value of 235.8.  \n<!-- image -->', ""### 3.3.3 Potential Problems  \nWhen we fit a linear regression model to a particular data set, many problems may occur. Most common among these are the following:  \n1. Non-linearity of the response-predictor relationships.\n2. Correlation of error terms.\n3. Non-constant variance of error terms.\n4. Outliers.\n5. High-leverage points.\n6. Collinearity.  \nIn practice, identifying and overcoming these problems is as much an art as a science. Many pages in countless books have been written on this topic. Since the linear regression model is not our primary focus here, we will provide only a brief summary of some key points.\n1. Non-linearity of the Data  \nFIGURE 3.9. Plots of residuals versus predicted (or fitted) values for the Auto data set. In each plot, the red line is a smooth fit to the residuals, intended to make it easier to identify a trend. Left: A linear regression of mpg on horsepower . A strong pattern in the residuals indicates non-linearity in the data. Right: A linear regression of mpg on horsepower and horsepower 2 . There is little pattern in the residuals.  \n<!-- image -->  \nThe linear regression model assumes that there is a straight-line relationship between the predictors and the response. If the true relationship is far from linear, then virtually all of the conclusions that we draw from the fit are suspect. In addition, the prediction accuracy of the model can be significantly reduced.  \nResidual plots are a useful graphical tool for identifying non-linearity. Given a simple linear regression model, we can plot the residuals, e i =  \ny i -ˆ y i , versus the predictor x i . In the case of a multiple regression model, since there are multiple predictors, we instead plot the residuals versus the predicted (or fitted ) values ˆ y i . Ideally, the residual plot will show no discernible pattern. The presence of a pattern may indicate a problem with some aspect of the linear model.  \nIf the residual plot indicates that there are non-linear associations in the data, then a simple approach is to use non-linear transformations of the predictors, such as log X , √ X , and X 2 , in the regression model. In the later chapters of this book, we will discuss other more advanced non-linear approaches for addressing this issue.  \nThe left panel of Figure 3.9 displays a residual plot from the linear regression of mpg onto horsepower on the Auto data set that was illustrated in Figure 3.8. The red line is a smooth fit to the residuals, which is displayed in order to make it easier to identify any trends. The residuals exhibit a clear U-shape, which provides a strong indication of non-linearity in the data. In contrast, the right-hand panel of Figure 3.9 displays the residual plot that results from the model (3.36), which contains a quadratic term. There appears to be little pattern in the residuals, suggesting that the quadratic term improves the fit to the data.\n2. Correlation of Error Terms  \nAn important assumption of the linear regression model is that the error terms, /epsilon1 1  /epsilon1 2         /epsilon1 n , are uncorrelated. What does this mean? For instance, if the errors are uncorrelated, then the fact that /epsilon1 i is positive provides little or no information about the sign of /epsilon1 i +1 . The standard errors that are computed for the estimated regression coefficients or the fitted values are based on the assumption of uncorrelated error terms. If in fact there is correlation among the error terms, then the estimated standard errors will tend to underestimate the true standard errors. As a result, confidence and prediction intervals will be narrower than they should be. For example, a 95 % confidence interval may in reality have a much lower probability than 0   95 of containing the true value of the parameter. In addition, p -values associated with the model will be lower than they should be; this could cause us to erroneously conclude that a parameter is statistically significant. In short, if the error terms are correlated, we may have an unwarranted sense of confidence in our model.  \nWhy might correlations among the error terms occur? Such correlations frequently occur in the context of time series data, which consists of observations for which measurements are obtained at discrete points in time. In many cases, observations that are obtained at adjacent time points will have positively correlated errors. In order to determine if this is the case for a given data set, we can plot the residuals from our model as a function of time. If the errors are uncorrelated, then there should be no discernible pat-  \nAs an extreme example, suppose we accidentally doubled our data, leading to observations and error terms identical in pairs. If we ignored this, our standard error calculations would be as if we had a sample of size 2 n , when in fact we have only n samples. Our estimated parameters would be the same for the 2 n samples as for the n samples, but the confidence intervals would be narrower by a factor of √ 2 !  \nfitted time series  \nFIGURE 3.10. Plots of residuals from simulated time series data sets generated with differing levels of correlation ρ between error terms for adjacent time points.  \n<!-- image -->  \ntern. On the other hand, if the error terms are positively correlated, then we may see tracking in the residuals-that is, adjacent residuals may have similar values. Figure 3.10 provides an illustration. In the top panel, we see the residuals from a linear regression fit to data generated with uncorrelated errors. There is no evidence of a time-related trend in the residuals. In contrast, the residuals in the bottom panel are from a data set in which adjacent errors had a correlation of 0   9 . Now there is a clear pattern in the residuals-adjacent residuals tend to take on similar values. Finally, the center panel illustrates a more moderate case in which the residuals had a correlation of 0   5 . There is still evidence of tracking, but the pattern is less clear.  \nMany methods have been developed to properly take account of correlations in the error terms in time series data. Correlation among the error terms can also occur outside of time series data. For instance, consider a study in which individuals' heights are predicted from their weights. The assumption of uncorrelated errors could be violated if some of the individuals in the study are members of the same family, eat the same diet, or have been exposed to the same environmental factors. In general, the assumption of uncorrelated errors is extremely important for linear regression as well as for other statistical methods, and good experimental design is crucial in order to mitigate the risk of such correlations.  \ntracking  \nFIGURE 3.11. Residual plots. In each plot, the red line is a smooth fit to the residuals, intended to make it easier to identify a trend. The blue lines track the outer quantiles of the residuals, and emphasize patterns. Left: The funnel shape indicates heteroscedasticity. Right: The response has been log transformed, and there is now no evidence of heteroscedasticity.  \n<!-- image -->\n3. Non-constant Variance of Error Terms  \nAnother important assumption of the linear regression model is that the error terms have a constant variance, Var ( /epsilon1 i ) = σ 2 . The standard errors, confidence intervals, and hypothesis tests associated with the linear model rely upon this assumption.  \nSometimes we have a good idea of the variance of each response. For example, the i th response could be an average of n i raw observations. If each of these raw observations is uncorrelated with variance σ 2 , then their average has variance σ 2 i = σ 2 glyph[triangleleft]n i . In this case a simple remedy is to fit our model by weighted least squares , with weights proportional to the inverse variances-i.e. w i = n i in this case. Most linear regression software allows for observation weights.  \nUnfortunately, it is often the case that the variances of the error terms are non-constant. For instance, the variances of the error terms may increase with the value of the response. One can identify non-constant variances in the errors, or heteroscedasticity , from the presence of a funnel shape in the residual plot. An example is shown in the left-hand panel of Figure 3.11, in which the magnitude of the residuals tends to increase with the fitted values. When faced with this problem, one possible solution is to transform the response Y using a concave function such as log Y or √ Y . Such a transformation results in a greater amount of shrinkage of the larger responses, leading to a reduction in heteroscedasticity. The right-hand panel of Figure 3.11 displays the residual plot after transforming the response using log Y . The residuals now appear to have constant variance, though there is some evidence of a slight non-linear relationship in the data.\n4. Outliers  \nAn outlier is a point for which y i is far from the value predicted by the heteroscedasticity  \nweighted least squares outlier  \nFIGURE 3.12. Left: The least squares regression line is shown in red, and the regression line after removing the outlier is shown in blue. Center: The residual plot clearly identifies the outlier. Right: The outlier has a studentized residual of 6 ; typically we expect values between -3 and 3 .  \n<!-- image -->  \nmodel. Outliers can arise for a variety of reasons, such as incorrect recording of an observation during data collection.  \nResidual plots can be used to identify outliers. In this example, the outlier is clearly visible in the residual plot illustrated in the center panel of Figure 3.12. But in practice, it can be difficult to decide how large a residual needs to be before we consider the point to be an outlier. To address this problem, instead of plotting the residuals, we can plot the studentized residuals , computed by dividing each residual e i by its estimated standard error. Observations whose studentized residuals are greater than 3 in absolute value are possible outliers. In the right-hand panel of Figure 3.12, the outlier's studentized residual exceeds 6 , while all other observations have studentized residuals between -2 and 2 .  \nThe red point (observation 20) in the left-hand panel of Figure 3.12 illustrates a typical outlier. The red solid line is the least squares regression fit, while the blue dashed line is the least squares fit after removal of the outlier. In this case, removing the outlier has little effect on the least squares line: it leads to almost no change in the slope, and a miniscule reduction in the intercept. It is typical for an outlier that does not have an unusual predictor value to have little effect on the least squares fit. However, even if an outlier does not have much effect on the least squares fit, it can cause other problems. For instance, in this example, the RSE is 1   09 when the outlier is included in the regression, but it is only 0   77 when the outlier is removed. Since the RSE is used to compute all confidence intervals and p -values, such a dramatic increase caused by a single data point can have implications for the interpretation of the fit. Similarly, inclusion of the outlier causes the R 2 to decline from 0   892 to 0   805 .  \nIf we believe that an outlier has occurred due to an error in data collection or recording, then one solution is to simply remove the observation. However, care should be taken, since an outlier may instead indicate a deficiency with the model, such as a missing predictor.\n5. High Leverage Points  \nWe just saw that outliers are observations for which the response y i is unusual given the predictor x i . In contrast, observations with high leverage have an unusual value for x i . For example, observation 41 in the left-hand studentized residual  \nhigh leverage  \nFIGURE 3.13. Left: Observation 41 is a high leverage point, while 20 is not. The red line is the fit to all the data, and the blue line is the fit with observation 41 removed. Center: The red observation is not unusual in terms of its X 1 value or its X 2 value, but still falls outside the bulk of the data, and hence has high leverage. Right: Observation 41 has a high leverage and a high residual.  \n<!-- image -->  \npanel of Figure 3.13 has high leverage, in that the predictor value for this observation is large relative to the other observations. (Note that the data displayed in Figure 3.13 are the same as the data displayed in Figure 3.12, but with the addition of a single high leverage observation.) The red solid line is the least squares fit to the data, while the blue dashed line is the fit produced when observation 41 is removed. Comparing the left-hand panels of Figures 3.12 and 3.13, we observe that removing the high leverage observation has a much more substantial impact on the least squares line than removing the outlier. In fact, high leverage observations tend to have a sizable impact on the estimated regression line. It is cause for concern if the least squares line is heavily affected by just a couple of observations, because any problems with these points may invalidate the entire fit. For this reason, it is important to identify high leverage observations.  \nIn order to quantify an observation's leverage, we compute the leverage statistic . A large value of this statistic indicates an observation with high leverage. For a simple linear regression,  \nIn a simple linear regression, high leverage observations are fairly easy to identify, since we can simply look for observations for which the predictor value is outside of the normal range of the observations. But in a multiple linear regression with many predictors, it is possible to have an observation that is well within the range of each individual predictor's values, but that is unusual in terms of the full set of predictors. An example is shown in the center panel of Figure 3.13, for a data set with two predictors, X 1 and X 2 . Most of the observations' predictor values fall within the blue dashed ellipse, but the red observation is well outside of this range. But neither its value for X 1 nor its value for X 2 is unusual. So if we examine just X 1 or just X 2 , we will fail to notice this high leverage point. This problem is more pronounced in multiple regression settings with more than two predictors, because then there is no simple way to plot all dimensions of the data simultaneously.  \n$$h _ { i } = \\frac { 1 } { n } + \\frac { ( x _ { i } - \\bar { x } ) ^ { 2 } } { \\sum _ { i ^ { \\prime } = 1 } ^ { n } ( x _ { i ^ { \\prime } } - \\bar { x } ) ^ { 2 } } .$$  \nleverage statistic  \nFIGURE 3.14. Scatterplots of the observations from the Credit data set. Left: A plot of age versus limit . These two variables are not collinear. Right: A plot of rating versus limit . There is high collinearity.  \n<!-- image -->  \nIt is clear from this equation that h i increases with the distance of x i from ¯ x . There is a simple extension of h i to the case of multiple predictors, though we do not provide the formula here. The leverage statistic h i is always between 1 glyph[triangleleft]n and 1 , and the average leverage for all the observations is always equal to ( p +1) glyph[triangleleft]n . So if a given observation has a leverage statistic that greatly exceeds ( p +1) glyph[triangleleft]n , then we may suspect that the corresponding point has high leverage.  \nThe right-hand panel of Figure 3.13 provides a plot of the studentized residuals versus h i for the data in the left-hand panel of Figure 3.13. Observation 41 stands out as having a very high leverage statistic as well as a high studentized residual. In other words, it is an outlier as well as a high leverage observation. This is a particularly dangerous combination! This plot also reveals the reason that observation 20 had relatively little effect on the least squares fit in Figure 3.12: it has low leverage.\n6. Collinearity  \nCollinearity refers to the situation in which two or more predictor variables are closely related to one another. The concept of collinearity is illustrated in Figure 3.14 using the Credit data set. In the left-hand panel of Figure 3.14, the two predictors limit and age appear to have no obvious relationship. In contrast, in the right-hand panel of Figure 3.14, the predictors limit and rating are very highly correlated with each other, and we say that they are collinear . The presence of collinearity can pose problems in the regression context, since it can be difficult to separate out the individual effects of collinear variables on the response. In other words, since limit and rating tend to increase or decrease together, it can be difficult to determine how each one separately is associated with the response, balance .  \nFigure 3.15 illustrates some of the difficulties that can result from collinearity. The left-hand panel of Figure 3.15 is a contour plot of the RSS (3.22) associated with different possible coefficient estimates for the regression of balance on limit and age . Each ellipse represents a set of coefficients collinearity  \nFIGURE 3.15. Contour plots for the RSS values as a function of the parameters β for various regressions involving the Credit data set. In each plot, the black dots represent the coefficient values corresponding to the minimum RSS. Left: A contour plot of RSS for the regression of balance onto age and limit . The minimum value is well defined. Right: A contour plot of RSS for the regression of balance onto rating and limit . Because of the collinearity, there are many pairs ( β Limit  β Rating ) with a similar value for RSS.  \n<!-- image -->  \nthat correspond to the same RSS, with ellipses nearest to the center taking on the lowest values of RSS. The black dots and associated dashed lines represent the coefficient estimates that result in the smallest possible RSS-in other words, these are the least squares estimates. The axes for limit and age have been scaled so that the plot includes possible coefficient estimates that are up to four standard errors on either side of the least squares estimates. Thus the plot includes all plausible values for the coefficients. For example, we see that the true limit coefficient is almost certainly somewhere between 0   15 and 0   20 .  \nSince collinearity reduces the accuracy of the estimates of the regression coefficients, it causes the standard error for ˆ β j to grow. Recall that the t -statistic for each predictor is calculated by dividing ˆ β j by its standard  \nIn contrast, the right-hand panel of Figure 3.15 displays contour plots of the RSS associated with possible coefficient estimates for the regression of balance onto limit and rating , which we know to be highly collinear. Now the contours run along a narrow valley; there is a broad range of values for the coefficient estimates that result in equal values for RSS. Hence a small change in the data could cause the pair of coefficient values that yield the smallest RSS-that is, the least squares estimates-to move anywhere along this valley. This results in a great deal of uncertainty in the coefficient estimates. Notice that the scale for the limit coefficient now runs from roughly -0   2 to 0   2 ; this is an eight-fold increase over the plausible range of the limit coefficient in the regression with age . Interestingly, even though the limit and rating coefficients now have much more individual uncertainty, they will almost certainly lie somewhere in this contour valley. For example, we would not expect the true value of the limit and rating coefficients to be -0   1 and 1 respectively, even though such a value is plausible for each coefficient individually.  \nTABLE 3.11. The results for two multiple regression models involving the Credit data set are shown. Model 1 is a regression of balance on age and limit , and Model 2 a regression of balance on rating and limit . The standard error of ˆ β limit increases 12-fold in the second regression, due to collinearity.  \n|         |           | Coefficient   |   Std. error | t -statistic   | p -value                      |\n|---------|-----------|---------------|--------------|----------------|-------------------------------|\n| Model 1 | Intercept | - 173.411     |       43.828 | - 3.957        | < 0   0001 |\n|         | age       | - 2.292       |        0.672 | - 3.407        | 0   0007   |\n|         | limit     | 0.173         |        0.005 | 34.496         | < 0   0001 |\n|         | Intercept | - 377.537     |       45.254 | - 8.343        | < 0   0001 |\n| Model 2 | rating    | 2.202         |        0.952 | 2.312          | 0.0213                        |\n|         | limit     | 0.025         |        0.064 | 0.384          | 0.7012                        |  \nerror. Consequently, collinearity results in a decline in the t -statistic. As a result, in the presence of collinearity, we may fail to reject H 0 : β j = 0 . This means that the power of the hypothesis test-the probability of correctly detecting a non-zero coefficient-is reduced by collinearity.  \nA simple way to detect collinearity is to look at the correlation matrix of the predictors. An element of this matrix that is large in absolute value indicates a pair of highly correlated variables, and therefore a collinearity problem in the data. Unfortunately, not all collinearity problems can be detected by inspection of the correlation matrix: it is possible for collinearity to exist between three or more variables even if no pair of variables has a particularly high correlation. We call this situation multicollinearity . Instead of inspecting the correlation matrix, a better way to assess multicollinearity is to compute the variance inflation factor (VIF). The VIF is the ratio of the variance of ˆ β j when fitting the full model divided by the variance of ˆ β j if fit on its own. The smallest possible value for VIF is 1 , which indicates the complete absence of collinearity. Typically in practice there is a small amount of collinearity among the predictors. As a rule of thumb, a VIF value that exceeds 5 or 10 indicates a problematic amount of collinearity. The VIF for each variable can be computed using the formula  \nTable 3.11 compares the coefficient estimates obtained from two separate multiple regression models. The first is a regression of balance on age and limit , and the second is a regression of balance on rating and limit . In the first regression, both age and limit are highly significant with very small p -values. In the second, the collinearity between limit and rating has caused the standard error for the limit coefficient estimate to increase by a factor of 12 and the p -value to increase to 0   701 . In other words, the importance of the limit variable has been masked due to the presence of collinearity. To avoid such a situation, it is desirable to identify and address potential collinearity problems while fitting the model.  \n$$V I F ( \\hat { \\beta } _ { j } ) = \\frac { 1 } { 1 - R _ { X _ { j } | X _ { - j } } ^ { 2 } } ,$$  \nwhere R 2 X j ♣ X -j is the R 2 from a regression of X j onto all of the other predictors. If R 2 X j ♣ X -j is close to one, then collinearity is present, and so the VIF will be large.  \npower multicollinearity variance inflation factor  \nIn the Credit data, a regression of balance on age , rating , and limit indicates that the predictors have VIF values of 1.01, 160.67, and 160.59. As we suspected, there is considerable collinearity in the data!  \nWhen faced with the problem of collinearity, there are two simple solutions. The first is to drop one of the problematic variables from the regression. This can usually be done without much compromise to the regression fit, since the presence of collinearity implies that the information that this variable provides about the response is redundant in the presence of the other variables. For instance, if we regress balance onto age and limit , without the rating predictor, then the resulting VIF values are close to the minimum possible value of 1 , and the R 2 drops from 0   754 to 0   75 . So dropping rating from the set of predictors has effectively solved the collinearity problem without compromising the fit. The second solution is to combine the collinear variables together into a single predictor. For instance, we might take the average of standardized versions of limit and rating in order to create a new variable that measures credit worthiness ."", ""### 8.1.1 Regression Trees  \nIn order to motivate regression trees , we begin with a simple example.  \ndecision tree regression tree  \nFIGURE 8.1. For the Hitters data, a regression tree for predicting the log salary of a baseball player, based on the number of years that he has played in the major leagues and the number of hits that he made in the previous year. At a given internal node, the label (of the form X j &lt; t k ) indicates the left-hand branch emanating from that split, and the right-hand branch corresponds to X j ≥ t k . For instance, the split at the top of the tree results in two large branches. The left-hand branch corresponds to Years&lt;4.5 , and the right-hand branch corresponds to Years&gt;=4.5 . The tree has two internal nodes and three terminal nodes, or leaves. The number in each leaf is the mean of the response for the observations that fall there.  \n<!-- image -->\nPredicting Baseball Players' Salaries Using Regression Trees  \nWe use the Hitters data set to predict a baseball player's Salary based on Years (the number of years that he has played in the major leagues) and Hits (the number of hits that he made in the previous year). We first remove observations that are missing Salary values, and log-transform Salary so that its distribution has more of a typical bell-shape. (Recall that Salary is measured in thousands of dollars.)  \nFigure 8.1 shows a regression tree fit to this data. It consists of a series of splitting rules, starting at the top of the tree. The top split assigns observations having Years&lt;4.5 to the left branch. 1 The predicted salary for these players is given by the mean response value for the players in the data set with Years&lt;4.5 . For such players, the mean log salary is 5   107 , and so we make a prediction of e 5   107 thousands of dollars, i.e. $165,174, for these players. Players with Years&gt;=4.5 are assigned to the right branch, and then that group is further subdivided by Hits . Overall, the tree stratifies or segments the players into three regions of predictor space: players who have played for four or fewer years, players who have played for five or more years and who made fewer than 118 hits last year, and players who have played for five or more years and who made at least 118 hits last year. These three regions can be written as R 1 = {X ♣ Years&lt;4.5 }, R 2 = {X ♣ Years&gt;=4.5 , Hits&lt;117.5 ♦ , and R 3 = {X ♣ Years&gt;=4.5 , Hits&gt;=117.5 }. Figure 8.2 illustrates  \n1 Both Years and Hits are integers in these data; the function used to fit this tree labels the splits at the midpoint between two adjacent values.  \nFIGURE 8.2. The three-region partition for the Hitters data set from the regression tree illustrated in Figure 8.1.  \n<!-- image -->  \nthe regions as a function of Years and Hits . The predicted salaries for these three groups are $1,000 × e 5   107 = $165,174, $1,000 × e 5   999 = $402,834, and $1,000 × e 6   740 = $845,346 respectively.  \nWe might interpret the regression tree displayed in Figure 8.1 as follows: Years is the most important factor in determining Salary , and players with less experience earn lower salaries than more experienced players. Given that a player is less experienced, the number of hits that he made in the previous year seems to play little role in his salary. But among players who have been in the major leagues for five or more years, the number of hits made in the previous year does affect salary, and players who made more hits last year tend to have higher salaries. The regression tree shown in Figure 8.1 is likely an over-simplification of the true relationship between Hits , Years , and Salary . However, it has advantages over other types of regression models (such as those seen in Chapters 3 and 6): it is easier to interpret, and has a nice graphical representation.  \nIn keeping with the tree analogy, the regions R 1 , R 2 , and R 3 are known as terminal nodes or leaves of the tree. As is the case for Figure 8.1, decision trees are typically drawn upside down , in the sense that the leaves are at the bottom of the tree. The points along the tree where the predictor space is split are referred to as internal nodes . In Figure 8.1, the two internal nodes are indicated by the text Years&lt;4.5 and Hits&lt;117.5 . We refer to the segments of the trees that connect the nodes as branches .\nPrediction via Stratification of the Feature Space  \nWe now discuss the process of building a regression tree. Roughly speaking, there are two steps.  \n1. We divide the predictor space - that is, the set of possible values for X 1  X 2         X p - into J distinct and non-overlapping regions, R 1  R 2         R J .  \nterminal node leaf internal node branch  \n2. For every observation that falls into the region R j , we make the same prediction, which is simply the mean of the response values for the training observations in R j .  \nFor instance, suppose that in Step 1 we obtain two regions, R 1 and R 2 , and that the response mean of the training observations in the first region is 10 , while the response mean of the training observations in the second region is 20 . Then for a given observation X = x , if x ∈ R 1 we will predict a value of 10 , and if x ∈ R 2 we will predict a value of 20 .  \nWe now elaborate on Step 1 above. How do we construct the regions R 1         R J ? In theory, the regions could have any shape. However, we choose to divide the predictor space into high-dimensional rectangles, or boxes , for simplicity and for ease of interpretation of the resulting predictive model. The goal is to find boxes R 1         R J that minimize the RSS, given by  \n$$\\sum _ { j = 1 } ^ { J } \\sum _ { i \\in R _ { j } } ( y _ { i } - \\hat { y } _ { R _ { j } } ) ^ { 2 } ,$$  \nwhere ˆ y Rj is the mean response for the training observations within the j th box. Unfortunately, it is computationally infeasible to consider every possible partition of the feature space into J boxes. For this reason, we take a top-down , greedy approach that is known as recursive binary splitting . The approach is top-down because it begins at the top of the tree (at which point all observations belong to a single region) and then successively splits the predictor space; each split is indicated via two new branches further down on the tree. It is greedy because at each step of the tree-building process, the best split is made at that particular step, rather than looking ahead and picking a split that will lead to a better tree in some future step.  \nIn order to perform recursive binary splitting, we first select the predictor X j and the cutpoint s such that splitting the predictor space into the regions ¶ X ♣ X j &lt; s ♦ and ¶ X ♣ X j ≥ s ♦ leads to the greatest possible reduction in RSS. (The notation ¶ X ♣ X j &lt; s ♦ means the region of predictor space in which X j takes on a value less than s .) That is, we consider all predictors X 1         X p , and all possible values of the cutpoint s for each of the predictors, and then choose the predictor and cutpoint such that the resulting tree has the lowest RSS. In greater detail, for any j and s , we define the pair of half-planes  \n$$R _ { 1 } ( j , s ) = \\{ X | X _ { j } < s \\} \\ \\text { and } \\ R _ { 2 } ( j , s ) = \\{ X | X _ { j } \\geq s \\} ,$$  \nand we seek the value of j and s that minimize the equation  \n$$\\sum _ { i \\colon x _ { i } \\in R _ { 1 } ( j , s ) } ( y _ { i } - \\hat { y } _ { R _ { 1 } } ) ^ { 2 } + \\sum _ { i \\colon x _ { i } \\in R _ { 2 } ( j , s ) } ( y _ { i } - \\hat { y } _ { R _ { 2 } } ) ^ { 2 } ,$$  \nwhere ˆ y R 1 is the mean response for the training observations in R 1 ( j s ) , and ˆ y R 2 is the mean response for the training observations in R 2 ( j s ) . Finding the values of j and s that minimize (8.3) can be done quite quickly, especially when the number of features p is not too large.  \nNext, we repeat the process, looking for the best predictor and best cutpoint in order to split the data further so as to minimize the RSS within recursive binary splitting  \nFIGURE 8.3. Top Left: A partition of two-dimensional feature space that could not result from recursive binary splitting. Top Right: The output of recursive binary splitting on a two-dimensional example. Bottom Left: A tree corresponding to the partition in the top right panel. Bottom Right: A perspective plot of the prediction surface corresponding to that tree.  \n<!-- image -->  \neach of the resulting regions. However, this time, instead of splitting the entire predictor space, we split one of the two previously identified regions. We now have three regions. Again, we look to split one of these three regions further, so as to minimize the RSS. The process continues until a stopping criterion is reached; for instance, we may continue until no region contains more than five observations.  \nA five-region example of this approach is shown in Figure 8.3.  \nOnce the regions R 1         R J have been created, we predict the response for a given test observation using the mean of the training observations in the region to which that test observation belongs.\nTree Pruning  \nThe process described above may produce good predictions on the training set, but is likely to overfit the data, leading to poor test set performance. This is because the resulting tree might be too complex. A smaller tree  \nwith fewer splits (that is, fewer regions R 1         R J ) might lead to lower variance and better interpretation at the cost of a little bias. One possible alternative to the process described above is to build the tree only so long as the decrease in the RSS due to each split exceeds some (high) threshold. This strategy will result in smaller trees, but is too short-sighted since a seemingly worthless split early on in the tree might be followed by a very good split-that is, a split that leads to a large reduction in RSS later on.  \nCost complexity pruning -also known as weakest link pruning -gives us a way to do just this. Rather than considering every possible subtree, we consider a sequence of trees indexed by a nonnegative tuning parameter α . For each value of α there corresponds a subtree T ⊂ T 0 such that  \nTherefore, a better strategy is to grow a very large tree T 0 , and then prune it back in order to obtain a subtree . How do we determine the best way to prune the tree? Intuitively, our goal is to select a subtree that leads to the lowest test error rate. Given a subtree, we can estimate its test error using cross-validation or the validation set approach. However, estimating the cross-validation error for every possible subtree would be too cumbersome, since there is an extremely large number of possible subtrees. Instead, we need a way to select a small set of subtrees for consideration.  \n$$\\sum _ { m = 1 } ^ { | T | } \\sum _ { i \\colon x _ { i } \\in R _ { m } } ( y _ { i } - \\hat { y } _ { R _ { m } } ) ^ { 2 } + \\alpha | T | \\\\$$  \nis as small as possible. Here ♣ T ♣ indicates the number of terminal nodes of the tree T , R m is the rectangle (i.e. the subset of predictor space) corresponding to the m th terminal node, and ˆ y Rm is the predicted response associated with R m -that is, the mean of the training observations in R m . The tuning parameter α controls a trade-off between the subtree's complexity and its fit to the training data. When α = 0 , then the subtree T will simply equal T 0 , because then (8.4) just measures the training error. However, as α increases, there is a price to pay for having a tree with many terminal nodes, and so the quantity (8.4) will tend to be minimized for a smaller subtree. Equation 8.4 is reminiscent of the lasso (6.7) from Chapter 6, in which a similar formulation was used in order to control the complexity of a linear model.  \nIt turns out that as we increase α from zero in (8.4), branches get pruned from the tree in a nested and predictable fashion, so obtaining the whole sequence of subtrees as a function of α is easy. We can select a value of α using a validation set or using cross-validation. We then return to the full data set and obtain the subtree corresponding to α . This process is summarized in Algorithm 8.1.  \nFigures 8.4 and 8.5 display the results of fitting and pruning a regression tree on the Hitters data, using nine of the features. First, we randomly divided the data set in half, yielding 132 observations in the training set and 131 observations in the test set. We then built a large regression tree on the training data and varied α in (8.4) in order to create subtrees with different numbers of terminal nodes. Finally, we performed six-fold crossvalidation in order to estimate the cross-validated MSE of the trees as prune subtree  \ncost complexity pruning weakest link pruning\nAlgorithm 8.1 Building a Regression Tree  \n1. Use recursive binary splitting to grow a large tree on the training data, stopping only when each terminal node has fewer than some minimum number of observations.\n2. Apply cost complexity pruning to the large tree in order to obtain a sequence of best subtrees, as a function of α .\n3. Use K-fold cross-validation to choose α . That is, divide the training observations into K folds. For each k = 1         K :\n4. (a) Repeat Steps 1 and 2 on all but the k th fold of the training data.\n5. (b) Evaluate the mean squared prediction error on the data in the left-out k th fold, as a function of α .  \nAverage the results for each value of α , and pick α to minimize the average error.  \n4. Return the subtree from Step 2 that corresponds to the chosen value of α .  \na function of α . (We chose to perform six-fold cross-validation because 132 is an exact multiple of six.) The unpruned regression tree is shown in Figure 8.4. The green curve in Figure 8.5 shows the CV error as a function of the number of leaves, 2 while the orange curve indicates the test error. Also shown are standard error bars around the estimated errors. For reference, the training error curve is shown in black. The CV error is a reasonable approximation of the test error: the CV error takes on its minimum for a three-node tree, while the test error also dips down at the three-node tree (though it takes on its lowest value at the ten-node tree). The pruned tree containing three terminal nodes is shown in Figure 8.1."", ""### 3.1.3 Assessing the Accuracy of the Model  \nOnce we have rejected the null hypothesis (3.12) in favor of the alternative hypothesis (3.13), it is natural to want to quantify the extent to which the model fits the data . The quality of a linear regression fit is typically assessed using two related quantities: the residual standard error (RSE) and the R 2 statistic.  \n4 In Table 3.1, a small p -value for the intercept indicates that we can reject the null hypothesis that β 0 = 0 , and a small p -value for TV indicates that we can reject the null hypothesis that β 1 = 0 . Rejecting the latter null hypothesis allows us to conclude that there is a relationship between TV and sales . Rejecting the former allows us to conclude that in the absence of TV expenditure, sales are non-zero.  \np -value  \nR 2  \nTABLE 3.2. For the Advertising data, more information about the least squares model for the regression of number of units sold on TV advertising budget.  \n| Quantity                |   Value |\n|-------------------------|---------|\n| Residual standard error |   3.26  |\n| R 2                     |   0.612 |\n| F -statistic            | 312.1   |  \nTable 3.2 displays the RSE, the R 2 statistic, and the F -statistic (to be described in Section 3.2.2) for the linear regression of number of units sold on TV advertising budget.\nResidual Standard Error  \nRecall from the model (3.5) that associated with each observation is an error term /epsilon1 . Due to the presence of these error terms, even if we knew the true regression line (i.e. even if β 0 and β 1 were known), we would not be able to perfectly predict Y from X . The RSE is an estimate of the standard deviation of /epsilon1 . Roughly speaking, it is the average amount that the response will deviate from the true regression line. It is computed using the formula  \n$$\\text {deactivate from the true regression line. It is computed using the formula} \\\\ R S E = \\sqrt { \\frac { 1 } { n - 2 } } R S S = \\sqrt { \\frac { 1 } { n - 2 } } \\sum _ { i = 1 } ^ { n } ( y _ { i } - \\hat { y } _ { i } ) ^ { 2 } . \\\\ \\text {that RSS was defined in Section 3.1.1, and is given by the formula}$$  \nNote that RSS was defined in Section 3.1.1, and is given by the formula  \n$$R S S = \\sum _ { i = 1 } ^ { n } ( y _ { i } - \\hat { y } _ { i } ) ^ { 2 } . \\\\$$  \nIn the case of the advertising data, we see from the linear regression output in Table 3.2 that the RSE is 3   26 . In other words, actual sales in each market deviate from the true regression line by approximately 3  260 units, on average. Another way to think about this is that even if the model were correct and the true values of the unknown coefficients β 0 and β 1 were known exactly, any prediction of sales on the basis of TV advertising would still be off by about 3  260 units on average. Of course, whether or not 3  260 units is an acceptable prediction error depends on the problem context. In the advertising data set, the mean value of sales over all markets is approximately 14  000 units, and so the percentage error is 3  260 glyph[triangleleft] 14  000 = 23 %.  \nThe RSE is considered a measure of the lack of fit of the model (3.5) to the data. If the predictions obtained using the model are very close to the true outcome values-that is, if ˆ y i ≈ y i for i = 1         n -then (3.15) will be small, and we can conclude that the model fits the data very well. On the other hand, if ˆ y i is very far from y i for one or more observations, then the RSE may be quite large, indicating that the model doesn't fit the data well.\nR 2 Statistic  \nThe RSE provides an absolute measure of lack of fit of the model (3.5) to the data. But since it is measured in the units of Y , it is not always  \nclear what constitutes a good RSE. The R 2 statistic provides an alternative measure of fit. It takes the form of a proportion -the proportion of variance explained-and so it always takes on a value between 0 and 1 , and is independent of the scale of Y .  \nTo calculate R 2 , we use the formula  \n$$R ^ { 2 } = \\frac { \\text {TSSS} - \\text {RSS} } { \\text {TSSS} } = 1 - \\frac { \\text {RSS} } { \\text {TSSS} }$$  \nwhere TSS = ∑ ( y i -¯ y ) 2 is the total sum of squares , and RSS is defined in (3.16). TSS measures the total variance in the response Y , and can be thought of as the amount of variability inherent in the response before the regression is performed. In contrast, RSS measures the amount of variability that is left unexplained after performing the regression. Hence, TSS -RSS measures the amount of variability in the response that is explained (or removed) by performing the regression, and R 2 measures the proportion of variability in Y that can be explained using X . An R 2 statistic that is close to 1 indicates that a large proportion of the variability in the response is explained by the regression. A number near 0 indicates that the regression does not explain much of the variability in the response; this might occur because the linear model is wrong, or the error variance σ 2 is high, or both. In Table 3.2, the R 2 was 0   61 , and so just under two-thirds of the variability in sales is explained by a linear regression on TV .  \nThe R 2 statistic is a measure of the linear relationship between X and Y . Recall that correlation , defined as  \nThe R 2 statistic (3.17) has an interpretational advantage over the RSE (3.15), since unlike the RSE, it always lies between 0 and 1. However, it can still be challenging to determine what is a good R 2 value, and in general, this will depend on the application. For instance, in certain problems in physics, we may know that the data truly comes from a linear model with a small residual error. In this case, we would expect to see an R 2 value that is extremely close to 1 , and a substantially smaller R 2 value might indicate a serious problem with the experiment in which the data were generated. On the other hand, in typical applications in biology, psychology, marketing, and other domains, the linear model (3.5) is at best an extremely rough approximation to the data, and residual errors due to other unmeasured factors are often very large. In this setting, we would expect only a very small proportion of the variance in the response to be explained by the predictor, and an R 2 value well below 0   1 might be more realistic!  \n$$\\text {cor} ( X , Y ) = \\frac { \\sum _ { i = 1 } ^ { n } ( x _ { i } - \\bar { x } ) ( y _ { i } - \\bar { y } ) } { \\sqrt { \\sum _ { i = 1 } ^ { n } ( x _ { i } - \\bar { x } ) ^ { 2 } } \\sqrt { \\sum _ { i = 1 } ^ { n } ( y _ { i } - \\bar { y } ) ^ { 2 } } } , \\\\ \\text {also a measure of the linear relationship between } X \\text { and } Y . 5 \\text { This sur-}$$  \nis also a measure of the linear relationship between X and Y . 5 This suggests that we might be able to use r = Cor ( XY ) instead of R 2 in order to assess the fit of the linear model. In fact, it can be shown that in the simple  \n5 We note that in fact, the right-hand side of (3.18) is the sample correlation; thus, it would be more correct to write ̂ Cor ( XY ) ; however, we omit the 'hat' for ease of notation.  \ntotal sum of squares correlation  \nSimple regression of sales on radio  \n|           |   Coefficient |   Std. error |   t -statistic | p -value                      |\n|-----------|---------------|--------------|----------------|-------------------------------|\n| Intercept |         9.312 |        0.563 |          16.54 | < 0   0001 |\n| radio     |         0.203 |        0.02  |           9.92 | < 0   0001 |\nSimple regression of sales on newspaper  \nTABLE 3.3. More simple linear regression models for the Advertising data. Coefficients of the simple linear regression model for number of units sold on Top: radio advertising budget and Bottom: newspaper advertising budget. A $ 1  000 increase in spending on radio advertising is associated with an average increase in sales by around 203 units, while the same increase in spending on newspaper advertising is associated with an average increase in sales by around 55 units. (Note that the sales variable is in thousands of units, and the radio and newspaper variables are in thousands of dollars.)  \n|           |   Coefficient |   Std. error |   t -statistic | p -value                      |\n|-----------|---------------|--------------|----------------|-------------------------------|\n| Intercept |        12.351 |        0.621 |          19.88 | < 0   0001 |\n| newspaper |         0.055 |        0.017 |           3.3  | 0   00115  |  \nlinear regression setting, R 2 = r 2 . In other words, the squared correlation and the R 2 statistic are identical. However, in the next section we will discuss the multiple linear regression problem, in which we use several predictors simultaneously to predict the response. The concept of correlation between the predictors and the response does not extend automatically to this setting, since correlation quantifies the association between a single pair of variables rather than between a larger number of variables. We will see that R 2 fills this role.""]","La distancia basada en la correlación entre dos observaciones se define considerando la correlación entre los perfiles de observación de cada par de observaciones. En el contexto de la agrupación jerárquica, se menciona que si cada observación ha sido centrada para tener media cero y desviación estándar uno, y si denotamos \( r_{ij} \) como la correlación entre las observaciones \( i \) y \( j \), entonces la cantidad \( 1 - r_{ij} \) es proporcional a la distancia euclidiana al cuadrado entre las observaciones \( i \) y \( j \). Esto implica que la distancia basada en la correlación se centra en las formas de los perfiles de observación en lugar de sus magnitudes.",La distancia basada en la correlación considera que dos observaciones son similares si sus características están altamente correlacionadas. La matriz de disimilitud se puede calcular como uno menos la matriz de correlación entre las observaciones.,1.0,,0.7928571428373213,1.0
