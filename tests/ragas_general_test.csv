user_input,retrieved_contexts,response,reference,faithfulness,answer_relevancy,context_precision,context_recall
What is the primary motivation for supervised learning?,"[""### 2.1.4 Supervised Versus Unsupervised Learning  \nMost statistical learning problems fall into one of two categories: supervised or unsupervised . The examples that we have discussed so far in this chapter all fall into the supervised learning domain. For each observation of the predictor measurement(s) x i , i = 1         n there is an associated response measurement y i . We wish to fit a model that relates the response to the predictors, with the aim of accurately predicting the response for future observations (prediction) or better understanding the relationship between the response and the predictors (inference). Many classical statistical learning methods such as linear regression and logistic regression (Chapter 4), as well as more modern approaches such as GAM, boosting, and support vector machines, operate in the supervised learning domain. The vast majority of this book is devoted to this setting.  \nBy contrast, unsupervised learning describes the somewhat more challenging situation in which for every observation i = 1         n , we observe a vector of measurements x i but no associated response y i . It is not possible to fit a linear regression model, since there is no response variable to predict. In this setting, we are in some sense working blind; the situation is referred to as unsupervised because we lack a response variable that can supervise our analysis. What sort of statistical analysis is possible? We can seek to understand the relationships between the variables or between the observations. One statistical learning tool that we may use in this setting is cluster analysis , or clustering. The goal of cluster analysis is to ascertain, on the basis of x 1         x n , whether the observations fall into relatively distinct groups. For example, in a market segmentation study we might observe multiple characteristics (variables) for potential customers, such as zip code, family income, and shopping habits. We might believe that the customers fall into different groups, such as big spenders versus low spenders. If the information about each customer's spending patterns were available, then a supervised analysis would be possible. However, this information is not available-that is, we do not know whether each potential customer is a big spender or not. In this setting, we can try to cluster the customers on the basis of the variables measured, in order to identify supervised unsupervised  \nlogistic regression cluster analysis  \nFIGURE 2.8. A clustering data set involving three groups. Each group is shown using a different colored symbol. Left: The three groups are well-separated. In this setting, a clustering approach should successfully identify the three groups. Right: There is some overlap among the groups. Now the clustering task is more challenging.  \n<!-- image -->  \ndistinct groups of potential customers. Identifying such groups can be of interest because it might be that the groups differ with respect to some property of interest, such as spending habits.  \nIn the examples shown in Figure 2.8, there are only two variables, and so one can simply visually inspect the scatterplots of the observations in order to identify clusters. However, in practice, we often encounter data sets that contain many more than two variables. In this case, we cannot easily plot the observations. For instance, if there are p variables in our data set, then p ( p -1) glyph[triangleleft] 2 distinct scatterplots can be made, and visual inspection is simply not a viable way to identify clusters. For this reason, automated clustering methods are important. We discuss clustering and other unsupervised learning approaches in Chapter 12.  \nFigure 2.8 provides a simple illustration of the clustering problem. We have plotted 150 observations with measurements on two variables, X 1 and X 2 . Each observation corresponds to one of three distinct groups. For illustrative purposes, we have plotted the members of each group using different colors and symbols. However, in practice the group memberships are unknown, and the goal is to determine the group to which each observation belongs. In the left-hand panel of Figure 2.8, this is a relatively easy task because the groups are well-separated. By contrast, the right-hand panel illustrates a more challenging setting in which there is some overlap between the groups. A clustering method could not be expected to assign all of the overlapping points to their correct group (blue, green, or orange).  \nMany problems fall naturally into the supervised or unsupervised learning paradigms. However, sometimes the question of whether an analysis should be considered supervised or unsupervised is less clear-cut. For instance, suppose that we have a set of n observations. For m of the observations, where m&lt;n , we have both predictor measurements and a response  \nmeasurement. For the remaining n -m observations, we have predictor measurements but no response measurement. Such a scenario can arise if the predictors can be measured relatively cheaply but the corresponding responses are much more expensive to collect. We refer to this setting as a semi-supervised learning problem. In this setting, we wish to use a statistical learning method that can incorporate the m observations for which response measurements are available as well as the n -m observations for which they are not. Although this is an interesting topic, it is beyond the scope of this book."", ""## 12.1 The Challenge of Unsupervised Learning  \nSupervised learning is a well-understood area. In fact, if you have read the preceding chapters in this book, then you should by now have a good grasp of supervised learning. For instance, if you are asked to predict a binary outcome from a data set, you have a very well developed set of tools at your disposal (such as logistic regression, linear discriminant analysis, classification trees, support vector machines, and more) as well as a clear  \nhttps://doi.org/10.1007/978-3-031-38747-0\\_12  \nunderstanding of how to assess the quality of the results obtained (using cross-validation, validation on an independent test set, and so forth).  \nTechniques for unsupervised learning are of growing importance in a number of fields. A cancer researcher might assay gene expression levels in 100 patients with breast cancer. He or she might then look for subgroups among the breast cancer samples, or among the genes, in order to obtain a better understanding of the disease. An online shopping site might try to identify groups of shoppers with similar browsing and purchase histories, as well as items that are of particular interest to the shoppers within each group. Then an individual shopper can be preferentially shown the items in which he or she is particularly likely to be interested, based on the purchase histories of similar shoppers. A search engine might choose which search results to display to a particular individual based on the click histories of other individuals with similar search patterns. These statistical learning tasks, and many more, can be performed via unsupervised learning techniques.  \nIn contrast, unsupervised learning is often much more challenging. The exercise tends to be more subjective, and there is no simple goal for the analysis, such as prediction of a response. Unsupervised learning is often performed as part of an exploratory data analysis . Furthermore, it can be hard to assess the results obtained from unsupervised learning methods, since there is no universally accepted mechanism for performing crossvalidation or validating results on an independent data set. The reason for this difference is simple. If we fit a predictive model using a supervised learning technique, then it is possible to check our work by seeing how well our model predicts the response Y on observations not used in fitting the model. However, in unsupervised learning, there is no way to check our work because we don't know the true answer-the problem is unsupervised."", ""## A Brief History of Statistical Learning  \nThough the term statistical learning is fairly new, many of the concepts that underlie the field were developed long ago. At the beginning of the nineteenth century, the method of least squares was developed, implementing the earliest form of what is now known as linear regression . The approach was first successfully applied to problems in astronomy. Linear regression is used for predicting quantitative values, such as an individual's salary. In order to predict qualitative values, such as whether a patient survives or dies, or whether the stock market increases or decreases, linear discriminant analysis was proposed in 1936. In the 1940s, various authors put forth an alternative approach, logistic regression . In the early 1970s, the term generalized linear model was developed to describe an entire class of statistical learning methods that include both linear and logistic regression as special cases.  \nSince that time, statistical learning has emerged as a new subfield in statistics, focused on supervised and unsupervised modeling and prediction. In recent years, progress in statistical learning has been marked by the increasing availability of powerful and relatively user-friendly software, such as the popular and freely available Python system. This has the potential to continue the transformation of the field from a set of techniques used and  \nBy the end of the 1970s, many more techniques for learning from data were available. However, they were almost exclusively linear methods because fitting non-linear relationships was computationally difficult at the time. By the 1980s, computing technology had finally improved sufficiently that non-linear methods were no longer computationally prohibitive. In the mid 1980s, classification and regression trees were developed, followed shortly by generalized additive models . Neural networks gained popularity in the 1980s, and support vector machines arose in the 1990s.  \ndeveloped by statisticians and computer scientists to an essential toolkit for a much broader community.""]","The primary motivation for supervised learning is to fit a model that relates the response to the predictors, with the aim of accurately predicting the response for future observations (prediction) or better understanding the relationship between the response and the predictors (inference).","The primary motivation for supervised learning is to build a statistical model for predicting or estimating an output based on one or more inputs. This is useful in scenarios where we have a set of features (predictors) and a known outcome (response) and we want to predict the outcome for new, unseen data.",1.0,,0.9999999999666667,1.0
What are the two main types of problems that supervised learning addresses?,"[""### 2.1.4 Supervised Versus Unsupervised Learning  \nMost statistical learning problems fall into one of two categories: supervised or unsupervised . The examples that we have discussed so far in this chapter all fall into the supervised learning domain. For each observation of the predictor measurement(s) x i , i = 1         n there is an associated response measurement y i . We wish to fit a model that relates the response to the predictors, with the aim of accurately predicting the response for future observations (prediction) or better understanding the relationship between the response and the predictors (inference). Many classical statistical learning methods such as linear regression and logistic regression (Chapter 4), as well as more modern approaches such as GAM, boosting, and support vector machines, operate in the supervised learning domain. The vast majority of this book is devoted to this setting.  \nBy contrast, unsupervised learning describes the somewhat more challenging situation in which for every observation i = 1         n , we observe a vector of measurements x i but no associated response y i . It is not possible to fit a linear regression model, since there is no response variable to predict. In this setting, we are in some sense working blind; the situation is referred to as unsupervised because we lack a response variable that can supervise our analysis. What sort of statistical analysis is possible? We can seek to understand the relationships between the variables or between the observations. One statistical learning tool that we may use in this setting is cluster analysis , or clustering. The goal of cluster analysis is to ascertain, on the basis of x 1         x n , whether the observations fall into relatively distinct groups. For example, in a market segmentation study we might observe multiple characteristics (variables) for potential customers, such as zip code, family income, and shopping habits. We might believe that the customers fall into different groups, such as big spenders versus low spenders. If the information about each customer's spending patterns were available, then a supervised analysis would be possible. However, this information is not available-that is, we do not know whether each potential customer is a big spender or not. In this setting, we can try to cluster the customers on the basis of the variables measured, in order to identify supervised unsupervised  \nlogistic regression cluster analysis  \nFIGURE 2.8. A clustering data set involving three groups. Each group is shown using a different colored symbol. Left: The three groups are well-separated. In this setting, a clustering approach should successfully identify the three groups. Right: There is some overlap among the groups. Now the clustering task is more challenging.  \n<!-- image -->  \ndistinct groups of potential customers. Identifying such groups can be of interest because it might be that the groups differ with respect to some property of interest, such as spending habits.  \nIn the examples shown in Figure 2.8, there are only two variables, and so one can simply visually inspect the scatterplots of the observations in order to identify clusters. However, in practice, we often encounter data sets that contain many more than two variables. In this case, we cannot easily plot the observations. For instance, if there are p variables in our data set, then p ( p -1) glyph[triangleleft] 2 distinct scatterplots can be made, and visual inspection is simply not a viable way to identify clusters. For this reason, automated clustering methods are important. We discuss clustering and other unsupervised learning approaches in Chapter 12.  \nFigure 2.8 provides a simple illustration of the clustering problem. We have plotted 150 observations with measurements on two variables, X 1 and X 2 . Each observation corresponds to one of three distinct groups. For illustrative purposes, we have plotted the members of each group using different colors and symbols. However, in practice the group memberships are unknown, and the goal is to determine the group to which each observation belongs. In the left-hand panel of Figure 2.8, this is a relatively easy task because the groups are well-separated. By contrast, the right-hand panel illustrates a more challenging setting in which there is some overlap between the groups. A clustering method could not be expected to assign all of the overlapping points to their correct group (blue, green, or orange).  \nMany problems fall naturally into the supervised or unsupervised learning paradigms. However, sometimes the question of whether an analysis should be considered supervised or unsupervised is less clear-cut. For instance, suppose that we have a set of n observations. For m of the observations, where m&lt;n , we have both predictor measurements and a response  \nmeasurement. For the remaining n -m observations, we have predictor measurements but no response measurement. Such a scenario can arise if the predictors can be measured relatively cheaply but the corresponding responses are much more expensive to collect. We refer to this setting as a semi-supervised learning problem. In this setting, we wish to use a statistical learning method that can incorporate the m observations for which response measurements are available as well as the n -m observations for which they are not. Although this is an interesting topic, it is beyond the scope of this book."", ""## 4.1 An Overview of Classification  \nClassification problems occur often, perhaps even more so than regression problems. Some examples include:  \nqualitative classification  \nclassifier logistic  \nregression linear discriminant analysis quadratic discriminant analysis naive Bayes K -nearest neighbors generalized linear models Poisson regression  \n1. A person arrives at the emergency room with a set of symptoms that could possibly be attributed to one of three medical conditions. Which of the three conditions does the individual have?\n2. An online banking service must be able to determine whether or not a transaction being performed on the site is fraudulent, on the basis of the user's IP address, past transaction history, and so forth.\n3. On the basis of DNA sequence data for a number of patients with and without a given disease, a biologist would like to figure out which DNA mutations are deleterious (disease-causing) and which are not.  \nJust as in the regression setting, in the classification setting we have a set of training observations ( x 1  y 1 )         ( x n  y n ) that we can use to build a classifier. We want our classifier to perform well not only on the training data, but also on test observations that were not used to train the classifier.  \nIt is worth noting that Figure 4.1 displays a very pronounced relationship between the predictor balance and the response default . In most real applications, the relationship between the predictor and the response will not be nearly so strong. However, for the sake of illustrating the classification procedures discussed in this chapter, we use an example in which the relationship between the predictor and the response is somewhat exaggerated.  \nIn this chapter, we will illustrate the concept of classification using the simulated Default data set. We are interested in predicting whether an individual will default on his or her credit card payment, on the basis of annual income and monthly credit card balance. The data set is displayed in Figure 4.1. In the left-hand panel of Figure 4.1, we have plotted annual income and monthly credit card balance for a subset of 10  000 individuals. The individuals who defaulted in a given month are shown in orange, and those who did not in blue. (The overall default rate is about 3 %, so we have plotted only a fraction of the individuals who did not default.) It appears that individuals who defaulted tended to have higher credit card balances than those who did not. In the center and right-hand panels of Figure 4.1, two pairs of boxplots are shown. The first shows the distribution of balance split by the binary default variable; the second is a similar plot for income . In this chapter, we learn how to build a model to predict default ( Y ) for any given value of balance ( X 1 ) and income ( X 2 ). Since Y is not quantitative, the simple linear regression model of Chapter 3 is not a good choice: we will elaborate on this further in Section 4.2."", ""### 2.1.5 Regression Versus Classification Problems  \nVariables can be characterized as either quantitative or qualitative (also known as categorical ). Quantitative variables take on numerical values. Examples include a person's age, height, or income, the value of a house, and the price of a stock. In contrast, qualitative variables take on values in one of K different classes , or categories. Examples of qualitative variables include a person's marital status (married or not), the brand of product purchased (brand A, B, or C), whether a person defaults on a debt (yes or no), or a cancer diagnosis (Acute Myelogenous Leukemia, Acute Lymphoblastic Leukemia, or No Leukemia). We tend to refer to problems with a quantitative response as regression problems, while those involving a qualitative response are often referred to as classification problems. However, the distinction is not always that crisp. Least squares linear regression (Chapter 3) is used with a quantitative response, whereas logistic regression (Chapter 4) is typically used with a qualitative (two-class, or binary ) response. Thus, despite its name, logistic regression is a classification method. But since it estimates class probabilities, it can be thought of as a regression method as well. Some statistical methods, such as K -nearest neighbors (Chapters 2 and 4) and boosting (Chapter 8), can be used in the case of either quantitative or qualitative responses.  \nWe tend to select statistical learning methods on the basis of whether the response is quantitative or qualitative; i.e. we might use linear regression when quantitative and logistic regression when qualitative. However, whether the predictors are qualitative or quantitative is generally considered less important. Most of the statistical learning methods discussed in this book can be applied regardless of the predictor variable type, provided that any qualitative predictors are properly coded before the analysis is performed. This is discussed in Chapter 3."", ""## 12.1 The Challenge of Unsupervised Learning  \nSupervised learning is a well-understood area. In fact, if you have read the preceding chapters in this book, then you should by now have a good grasp of supervised learning. For instance, if you are asked to predict a binary outcome from a data set, you have a very well developed set of tools at your disposal (such as logistic regression, linear discriminant analysis, classification trees, support vector machines, and more) as well as a clear  \nhttps://doi.org/10.1007/978-3-031-38747-0\\_12  \nunderstanding of how to assess the quality of the results obtained (using cross-validation, validation on an independent test set, and so forth).  \nTechniques for unsupervised learning are of growing importance in a number of fields. A cancer researcher might assay gene expression levels in 100 patients with breast cancer. He or she might then look for subgroups among the breast cancer samples, or among the genes, in order to obtain a better understanding of the disease. An online shopping site might try to identify groups of shoppers with similar browsing and purchase histories, as well as items that are of particular interest to the shoppers within each group. Then an individual shopper can be preferentially shown the items in which he or she is particularly likely to be interested, based on the purchase histories of similar shoppers. A search engine might choose which search results to display to a particular individual based on the click histories of other individuals with similar search patterns. These statistical learning tasks, and many more, can be performed via unsupervised learning techniques.  \nIn contrast, unsupervised learning is often much more challenging. The exercise tends to be more subjective, and there is no simple goal for the analysis, such as prediction of a response. Unsupervised learning is often performed as part of an exploratory data analysis . Furthermore, it can be hard to assess the results obtained from unsupervised learning methods, since there is no universally accepted mechanism for performing crossvalidation or validating results on an independent data set. The reason for this difference is simple. If we fit a predictive model using a supervised learning technique, then it is possible to check our work by seeing how well our model predicts the response Y on observations not used in fitting the model. However, in unsupervised learning, there is no way to check our work because we don't know the true answer-the problem is unsupervised.""]","Supervised learning primarily addresses two main types of problems:

1. **Regression Problems**: These involve predicting a quantitative response. Examples of quantitative variables include a person's age, height, or income. In regression problems, methods like linear regression are typically used.

2. **Classification Problems**: These involve predicting a qualitative response, which takes on values in one of several classes or categories. Examples include determining a person's marital status (married or not) or whether a person defaults on a debt (yes or no). Logistic regression is a common method used for classification problems, despite its name suggesting a regression method.

In summary, regression problems deal with quantitative responses, while classification problems deal with qualitative responses.","Supervised learning addresses two main types of problems: regression and classification. Regression problems involve predicting a continuous or quantitative output, while classification problems involve predicting a qualitative or categorical output.",1.0,,0.9999999999666667,1.0
Describe the difference between training data and test data in the context of machine learning.,"['## 5.1 Cross-Validation  \nIn Chapter 2 we discuss the distinction between the test error rate and the training error rate . The test error is the average error that results from using a statistical learning method to predict the response on a new observationthat is, a measurement that was not used in training the method. Given a data set, the use of a particular statistical learning method is warranted if it results in a low test error. The test error can be easily calculated if a designated test set is available. Unfortunately, this is usually not the case. In contrast, the training error can be easily calculated by applying the statistical learning method to the observations used in its training. But as we saw in Chapter 2, the training error rate often is quite different from the test error rate, and in particular the former can dramatically underestimate the latter.  \nIn Sections 5.1.1-5.1.4, for simplicity we assume that we are interested in performing regression with a quantitative response. In Section 5.1.5 we consider the case of classification with a qualitative response. As we will see, the key concepts remain the same regardless of whether the response is quantitative or qualitative.  \nIn the absence of a very large designated test set that can be used to directly estimate the test error rate, a number of techniques can be used to estimate this quantity using the available training data. Some methods make a mathematical adjustment to the training error rate in order to estimate the test error rate. Such approaches are discussed in Chapter 6. In this section, we instead consider a class of methods that estimate the test error rate by holding out a subset of the training observations from the fitting process, and then applying the statistical learning method to those held out observations.', '### Applied  \n8. In this exercise, we will generate simulated data, and will then use this data to perform forward and backward stepwise selection.\n2. (a) Create a random number generator and use its normal() method to generate a predictor X of length n = 100 , as well as a noise vector /epsilon1 of length n = 100 .\n3. (b) Generate a response vector Y of length n = 100 according to the model  \n$$Y = \\beta _ { 0 } + \\beta _ { 1 } X + \\beta _ { 2 } X ^ { 2 } + \\beta _ { 3 } X ^ { 3 } + \\epsilon ,$$  \nwhere β 0 , β 1 , β 2 , and β 3 are constants of your choice.  \n- (c) Use forward stepwise selection in order to select a model containing the predictors XX 2         X 10 . What is the model obtained according to C p ? Report the coefficients of the model obtained.\n- (d) Repeat (c), using backwards stepwise selection. How does your answer compare to the results in (c)?\n- (e) Now fit a lasso model to the simulated data, again using XX 2         X 10 as predictors. Use cross-validation to select the optimal value of λ . Create plots of the cross-validation error as a function of λ . Report the resulting coefficient estimates, and discuss the results obtained.\n- (f) Now generate a response vector Y according to the model  \n$$Y = \\beta _ { 0 } + \\beta _ { 7 } X ^ { 7 } + \\epsilon ,$$  \nand perform forward stepwise selection and the lasso. Discuss the results obtained.  \n9. In this exercise, we will predict the number of applications received using the other variables in the College data set.\n2. (a) Split the data set into a training set and a test set.\n3. (b) Fit a linear model using least squares on the training set, and report the test error obtained.\n4. (c) Fit a ridge regression model on the training set, with λ chosen by cross-validation. Report the test error obtained.\n5. (d) Fit a lasso model on the training set, with λ chosen by crossvalidation. Report the test error obtained, along with the number of non-zero coefficient estimates.\n6. (e) Fit a PCR model on the training set, with M chosen by crossvalidation. Report the test error obtained, along with the value of M selected by cross-validation.\n7. (f) Fit a PLS model on the training set, with M chosen by crossvalidation. Report the test error obtained, along with the value of M selected by cross-validation.  \n- (g) Comment on the results obtained. How accurately can we predict the number of college applications received? Is there much difference among the test errors resulting from these five approaches?\n10. We have seen that as the number of features used in a model increases, the training error will necessarily decrease, but the test error may not. We will now explore this in a simulated data set.\n- (a) Generate a data set with p = 20 features, n = 1  000 observations, and an associated quantitative response vector generated according to the model  \n$$Y = X \\beta + \\epsilon ,$$  \nwhere β has some elements that are exactly equal to zero.  \n- (b) Split your data set into a training set containing 100 observations and a test set containing 900 observations.\n- (c) Perform best subset selection on the training set, and plot the training set MSE associated with the best model of each size.\n- (d) Plot the test set MSE associated with the best model of each size.\n- (e) For which model size does the test set MSE take on its minimum value? Comment on your results. If it takes on its minimum value for a model containing only an intercept or a model containing all of the features, then play around with the way that you are generating the data in (a) until you come up with a scenario in which the test set MSE is minimized for an intermediate model size.\n- (f) How does the model at which the test set MSE is minimized compare to the true model used to generate the data? Comment on the coefficient values.\n- (g) Create a plot displaying √ ∑ p j =1 ( β j -ˆ β r j ) 2 for a range of values of r , where ˆ β r j is the j th coefficient estimate for the best model containing r coefficients. Comment on what you observe. How does this compare to the test MSE plot from (d)?\n11. We will now try to predict per capita crime rate in the Boston data set.\n- (a) Try out some of the regression methods explored in this chapter, such as best subset selection, the lasso, ridge regression, and PCR. Present and discuss results for the approaches that you consider.\n- (b) Propose a model (or set of models) that seem to perform well on this data set, and justify your answer. Make sure that you are evaluating model performance using validation set error, crossvalidation, or some other reasonable alternative, as opposed to using training error.  \n- (c) Does your chosen model involve all of the features in the data set? Why or why not?', ""### 2.2.1 Measuring the Quality of Fit  \nIn order to evaluate the performance of a statistical learning method on a given data set, we need some way to measure how well its predictions actually match the observed data. That is, we need to quantify the extent to which the predicted response value for a given observation is close to the true response value for that observation. In the regression setting, the most commonly-used measure is the mean squared error (MSE), given by  \n$$M S E = \\frac { 1 } { n } \\sum _ { i = 1 } ^ { n } ( y _ { i } - \\hat { f } ( x _ { i } ) ) ^ { 2 } , \\quad \\ \\ ( 2 . 5 ) ^ { \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\$$  \nwhere ˆ f ( x i ) is the prediction that ˆ f gives for the i th observation. The MSE will be small if the predicted responses are very close to the true responses, and will be large if for some of the observations, the predicted and true responses differ substantially.  \nTo state it more mathematically, suppose that we fit our statistical learning method on our training observations ¶ ( x 1  y 1 )  ( x 2  y 2 )         ( x n  y n ) ♦ , and we obtain the estimate ˆ f . We can then compute ˆ f ( x 1 )  ˆ f ( x 2 )         ˆ f ( x n ) .  \nThe MSE in (2.5) is computed using the training data that was used to fit the model, and so should more accurately be referred to as the training MSE . But in general, we do not really care how well the method works on the training data. Rather, we are interested in the accuracy of the predictions that we obtain when we apply our method to previously unseen test data . Why is this what we care about? Suppose that we are interested in developing an algorithm to predict a stock's price based on previous stock returns. We can train the method using stock returns from the past 6 months. But we don't really care how well our method predicts last week's stock price. We instead care about how well it will predict tomorrow's price or next month's price. On a similar note, suppose that we have clinical measurements (e.g. weight, blood pressure, height, age, family history of disease) for a number of patients, as well as information about whether each patient has diabetes. We can use these patients to train a statistical learning method to predict risk of diabetes based on clinical measurements. In practice, we want this method to accurately predict diabetes risk for future patients based on their clinical measurements. We are not very interested in whether or not the method accurately predicts diabetes risk for patients used to train the model, since we already know which of those patients have diabetes.  \nmean squared error training MSE  \ntest data  \nFIGURE 2.9. Left: Data simulated from f , shown in black. Three estimates of f are shown: the linear regression line (orange curve), and two smoothing spline fits (blue and green curves). Right: Training MSE (grey curve), test MSE (red curve), and minimum possible test MSE over all methods (dashed line). Squares represent the training and test MSEs for the three fits shown in the left-hand panel.  \n<!-- image -->  \nIf these are approximately equal to y 1  y 2         y n , then the training MSE given by (2.5) is small. However, we are really not interested in whether ˆ f ( x i ) ≈ y i ; instead, we want to know whether ˆ f ( x 0 ) is approximately equal to y 0 , where ( x 0  y 0 ) is a previously unseen test observation not used to train the statistical learning method . We want to choose the method that gives the lowest test MSE , as opposed to the lowest training MSE. In other words, if we had a large number of test observations, we could compute  \n$$\\ A v e ( y _ { 0 } - \\hat { f } ( x _ { 0 } ) ) ^ { 2 } , \\\\ \\quad \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\$$  \nHow can we go about trying to select a method that minimizes the test MSE? In some settings, we may have a test data set available-that is, we may have access to a set of observations that were not used to train the statistical learning method. We can then simply evaluate (2.6) on the test observations, and select the learning method for which the test MSE is smallest. But what if no test observations are available? In that case, one might imagine simply selecting a statistical learning method that minimizes the training MSE (2.5). This seems like it might be a sensible approach, since the training MSE and the test MSE appear to be closely related. Unfortunately, there is a fundamental problem with this strategy: there is no guarantee that the method with the lowest training MSE will also have the lowest test MSE. Roughly speaking, the problem is that many statistical methods specifically estimate coefficients so as to minimize the training set MSE. For these methods, the training set MSE can be quite small, but the test MSE is often much larger.  \nthe average squared prediction error for these test observations ( x 0  y 0 ) . We'd like to select the model for which this quantity is as small as possible.  \nFigure 2.9 illustrates this phenomenon on a simple example. In the lefthand panel of Figure 2.9, we have generated observations from (2.1) with test MSE  \nthe true f given by the black curve. The orange, blue and green curves illustrate three possible estimates for f obtained using methods with increasing levels of flexibility. The orange line is the linear regression fit, which is relatively inflexible. The blue and green curves were produced using smoothing splines , discussed in Chapter 7, with different levels of smoothness. It is clear that as the level of flexibility increases, the curves fit the observed data more closely. The green curve is the most flexible and matches the data very well; however, we observe that it fits the true f (shown in black) poorly because it is too wiggly. By adjusting the level of flexibility of the smoothing spline fit, we can produce many different fits to this data.  \nIn this example, we know the true function f , and so we can also compute the test MSE over a very large test set, as a function of flexibility. (Of course, in general f is unknown, so this will not be possible.) The test MSE is displayed using the red curve in the right-hand panel of Figure 2.9. As with the training MSE, the test MSE initially declines as the level of flexibility increases. However, at some point the test MSE levels off and then starts to increase again. Consequently, the orange and green curves both have high test MSE. The blue curve minimizes the test MSE, which should not be surprising given that visually it appears to estimate f the best in the left-hand panel of Figure 2.9. The horizontal dashed line indicates Var ( /epsilon1 ) , the irreducible error in (2.3), which corresponds to the lowest achievable test MSE among all possible methods. Hence, the smoothing spline represented by the blue curve is close to optimal.  \nWe now move on to the right-hand panel of Figure 2.9. The grey curve displays the average training MSE as a function of flexibility, or more formally the degrees of freedom , for a number of smoothing splines. The degrees of freedom is a quantity that summarizes the flexibility of a curve; it is discussed more fully in Chapter 7. The orange, blue and green squares indicate the MSEs associated with the corresponding curves in the lefthand panel. A more restricted and hence smoother curve has fewer degrees of freedom than a wiggly curve-note that in Figure 2.9, linear regression is at the most restrictive end, with two degrees of freedom. The training MSE declines monotonically as flexibility increases. In this example the true f is non-linear, and so the orange linear fit is not flexible enough to estimate f well. The green curve has the lowest training MSE of all three methods, since it corresponds to the most flexible of the three curves fit in the left-hand panel.  \nIn the right-hand panel of Figure 2.9, as the flexibility of the statistical learning method increases, we observe a monotone decrease in the training MSE and a U-shape in the test MSE. This is a fundamental property of statistical learning that holds regardless of the particular data set at hand and regardless of the statistical method being used. As model flexibility increases, the training MSE will decrease, but the test MSE may not. When a given method yields a small training MSE but a large test MSE, we are said to be overfitting the data. This happens because our statistical learning procedure is working too hard to find patterns in the training data, and may be picking up some patterns that are just caused by random chance rather than by true properties of the unknown function f . When we overfit the training data, the test MSE will be very large because the supposed smoothing spline  \ndegrees of freedom  \nFIGURE 2.10. Details are as in Figure 2.9, using a different true f that is much closer to linear. In this setting, linear regression provides a very good fit to the data.  \n<!-- image -->  \npatterns that the method found in the training data simply don't exist in the test data. Note that regardless of whether or not overfitting has occurred, we almost always expect the training MSE to be smaller than the test MSE because most statistical learning methods either directly or indirectly seek to minimize the training MSE. Overfitting refers specifically to the case in which a less flexible model would have yielded a smaller test MSE.  \nIn practice, one can usually compute the training MSE with relative ease, but estimating the test MSE is considerably more difficult because usually no test data are available. As the previous three examples illustrate, the flexibility level corresponding to the model with the minimal test MSE can vary considerably among data sets. Throughout this book, we discuss a variety of approaches that can be used in practice to estimate this minimum point. One important method is cross-validation (Chapter 5), which is a method for estimating the test MSE using the training data.  \nFigure 2.10 provides another example in which the true f is approximately linear. Again we observe that the training MSE decreases monotonically as the model flexibility increases, and that there is a U-shape in the test MSE. However, because the truth is close to linear, the test MSE only decreases slightly before increasing again, so that the orange least squares fit is substantially better than the highly flexible green curve. Finally, Figure 2.11 displays an example in which f is highly non-linear. The training and test MSE curves still exhibit the same general patterns, but now there is a rapid decrease in both curves before the test MSE starts to increase slowly.""]","In the context of machine learning, training data and test data serve distinct purposes:

- **Training Data**: This is the dataset used to train a statistical learning method or model. The model learns from this data by adjusting its parameters to minimize the error in its predictions. The training data includes both the input features and the corresponding output labels or responses. The goal is to fit the model to this data as accurately as possible, which is often measured using metrics like the training mean squared error (MSE).

- **Test Data**: This dataset is used to evaluate the performance of the trained model. It consists of new, unseen observations that were not used during the training process. The test data helps in assessing how well the model generalizes to new data, which is crucial for understanding its predictive power. The test error, often measured as test MSE, indicates how accurately the model predicts the responses for these new observations. The test error is typically larger than the training error due to overfitting, where the model may perform well on the training data but poorly on unseen data.

The key difference is that training data is used to build the model, while test data is used to evaluate its performance on new, unseen data.","Training data is the set of observations used to teach or train a statistical learning model. Test data, on the other hand, consists of previously unseen observations that are used to evaluate the performance and accuracy of the trained model.",0.9375,,0.8333333332916666,1.0
¿Cuál es la diferencia entre un problema de regresión y uno de clasificación?,"[""### 2.1.5 Regression Versus Classification Problems  \nVariables can be characterized as either quantitative or qualitative (also known as categorical ). Quantitative variables take on numerical values. Examples include a person's age, height, or income, the value of a house, and the price of a stock. In contrast, qualitative variables take on values in one of K different classes , or categories. Examples of qualitative variables include a person's marital status (married or not), the brand of product purchased (brand A, B, or C), whether a person defaults on a debt (yes or no), or a cancer diagnosis (Acute Myelogenous Leukemia, Acute Lymphoblastic Leukemia, or No Leukemia). We tend to refer to problems with a quantitative response as regression problems, while those involving a qualitative response are often referred to as classification problems. However, the distinction is not always that crisp. Least squares linear regression (Chapter 3) is used with a quantitative response, whereas logistic regression (Chapter 4) is typically used with a qualitative (two-class, or binary ) response. Thus, despite its name, logistic regression is a classification method. But since it estimates class probabilities, it can be thought of as a regression method as well. Some statistical methods, such as K -nearest neighbors (Chapters 2 and 4) and boosting (Chapter 8), can be used in the case of either quantitative or qualitative responses.  \nWe tend to select statistical learning methods on the basis of whether the response is quantitative or qualitative; i.e. we might use linear regression when quantitative and logistic regression when qualitative. However, whether the predictors are qualitative or quantitative is generally considered less important. Most of the statistical learning methods discussed in this book can be applied regardless of the predictor variable type, provided that any qualitative predictors are properly coded before the analysis is performed. This is discussed in Chapter 3."", ""## 4.1 An Overview of Classification  \nClassification problems occur often, perhaps even more so than regression problems. Some examples include:  \nqualitative classification  \nclassifier logistic  \nregression linear discriminant analysis quadratic discriminant analysis naive Bayes K -nearest neighbors generalized linear models Poisson regression  \n1. A person arrives at the emergency room with a set of symptoms that could possibly be attributed to one of three medical conditions. Which of the three conditions does the individual have?\n2. An online banking service must be able to determine whether or not a transaction being performed on the site is fraudulent, on the basis of the user's IP address, past transaction history, and so forth.\n3. On the basis of DNA sequence data for a number of patients with and without a given disease, a biologist would like to figure out which DNA mutations are deleterious (disease-causing) and which are not.  \nJust as in the regression setting, in the classification setting we have a set of training observations ( x 1  y 1 )         ( x n  y n ) that we can use to build a classifier. We want our classifier to perform well not only on the training data, but also on test observations that were not used to train the classifier.  \nIt is worth noting that Figure 4.1 displays a very pronounced relationship between the predictor balance and the response default . In most real applications, the relationship between the predictor and the response will not be nearly so strong. However, for the sake of illustrating the classification procedures discussed in this chapter, we use an example in which the relationship between the predictor and the response is somewhat exaggerated.  \nIn this chapter, we will illustrate the concept of classification using the simulated Default data set. We are interested in predicting whether an individual will default on his or her credit card payment, on the basis of annual income and monthly credit card balance. The data set is displayed in Figure 4.1. In the left-hand panel of Figure 4.1, we have plotted annual income and monthly credit card balance for a subset of 10  000 individuals. The individuals who defaulted in a given month are shown in orange, and those who did not in blue. (The overall default rate is about 3 %, so we have plotted only a fraction of the individuals who did not default.) It appears that individuals who defaulted tended to have higher credit card balances than those who did not. In the center and right-hand panels of Figure 4.1, two pairs of boxplots are shown. The first shows the distribution of balance split by the binary default variable; the second is a similar plot for income . In this chapter, we learn how to build a model to predict default ( Y ) for any given value of balance ( X 1 ) and income ( X 2 ). Since Y is not quantitative, the simple linear regression model of Chapter 3 is not a good choice: we will elaborate on this further in Section 4.2."", '### 8.1.3 Trees Versus Linear Models  \nRegression and classification trees have a very different flavor from the more classical approaches for regression and classification presented in Chapters 3 and 4. In particular, linear regression assumes a model of the form  \n$$f ( X ) = \\beta _ { 0 } + \\sum _ { j = 1 } ^ { p } X _ { j } \\beta _ { j } ,$$  \nwhereas regression trees assume a model of the form  \n$$f ( X ) = \\sum _ { m = 1 } ^ { M } c _ { m } \\cdot 1 _ { ( X \\in R _ { m } ) } \\\\$$  \nwhere R 1         R M represent a partition of feature space, as in Figure 8.3.  \nOf course, other considerations beyond simply test error may come into play in selecting a statistical learning method; for instance, in certain settings, prediction using a tree may be preferred for the sake of interpretability and visualization.  \nWhich model is better? It depends on the problem at hand. If the relationship between the features and the response is well approximated by a linear model as in (8.8), then an approach such as linear regression will likely work well, and will outperform a method such as a regression tree that does not exploit this linear structure. If instead there is a highly nonlinear and complex relationship between the features and the response as indicated by model (8.9), then decision trees may outperform classical approaches. An illustrative example is displayed in Figure 8.7. The relative performances of tree-based and classical approaches can be assessed by estimating the test error, using either cross-validation or the validation set approach (Chapter 5).']","La diferencia principal entre un problema de regresión y uno de clasificación radica en el tipo de variable de respuesta que se está prediciendo:

- **Regresión**: Se refiere a problemas donde la variable de respuesta es cuantitativa, es decir, toma valores numéricos. Ejemplos de variables cuantitativas incluyen la edad de una persona, la altura, el ingreso, el valor de una casa o el precio de una acción. En estos casos, se utilizan métodos como la regresión lineal para predecir el valor numérico de la variable de respuesta.

- **Clasificación**: Se refiere a problemas donde la variable de respuesta es cualitativa o categórica, es decir, toma valores en una de varias clases o categorías. Ejemplos de variables cualitativas incluyen el estado civil de una persona (casado o no), la marca de un producto comprado (marca A, B o C), si una persona incumple con una deuda (sí o no), o un diagnóstico de cáncer (Leucemia Mieloide Aguda, Leucemia Linfoblástica Aguda o Sin Leucemia). En estos casos, se utilizan métodos como la regresión logística, que a pesar de su nombre, es un método de clasificación.

En resumen, la elección del método estadístico de aprendizaje se basa en si la respuesta es cuantitativa o cualitativa.","Un problema de regresión se refiere a la predicción de un valor de salida cuantitativo o continuo, como el precio de una acción. Un problema de clasificación se refiere a la predicción de un valor de salida cualitativo o categórico, como si un correo electrónico es spam o no.",1.0,,0.8333333332916666,1.0
What is the primary purpose of cross-validation?,"['### 5.1.1 The Validation Set Approach  \nSuppose that we would like to estimate the test error associated with fitting a particular statistical learning method on a set of observations. The validation set approach , displayed in Figure 5.1, is a very simple strategy for this task. It involves randomly dividing the available set of observations into two parts, a training set and a validation set or hold-out set . The model is fit on the training set, and the fitted model is used to predict the responses for the observations in the validation set. The resulting validation set error rate-typically assessed using MSE in the case of a quantitative response-provides an estimate of the test error rate.  \nWeillustrate the validation set approach on the Auto data set. Recall from Chapter 3 that there appears to be a non-linear relationship between mpg and horsepower , and that a model that predicts mpg using horsepower and horsepower 2 gives better results than a model that uses only a linear term. It is natural to wonder whether a cubic or higher-order fit might provide even better results. We answer this question in Chapter 3 by looking at the p-values associated with a cubic term and higher-order polynomial terms in a linear regression. But we could also answer this question using the validation method. We randomly split the 392 observations into two validation set approach validation set hold-out set  \nFIGURE 5.1. A schematic display of the validation set approach. A set of n observations are randomly split into a training set (shown in blue, containing observations 7, 22, and 13, among others) and a validation set (shown in beige, and containing observation 91, among others). The statistical learning method is fit on the training set, and its performance is evaluated on the validation set.  \n<!-- image -->  \nsets, a training set containing 196 of the data points, and a validation set containing the remaining 196 observations. The validation set error rates that result from fitting various regression models on the training sample and evaluating their performance on the validation sample, using MSE as a measure of validation set error, are shown in the left-hand panel of Figure 5.2. The validation set MSE for the quadratic fit is considerably smaller than for the linear fit. However, the validation set MSE for the cubic fit is actually slightly larger than for the quadratic fit. This implies that including a cubic term in the regression does not lead to better prediction than simply using a quadratic term.  \nThe validation set approach is conceptually simple and is easy to implement. But it has two potential drawbacks:  \nRecall that in order to create the left-hand panel of Figure 5.2, we randomly divided the data set into two parts, a training set and a validation set. If we repeat the process of randomly splitting the sample set into two parts, we will get a somewhat different estimate for the test MSE. As an illustration, the right-hand panel of Figure 5.2 displays ten different validation set MSE curves from the Auto data set, produced using ten different random splits of the observations into training and validation sets. All ten curves indicate that the model with a quadratic term has a dramatically smaller validation set MSE than the model with only a linear term. Furthermore, all ten curves indicate that there is not much benefit in including cubic or higher-order polynomial terms in the model. But it is worth noting that each of the ten curves results in a different test MSE estimate for each of the ten regression models considered. And there is no consensus among the curves as to which model results in the smallest validation set MSE. Based on the variability among these curves, all that we can conclude with any confidence is that the linear fit is not adequate for this data.  \n1. As is shown in the right-hand panel of Figure 5.2, the validation estimate of the test error rate can be highly variable, depending on precisely which observations are included in the training set and which observations are included in the validation set.\n2. In the validation approach, only a subset of the observations-those that are included in the training set rather than in the validation set-are used to fit the model. Since statistical methods tend to perform worse when trained on fewer observations, this suggests that the  \nFIGURE 5.2. The validation set approach was used on the Auto data set in order to estimate the test error that results from predicting mpg using polynomial functions of horsepower . Left: Validation error estimates for a single split into training and validation data sets. Right: The validation method was repeated ten times, each time using a different random split of the observations into a training set and a validation set. This illustrates the variability in the estimated test MSE that results from this approach.  \n<!-- image -->  \nvalidation set error rate may tend to overestimate the test error rate for the model fit on the entire data set.  \nIn the coming subsections, we will present cross-validation , a refinement of the validation set approach that addresses these two issues.', '### 5.1.3 k -Fold Cross-Validation  \nAn alternative to LOOCV is k -fold CV . This approach involves randomly dividing the set of observations into k groups, or folds , of approximately equal size. The first fold is treated as a validation set, and the method is fit on the remaining k -1 folds. The mean squared error, MSE 1 , is then computed on the observations in the held-out fold. This procedure is repeated k times; each time, a different group of observations is treated as a validation set. This process results in k estimates of the test error, MSE 1  MSE 2         MSE k . The k -fold CV estimate is computed by averaging these values,  \n$$C V _ { ( k ) } = \\frac { 1 } { k } \\sum _ { i = 1 } ^ { k } M S E _ { i } .$$  \nFigure 5.5 illustrates the k -fold CV approach.  \n1 In the case of multiple linear regression, the leverage takes a slightly more complicated form than (3.37), but (5.2) still holds.  \nk -fold CV  \nFIGURE 5.5. A schematic display of 5 -fold CV. A set of n observations is randomly split into five non-overlapping groups. Each of these fifths acts as a validation set (shown in beige), and the remainder as a training set (shown in blue). The test error is estimated by averaging the five resulting MSE estimates.  \n<!-- image -->  \nIt is not hard to see that LOOCV is a special case of k -fold CV in which k is set to equal n . In practice, one typically performs k -fold CV using k = 5 or k = 10 . What is the advantage of using k = 5 or k = 10 rather than k = n ? The most obvious advantage is computational. LOOCV requires fitting the statistical learning method n times. This has the potential to be computationally expensive (except for linear models fit by least squares, in which case formula (5.2) can be used). But cross-validation is a very general approach that can be applied to almost any statistical learning method. Some statistical learning methods have computationally intensive fitting procedures, and so performing LOOCV may pose computational problems, especially if n is extremely large. In contrast, performing 10 -fold CV requires fitting the learning procedure only ten times, which may be much more feasible. As we see in Section 5.1.4, there also can be other non-computational advantages to performing 5 -fold or 10 -fold CV, which involve the bias-variance trade-off.  \nWhen we examine real data, we do not know the true test MSE, and so it is difficult to determine the accuracy of the cross-validation estimate. However, if we examine simulated data, then we can compute the true test MSE, and can thereby evaluate the accuracy of our cross-validation results. In Figure 5.6, we plot the cross-validation estimates and true test error rates that result from applying smoothing splines to the simulated data sets illustrated in Figures 2.9-2.11 of Chapter 2. The true test MSE is displayed in blue. The black dashed and orange solid lines respectively show the estimated LOOCV and 10 -fold CV estimates. In all three plots, the two cross-validation estimates are very similar. In the right-hand panel  \nThe right-hand panel of Figure 5.4 displays nine different 10 -fold CV estimates for the Auto data set, each resulting from a different random split of the observations into ten folds. As we can see from the figure, there is some variability in the CV estimates as a result of the variability in how the observations are divided into ten folds. But this variability is typically much lower than the variability in the test error estimates that results from the validation set approach (right-hand panel of Figure 5.2).  \nFIGURE 5.6. True and estimated test MSE for the simulated data sets in Figures 2.9 ( left ), 2.10 ( center ), and 2.11 ( right ). The true test MSE is shown in blue, the LOOCV estimate is shown as a black dashed line, and the 10 -fold CV estimate is shown in orange. The crosses indicate the minimum of each of the MSE curves.  \n<!-- image -->  \nof Figure 5.6, the true test MSE and the cross-validation curves are almost identical. In the center panel of Figure 5.6, the two sets of curves are similar at the lower degrees of flexibility, while the CV curves overestimate the test set MSE for higher degrees of flexibility. In the left-hand panel of Figure 5.6, the CV curves have the correct general shape, but they underestimate the true test MSE.  \nWhen we perform cross-validation, our goal might be to determine how well a given statistical learning procedure can be expected to perform on independent data; in this case, the actual estimate of the test MSE is of interest. But at other times we are interested only in the location of the minimum point in the estimated test MSE curve . This is because we might be performing cross-validation on a number of statistical learning methods, or on a single method using different levels of flexibility, in order to identify the method that results in the lowest test error. For this purpose, the location of the minimum point in the estimated test MSE curve is important, but the actual value of the estimated test MSE is not. We find in Figure 5.6 that despite the fact that they sometimes underestimate the true test MSE, all of the CV curves come close to identifying the correct level of flexibility-that is, the flexibility level corresponding to the smallest test MSE.', ""### 6.1.3 Choosing the Optimal Model  \nBest subset selection, forward selection, and backward selection result in the creation of a set of models, each of which contains a subset of the p  \n3 Like forward stepwise selection, backward stepwise selection performs a guided search over model space, and so effectively considers substantially more than 1 + p ( p + 1) glyph[triangleleft] 2 models.  \npredictors. To apply these methods, we need a way to determine which of these models is best . As we discussed in Section 6.1.1, the model containing all of the predictors will always have the smallest RSS and the largest R 2 , since these quantities are related to the training error. Instead, we wish to choose a model with a low test error. As is evident here, and as we show in Chapter 2, the training error can be a poor estimate of the test error. Therefore, RSS and R 2 are not suitable for selecting the best model among a collection of models with different numbers of predictors.  \nIn order to select the best model with respect to test error, we need to estimate this test error. There are two common approaches:  \n1. We can indirectly estimate test error by making an adjustment to the training error to account for the bias due to overfitting.\n2. We can directly estimate the test error, using either a validation set approach or a cross-validation approach, as discussed in Chapter 5.  \nWe consider both of these approaches below.\nC p , AIC, BIC, and Adjusted R 2  \nWe show in Chapter 2 that the training set MSE is generally an underestimate of the test MSE. (Recall that MSE = RSS glyph[triangleleft]n .) This is because when we fit a model to the training data using least squares, we specifically estimate the regression coefficients such that the training RSS (but not the test RSS) is as small as possible. In particular, the training error will decrease as more variables are included in the model, but the test error may not. Therefore, training set RSS and training set R 2 cannot be used to select from among a set of models with different numbers of variables.  \nFor a fitted least squares model containing d predictors, the C p estimate of test MSE is computed using the equation  \nHowever, a number of techniques for adjusting the training error for the model size are available. These approaches can be used to select among a set of models with different numbers of variables. We now consider four such approaches: C p , Akaike information criterion (AIC), Bayesian information criterion (BIC), and adjusted R 2 . Figure 6.2 displays C p , BIC, and adjusted R 2 for the best model of each size produced by best subset selection on the Credit data set.  \n$$C _ { p } = \\frac { 1 } { n } \\left ( R S S + 2 d \\hat { \\sigma } ^ { 2 } \\right ) , \\\\ \\intertext { t i m a t e \\, o f \\, the \\, v a r i c e \\, o f \\, the \\, e r r $ \\epsilon $ \\, associated \\, with \\, each }$$  \nwhere ˆ σ 2 is an estimate of the variance of the error /epsilon1 associated with each response measurement in (6.1). 4 Typically ˆ σ 2 is estimated using the full model containing all predictors. Essentially, the C p statistic adds a penalty of 2 d ˆ σ 2 to the training RSS in order to adjust for the fact that the training error tends to underestimate the test error. Clearly, the penalty increases as the number of predictors in the model increases; this is intended to adjust  \n4 Mallow's C p is sometimes defined as C ′ p = RSS glyph[triangleleft] ˆ σ 2 +2 d -n . This is equivalent to the definition given above in the sense that C p = 1 n ˆ σ 2 ( C ′ p + n ) , and so the model with smallest C p also has smallest C ′ p .  \n<!-- image -->  \nC p Akaike information criterion Bayesian information criterion adjusted R 2  \nFIGURE 6.2. C p , BIC, and adjusted R 2 are shown for the best models of each size for the Credit data set (the lower frontier in Figure 6.1). C p and BIC are estimates of test MSE. In the middle plot we see that the BIC estimate of test error shows an increase after four variables are selected. The other two plots are rather flat after four variables are included.  \n<!-- image -->  \nfor the corresponding decrease in training RSS. Though it is beyond the scope of this book, one can show that if ˆ σ 2 is an unbiased estimate of σ 2 in (6.2), then C p is an unbiased estimate of test MSE. As a consequence, the C p statistic tends to take on a small value for models with a low test error, so when determining which of a set of models is best, we choose the model with the lowest C p value. In Figure 6.2, C p selects the six-variable model containing the predictors income , limit , rating , cards , age and student .  \nThe AIC criterion is defined for a large class of models fit by maximum likelihood. In the case of the model (6.1) with Gaussian errors, maximum likelihood and least squares are the same thing. In this case AIC is given by  \n$$A I C = \\frac { 1 } { n } \\left ( R S S + 2 d \\hat { \\sigma } ^ { 2 } \\right ) , \\\\$$  \nBIC is derived from a Bayesian point of view, but ends up looking similar to C p (and AIC) as well. For the least squares model with d predictors, the BIC is, up to irrelevant constants, given by where, for simplicity, we have omitted irrelevant constants. 5 Hence for least squares models, C p and AIC are proportional to each other, and so only C p is displayed in Figure 6.2.  \n$$B I C = \\frac { 1 } { n } \\left ( R S S + \\log ( n ) d \\hat { \\sigma } ^ { 2 } \\right ) . \\\\ \\intertext { B I C } R I C \\text { will } \\text { end } \\text { to take } \\text { on } a \\text { small value for } a \\text { mode } \\text { with } a$$  \nLike C p , the BIC will tend to take on a small value for a model with a low test error, and so generally we select the model that has the lowest BIC value. Notice that BIC replaces the 2 d ˆ σ 2 used by C p with a log( n ) d ˆ σ 2 term, where n is the number of observations. Since log n &gt; 2 for any n &gt; 7 ,  \n5 There are two formulas for AIC for least squares regression. The formula that we provide here requires an expression for σ 2 , which we obtain using the full model containing all predictors. The second formula is appropriate when σ 2 is unknown and we do not want to explicitly estimate it; that formula has a log( RSS ) term instead of an RSS term. Detailed derivations of these two formulas are outside of the scope of this book.  \nthe BIC statistic generally places a heavier penalty on models with many variables, and hence results in the selection of smaller models than C p . In Figure 6.2, we see that this is indeed the case for the Credit data set; BIC chooses a model that contains only the four predictors income , limit , cards , and student . In this case the curves are very flat and so there does not appear to be much difference in accuracy between the four-variable and six-variable models.  \nThe adjusted R 2 statistic is another popular approach for selecting among a set of models that contain different numbers of variables. Recall from Chapter 3 that the usual R 2 is defined as 1 -RSS glyph[triangleleft] TSS, where TSS = ∑ ( y i -y ) 2 is the total sum of squares for the response. Since RSS always decreases as more variables are added to the model, the R 2 always increases as more variables are added. For a least squares model with d variables, the adjusted R 2 statistic is calculated as  \n$$\\text {Adjusted} \\, R ^ { 2 } = 1 - \\frac { \\text {RSS} / ( n - d - 1 ) } { \\text {TSS} / ( n - 1 ) } .$$  \nUnlike C p , AIC, and BIC, for which a small value indicates a model with a low test error, a large value of adjusted R 2 indicates a model with a small test error. Maximizing the adjusted R 2 is equivalent to minimizing RSS n -d -1 . While RSS always decreases as the number of variables in the model increases, RSS n -d -1 may increase or decrease, due to the presence of d in the denominator.  \nC p , AIC, and BIC all have rigorous theoretical justifications that are beyond the scope of this book. These justifications rely on asymptotic arguments (scenarios where the sample size n is very large). Despite its popularity, and even though it is quite intuitive, the adjusted R 2 is not as well motivated in statistical theory as AIC, BIC, and C p . All of these measures are simple to use and compute. Here we have presented their formulas in the case of a linear model fit using least squares; however, AIC and BIC can also be defined for more general types of models.  \nThe intuition behind the adjusted R 2 is that once all of the correct variables have been included in the model, adding additional noise variables will lead to only a very small decrease in RSS. Since adding noise variables leads to an increase in d , such variables will lead to an increase in RSS n -d -1 , and consequently a decrease in the adjusted R 2 . Therefore, in theory, the model with the largest adjusted R 2 will have only correct variables and no noise variables. Unlike the R 2 statistic, the adjusted R 2 statistic pays a price for the inclusion of unnecessary variables in the model. Figure 6.2 displays the adjusted R 2 for the Credit data set. Using this statistic results in the selection of a model that contains seven variables, adding own to the model selected by C p and AIC.\nValidation and Cross-Validation  \nAs an alternative to the approaches just discussed, we can directly estimate the test error using the validation set and cross-validation methods discussed in Chapter 5. We can compute the validation set error or the cross-validation error for each model under consideration, and then select  \nFIGURE 6.3. For the Credit data set, three quantities are displayed for the best model containing d predictors, for d ranging from 1 to 11 . The overall best model, based on each of these quantities, is shown as a blue cross. Left: Square root of BIC. Center: Validation set errors. Right: Cross-validation errors.  \n<!-- image -->  \nthe model for which the resulting estimated test error is smallest. This procedure has an advantage relative to AIC, BIC, C p , and adjusted R 2 , in that it provides a direct estimate of the test error, and makes fewer assumptions about the true underlying model. It can also be used in a wider range of model selection tasks, even in cases where it is hard to pinpoint the model degrees of freedom (e.g. the number of predictors in the model) or hard to estimate the error variance σ 2 . Note that when cross-validation is used, the sequence of models M k in Algorithms 6.1-6.3 is determined separately for each training fold, and the validation errors are averaged over all folds for each model size k . This means, for example with best-subset regression, that M k , the best subset of size k , can differ across the folds. Once the best size k is chosen, we find the best model of that size on the full data set.  \nFigure 6.3 displays, as a function of d , the BIC, validation set errors, and cross-validation errors on the Credit data, for the best d -variable model. The validation errors were calculated by randomly selecting three-quarters of the observations as the training set, and the remainder as the validation set. The cross-validation errors were computed using k = 10 folds. In this case, the validation and cross-validation methods both result in a six-variable model. However, all three approaches suggest that the four-, five-, and six-variable models are roughly equivalent in terms of their test errors.  \nIn the past, performing cross-validation was computationally prohibitive for many problems with large p and/or large n , and so AIC, BIC, C p , and adjusted R 2 were more attractive approaches for choosing among a set of models. However, nowadays with fast computers, the computations required to perform cross-validation are hardly ever an issue. Thus, crossvalidation is a very attractive approach for selecting from among a number of models under consideration.  \nIn fact, the estimated test error curves displayed in the center and righthand panels of Figure 6.3 are quite flat. While a three-variable model clearly has lower estimated test error than a two-variable model, the estimated test errors of the 3- to 11-variable models are quite similar. Furthermore, if we  \nrepeated the validation set approach using a different split of the data into a training set and a validation set, or if we repeated cross-validation using a different set of cross-validation folds, then the precise model with the lowest estimated test error would surely change. In this setting, we can select a model using the one-standard-error rule . We first calculate the standard error of the estimated test MSE for each model size, and then select the smallest model for which the estimated test error is within one standard error of the lowest point on the curve. The rationale here is that if a set of models appear to be more or less equally good, then we might as well choose the simplest model-that is, the model with the smallest number of predictors. In this case, applying the one-standard-error rule to the validation set or cross-validation approach leads to selection of the three-variable model."", ""### 5.1.4 Bias-Variance Trade-Off for k -Fold Cross-Validation  \nWe mentioned in Section 5.1.3 that k -fold CV with k &lt; n has a computational advantage to LOOCV. But putting computational issues aside, a less obvious but potentially more important advantage of k -fold CV is that it often gives more accurate estimates of the test error rate than does LOOCV. This has to do with a bias-variance trade-off.  \nIt was mentioned in Section 5.1.1 that the validation set approach can lead to overestimates of the test error rate, since in this approach the training set used to fit the statistical learning method contains only half the observations of the entire data set. Using this logic, it is not hard to see  \nthat LOOCV will give approximately unbiased estimates of the test error, since each training set contains n -1 observations, which is almost as many as the number of observations in the full data set. And performing k -fold CV for, say, k = 5 or k = 10 will lead to an intermediate level of bias, since each training set contains approximately ( k -1) nglyph[triangleleft]k observationsfewer than in the LOOCV approach, but substantially more than in the validation set approach. Therefore, from the perspective of bias reduction, it is clear that LOOCV is to be preferred to k -fold CV.  \nTo summarize, there is a bias-variance trade-off associated with the choice of k in k -fold cross-validation. Typically, given these considerations, one performs k -fold cross-validation using k = 5 or k = 10 , as these values have been shown empirically to yield test error rate estimates that suffer neither from excessively high bias nor from very high variance.  \nHowever, we know that bias is not the only source for concern in an estimating procedure; we must also consider the procedure's variance. It turns out that LOOCV has higher variance than does k -fold CV with k &lt; n . Why is this the case? When we perform LOOCV, we are in effect averaging the outputs of n fitted models, each of which is trained on an almost identical set of observations; therefore, these outputs are highly (positively) correlated with each other. In contrast, when we perform k -fold CV with k &lt; n , we are averaging the outputs of k fitted models that are somewhat less correlated with each other, since the overlap between the training sets in each model is smaller. Since the mean of many highly correlated quantities has higher variance than does the mean of many quantities that are not as highly correlated, the test error estimate resulting from LOOCV tends to have higher variance than does the test error estimate resulting from k -fold CV.""]","The primary purpose of cross-validation is to estimate the test error associated with a statistical learning method. It helps in determining how well a given statistical learning procedure can be expected to perform on independent data. Cross-validation provides a more accurate estimate of the test error by averaging the results over multiple training and validation sets, thereby addressing issues like variability and overfitting that can arise with simpler validation methods.","Cross-validation is a technique used to estimate the test error of a statistical learning method, which helps in evaluating its performance and selecting the appropriate level of model complexity.",1.0,,0.999999999975,1.0
Explain the difference between the validation set approach and leave-one-out cross-validation (LOOCV).,"[""### 5.1.2 Leave-One-Out Cross-Validation  \nLeave-one-out cross-validation (LOOCV) is closely related to the validation set approach of Section 5.1.1, but it attempts to address that method's drawbacks.  \nWe can repeat the procedure by selecting ( x 2  y 2 ) for the validation data, training the statistical learning procedure on the n -1 observations ¶ ( x 1  y 1 )  ( x 3  y 3 )         ( x n  y n ) ♦ , and computing MSE 2 = ( y 2 -ˆ y 2 ) 2 . Repeating this approach n times produces n squared errors, MSE 1         MSE n . The LOOCV estimate for the test MSE is the average of these n test error estimates:  \nLike the validation set approach, LOOCV involves splitting the set of observations into two parts. However, instead of creating two subsets of comparable size, a single observation ( x 1  y 1 ) is used for the validation set, and the remaining observations ¶ ( x 2  y 2 )         ( x n  y n ) ♦ make up the training set. The statistical learning method is fit on the n -1 training observations, and a prediction ˆ y 1 is made for the excluded observation, using its value x 1 . Since ( x 1  y 1 ) was not used in the fitting process, MSE 1 = ( y 1 -ˆ y 1 ) 2 provides an approximately unbiased estimate for the test error. But even though MSE 1 is unbiased for the test error, it is a poor estimate because it is highly variable, since it is based upon a single observation ( x 1  y 1 ) .  \n$$C V _ { ( n ) } = \\frac { 1 } { n } \\sum _ { i = 1 } ^ { n } M S E _ { i } .$$  \nleave-oneout crossvalidation  \nFIGURE 5.3. A schematic display of LOOCV. A set of n data points is repeatedly split into a training set (shown in blue) containing all but one observation, and a validation set that contains only that observation (shown in beige). The test error is then estimated by averaging the n resulting MSEs. The first training set contains all but observation 1, the second training set contains all but observation 2, and so forth.  \n<!-- image -->  \nA schematic of the LOOCV approach is illustrated in Figure 5.3.  \nWe used LOOCV on the Auto data set in order to obtain an estimate of the test set MSE that results from fitting a linear regression model to predict mpg using polynomial functions of horsepower . The results are shown in the left-hand panel of Figure 5.4.  \nLOOCV has a couple of major advantages over the validation set approach. First, it has far less bias. In LOOCV, we repeatedly fit the statistical learning method using training sets that contain n -1 observations, almost as many as are in the entire data set. This is in contrast to the validation set approach, in which the training set is typically around half the size of the original data set. Consequently, the LOOCV approach tends not to overestimate the test error rate as much as the validation set approach does. Second, in contrast to the validation approach which will yield different results when applied repeatedly due to randomness in the training/validation set splits, performing LOOCV multiple times will always yield the same results: there is no randomness in the training/validation set splits.  \nLOOCV has the potential to be expensive to implement, since the model has to be fit n times. This can be very time consuming if n is large, and if each individual model is slow to fit. With least squares linear or polynomial regression, an amazing shortcut makes the cost of LOOCV the same as that of a single model fit! The following formula holds:  \n$$C V _ { ( n ) } = \\frac { 1 } { n } \\sum _ { i = 1 } ^ { n } \\left ( \\frac { y _ { i } - \\hat { y } _ { i } } { 1 - h _ { i } } \\right ) ^ { 2 } \\, ,$$  \nFIGURE 5.4. Cross-validation was used on the Auto data set in order to estimate the test error that results from predicting mpg using polynomial functions of horsepower . Left: The LOOCV error curve. Right: 10 -fold CV was run nine separate times, each with a different random split of the data into ten parts. The figure shows the nine slightly different CV error curves.  \n<!-- image -->  \nwhere ˆ y i is the i th fitted value from the original least squares fit, and h i is the leverage defined in (3.37) on page 105. 1 This is like the ordinary MSE, except the i th residual is divided by 1 -h i . The leverage lies between 1 glyph[triangleleft]n and 1 , and reflects the amount that an observation influences its own fit. Hence the residuals for high-leverage points are inflated in this formula by exactly the right amount for this equality to hold.  \nLOOCV is a very general method, and can be used with any kind of predictive modeling. For example we could use it with logistic regression or linear discriminant analysis, or any of the methods discussed in later chapters. The magic formula (5.2) does not hold in general, in which case the model has to be refit n times."", '## 5.4 Exercises  \n### Conceptual  \n1. Using basic statistical properties of the variance, as well as singlevariable calculus, derive (5.6). In other words, prove that α given by (5.6) does indeed minimize Var( α X +(1 -α ) Y ) .\n2. We will now derive the probability that a given observation is part of a bootstrap sample. Suppose that we obtain a bootstrap sample from a set of n observations.\n3. (a) What is the probability that the first bootstrap observation is not the j th observation from the original sample? Justify your answer.\n4. (b) What is the probability that the second bootstrap observation is not the j th observation from the original sample?\n5. (c) Argue that the probability that the j th observation is not in the bootstrap sample is (1 -1 glyph[triangleleft]n ) n .\n6. (d) When n = 5 , what is the probability that the j th observation is in the bootstrap sample?\n7. (e) When n = 100 , what is the probability that the j th observation is in the bootstrap sample?\n8. (f) When n = 10  000 , what is the probability that the j th observation is in the bootstrap sample?\n9. (g) Create a plot that displays, for each integer value of n from 1 to 100  000 , the probability that the j th observation is in the bootstrap sample. Comment on what you observe.\n10. (h) We will now investigate numerically the probability that a bootstrap sample of size n = 100 contains the j th observation. Here j = 4 . We first create an array store with values that will subsequently be overwritten using the function np.empty() . We then  \nrepeatedly create bootstrap samples, and each time we record whether or not the fifth observation is contained in the bootstrap sample.  \n```\nrng = np.random.default_rng(10) store = np.empty(10000) for i in range(10000): store[i] = np.sum(rng.choice(100, replace=True) == 4) > 0 np.mean(store)\n```  \nComment on the results obtained.  \n3. We now review k -fold cross-validation.\n2. (a) Explain how k -fold cross-validation is implemented.\n3. (b) What are the advantages and disadvantages of k -fold crossvalidation relative to:\n4. Suppose that we use some statistical learning method to make a prediction for the response Y for a particular value of the predictor X . Carefully describe how we might estimate the standard deviation of our prediction.  \n```\ni. The validation set approach? ii. LOOCV?\n```']","The validation set approach and leave-one-out cross-validation (LOOCV) are both methods used to estimate the test error of a statistical learning model, but they differ in how they split the data and their characteristics:

### Validation Set Approach:
- **Data Splitting**: The data set is split into two parts: a training set and a validation set, typically of comparable size.
- **Training and Validation**: The model is trained on the training set and evaluated on the validation set to estimate the test error.
- **Bias and Variability**: This approach can have high bias because the training set is smaller than the full data set. It can also have high variability because the results depend on the specific random split of the data.
- **Randomness**: Different random splits can lead to different results, making the approach less stable.

### Leave-One-Out Cross-Validation (LOOCV):
- **Data Splitting**: Each observation in the data set is used once as a validation set, while the remaining observations form the training set. This process is repeated for each observation.
- **Training and Validation**: The model is trained on \( n-1 \) observations and validated on the single left-out observation. This is repeated \( n \) times (where \( n \) is the number of observations), and the test error is estimated by averaging the errors from each iteration.
- **Bias and Variability**: LOOCV has less bias because the training set is almost as large as the full data set. It also has less variability because there is no randomness in the data splits.
- **Computational Cost**: LOOCV can be computationally expensive because the model is fit \( n \) times, although there are shortcuts for certain models like least squares regression that reduce this cost.

In summary, LOOCV tends to provide a more stable and less biased estimate of the test error compared to the validation set approach, but it can be more computationally intensive.","In the validation set approach, the data is randomly split into a training set and a validation set. The model is trained on the training set and evaluated on the validation set. In LOOCV, each observation is used as a validation set once, and the model is trained on the remaining n-1 observations. This process is repeated n times, and the test error is estimated by averaging the n resulting MSEs.",,,0.99999999995,1.0
What is the 'bias-variance trade-off' in the context of k-fold cross-validation?,"[""### 5.1.4 Bias-Variance Trade-Off for k -Fold Cross-Validation  \nWe mentioned in Section 5.1.3 that k -fold CV with k &lt; n has a computational advantage to LOOCV. But putting computational issues aside, a less obvious but potentially more important advantage of k -fold CV is that it often gives more accurate estimates of the test error rate than does LOOCV. This has to do with a bias-variance trade-off.  \nIt was mentioned in Section 5.1.1 that the validation set approach can lead to overestimates of the test error rate, since in this approach the training set used to fit the statistical learning method contains only half the observations of the entire data set. Using this logic, it is not hard to see  \nthat LOOCV will give approximately unbiased estimates of the test error, since each training set contains n -1 observations, which is almost as many as the number of observations in the full data set. And performing k -fold CV for, say, k = 5 or k = 10 will lead to an intermediate level of bias, since each training set contains approximately ( k -1) nglyph[triangleleft]k observationsfewer than in the LOOCV approach, but substantially more than in the validation set approach. Therefore, from the perspective of bias reduction, it is clear that LOOCV is to be preferred to k -fold CV.  \nTo summarize, there is a bias-variance trade-off associated with the choice of k in k -fold cross-validation. Typically, given these considerations, one performs k -fold cross-validation using k = 5 or k = 10 , as these values have been shown empirically to yield test error rate estimates that suffer neither from excessively high bias nor from very high variance.  \nHowever, we know that bias is not the only source for concern in an estimating procedure; we must also consider the procedure's variance. It turns out that LOOCV has higher variance than does k -fold CV with k &lt; n . Why is this the case? When we perform LOOCV, we are in effect averaging the outputs of n fitted models, each of which is trained on an almost identical set of observations; therefore, these outputs are highly (positively) correlated with each other. In contrast, when we perform k -fold CV with k &lt; n , we are averaging the outputs of k fitted models that are somewhat less correlated with each other, since the overlap between the training sets in each model is smaller. Since the mean of many highly correlated quantities has higher variance than does the mean of many quantities that are not as highly correlated, the test error estimate resulting from LOOCV tends to have higher variance than does the test error estimate resulting from k -fold CV."", ""### 2.2.2 The Bias-Variance Trade-Off  \nThe U-shape observed in the test MSE curves (Figures 2.9-2.11) turns out to be the result of two competing properties of statistical learning methods.  \ncrossvalidation  \nFIGURE 2.11. Details are as in Figure 2.9, using a different f that is far from linear. In this setting, linear regression provides a very poor fit to the data.  \n<!-- image -->  \nThough the mathematical proof is beyond the scope of this book, it is possible to show that the expected test MSE, for a given value x 0 , can always be decomposed into the sum of three fundamental quantities: the variance of ˆ f ( x 0 ) , the squared bias of ˆ f ( x 0 ) and the variance of the error terms /epsilon1 . That is,  \n$$E \\left ( y _ { 0 } - \\hat { f } ( x _ { 0 } ) \\right ) ^ { 2 } = \\text {Var} ( \\hat { f } ( x _ { 0 } ) ) + \\left [ \\text {Bias} ( \\hat { f } ( x _ { 0 } ) ) \\right ] ^ { 2 } + \\text {Var} ( \\epsilon ) . \\\\$$  \nEquation 2.7 tells us that in order to minimize the expected test error, we need to select a statistical learning method that simultaneously achieves low variance and low bias . Note that variance is inherently a nonnegative quantity, and squared bias is also nonnegative. Hence, we see that the expected test MSE can never lie below Var ( /epsilon1 ) , the irreducible error from (2.3).  \nHere the notation E ( y 0 -ˆ f ( x 0 ) ) 2 defines the expected test MSE at x 0 , and refers to the average test MSE that we would obtain if we repeatedly estimated f using a large number of training sets, and tested each at x 0 . The overall expected test MSE can be computed by averaging E ( y 0 -ˆ f ( x 0 ) ) 2 over all possible values of x 0 in the test set.  \nWhat do we mean by the variance and bias of a statistical learning method? Variance refers to the amount by which ˆ f would change if we estimated it using a different training data set. Since the training data are used to fit the statistical learning method, different training data sets will result in a different ˆ f . But ideally the estimate for f should not vary too much between training sets. However, if a method has high variance then small changes in the training data can result in large changes in ˆ f . In general, more flexible statistical methods have higher variance. Consider the green and orange curves in Figure 2.9. The flexible green curve is following the observations very closely. It has high variance because changing any one of these data points may cause the estimate ˆ f to change considerably.  \nvariance bias expected test MSE  \nFIGURE 2.12. Squared bias (blue curve), variance (orange curve), Var ( /epsilon1 ) (dashed line), and test MSE (red curve) for the three data sets in Figures 2.9-2.11. The vertical dotted line indicates the flexibility level corresponding to the smallest test MSE.  \n<!-- image -->  \nIn contrast, the orange least squares line is relatively inflexible and has low variance, because moving any single observation will likely cause only a small shift in the position of the line.  \nAs a general rule, as we use more flexible methods, the variance will increase and the bias will decrease. The relative rate of change of these two quantities determines whether the test MSE increases or decreases. As we increase the flexibility of a class of methods, the bias tends to initially decrease faster than the variance increases. Consequently, the expected test MSE declines. However, at some point increasing flexibility has little impact on the bias but starts to significantly increase the variance. When this happens the test MSE increases. Note that we observed this pattern of decreasing test MSE followed by increasing test MSE in the right-hand panels of Figures 2.9-2.11.  \nOn the other hand, bias refers to the error that is introduced by approximating a real-life problem, which may be extremely complicated, by a much simpler model. For example, linear regression assumes that there is a linear relationship between Y and X 1  X 2         X p . It is unlikely that any real-life problem truly has such a simple linear relationship, and so performing linear regression will undoubtedly result in some bias in the estimate of f . In Figure 2.11, the true f is substantially non-linear, so no matter how many training observations we are given, it will not be possible to produce an accurate estimate using linear regression. In other words, linear regression results in high bias in this example. However, in Figure 2.10 the true f is very close to linear, and so given enough data, it should be possible for linear regression to produce an accurate estimate. Generally, more flexible methods result in less bias.  \nThe three plots in Figure 2.12 illustrate Equation 2.7 for the examples in Figures 2.9-2.11. In each case the blue solid curve represents the squared bias, for different levels of flexibility, while the orange curve corresponds to the variance. The horizontal dashed line represents Var ( /epsilon1 ) , the irreducible error. Finally, the red curve, corresponding to the test set MSE, is the sum  \nof these three quantities. In all three cases, the variance increases and the bias decreases as the method's flexibility increases. However, the flexibility level corresponding to the optimal test MSE differs considerably among the three data sets, because the squared bias and variance change at different rates in each of the data sets. In the left-hand panel of Figure 2.12, the bias initially decreases rapidly, resulting in an initial sharp decrease in the expected test MSE. On the other hand, in the center panel of Figure 2.12 the true f is close to linear, so there is only a small decrease in bias as flexibility increases, and the test MSE only declines slightly before increasing rapidly as the variance increases. Finally, in the right-hand panel of Figure 2.12, as flexibility increases, there is a dramatic decline in bias because the true f is very non-linear. There is also very little increase in variance as flexibility increases. Consequently, the test MSE declines substantially before experiencing a small increase as model flexibility increases.  \nIn a real-life situation in which f is unobserved, it is generally not possible to explicitly compute the test MSE, bias, or variance for a statistical learning method. Nevertheless, one should always keep the bias-variance trade-off in mind. In this book we explore methods that are extremely flexible and hence can essentially eliminate bias. However, this does not guarantee that they will outperform a much simpler method such as linear regression. To take an extreme example, suppose that the true f is linear. In this situation linear regression will have no bias, making it very hard for a more flexible method to compete. In contrast, if the true f is highly non-linear and we have an ample number of training observations, then we may do better using a highly flexible approach, as in Figure 2.11. In Chapter 5 we discuss cross-validation, which is a way to estimate the test MSE using the training data.  \nThe relationship between bias, variance, and test set MSE given in Equation 2.7 and displayed in Figure 2.12 is referred to as the bias-variance trade-off . Good test set performance of a statistical learning method requires low variance as well as low squared bias. This is referred to as a trade-off because it is easy to obtain a method with extremely low bias but high variance (for instance, by drawing a curve that passes through every single training observation) or a method with very low variance but high bias (by fitting a horizontal line to the data). The challenge lies in finding a method for which both the variance and the squared bias are low. This trade-off is one of the most important recurring themes in this book.""]","The 'bias-variance trade-off' in the context of k-fold cross-validation refers to the balance between bias and variance when estimating the test error rate. Here's a summary of how it applies:

- **Bias**: In k-fold cross-validation, the bias is related to how closely the training set used to fit the model resembles the full dataset. LOOCV (Leave-One-Out Cross-Validation) tends to have lower bias because each training set contains \( n-1 \) observations, which is almost the entire dataset. In contrast, k-fold CV with \( k < n \) has a slightly higher bias because each training set contains fewer observations than LOOCV, but more than the validation set approach.

- **Variance**: Variance in this context refers to the variability of the test error estimate. LOOCV has higher variance because it averages the outputs of \( n \) fitted models, each trained on almost identical datasets, leading to highly correlated outputs. On the other hand, k-fold CV with \( k < n \) results in lower variance because it averages the outputs of \( k \) models that are less correlated, due to smaller overlap between training sets.

The trade-off is about choosing a value of \( k \) that balances these two aspects. Typically, \( k = 5 \) or \( k = 10 \) is chosen because these values have been empirically shown to provide test error rate estimates that do not suffer from excessively high bias or variance.","In k-fold cross-validation, there is a trade-off between bias and variance. A smaller value of k (like 5 or 10) leads to a more biased estimate of the test error because the training sets are smaller than the full dataset. A larger value of k (like in LOOCV) leads to a less biased estimate but higher variance because the training sets are highly correlated. Typically, k=5 or k=10 is used as a compromise between bias and variance.",1.0,,0.99999999995,1.0
¿Qué es el 'bootstrap' y para qué se utiliza en el aprendizaje estadístico?,"['## 5.2 The Bootstrap  \nThe bootstrap is a widely applicable and extremely powerful statistical tool that can be used to quantify the uncertainty associated with a given estimator or statistical learning method. As a simple example, the bootstrap can be used to estimate the standard errors of the coefficients from a linear regression fit. In the specific case of linear regression, this is not particularly useful, since we saw in Chapter 3 that standard statistical software such as R outputs such standard errors automatically. However, the power of the bootstrap lies in the fact that it can be easily applied to a wide range of statistical learning methods, including some for which a measure of variability is otherwise difficult to obtain and is not automatically output by statistical software.  \nSuppose that we wish to invest a fixed sum of money in two financial assets that yield returns of X and Y , respectively, where X and Y are random quantities. We will invest a fraction α of our money in X , and will invest the remaining 1 -α in Y . Since there is variability associated with the returns on these two assets, we wish to choose α to minimize the total risk, or variance, of our investment. In other words, we want to minimize Var( α X +(1 -α ) Y ) . One can show that the value that minimizes the risk is given by  \nIn this section we illustrate the bootstrap on a toy example in which we wish to determine the best investment allocation under a simple model. In Section 5.3 we explore the use of the bootstrap to assess the variability associated with the regression coefficients in a linear model fit.  \n$$\\alpha = \\frac { \\sigma _ { Y } ^ { 2 } - \\sigma _ { X Y } } { \\sigma _ { X } ^ { 2 } + \\sigma _ { Y } ^ { 2 } - 2 \\sigma _ { X Y } } , \\\\ \\quad ? \\quad U _ { X } ( U _ { Y } ) _ { 1 } + U _ { X } ( U _ { Y } ) _ { 2 }$$  \nwhere σ 2 X = Var( X )  σ 2 Y = Var( Y ) , and σ XY = Cov( XY ) .  \nIn reality, the quantities σ 2 X , σ 2 Y , and σ XY are unknown. We can compute estimates for these quantities, ˆ σ 2 X , ˆ σ 2 Y , and ˆ σ XY , using a data set that contains past measurements for X and Y . We can then estimate the value of α that minimizes the variance of our investment using  \n$$\\hat { \\alpha } = \\frac { \\hat { \\sigma } _ { Y } ^ { 2 } - \\hat { \\sigma } _ { X Y } } { \\hat { \\sigma } _ { X } ^ { 2 } + \\hat { \\sigma } _ { Y } ^ { 2 } - 2 \\hat { \\sigma } _ { X Y } } .$$  \nFigure 5.9 illustrates this approach for estimating α on a simulated data set. In each panel, we simulated 100 pairs of returns for the investments X and Y . We used these returns to estimate σ 2 X  σ 2 Y , and σ XY , which we then substituted into (5.7) in order to obtain estimates for α . The value of ˆ α resulting from each simulated data set ranges from 0   532 to 0   657 .  \nIt is natural to wish to quantify the accuracy of our estimate of α . To estimate the standard deviation of ˆ α , we repeated the process of simulating 100 paired observations of X and Y , and estimating α using (5.7), 1,000 times. We thereby obtained 1,000 estimates for α , which we can call ˆ α 1  ˆ α 2         ˆ α 1  000 . The left-hand panel of Figure 5.10 displays a histogram of the resulting estimates. For these simulations the parameters were set to σ 2 X = 1  σ 2 Y = 1   25 , and σ XY = 0   5 , and so we know that the true value of α is 0   6 . We indicated this value using a solid vertical line on the histogram.  \nbootstrap  \nFIGURE 5.9. Each panel displays 100 simulated returns for investments X and Y . From left to right and top to bottom, the resulting estimates for α are 0   576 , 0   532 , 0   657 , and 0   651 .  \n<!-- image -->  \nThe mean over all 1,000 estimates for α is  \n$$\\bar { \\alpha } = \\frac { 1 } { 1 0 0 0 } \\sum _ { r = 1 } ^ { 1 0 0 0 } \\hat { \\alpha } _ { r } = 0 . 5 9 9 6 , \\\\$$  \nvery close to α = 0   6 , and the standard deviation of the estimates is  \n$$\\sqrt { \\frac { 1 } { 1 0 0 0 - 1 } \\sum _ { r = 1 } ^ { 1 0 0 0 } ( \\hat { \\alpha } _ { r } - \\bar { \\alpha } ) ^ { 2 } } = 0 . 0 8 3 . \\\\ \\intertext { a v e r g o o d i e a f t h e a c c u r a c y f \\hat { \\alpha } \\colon S E ( \\hat { \\alpha } ) }$$  \nIn practice, however, the procedure for estimating SE(ˆ α ) outlined above cannot be applied, because for real data we cannot generate new samples from the original population. However, the bootstrap approach allows us to use a computer to emulate the process of obtaining new sample sets, so that we can estimate the variability of ˆ α without generating additional samples. Rather than repeatedly obtaining independent data sets from the population, we instead obtain distinct data sets by repeatedly sampling observations from the original data set .  \nThis gives us a very good idea of the accuracy of ˆ α : SE(ˆ α ) ≈ 0   083 . So roughly speaking, for a random sample from the population, we would expect ˆ α to differ from α by approximately 0   08 , on average.  \nThis approach is illustrated in Figure 5.11 on a simple data set, which we call Z , that contains only n = 3 observations. We randomly select n observations from the data set in order to produce a bootstrap data set,  \nFIGURE 5.10. Left: A histogram of the estimates of α obtained by generating 1,000 simulated data sets from the true population. Center: A histogram of the estimates of α obtained from 1,000 bootstrap samples from a single data set. Right: The estimates of α displayed in the left and center panels are shown as boxplots. In each panel, the pink line indicates the true value of α .  \n<!-- image -->  \nZ ∗ 1 . The sampling is performed with replacement , which means that the same observation can occur more than once in the bootstrap data set. In this example, Z ∗ 1 contains the third observation twice, the first observation once, and no instances of the second observation. Note that if an observation is contained in Z ∗ 1 , then both its X and Y values are included. We can use Z ∗ 1 to produce a new bootstrap estimate for α , which we call ˆ α ∗ 1 . This procedure is repeated B times for some large value of B , in order to produce B different bootstrap data sets, Z ∗ 1  Z ∗ 2         Z ∗ B , and B corresponding α estimates, ˆ α ∗ 1  ˆ α ∗ 2         ˆ α ∗ B . We can compute the standard error of these bootstrap estimates using the formula  \n$$\\text {strap estimates using the formula} \\\\ \\text {SE} _ { B } ( \\hat { \\alpha } ) = \\sqrt { \\frac { 1 } { B - 1 } \\sum _ { r = 1 } ^ { B } \\left ( \\hat { \\alpha } ^ { * r } - \\frac { 1 } { B } \\sum _ { r ^ { \\prime } = 1 } ^ { B } \\hat { \\alpha } ^ { * r ^ { \\prime } } \\right ) ^ { 2 } } \\, . \\\\ \\text {serves as an estimate of the standard error of } \\hat { \\alpha } \\text { estimated from the}$$  \nThe bootstrap approach is illustrated in the center panel of Figure 5.10, which displays a histogram of 1,000 bootstrap estimates of α , each computed using a distinct bootstrap data set. This panel was constructed on the basis of a single data set, and hence could be created using real data. Note that the histogram looks very similar to the left-hand panel, which displays the idealized histogram of the estimates of α obtained by generating 1,000 simulated data sets from the true population. In particular the bootstrap estimate SE(ˆ α ) from (5.8) is 0   087 , very close to the estimate of 0   083 obtained using 1,000 simulated data sets. The right-hand panel displays the information in the center and left panels in a different way, via boxplots of the estimates for α obtained by generating 1,000 simulated data sets from the true population and using the bootstrap approach. Again, the boxplots have similar spreads, indicating that the bootstrap approach can be used to effectively estimate the variability associated with ˆ α .  \nThis serves as an estimate of the standard error of ˆ α estimated from the original data set.  \nwith replacement  \nFIGURE 5.11. A graphical illustration of the bootstrap approach on a small sample containing n = 3 observations. Each bootstrap data set contains n observations, sampled with replacement from the original data set. Each bootstrap data set is used to obtain an estimate of α .  \n<!-- image -->', ""### 5.3.3 The Bootstrap  \nWe illustrate the use of the bootstrap in the simple example of Section 5.2, as well as on an example involving estimating the accuracy of the linear regression model on the Auto data set.\nEstimating the Accuracy of a Statistic of Interest  \nOne of the great advantages of the bootstrap approach is that it can be applied in almost all situations. No complicated mathematical calculations are required. While there are several implementations of the bootstrap in Python, its use for estimating standard error is simple enough that we write our own function below for the case when our data is stored in a dataframe.  \nTo illustrate the bootstrap, we start with a simple example. The Portfolio data set in the ISLP package is described in Section 5.2. The goal is to estimate the sampling variance of the parameter α given in formula (5.7). We will create a function alpha\\_func() , which takes as input a dataframe D assumed to have columns X and Y , as well as a vector idx indicating which observations should be used to estimate α . The function then outputs the estimate for α based on the selected observations.  \n```\nIn[15]: Portfolio = load_data('Portfolio') def alpha_func(D, idx): cov_ = np.cov(D[['X','Y']].loc[idx], rowvar=False) return ((cov_[1,1] -cov_[0,1]) / (cov_[0,0]+cov_[1,1]-2*cov_[0,1]))\n```  \nThis function returns an estimate for α based on applying the minimum variance formula (5.7) to the observations indexed by the argument idx . For instance, the following command estimates α using all 100 observations.  \n```\nIn[16]: alpha_func(Portfolio, range(100))\n```\nOut[16]: 0.5758  \nNext we randomly select 100 observations from range(100) , with replacement. This is equivalent to constructing a new bootstrap data set and recomputing ˆ α based on the new data set.  \n```\nIn[17]: rng = np.random.default_rng(0) alpha_func(Portfolio, rng.choice(100, 100, replace=True))\n```\nOut[17]: 0.6074  \nThis process can be generalized to create a simple function boot\\_SE() for computing the bootstrap standard error for arbitrary functions that take only a data frame as an argument.  \n```\nIn[18]: def boot_SE(func, D, n=None, B=1000, seed=0): rng = np.random.default_rng(seed) first_, second_ = 0, 0 n = n or D.shape[0] for _ in range(B): idx = rng.choice(D.index, n, replace=True) value = func(D, idx) first_ += value second_ += value**2 return np.sqrt(second_ / B -(first_ / B)**2)\n```  \nNotice the use of \\_ as a loop variable in for \\_ in range(B) . This is often used if the value of the counter is unimportant and simply makes sure the loop is executed B times.  \nLet's use our function to evaluate the accuracy of our estimate of α using B = 1  000 bootstrap replications.  \n```\nIn[19]: alpha_SE = boot_SE(alpha_func, Portfolio, B=1000, seed=0) alpha_SE\n```\nOut[19]: 0.0912  \nThe final output shows that the bootstrap estimate for SE(ˆ α ) is 0   0912 .\nEstimating the Accuracy of a Linear Regression Model  \nThe bootstrap approach can be used to assess the variability of the coefficient estimates and predictions from a statistical learning method. Here we use the bootstrap approach in order to assess the variability of the  \nestimates for β 0 and β 1 , the intercept and slope terms for the linear regression model that uses horsepower to predict mpg in the Auto data set. We will compare the estimates obtained using the bootstrap to those obtained using the formulas for SE( ˆ β 0 ) and SE( ˆ β 1 ) described in Section 3.1.2.  \nWe start by writing a generic function boot\\_OLS() for bootstrapping a regression model that takes a formula to define the corresponding regression. We use the clone() function to make a copy of the formula that can be refit to the new dataframe. This means that any derived features such as those defined by poly() (which we will see shortly), will be re-fit on the resampled data frame.  \nTo use our boot\\_SE() function, we must write a function (its first argument) that takes a data frame D and indices idx as its only arguments. But here we want to bootstrap a specific regression model, specified by a model formula and data. We show how to do this in a few simple steps.  \n```\nIn[20]: def boot_OLS(model_matrix, response, D, idx): D_ = D.loc[idx] Y_ = D_[response] X_ = clone(model_matrix).fit_transform(D_) return sm.OLS(Y_, X_).fit().params\n```  \nThis is not quite what is needed as the first argument to boot\\_SE() . The first two arguments which specify the model will not change in the bootstrap process, and we would like to freeze them. The function partial() from the functools module does precisely this: it takes a function as an argument, and freezes some of its arguments, starting from the left. We use it to freeze the first two model-formula arguments of boot\\_OLS() .  \n```\nIn[21]: hp_func = partial(boot_OLS, MS(['horsepower']), 'mpg')\n```  \nTyping hp\\_func? will show that it has two arguments D and idx - it is a version of boot\\_OLS() with the first two arguments frozen - and hence is ideal as the first argument for boot\\_SE() .  \nThe hp\\_func() function can now be used in order to create bootstrap estimates for the intercept and slope terms by randomly sampling from among the observations with replacement. We first demonstrate its utility on 10 bootstrap samples.  \n```\nIn[22]: rng = np.random.default_rng(0) np.array([hp_func(Auto, rng.choice(392, 392, replace=True)) for _ in range(10)])\n```  \n```\nOut[22]: array([[39.8806, -0.1568], [38.733 , -0.147 ], [38.3173, -0.1444], [39.9145, -0.1578], [39.4335, -0.1507], [40.3663, -0.1591], [39.6233, -0.1545], [39.0581, -0.1495], [38.6669, -0.1452], [39.6428, -0.1556]])\n```  \nclone()  \npartial()  \nNext, we use the boot\\_SE() function to compute the standard errors of 1,000 bootstrap estimates for the intercept and slope terms.  \n```\nIn[23]: hp_se = boot_SE(hp_func, Auto, B=1000, seed=10) hp_se\n```  \n```\nOut[23]: intercept 0.8488 horsepower 0.0074 dtype: float64\n```  \nThis indicates that the bootstrap estimate for SE( ˆ β 0 ) is 0.85, and that the bootstrap estimate for SE( ˆ β 1 ) is 0.0074. As discussed in Section 3.1.2, standard formulas can be used to compute the standard errors for the regression coefficients in a linear model. These can be obtained using the summarize() function from ISLP.sm .  \n```\nIn[24]: hp_model.fit(Auto, Auto['mpg']) model_se = summarize(hp_model.results_)['std err'] model_se\n```  \n```\nOut[24]: intercept 0.717 horsepower 0.006 Name: std err, dtype: float64\n```  \nThe standard error estimates for ˆ β 0 and ˆ β 1 obtained using the formulas from Section 3.1.2 are 0.717 for the intercept and 0.006 for the slope. Interestingly, these are somewhat different from the estimates obtained using the bootstrap. Does this indicate a problem with the bootstrap? In fact, it suggests the opposite. Recall that the standard formulas given in Equation 3.8 on page 75 rely on certain assumptions. For example, they depend on the unknown parameter σ 2 , the noise variance. We then estimate σ 2 using the RSS. Now although the formula for the standard errors do not rely on the linear model being correct, the estimate for σ 2 does. We see in Figure 3.8 on page 99 that there is a non-linear relationship in the data, and so the residuals from a linear fit will be inflated, and so will ˆ σ 2 . Secondly, the standard formulas assume (somewhat unrealistically) that the x i are fixed, and all the variability comes from the variation in the errors /epsilon1 i . The bootstrap approach does not rely on any of these assumptions, and so it is likely giving a more accurate estimate of the standard errors of ˆ β 0 and ˆ β 1 than the results from sm.OLS .  \nBelow we compute the bootstrap standard error estimates and the standard linear regression estimates that result from fitting the quadratic model to the data. Since this model provides a good fit to the data (Figure 3.8), there is now a better correspondence between the bootstrap estimates and the standard estimates of SE( ˆ β 0 ) , SE( ˆ β 1 ) and SE( ˆ β 2 ) .  \n```\nIn[25]: quad_model = MS([poly('horsepower', 2, raw=True)]) quad_func = partial(boot_OLS, quad_model, 'mpg') boot_SE(quad_func, Auto, B=1000)\n```  \n```\nOut[25]: intercept 2.067840 poly(horsepower, 2, raw=True)[0] 0.033019 poly(horsepower, 2, raw=True)[1] 0.000120 dtype: float64 We compare the results to the standard errors computed using sm.OLS() . In[26]: M = sm.OLS(Auto['mpg'], quad_model.fit_transform(Auto)) summarize(M.fit())['std err'] Out[26]: intercept 1.800\n```  \n```\npoly(horsepower, 2, raw=True)[0] 0.031 poly(horsepower, 2, raw=True)[1] 0.000 Name: std err, dtype: float64\n```"", '### 8.2.1 Bagging  \nThe bootstrap, introduced in Chapter 5, is an extremely powerful idea. It is used in many situations in which it is hard or even impossible to directly compute the standard deviation of a quantity of interest. We see here that the bootstrap can be used in a completely different context, in order to improve statistical learning methods such as decision trees.  \nRecall that given a set of n independent observations Z 1         Z n , each with variance σ 2 , the variance of the mean ¯ Z of the observations is given by σ 2 glyph[triangleleft]n . In other words, averaging a set of observations reduces variance . Hence a natural way to reduce the variance and increase the test set accuracy of a statistical learning method is to take many training sets from the population, build a separate prediction model using each training set, and average the resulting predictions. In other words, we could calculate ˆ f 1 ( x )  ˆ f 2 ( x )         ˆ f B ( x ) using B separate training sets, and average them in order to obtain a single low-variance statistical learning model, given by  \nThe decision trees discussed in Section 8.1 suffer from high variance . This means that if we split the training data into two parts at random, and fit a decision tree to both halves, the results that we get could be quite different. In contrast, a procedure with low variance will yield similar results if applied repeatedly to distinct data sets; linear regression tends to have low variance, if the ratio of n to p is moderately large. Bootstrap aggregation , or bagging , is a general-purpose procedure for reducing the variance of a statistical learning method; we introduce it here because it is particularly useful and frequently used in the context of decision trees.  \n$$\\hat { f } _ { a v g } ( x ) = \\frac { 1 } { B } \\sum _ { b = 1 } ^ { B } \\hat { f } ^ { b } ( x ) . \\\\$$  \nOf course, this is not practical because we generally do not have access to multiple training sets. Instead, we can bootstrap, by taking repeated samples from the (single) training data set. In this approach we generate B different bootstrapped training data sets. We then train our method on the b th bootstrapped training set in order to get ˆ f ∗ b ( x ) , and finally average all the predictions, to obtain  \n$$\\hat { f } _ { b a g } ( x ) = \\frac { 1 } { B } \\sum _ { b = 1 } ^ { B } \\hat { f } ^ { * b } ( x ) .$$  \nensemble weak learners  \nbagging  \nFIGURE 8.8. Bagging and random forest results for the Heart data. The test error (black and orange) is shown as a function of B , the number of bootstrapped training sets used. Random forests were applied with m = √ p . The dashed line indicates the test error resulting from a single classification tree. The green and blue traces show the OOB error, which in this case is - by chance - considerably lower.  \n<!-- image -->\nThis is called bagging.  \nThus far, we have described the bagging procedure in the regression context, to predict a quantitative outcome Y . How can bagging be extended to a classification problem where Y is qualitative? In that situation, there are a few possible approaches, but the simplest is as follows. For a given test observation, we can record the class predicted by each of the B trees, and take a majority vote : the overall prediction is the most commonly occurring class among the B predictions.  \nWhile bagging can improve predictions for many regression methods, it is particularly useful for decision trees. To apply bagging to regression trees, we simply construct B regression trees using B bootstrapped training sets, and average the resulting predictions. These trees are grown deep, and are not pruned. Hence each individual tree has high variance, but low bias. Averaging these B trees reduces the variance. Bagging has been demonstrated to give impressive improvements in accuracy by combining together hundreds or even thousands of trees into a single procedure.  \nFigure 8.8 shows the results from bagging trees on the Heart data. The test error rate is shown as a function of B , the number of trees constructed using bootstrapped training data sets. We see that the bagging test error rate is slightly lower in this case than the test error rate obtained from a single tree. The number of trees B is not a critical parameter with bagging; using a very large value of B will not lead to overfitting. In practice we majority vote  \nuse a value of B sufficiently large that the error has settled down. Using B = 100 is sufficient to achieve good performance in this example.\nOut-of-Bag Error Estimation  \nIt turns out that there is a very straightforward way to estimate the test error of a bagged model, without the need to perform cross-validation or the validation set approach. Recall that the key to bagging is that trees are repeatedly fit to bootstrapped subsets of the observations. One can show that on average, each bagged tree makes use of around two-thirds of the observations. 3 The remaining one-third of the observations not used to fit a given bagged tree are referred to as the out-of-bag (OOB) observations. We can predict the response for the i th observation using each of the trees in which that observation was OOB. This will yield around Bglyph[triangleleft] 3 predictions for the i th observation. In order to obtain a single prediction for the i th observation, we can average these predicted responses (if regression is the goal) or can take a majority vote (if classification is the goal). This leads to a single OOB prediction for the i th observation. An OOB prediction can be obtained in this way for each of the n observations, from which the overall OOB MSE (for a regression problem) or classification error (for a classification problem) can be computed. The resulting OOB error is a valid estimate of the test error for the bagged model, since the response for each observation is predicted using only the trees that were not fit using that observation. Figure 8.8 displays the OOB error on the Heart data. It can be shown that with B sufficiently large, OOB error is virtually equivalent to leave-one-out cross-validation error. The OOB approach for estimating the test error is particularly convenient when performing bagging on large data sets for which cross-validation would be computationally onerous.\nVariable Importance Measures  \nAs we have discussed, bagging typically results in improved accuracy over prediction using a single tree. Unfortunately, however, it can be difficult to interpret the resulting model. Recall that one of the advantages of decision trees is the attractive and easily interpreted diagram that results, such as the one displayed in Figure 8.1. However, when we bag a large number of trees, it is no longer possible to represent the resulting statistical learning procedure using a single tree, and it is no longer clear which variables are most important to the procedure. Thus, bagging improves prediction accuracy at the expense of interpretability.  \nAlthough the collection of bagged trees is much more difficult to interpret than a single tree, one can obtain an overall summary of the importance of each predictor using the RSS (for bagging regression trees) or the Gini index (for bagging classification trees). In the case of bagging regression trees, we can record the total amount that the RSS (8.1) is decreased due to splits over a given predictor, averaged over all B trees. A large value indicates an important predictor. Similarly, in the context of bagging classification  \n3 This relates to Exercise 2 of Chapter 5.  \nout-of-bag  \nFIGURE 8.9. A variable importance plot for the Heart data. Variable importance is computed using the mean decrease in Gini index, and expressed relative to the maximum.  \n<!-- image -->  \ntrees, we can add up the total amount that the Gini index (8.6) is decreased by splits over a given predictor, averaged over all B trees.  \nA graphical representation of the variable importances in the Heart data is shown in Figure 8.9. We see the mean decrease in Gini index for each variable, relative to the largest. The variables with the largest mean decrease in Gini index are Thal , Ca , and ChestPain .']","El 'bootstrap' es una herramienta estadística poderosa y ampliamente aplicable que se utiliza para cuantificar la incertidumbre asociada con un estimador o método de aprendizaje estadístico. Su principal ventaja es que puede aplicarse fácilmente a una amplia gama de métodos de aprendizaje estadístico, incluyendo aquellos para los cuales es difícil obtener una medida de variabilidad y que no son proporcionados automáticamente por el software estadístico.

Por ejemplo, el bootstrap puede usarse para estimar los errores estándar de los coeficientes de una regresión lineal. Aunque en el caso específico de la regresión lineal esto no es particularmente útil, ya que el software estadístico estándar suele proporcionar estos errores estándar automáticamente, el verdadero poder del bootstrap radica en su aplicabilidad a métodos más complejos.

El bootstrap permite emular el proceso de obtener nuevos conjuntos de muestras mediante el uso de un ordenador, sin necesidad de generar muestras adicionales del conjunto de datos original. Esto se logra mediante el muestreo con reemplazo de las observaciones del conjunto de datos original para crear múltiples conjuntos de datos bootstrap, lo que permite estimar la variabilidad de un estimador sin necesidad de obtener nuevos datos.

En resumen, el bootstrap se utiliza para estimar la precisión de un estimador, como la desviación estándar de un parámetro, y es especialmente útil en situaciones donde no se pueden generar nuevas muestras del conjunto de datos original.",El bootstrap es una herramienta estadística que consiste en tomar muestras repetidas con reemplazo de un conjunto de datos para obtener información adicional sobre un modelo ajustado. Se utiliza comúnmente para estimar la variabilidad de un estimador de parámetros o de un método de aprendizaje estadístico.,1.0,,0.9999999999666667,1.0
What are two common metrics used to assess the accuracy of a linear regression model?,"[""### 2.2.1 Measuring the Quality of Fit  \nIn order to evaluate the performance of a statistical learning method on a given data set, we need some way to measure how well its predictions actually match the observed data. That is, we need to quantify the extent to which the predicted response value for a given observation is close to the true response value for that observation. In the regression setting, the most commonly-used measure is the mean squared error (MSE), given by  \n$$M S E = \\frac { 1 } { n } \\sum _ { i = 1 } ^ { n } ( y _ { i } - \\hat { f } ( x _ { i } ) ) ^ { 2 } , \\quad \\ \\ ( 2 . 5 ) ^ { \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\$$  \nwhere ˆ f ( x i ) is the prediction that ˆ f gives for the i th observation. The MSE will be small if the predicted responses are very close to the true responses, and will be large if for some of the observations, the predicted and true responses differ substantially.  \nTo state it more mathematically, suppose that we fit our statistical learning method on our training observations ¶ ( x 1  y 1 )  ( x 2  y 2 )         ( x n  y n ) ♦ , and we obtain the estimate ˆ f . We can then compute ˆ f ( x 1 )  ˆ f ( x 2 )         ˆ f ( x n ) .  \nThe MSE in (2.5) is computed using the training data that was used to fit the model, and so should more accurately be referred to as the training MSE . But in general, we do not really care how well the method works on the training data. Rather, we are interested in the accuracy of the predictions that we obtain when we apply our method to previously unseen test data . Why is this what we care about? Suppose that we are interested in developing an algorithm to predict a stock's price based on previous stock returns. We can train the method using stock returns from the past 6 months. But we don't really care how well our method predicts last week's stock price. We instead care about how well it will predict tomorrow's price or next month's price. On a similar note, suppose that we have clinical measurements (e.g. weight, blood pressure, height, age, family history of disease) for a number of patients, as well as information about whether each patient has diabetes. We can use these patients to train a statistical learning method to predict risk of diabetes based on clinical measurements. In practice, we want this method to accurately predict diabetes risk for future patients based on their clinical measurements. We are not very interested in whether or not the method accurately predicts diabetes risk for patients used to train the model, since we already know which of those patients have diabetes.  \nmean squared error training MSE  \ntest data  \nFIGURE 2.9. Left: Data simulated from f , shown in black. Three estimates of f are shown: the linear regression line (orange curve), and two smoothing spline fits (blue and green curves). Right: Training MSE (grey curve), test MSE (red curve), and minimum possible test MSE over all methods (dashed line). Squares represent the training and test MSEs for the three fits shown in the left-hand panel.  \n<!-- image -->  \nIf these are approximately equal to y 1  y 2         y n , then the training MSE given by (2.5) is small. However, we are really not interested in whether ˆ f ( x i ) ≈ y i ; instead, we want to know whether ˆ f ( x 0 ) is approximately equal to y 0 , where ( x 0  y 0 ) is a previously unseen test observation not used to train the statistical learning method . We want to choose the method that gives the lowest test MSE , as opposed to the lowest training MSE. In other words, if we had a large number of test observations, we could compute  \n$$\\ A v e ( y _ { 0 } - \\hat { f } ( x _ { 0 } ) ) ^ { 2 } , \\\\ \\quad \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\$$  \nHow can we go about trying to select a method that minimizes the test MSE? In some settings, we may have a test data set available-that is, we may have access to a set of observations that were not used to train the statistical learning method. We can then simply evaluate (2.6) on the test observations, and select the learning method for which the test MSE is smallest. But what if no test observations are available? In that case, one might imagine simply selecting a statistical learning method that minimizes the training MSE (2.5). This seems like it might be a sensible approach, since the training MSE and the test MSE appear to be closely related. Unfortunately, there is a fundamental problem with this strategy: there is no guarantee that the method with the lowest training MSE will also have the lowest test MSE. Roughly speaking, the problem is that many statistical methods specifically estimate coefficients so as to minimize the training set MSE. For these methods, the training set MSE can be quite small, but the test MSE is often much larger.  \nthe average squared prediction error for these test observations ( x 0  y 0 ) . We'd like to select the model for which this quantity is as small as possible.  \nFigure 2.9 illustrates this phenomenon on a simple example. In the lefthand panel of Figure 2.9, we have generated observations from (2.1) with test MSE  \nthe true f given by the black curve. The orange, blue and green curves illustrate three possible estimates for f obtained using methods with increasing levels of flexibility. The orange line is the linear regression fit, which is relatively inflexible. The blue and green curves were produced using smoothing splines , discussed in Chapter 7, with different levels of smoothness. It is clear that as the level of flexibility increases, the curves fit the observed data more closely. The green curve is the most flexible and matches the data very well; however, we observe that it fits the true f (shown in black) poorly because it is too wiggly. By adjusting the level of flexibility of the smoothing spline fit, we can produce many different fits to this data.  \nIn this example, we know the true function f , and so we can also compute the test MSE over a very large test set, as a function of flexibility. (Of course, in general f is unknown, so this will not be possible.) The test MSE is displayed using the red curve in the right-hand panel of Figure 2.9. As with the training MSE, the test MSE initially declines as the level of flexibility increases. However, at some point the test MSE levels off and then starts to increase again. Consequently, the orange and green curves both have high test MSE. The blue curve minimizes the test MSE, which should not be surprising given that visually it appears to estimate f the best in the left-hand panel of Figure 2.9. The horizontal dashed line indicates Var ( /epsilon1 ) , the irreducible error in (2.3), which corresponds to the lowest achievable test MSE among all possible methods. Hence, the smoothing spline represented by the blue curve is close to optimal.  \nWe now move on to the right-hand panel of Figure 2.9. The grey curve displays the average training MSE as a function of flexibility, or more formally the degrees of freedom , for a number of smoothing splines. The degrees of freedom is a quantity that summarizes the flexibility of a curve; it is discussed more fully in Chapter 7. The orange, blue and green squares indicate the MSEs associated with the corresponding curves in the lefthand panel. A more restricted and hence smoother curve has fewer degrees of freedom than a wiggly curve-note that in Figure 2.9, linear regression is at the most restrictive end, with two degrees of freedom. The training MSE declines monotonically as flexibility increases. In this example the true f is non-linear, and so the orange linear fit is not flexible enough to estimate f well. The green curve has the lowest training MSE of all three methods, since it corresponds to the most flexible of the three curves fit in the left-hand panel.  \nIn the right-hand panel of Figure 2.9, as the flexibility of the statistical learning method increases, we observe a monotone decrease in the training MSE and a U-shape in the test MSE. This is a fundamental property of statistical learning that holds regardless of the particular data set at hand and regardless of the statistical method being used. As model flexibility increases, the training MSE will decrease, but the test MSE may not. When a given method yields a small training MSE but a large test MSE, we are said to be overfitting the data. This happens because our statistical learning procedure is working too hard to find patterns in the training data, and may be picking up some patterns that are just caused by random chance rather than by true properties of the unknown function f . When we overfit the training data, the test MSE will be very large because the supposed smoothing spline  \ndegrees of freedom  \nFIGURE 2.10. Details are as in Figure 2.9, using a different true f that is much closer to linear. In this setting, linear regression provides a very good fit to the data.  \n<!-- image -->  \npatterns that the method found in the training data simply don't exist in the test data. Note that regardless of whether or not overfitting has occurred, we almost always expect the training MSE to be smaller than the test MSE because most statistical learning methods either directly or indirectly seek to minimize the training MSE. Overfitting refers specifically to the case in which a less flexible model would have yielded a smaller test MSE.  \nIn practice, one can usually compute the training MSE with relative ease, but estimating the test MSE is considerably more difficult because usually no test data are available. As the previous three examples illustrate, the flexibility level corresponding to the model with the minimal test MSE can vary considerably among data sets. Throughout this book, we discuss a variety of approaches that can be used in practice to estimate this minimum point. One important method is cross-validation (Chapter 5), which is a method for estimating the test MSE using the training data.  \nFigure 2.10 provides another example in which the true f is approximately linear. Again we observe that the training MSE decreases monotonically as the model flexibility increases, and that there is a U-shape in the test MSE. However, because the truth is close to linear, the test MSE only decreases slightly before increasing again, so that the orange least squares fit is substantially better than the highly flexible green curve. Finally, Figure 2.11 displays an example in which f is highly non-linear. The training and test MSE curves still exhibit the same general patterns, but now there is a rapid decrease in both curves before the test MSE starts to increase slowly."", ""### 3.1.3 Assessing the Accuracy of the Model  \nOnce we have rejected the null hypothesis (3.12) in favor of the alternative hypothesis (3.13), it is natural to want to quantify the extent to which the model fits the data . The quality of a linear regression fit is typically assessed using two related quantities: the residual standard error (RSE) and the R 2 statistic.  \n4 In Table 3.1, a small p -value for the intercept indicates that we can reject the null hypothesis that β 0 = 0 , and a small p -value for TV indicates that we can reject the null hypothesis that β 1 = 0 . Rejecting the latter null hypothesis allows us to conclude that there is a relationship between TV and sales . Rejecting the former allows us to conclude that in the absence of TV expenditure, sales are non-zero.  \np -value  \nR 2  \nTABLE 3.2. For the Advertising data, more information about the least squares model for the regression of number of units sold on TV advertising budget.  \n| Quantity                |   Value |\n|-------------------------|---------|\n| Residual standard error |   3.26  |\n| R 2                     |   0.612 |\n| F -statistic            | 312.1   |  \nTable 3.2 displays the RSE, the R 2 statistic, and the F -statistic (to be described in Section 3.2.2) for the linear regression of number of units sold on TV advertising budget.\nResidual Standard Error  \nRecall from the model (3.5) that associated with each observation is an error term /epsilon1 . Due to the presence of these error terms, even if we knew the true regression line (i.e. even if β 0 and β 1 were known), we would not be able to perfectly predict Y from X . The RSE is an estimate of the standard deviation of /epsilon1 . Roughly speaking, it is the average amount that the response will deviate from the true regression line. It is computed using the formula  \n$$\\text {deactivate from the true regression line. It is computed using the formula} \\\\ R S E = \\sqrt { \\frac { 1 } { n - 2 } } R S S = \\sqrt { \\frac { 1 } { n - 2 } } \\sum _ { i = 1 } ^ { n } ( y _ { i } - \\hat { y } _ { i } ) ^ { 2 } . \\\\ \\text {that RSS was defined in Section 3.1.1, and is given by the formula}$$  \nNote that RSS was defined in Section 3.1.1, and is given by the formula  \n$$R S S = \\sum _ { i = 1 } ^ { n } ( y _ { i } - \\hat { y } _ { i } ) ^ { 2 } . \\\\$$  \nIn the case of the advertising data, we see from the linear regression output in Table 3.2 that the RSE is 3   26 . In other words, actual sales in each market deviate from the true regression line by approximately 3  260 units, on average. Another way to think about this is that even if the model were correct and the true values of the unknown coefficients β 0 and β 1 were known exactly, any prediction of sales on the basis of TV advertising would still be off by about 3  260 units on average. Of course, whether or not 3  260 units is an acceptable prediction error depends on the problem context. In the advertising data set, the mean value of sales over all markets is approximately 14  000 units, and so the percentage error is 3  260 glyph[triangleleft] 14  000 = 23 %.  \nThe RSE is considered a measure of the lack of fit of the model (3.5) to the data. If the predictions obtained using the model are very close to the true outcome values-that is, if ˆ y i ≈ y i for i = 1         n -then (3.15) will be small, and we can conclude that the model fits the data very well. On the other hand, if ˆ y i is very far from y i for one or more observations, then the RSE may be quite large, indicating that the model doesn't fit the data well.\nR 2 Statistic  \nThe RSE provides an absolute measure of lack of fit of the model (3.5) to the data. But since it is measured in the units of Y , it is not always  \nclear what constitutes a good RSE. The R 2 statistic provides an alternative measure of fit. It takes the form of a proportion -the proportion of variance explained-and so it always takes on a value between 0 and 1 , and is independent of the scale of Y .  \nTo calculate R 2 , we use the formula  \n$$R ^ { 2 } = \\frac { \\text {TSSS} - \\text {RSS} } { \\text {TSSS} } = 1 - \\frac { \\text {RSS} } { \\text {TSSS} }$$  \nwhere TSS = ∑ ( y i -¯ y ) 2 is the total sum of squares , and RSS is defined in (3.16). TSS measures the total variance in the response Y , and can be thought of as the amount of variability inherent in the response before the regression is performed. In contrast, RSS measures the amount of variability that is left unexplained after performing the regression. Hence, TSS -RSS measures the amount of variability in the response that is explained (or removed) by performing the regression, and R 2 measures the proportion of variability in Y that can be explained using X . An R 2 statistic that is close to 1 indicates that a large proportion of the variability in the response is explained by the regression. A number near 0 indicates that the regression does not explain much of the variability in the response; this might occur because the linear model is wrong, or the error variance σ 2 is high, or both. In Table 3.2, the R 2 was 0   61 , and so just under two-thirds of the variability in sales is explained by a linear regression on TV .  \nThe R 2 statistic is a measure of the linear relationship between X and Y . Recall that correlation , defined as  \nThe R 2 statistic (3.17) has an interpretational advantage over the RSE (3.15), since unlike the RSE, it always lies between 0 and 1. However, it can still be challenging to determine what is a good R 2 value, and in general, this will depend on the application. For instance, in certain problems in physics, we may know that the data truly comes from a linear model with a small residual error. In this case, we would expect to see an R 2 value that is extremely close to 1 , and a substantially smaller R 2 value might indicate a serious problem with the experiment in which the data were generated. On the other hand, in typical applications in biology, psychology, marketing, and other domains, the linear model (3.5) is at best an extremely rough approximation to the data, and residual errors due to other unmeasured factors are often very large. In this setting, we would expect only a very small proportion of the variance in the response to be explained by the predictor, and an R 2 value well below 0   1 might be more realistic!  \n$$\\text {cor} ( X , Y ) = \\frac { \\sum _ { i = 1 } ^ { n } ( x _ { i } - \\bar { x } ) ( y _ { i } - \\bar { y } ) } { \\sqrt { \\sum _ { i = 1 } ^ { n } ( x _ { i } - \\bar { x } ) ^ { 2 } } \\sqrt { \\sum _ { i = 1 } ^ { n } ( y _ { i } - \\bar { y } ) ^ { 2 } } } , \\\\ \\text {also a measure of the linear relationship between } X \\text { and } Y . 5 \\text { This sur-}$$  \nis also a measure of the linear relationship between X and Y . 5 This suggests that we might be able to use r = Cor ( XY ) instead of R 2 in order to assess the fit of the linear model. In fact, it can be shown that in the simple  \n5 We note that in fact, the right-hand side of (3.18) is the sample correlation; thus, it would be more correct to write ̂ Cor ( XY ) ; however, we omit the 'hat' for ease of notation.  \ntotal sum of squares correlation  \nSimple regression of sales on radio  \n|           |   Coefficient |   Std. error |   t -statistic | p -value                      |\n|-----------|---------------|--------------|----------------|-------------------------------|\n| Intercept |         9.312 |        0.563 |          16.54 | < 0   0001 |\n| radio     |         0.203 |        0.02  |           9.92 | < 0   0001 |\nSimple regression of sales on newspaper  \nTABLE 3.3. More simple linear regression models for the Advertising data. Coefficients of the simple linear regression model for number of units sold on Top: radio advertising budget and Bottom: newspaper advertising budget. A $ 1  000 increase in spending on radio advertising is associated with an average increase in sales by around 203 units, while the same increase in spending on newspaper advertising is associated with an average increase in sales by around 55 units. (Note that the sales variable is in thousands of units, and the radio and newspaper variables are in thousands of dollars.)  \n|           |   Coefficient |   Std. error |   t -statistic | p -value                      |\n|-----------|---------------|--------------|----------------|-------------------------------|\n| Intercept |        12.351 |        0.621 |          19.88 | < 0   0001 |\n| newspaper |         0.055 |        0.017 |           3.3  | 0   00115  |  \nlinear regression setting, R 2 = r 2 . In other words, the squared correlation and the R 2 statistic are identical. However, in the next section we will discuss the multiple linear regression problem, in which we use several predictors simultaneously to predict the response. The concept of correlation between the predictors and the response does not extend automatically to this setting, since correlation quantifies the association between a single pair of variables rather than between a larger number of variables. We will see that R 2 fills this role."", ""### 3.1.2 Assessing the Accuracy of the Coefficient Estimates  \nRecall from (2.1) that we assume that the true relationship between X and Y takes the form Y = f ( X ) + /epsilon1 for some unknown function f , where /epsilon1 is a mean-zero random error term. If f is to be approximated by a linear function, then we can write this relationship as  \n$$Y = \\beta _ { 0 } + \\beta _ { 1 } X + \\epsilon .$$  \nHere β 0 is the intercept term-that is, the expected value of Y when X = 0 , and β 1 is the slope-the average increase in Y associated with a one-unit increase in X . The error term is a catch-all for what we miss with this simple model: the true relationship is probably not linear, there may be other variables that cause variation in Y , and there may be measurement error. We typically assume that the error term is independent of X .  \nFIGURE 3.2. Contour and three-dimensional plots of the RSS on the Advertising data, using sales as the response and TV as the predictor. The red dots correspond to the least squares estimates ˆ β 0 and ˆ β 1 , given by (3.4).  \n<!-- image -->  \nThe model given by (3.5) defines the population regression line , which is the best linear approximation to the true relationship between X and Y . 1 The least squares regression coefficient estimates (3.4) characterize the least squares line (3.2). The left-hand panel of Figure 3.3 displays these two lines in a simple simulated example. We created 100 random X s, and generated 100 corresponding Y s from the model  \n$$Y = 2 + 3 X + \\epsilon ,$$  \nwhere /epsilon1 was generated from a normal distribution with mean zero. The red line in the left-hand panel of Figure 3.3 displays the true relationship, f ( X ) = 2 + 3 X , while the blue line is the least squares estimate based on the observed data. The true relationship is generally not known for real data, but the least squares line can always be computed using the coefficient estimates given in (3.4). In other words, in real applications, we have access to a set of observations from which we can compute the least squares line; however, the population regression line is unobserved. In the right-hand panel of Figure 3.3 we have generated ten different data sets from the model given by (3.6) and plotted the corresponding ten least squares lines. Notice that different data sets generated from the same true model result in slightly different least squares lines, but the unobserved population regression line does not change.  \nAt first glance, the difference between the population regression line and the least squares line may seem subtle and confusing. We only have one data set, and so what does it mean that two different lines describe the relationship between the predictor and the response? Fundamentally, the concept of these two lines is a natural extension of the standard statistical approach of using information from a sample to estimate characteristics of a large population. For example, suppose that we are interested in knowing  \n1 The assumption of linearity is often a useful working model. However, despite what many textbooks might tell us, we seldom believe that the true relationship is linear.  \npopulation regression line least squares line  \nFIGURE 3.3. A simulated data set. Left: The red line represents the true relationship, f ( X ) = 2 + 3 X , which is known as the population regression line. The blue line is the least squares line; it is the least squares estimate for f ( X ) based on the observed data, shown in black. Right: The population regression line is again shown in red, and the least squares line in dark blue. In light blue, ten least squares lines are shown, each computed on the basis of a separate random set of observations. Each least squares line is different, but on average, the least squares lines are quite close to the population regression line.  \n<!-- image -->  \nthe population mean θ of some random variable Y . Unfortunately, θ is unknown, but we do have access to n observations from Y , y 1         y n , which we can use to estimate θ . A reasonable estimate is ˆ θ = ¯ y , where ¯ y = 1 n ∑ n i =1 y i is the sample mean. The sample mean and the population mean are different, but in general the sample mean will provide a good estimate of the population mean. In the same way, the unknown coefficients β 0 and β 1 in linear regression define the population regression line. We seek to estimate these unknown coefficients using ˆ β 0 and ˆ β 1 given in (3.4). These coefficient estimates define the least squares line.  \nThe analogy between linear regression and estimation of the mean of a random variable is an apt one based on the concept of bias . If we use the sample mean ˆ θ to estimate θ , this estimate is unbiased , in the sense that on average, we expect ˆ θ to equal θ . What exactly does this mean? It means that on the basis of one particular set of observations y 1         y n , ˆ θ might overestimate θ , and on the basis of another set of observations, ˆ θ might underestimate θ . But if we could average a huge number of estimates of θ obtained from a huge number of sets of observations, then this average would exactly equal θ . Hence, an unbiased estimator does not systematically over- or under-estimate the true parameter. The property of unbiasedness holds for the least squares coefficient estimates given by (3.4) as well: if we estimate β 0 and β 1 on the basis of a particular data set, then our estimates won't be exactly equal to β 0 and β 1 . But if we could average the estimates obtained over a huge number of data sets, then the average of these estimates would be spot on! In fact, we can see from the righthand panel of Figure 3.3 that the average of many least squares lines, each bias unbiased  \nestimated from a separate data set, is pretty close to the true population regression line.  \nWe continue the analogy with the estimation of the population mean θ of a random variable Y . A natural question is as follows: how accurate is the sample mean ˆ θ as an estimate of θ ? We have established that the average of ˆ θ 's over many data sets will be very close to θ , but that a single estimate ˆ θ may be a substantial underestimate or overestimate of θ . How far off will that single estimate of ˆ θ be? In general, we answer this question by computing the standard error of ˆ θ , written as SE(ˆ θ ) . We have the well-known formula  \n$$V a r ( \\hat { \\mu } ) = \\text {SE} ( \\hat { \\mu } ) ^ { 2 } = \\frac { \\sigma ^ { 2 } } { n } ,$$  \nwhere σ is the standard deviation of each of the realizations y i of Y . 2 Roughly speaking, the standard error tells us the average amount that this estimate ˆ θ differs from the actual value of θ . Equation 3.7 also tells us how this deviation shrinks with n -the more observations we have, the smaller the standard error of ˆ θ . In a similar vein, we can wonder how close ˆ β 0 and ˆ β 1 are to the true values β 0 and β 1 . To compute the standard errors associated with ˆ β 0 and ˆ β 1 , we use the following formulas:  \n$$\\intertext { s u c h s c r { E } ( \\hat { \\beta } _ { 0 } ) ^ { 2 } = \\sigma ^ { 2 } \\left [ \\frac { 1 } { n } + \\frac { \\bar { x } ^ { 2 } } { \\sum _ { i = 1 } ^ { n } ( x _ { i } - \\bar { x } ) ^ { 2 } } \\right ] , \\ \\, \\text {SE} ( \\hat { \\beta } _ { 1 } ) ^ { 2 } = \\frac { \\sigma ^ { 2 } } { \\sum _ { i = 1 } ^ { n } ( x _ { i } - \\bar { x } ) ^ { 2 } } , \\ ( 3 . 8 ) } \\\\ \\text {where } \\sigma ^ { 2 } = \\text {Var} ( \\epsilon ) . \\text { For these formulas to be strictly valid, we need to }$$  \nStandard errors can be used to compute confidence intervals . A 95% confidence interval is defined as a range of values such that with 95 % probability, the range will contain the true unknown value of the parameter. The range is defined in terms of lower and upper limits computed from the sample of data. A 95% confidence interval has the following property: if we take repeated samples and construct the confidence interval for each sample, 95% of the intervals will contain the true unknown value of the parameter. For linear regression, the 95 % confidence interval for β 1 approximately takes the form where σ 2 = Var ( /epsilon1 ) . For these formulas to be strictly valid, we need to assume that the errors /epsilon1 i for each observation have common variance σ 2 and are uncorrelated. This is clearly not true in Figure 3.1, but the formula still turns out to be a good approximation. Notice in the formula that SE( ˆ β 1 ) is smaller when the x i are more spread out; intuitively we have more leverage to estimate a slope when this is the case. We also see that SE( ˆ β 0 ) would be the same as SE(ˆ θ ) if ¯ x were zero (in which case ˆ β 0 would be equal to ¯ y ). In general, σ 2 is not known, but can be estimated from the data. This estimate of σ is known as the residual standard error , and is given by the formula RSE = √ RSS glyph[triangleleft] ( n -2) . Strictly speaking, when σ 2 is estimated from the data we should write ̂ SE( ˆ β 1 ) to indicate that an estimate has been made, but for simplicity of notation we will drop this extra 'hat'.  \n$$\\hat { \\beta } _ { 1 } \\pm 2 \\cdot \\text {SE} ( \\hat { \\beta } _ { 1 } ) .$$  \n2 This formula holds provided that the n observations are uncorrelated.  \nstandard error residual standard error  \nconfidence interval  \nThat is, there is approximately a 95 % chance that the interval  \n$$\\left [ \\hat { \\beta } _ { 1 } - 2 \\cdot \\text {SE} ( \\hat { \\beta } _ { 1 } ) , \\, \\hat { \\beta } _ { 1 } + 2 \\cdot \\text {SE} ( \\hat { \\beta } _ { 1 } ) \\right ] \\\\ \\\\ \\left [ \\hat { \\beta } _ { 1 } - 2 \\cdot \\text {SE} ( \\hat { \\beta } _ { 1 } ) , \\, \\hat { \\beta } _ { 1 } + 2 \\cdot \\text {SE} ( \\hat { \\beta } _ { 1 } ) \\right ] \\\\$$  \nwill contain the true value of β 1 . 3 Similarly, a confidence interval for β 0 approximately takes the form  \n$$\\hat { \\beta } _ { 0 } \\pm 2 \\cdot \\text {SE} ( \\hat { \\beta } _ { 0 } ) .$$  \nIn the case of the advertising data, the 95 % confidence interval for β 0 is [6   130  7   935] and the 95 % confidence interval for β 1 is [0   042  0   053] . Therefore, we can conclude that in the absence of any advertising, sales will, on average, fall somewhere between 6  130 and 7  935 units. Furthermore, for each $1  000 increase in television advertising, there will be an average increase in sales of between 42 and 53 units.  \nStandard errors can also be used to perform hypothesis tests on the coefficients. The most common hypothesis test involves testing the null hypothesis of  \n$$H _ { 0 } \\colon \\text {There is no relationship between } X \\text { and } Y$$  \nversus the alternative hypothesis  \n$$H _ { a } \\colon \\text {There is some relationship between } X \\text { and } Y .$$  \nMathematically, this corresponds to testing  \n$$H _ { 0 } \\colon \\beta _ { 1 } = 0$$  \n$$H _ { a } \\colon \\beta _ { 1 } \\neq 0 ,$$  \nsince if β 1 = 0 then the model (3.5) reduces to Y = β 0 + /epsilon1 , and X is not associated with Y . To test the null hypothesis, we need to determine whether ˆ β 1 , our estimate for β 1 , is sufficiently far from zero that we can be confident that β 1 is non-zero. How far is far enough? This of course depends on the accuracy of ˆ β 1 -that is, it depends on SE( ˆ β 1 ) . If SE( ˆ β 1 ) is small, then even relatively small values of ˆ β 1 may provide strong evidence that β 1 = 0 , and hence that there is a relationship between X and Y . In contrast, if SE( ˆ β 1 ) is large, then ˆ β 1 must be large in absolute value in order for us to reject the null hypothesis. In practice, we compute a t -statistic , given by  \n/negationslash  \n$$t = \\frac { \\hat { \\beta } _ { 1 } - 0 } { \\text {SE} ( \\hat { \\beta } _ { 1 } ) } ,$$  \n3 Approximately for several reasons. Equation 3.10 relies on the assumption that the errors are Gaussian. Also, the factor of 2 in front of the SE( ˆ β 1 ) term will vary slightly depending on the number of observations n in the linear regression. To be precise, rather than the number 2, (3.10) should contain the 97.5 % quantile of a t -distribution with n -2 degrees of freedom. Details of how to compute the 95 % confidence interval precisely in R will be provided later in this chapter.  \nversus  \n/negationslash hypothesis test  \nnull hypothesis alternative hypothesis  \nt -statistic  \nTABLE 3.1. For the Advertising data, coefficients of the least squares model for the regression of number of units sold on TV advertising budget. An increase of $1  000 in the TV advertising budget is associated with an increase in sales by around 50 units. (Recall that the sales variable is in thousands of units, and the TV variable is in thousands of dollars.)  \n|           |   Coefficient |   Std. error |   t -statistic | p -value                      |\n|-----------|---------------|--------------|----------------|-------------------------------|\n| Intercept |        7.0325 |       0.4578 |          15.36 | < 0   0001 |\n| TV        |        0.0475 |       0.0027 |          17.67 | < 0   0001 |  \nwhich measures the number of standard deviations that ˆ β 1 is away from 0 . If there really is no relationship between X and Y , then we expect that (3.14) will have a t -distribution with n -2 degrees of freedom. The t -distribution has a bell shape and for values of n greater than approximately 30 it is quite similar to the standard normal distribution. Consequently, it is a simple matter to compute the probability of observing any number equal to ♣ t ♣ or larger in absolute value, assuming β 1 = 0 . We call this probability the p -value . Roughly speaking, we interpret the p -value as follows: a small p -value indicates that it is unlikely to observe such a substantial association between the predictor and the response due to chance, in the absence of any real association between the predictor and the response. Hence, if we see a small p -value, then we can infer that there is an association between the predictor and the response. We reject the null hypothesis -that is, we declare a relationship to exist between X and Y -if the p -value is small enough. Typical p -value cutoffs for rejecting the null hypothesis are 5% or 1%, although this topic will be explored in much greater detail in Chapter 13. When n = 30 , these correspond to t -statistics (3.14) of around 2 and 2.75, respectively.  \n/negationslash  \nTable 3.1 provides details of the least squares model for the regression of number of units sold on TV advertising budget for the Advertising data. Notice that the coefficients for ˆ β 0 and ˆ β 1 are very large relative to their standard errors, so the t -statistics are also large; the probabilities of seeing such values if H 0 is true are virtually zero. Hence we can conclude that β 0 = 0 and β 1 = 0 . 4  \n/negationslash"", '### 2.1.2 How Do We Estimate f ?  \nThroughout this book, we explore many linear and non-linear approaches for estimating f . However, these methods generally share certain characteristics. We provide an overview of these shared characteristics in this section. We will always assume that we have observed a set of n different data points. For example in Figure 2.2 we observed n = 30 data points. These observations are called the training data because we will use these observations to train, or teach, our method how to estimate f . Let x ij represent the value of the j th predictor, or input, for observation i , where i = 1  2         n and j = 1  2         p . Correspondingly, let y i represent the response variable for the i th observation. Then our training data consist of ¶ ( x 1  y 1 )  ( x 2  y 2 )         ( x n  y n ) ♦ where x i = ( x i 1  x i 2         x ip ) T .  \nOur goal is to apply a statistical learning method to the training data in order to estimate the unknown function f . In other words, we want to find a function ˆ f such that Y ≈ ˆ f ( X ) for any observation ( XY ) . Broadly speaking, most statistical learning methods for this task can be characterized as either parametric or non-parametric . We now briefly discuss these two types of approaches.\nParametric Methods  \nParametric methods involve a two-step model-based approach.  \n1. First, we make an assumption about the functional form, or shape, of f . For example, one very simple assumption is that f is linear in X :  \nThis is a linear model , which will be discussed extensively in Chapter 3. Once we have assumed that f is linear, the problem of estimating f is greatly simplified. Instead of having to estimate an entirely arbitrary p -dimensional function f ( X ) , one only needs to estimate the p +1 coefficients β 0  β 1         β p .  \n$$f ( X ) = \\beta _ { 0 } + \\beta _ { 1 } X _ { 1 } + \\beta _ { 2 } X _ { 2 } + \\cdots + \\beta _ { p } X _ { p } . \\\\ \\vdots \\quad l \\vdots \\quad l _ { 1 } \\quad l _ { 2 } \\quad l _ { 1 } \\quad l _ { 2 } \\quad l \\vdots \\quad l _ { 1 } \\quad l _ { 2 } \\quad l _ { 1 } \\quad l _ { 2 }$$  \nlinear model training data  \nparametric nonparametric  \nFIGURE 2.4. A linear model fit by least squares to the Income data from Figure 2.3. The observations are shown in red, and the yellow plane indicates the least squares fit to the data.  \n<!-- image -->  \n2. After a model has been selected, we need a procedure that uses the training data to fit or train the model. In the case of the linear model (2.4), we need to estimate the parameters β 0  β 1         β p . That is, we want to find values of these parameters such that  \n$$Y \\approx \\beta _ { 0 } + \\beta _ { 1 } X _ { 1 } + \\beta _ { 2 } X _ { 2 } + \\dots + \\beta _ { p } X _ { p } .$$  \nThe most common approach to fitting the model (2.4) is referred to as (ordinary) least squares , which we discuss in Chapter 3. However, least squares is one of many possible ways to fit the linear model. In Chapter 6, we discuss other approaches for estimating the parameters in (2.4).  \nThe model-based approach just described is referred to as parametric ; it reduces the problem of estimating f down to one of estimating a set of parameters. Assuming a parametric form for f simplifies the problem of estimating f because it is generally much easier to estimate a set of parameters, such as β 0  β 1         β p in the linear model (2.4), than it is to fit an entirely arbitrary function f . The potential disadvantage of a parametric approach is that the model we choose will usually not match the true unknown form of f . If the chosen model is too far from the true f , then our estimate will be poor. We can try to address this problem by choosing flexible models that can fit many different possible functional forms for f . But in general, fitting a more flexible model requires estimating a greater number of parameters. These more complex models can lead to a phenomenon known as overfitting the data, which essentially means they follow the errors, or noise , too closely. These issues are discussed throughout this book.  \nFigure 2.4 shows an example of the parametric approach applied to the Income data from Figure 2.3. We have fit a linear model of the form  \n$$\\text {income} \\approx \\beta _ { 0 } + \\beta _ { 1 } \\times \\text {education} + \\beta _ { 2 } \\times \\text {sensitivity} .$$  \nfit train least squares  \nflexible overfitting noise  \nFIGURE 2.5. A smooth thin-plate spline fit to the Income data from Figure 2.3 is shown in yellow; the observations are displayed in red. Splines are discussed in Chapter 7.  \n<!-- image -->  \nSince we have assumed a linear relationship between the response and the two predictors, the entire fitting problem reduces to estimating β 0 , β 1 , and β 2 , which we do using least squares linear regression. Comparing Figure 2.3 to Figure 2.4, we can see that the linear fit given in Figure 2.4 is not quite right: the true f has some curvature that is not captured in the linear fit. However, the linear fit still appears to do a reasonable job of capturing the positive relationship between years of education and income , as well as the slightly less positive relationship between seniority and income . It may be that with such a small number of observations, this is the best we can do.\nNon-Parametric Methods  \nNon-parametric methods do not make explicit assumptions about the functional form of f . Instead they seek an estimate of f that gets as close to the data points as possible without being too rough or wiggly. Such approaches can have a major advantage over parametric approaches: by avoiding the assumption of a particular functional form for f , they have the potential to accurately fit a wider range of possible shapes for f . Any parametric approach brings with it the possibility that the functional form used to estimate f is very different from the true f , in which case the resulting model will not fit the data well. In contrast, non-parametric approaches completely avoid this danger, since essentially no assumption about the form of f is made. But non-parametric approaches do suffer from a major disadvantage: since they do not reduce the problem of estimating f to a small number of parameters, a very large number of observations (far more than is typically needed for a parametric approach) is required in order to obtain an accurate estimate for f .  \nAn example of a non-parametric approach to fitting the Income data is shown in Figure 2.5. A thin-plate spline is used to estimate f . This approach does not impose any pre-specified model on f . It instead attempts thin-plate spline  \nFIGURE 2.6. A rough thin-plate spline fit to the Income data from Figure 2.3. This fit makes zero errors on the training data.  \n<!-- image -->  \nto produce an estimate for f that is as close as possible to the observed data, subject to the fit-that is, the yellow surface in Figure 2.5-being smooth . In this case, the non-parametric fit has produced a remarkably accurate estimate of the true f shown in Figure 2.3. In order to fit a thin-plate spline, the data analyst must select a level of smoothness. Figure 2.6 shows the same thin-plate spline fit using a lower level of smoothness, allowing for a rougher fit. The resulting estimate fits the observed data perfectly! However, the spline fit shown in Figure 2.6 is far more variable than the true function f , from Figure 2.3. This is an example of overfitting the data, which we discussed previously. It is an undesirable situation because the fit obtained will not yield accurate estimates of the response on new observations that were not part of the original training data set. We discuss methods for choosing the correct amount of smoothness in Chapter 5. Splines are discussed in Chapter 7.  \nAs we have seen, there are advantages and disadvantages to parametric and non-parametric methods for statistical learning. We explore both types of methods throughout this book.']","Two common metrics used to assess the accuracy of a linear regression model are the **Residual Standard Error (RSE)** and the **R² statistic**.

1. **Residual Standard Error (RSE):**  
   The RSE is an estimate of the standard deviation of the error term in the regression model. It provides a measure of the average amount that the response will deviate from the true regression line. A smaller RSE indicates a better fit of the model to the data.

2. **R² Statistic:**  
   The R² statistic measures the proportion of variance in the response variable that is explained by the predictor variables in the model. It takes a value between 0 and 1, where a value closer to 1 indicates that a large proportion of the variability in the response is explained by the model, suggesting a good fit.","Two common metrics are the Residual Standard Error (RSE), which represents the average deviation of the response from the true regression line, and the R-squared (R²) statistic, which measures the proportion of variability in the response that can be explained by the predictors.",1.0,,0.5833333333041666,1.0
What is the interpretation of an interaction term in a multiple linear regression model?,"[""## 3.4 The Marketing Plan  \nWe now briefly return to the seven questions about the Advertising data that we set out to answer at the beginning of this chapter.  \n1. Is there a relationship between sales and advertising budget? This question can be answered by fitting a multiple regression model of sales onto TV , radio , and newspaper , as in (3.20), and testing the hypothesis H 0 : β TV = β radio = β newspaper = 0 . In Section 3.2.2, we showed that the F -statistic can be used to determine whether or not we should reject this null hypothesis. In this case the p -value corresponding to the F -statistic in Table 3.6 is very low, indicating clear evidence of a relationship between advertising and sales.\n2. How strong is the relationship?  \nWe discussed two measures of model accuracy in Section 3.1.3. First, the RSE estimates the standard deviation of the response from the population regression line. For the Advertising data, the RSE is 1   69 units while the mean value for the response is 14   022 , indicating a percentage error of roughly 12 %. Second, the R 2 statistic records the percentage of variability in the response that is explained by the predictors. The predictors explain almost 90 % of the variance in sales . The RSE and R 2 statistics are displayed in Table 3.6.  \n3. Which media are associated with sales?\n2. To answer this question, we can examine the p -values associated with each predictor's t -statistic (Section 3.1.2). In the multiple linear regression displayed in Table 3.4, the p -values for TV and radio are low, but the p -value for newspaper is not. This suggests that only TV and radio are related to sales . In Chapter 6 we explore this question in greater detail.  \n4. How large is the association between each medium and sales? We saw in Section 3.1.2 that the standard error of ˆ β j can be used to construct confidence intervals for β j . For the Advertising data, we can use the results in Table 3.4 to compute the 95 % confidence intervals for the coefficients in a multiple regression model using all three media budgets as predictors. The confidence intervals are as follows: (0   043  0   049) for TV , (0   172  0   206) for radio , and ( -0   013  0   011) for newspaper . The confidence intervals for TV and radio are narrow and far from zero, providing evidence that these media are related to sales . But the interval for newspaper includes zero, indicating that the variable is not statistically significant given the values of TV and  \nradio .  \nWe saw in Section 3.3.3 that collinearity can result in very wide standard errors. Could collinearity be the reason that the confidence interval associated with newspaper is so wide? The VIF scores are 1   005 , 1   145 , and 1   145 for TV , radio , and newspaper , suggesting no evidence of collinearity.  \nIn order to assess the association of each medium individually on sales, we can perform three separate simple linear regressions. Results are shown in Tables 3.1 and 3.3. There is evidence of an extremely strong association between TV and sales and between radio and sales . There is evidence of a mild association between newspaper and sales , when the values of TV and radio are ignored.  \n5. How accurately can we predict future sales?\n2. The response can be predicted using (3.21). The accuracy associated with this estimate depends on whether we wish to predict an individual response, Y = f ( X ) + /epsilon1 , or the average response, f ( X ) (Section 3.2.2). If the former, we use a prediction interval, and if the latter, we use a confidence interval. Prediction intervals will always be wider than confidence intervals because they account for the uncertainty associated with /epsilon1 , the irreducible error.\n6. Is the relationship linear?\n4. In Section 3.3.3, we saw that residual plots can be used in order to identify non-linearity. If the relationships are linear, then the residual plots should display no pattern. In the case of the Advertising data, we observe a non-linear effect in Figure 3.5, though this effect could also be observed in a residual plot. In Section 3.3.2, we discussed the inclusion of transformations of the predictors in the linear regression model in order to accommodate non-linear relationships.\n7. Is there synergy among the advertising media?\n6. The standard linear regression model assumes an additive relationship between the predictors and the response. An additive model is easy to interpret because the association between each predictor and the response is unrelated to the values of the other predictors. However, the additive assumption may be unrealistic for certain data sets. In Section 3.3.2, we showed how to include an interaction term  \nin the regression model in order to accommodate non-additive relationships. A small p -value associated with the interaction term indicates the presence of such relationships. Figure 3.5 suggested that the Advertising data may not be additive. Including an interaction term in the model results in a substantial increase in R 2 , from around 90 % to almost 97 %."", '### 3.3.2 Extensions of the Linear Model  \nThe standard linear regression model (3.19) provides interpretable results and works quite well on many real-world problems. However, it makes several highly restrictive assumptions that are often violated in practice. Two of the most important assumptions state that the relationship between the predictors and response are additive and linear . The additivity assumption means that the association between a predictor X j and the response Y does not depend on the values of the other predictors. The linearity assumption states that the change in the response Y associated with a one-unit change in X j is constant, regardless of the value of X j . In later chapters of this book, we examine a number of sophisticated methods that relax these two  \n12 There could still in theory be a difference between South and West, although the data here does not suggest any difference.  \ncontrast additive linear  \nassumptions. Here, we briefly examine some common classical approaches for extending the linear model.\nRemoving the Additive Assumption  \nIn our previous analysis of the Advertising data, we concluded that both TV and radio seem to be associated with sales . The linear models that formed the basis for this conclusion assumed that the effect on sales of increasing one advertising medium is independent of the amount spent on the other media. For example, the linear model (3.20) states that the average increase in sales associated with a one-unit increase in TV is always β 1 , regardless of the amount spent on radio .  \nConsider the standard linear regression model with two variables,  \nHowever, this simple model may be incorrect. Suppose that spending money on radio advertising actually increases the effectiveness of TV advertising, so that the slope term for TV should increase as radio increases. In this situation, given a fixed budget of $100  000 , spending half on radio and half on TV may increase sales more than allocating the entire amount to either TV or to radio . In marketing, this is known as a synergy effect, and in statistics it is referred to as an interaction effect. Figure 3.5 suggests that such an effect may be present in the advertising data. Notice that when levels of either TV or radio are low, then the true sales are lower than predicted by the linear model. But when advertising is split between the two media, then the model tends to underestimate sales .  \n$$Y = \\beta _ { 0 } + \\beta _ { 1 } X _ { 1 } + \\beta _ { 2 } X _ { 2 } + \\epsilon .$$  \nAccording to this model, a one-unit increase in X 1 is associated with an average increase in Y of β 1 units. Notice that the presence of X 2 does not alter this statement-that is, regardless of the value of X 2 , a oneunit increase in X 1 is associated with a β 1 -unit increase in Y . One way of extending this model is to include a third predictor, called an interaction term , which is constructed by computing the product of X 1 and X 2 . This results in the model  \n$$Y = \\beta _ { 0 } + \\beta _ { 1 } X _ { 1 } + \\beta _ { 2 } X _ { 2 } + \\beta _ { 3 } X _ { 1 } X _ { 2 } + \\epsilon .$$  \nHow does inclusion of this interaction term relax the additive assumption? Notice that (3.31) can be rewritten as  \n$$\\begin{array} { r c l } Y & = & \\beta _ { 0 } + ( \\beta _ { 1 } + \\beta _ { 3 } X _ { 2 } ) X _ { 1 } + \\beta _ { 2 } X _ { 2 } + \\epsilon \\\\ & = & \\beta _ { 0 } + \\tilde { \\beta } _ { 1 } X _ { 1 } + \\beta _ { 2 } X _ { 2 } + \\epsilon \\end{array}$$  \nwhere ˜ β 1 = β 1 + β 3 X 2 . Since ˜ β 1 is now a function of X 2 , the association between X 1 and Y is no longer constant: a change in the value of X 2 will change the association between X 1 and Y . A similar argument shows that a change in the value of X 1 changes the association between X 2 and Y .  \nFor example, suppose that we are interested in studying the productivity of a factory. We wish to predict the number of units produced on the basis of the number of production lines and the total number of workers . It seems likely that the effect of increasing the number of production lines  \nTABLE 3.9. For the Advertising data, least squares coefficient estimates associated with the regression of sales onto TV and radio , with an interaction term, as in (3.33).  \n|           |   Coefficient |   Std. error |   t -statistic | p -value                      |\n|-----------|---------------|--------------|----------------|-------------------------------|\n| Intercept |        6.7502 |        0.248 |          27.23 | < 0   0001 |\n| TV        |        0.0191 |        0.002 |          12.7  | < 0   0001 |\n| radio     |        0.0289 |        0.009 |           3.24 | 0.0014                        |\n| TV radio  |        0.0011 |        0     |          20.73 | < 0   0001 |  \n×  \nwill depend on the number of workers, since if no workers are available to operate the lines, then increasing the number of lines will not increase production. This suggests that it would be appropriate to include an interaction term between lines and workers in a linear model to predict units . Suppose that when we fit the model, we obtain  \n$$\\units \\ \\approx \\ 1 . 2 + 3 . 4 \\times \\ln e s + 0 . 2 2 \\times \\text {workers} + 1 . 4 \\times ( \\ln e s \\times \\text {workers} ) \\\\ = \\quad 1 . 2 + ( 3 . 4 + 1 . 4 \\times \\text {workers} ) \\times \\ln e s + 0 . 2 2 \\times \\text {workers} . \\\\$$  \nIn other words, adding an additional line will increase the number of units produced by 3   4 + 1   4 × workers . Hence the more workers we have, the stronger will be the effect of lines .  \nWe now return to the Advertising example. A linear model that uses radio , TV , and an interaction between the two to predict sales takes the form  \n$$\\begin{array} { r l } { \\ s a l e s } & { = } & { \\beta _ { 0 } + \\beta _ { 1 } \\times \\mathbf V + \\beta _ { 2 } \\times r a i d o + \\beta _ { 3 } \\times ( r a i d o \\times \\mathbf V ) + \\epsilon } \\\\ & { = } & { \\beta _ { 0 } + ( \\beta _ { 1 } + \\beta _ { 3 } \\times r a i d o ) \\times \\mathbf V + \\beta _ { 2 } \\times r a i d o + \\epsilon . } \\end{array}$$  \nWe can interpret β 3 as the increase in the effectiveness of TV advertising associated with a one-unit increase in radio advertising (or vice-versa). The coefficients that result from fitting the model (3.33) are given in Table 3.9.  \n/negationslash  \nIn this example, the p -values associated with TV , radio , and the interaction term all are statistically significant (Table 3.9), and so it is obvious that all three variables should be included in the model. However, it is sometimes the case that an interaction term has a very small p -value, but the associated main effects (in this case, TV and radio ) do not. The hierarchical principle states that if we include an interaction in a model, we  \nThe results in Table 3.9 strongly suggest that the model that includes the interaction term is superior to the model that contains only main effects . The p -value for the interaction term, TV × radio , is extremely low, indicating that there is strong evidence for H a : β 3 = 0 . In other words, it is clear that the true relationship is not additive. The R 2 for the model (3.33) is 96.8 %, compared to only 89.7 % for the model that predicts sales using TV and radio without an interaction term. This means that (96   8 -89   7) glyph[triangleleft] (100 -89   7) = 69 % of the variability in sales that remains after fitting the additive model has been explained by the interaction term. The coefficient estimates in Table 3.9 suggest that an increase in TV advertising of $1  000 is associated with increased sales of ( ˆ β 1 + ˆ β 3 × radio ) × 1  000 = 19+1   1 × radio units. And an increase in radio advertising of $1  000 will be associated with an increase in sales of ( ˆ β 2 + ˆ β 3 × TV ) × 1  000 = 29 + 1   1 × TV units.  \nmain effect  \nshould also include the main effects, even if the p -values associated with their coefficients are not significant. In other words, if the interaction between X 1 and X 2 seems important, then we should include both X 1 and X 2 in the model even if their coefficient estimates have large p -values. The rationale for this principle is that if X 1 × X 2 is related to the response, then whether or not the coefficients of X 1 or X 2 are exactly zero is of little interest. Also X 1 × X 2 is typically correlated with X 1 and X 2 , and so leaving them out tends to alter the meaning of the interaction.  \nIn the previous example, we considered an interaction between TV and radio , both of which are quantitative variables. However, the concept of interactions applies just as well to qualitative variables, or to a combination of quantitative and qualitative variables. In fact, an interaction between a qualitative variable and a quantitative variable has a particularly nice interpretation. Consider the Credit data set from Section 3.3.1, and suppose that we wish to predict balance using the income (quantitative) and student (qualitative) variables. In the absence of an interaction term, the model takes the form  \n$$\\text {takes the form} \\\\ \\text {balance} _ { i } \\ \\approx \\ \\beta _ { 0 } + \\beta _ { 1 } \\times \\text {income} _ { i } + \\begin{cases} \\beta _ { 2 } & \\text {if ith person is a student} \\\\ 0 & \\text {if ith person is not a student} \\end{cases} \\\\ = \\ \\beta _ { 1 } \\times \\text {income} _ { i } + \\begin{cases} \\beta _ { 0 } + \\beta _ { 2 } & \\text {if ith person is a student} \\\\ \\beta _ { 0 } & \\text {if ith person is not a student.} \\end{cases} \\\\$$  \nNotice that this amounts to fitting two parallel lines to the data, one for students and one for non-students. The lines for students and non-students have different intercepts, β 0 + β 2 versus β 0 , but the same slope, β 1 . This is illustrated in the left-hand panel of Figure 3.7. The fact that the lines are parallel means that the average effect on balance of a one-unit increase in income does not depend on whether or not the individual is a student. This represents a potentially serious limitation of the model, since in fact a change in income may have a very different effect on the credit card balance of a student versus a non-student.  \nThis limitation can be addressed by adding an interaction variable, created by multiplying income with the dummy variable for student . Our model now becomes  \n$$\\begin{array} { r l } { \\text {model now becomes} } \\\\ \\\\ { \\text {balance} _ { i } } & { \\approx \\beta _ { 0 } + \\beta _ { 1 } \\times \\text {income} _ { i } + \\left \\{ 0 \\right \\} } & { \\text {if student} } \\\\ & { \\quad \\left \\{ ( \\beta _ { 0 } + \\beta _ { 2 } ) + ( \\beta _ { 1 } + \\beta _ { 3 } ) \\times \\text {income} _ { i } \\right \\} \\text { if student} } \\\\ & { = } & { \\beta _ { 0 } + \\beta _ { 1 } \\times \\text {income} _ { i } } & { \\quad \\text {if not student.} } \\\\ & { \\quad } & & { ( 3 . 3 5 ) } \\\\ \\\\ { \\text {Once again, we have two different regression lines for the students and} } \\end{array}$$  \nOnce again, we have two different regression lines for the students and the non-students. But now those regression lines have different intercepts, β 0 + β 2 versus β 0 , as well as different slopes, β 1 + β 3 versus β 1 . This allows for the possibility that changes in income may affect the credit card balances of students and non-students differently. The right-hand panel of Figure 3.7  \nFIGURE 3.7. For the Credit data, the least squares lines are shown for prediction of balance from income for students and non-students. Left: The model (3.34) was fit. There is no interaction between income and student . Right: The model (3.35) was fit. There is an interaction term between income and student .  \n<!-- image -->  \nshows the estimated relationships between income and balance for students and non-students in the model (3.35). We note that the slope for students is lower than the slope for non-students. This suggests that increases in income are associated with smaller increases in credit card balance among students as compared to non-students.\nNon-linear Relationships  \nAs discussed previously, the linear regression model (3.19) assumes a linear relationship between the response and predictors. But in some cases, the true relationship between the response and the predictors may be nonlinear. Here we present a very simple way to directly extend the linear model to accommodate non-linear relationships, using polynomial regression . In later chapters, we will present more complex approaches for performing non-linear fits in more general settings.  \nConsider Figure 3.8, in which the mpg (gas mileage in miles per gallon) versus horsepower is shown for a number of cars in the Auto data set. The orange line represents the linear regression fit. There is a pronounced relationship between mpg and horsepower , but it seems clear that this relationship is in fact non-linear: the data suggest a curved relationship. A simple approach for incorporating non-linear associations in a linear model is to include transformed versions of the predictors. For example, the points in Figure 3.8 seem to have a quadratic shape, suggesting that a model of the form  \n$$\\mathbb { m } { P } g = \\beta _ { 0 } + \\beta _ { 1 } \\times \\text {horsepower} + \\beta _ { 2 } \\times \\text {horsepower} ^ { 2 } + \\epsilon$$  \nmay provide a better fit. Equation 3.36 involves predicting mpg using a non-linear function of horsepower . But it is still a linear model! That is, (3.36) is simply a multiple linear regression model with X 1 = horsepower and X 2 = horsepower 2 . So we can use standard linear regression software to estimate β 0  β 1 , and β 2 in order to produce a non-linear fit. The blue curve in Figure 3.8 shows the resulting quadratic fit to the data. The quadratic polynomial regression  \nquadratic  \nFIGURE 3.8. The Auto data set. For a number of cars, mpg and horsepower are shown. The linear regression fit is shown in orange. The linear regression fit for a model that includes horsepower 2 is shown as a blue curve. The linear regression fit for a model that includes all polynomials of horsepower up to fifth-degree is shown in green.  \n<!-- image -->  \nTABLE 3.10. For the Auto data set, least squares coefficient estimates associated with the regression of mpg onto horsepower and horsepower 2 .  \n|              | Coefficient   |   Std. error | t -statistic   | p -value                      |\n|--------------|---------------|--------------|----------------|-------------------------------|\n| Intercept    | 56.9001       |       1.8004 | 31.6           | < 0   0001 |\n| horsepower   | - 0.4662      |       0.0311 | - 15.0         | < 0   0001 |\n| horsepower 2 | 0.0012        |       0.0001 | 10.1           | < 0   0001 |  \nfit appears to be substantially better than the fit obtained when just the linear term is included. The R 2 of the quadratic fit is 0   688 , compared to 0   606 for the linear fit, and the p -value in Table 3.10 for the quadratic term is highly significant.  \nThe approach that we have just described for extending the linear model to accommodate non-linear relationships is known as polynomial regression , since we have included polynomial functions of the predictors in the regression model. We further explore this approach and other non-linear extensions of the linear model in Chapter 7.  \nIf including horsepower 2 led to such a big improvement in the model, why not include horsepower 3 , horsepower 4 , or even horsepower 5 ? The green curve in Figure 3.8 displays the fit that results from including all polynomials up to fifth degree in the model (3.36). The resulting fit seems unnecessarily wiggly-that is, it is unclear that including the additional terms really has led to a better fit to the data.']","In a multiple linear regression model, an interaction term is used to capture the effect of two predictors interacting with each other on the response variable. The inclusion of an interaction term allows the effect of one predictor on the response to depend on the level of another predictor. 

For example, consider a model with two predictors, \(X_1\) and \(X_2\), and an interaction term \(X_1 \times X_2\). The model can be written as:

\[ Y = \beta_0 + \beta_1 X_1 + \beta_2 X_2 + \beta_3 (X_1 \times X_2) + \epsilon \]

Here, \(\beta_3\) is the coefficient of the interaction term. The presence of this term means that the effect of \(X_1\) on \(Y\) is not constant but depends on the value of \(X_2\), and vice versa. Specifically, the effect of a one-unit increase in \(X_1\) on \(Y\) is \(\beta_1 + \beta_3 X_2\), which varies with \(X_2\). Similarly, the effect of a one-unit increase in \(X_2\) on \(Y\) is \(\beta_2 + \beta_3 X_1\), which varies with \(X_1\).

The interaction term thus relaxes the additive assumption of the standard linear model, allowing for a more flexible relationship between the predictors and the response. A small p-value associated with the interaction term indicates that the interaction effect is statistically significant, suggesting that the relationship between the predictors and the response is not purely additive.","An interaction term, such as Xj * Xk, in a multiple linear regression model suggests that the effect of one predictor on the response variable depends on the level of another predictor. This is also known as a synergy effect.",1.0,,0.99999999995,1.0
"How can you detect collinearity in a multiple regression model, and what are two ways to address it?","[""### 3.3.3 Potential Problems  \nWhen we fit a linear regression model to a particular data set, many problems may occur. Most common among these are the following:  \n1. Non-linearity of the response-predictor relationships.\n2. Correlation of error terms.\n3. Non-constant variance of error terms.\n4. Outliers.\n5. High-leverage points.\n6. Collinearity.  \nIn practice, identifying and overcoming these problems is as much an art as a science. Many pages in countless books have been written on this topic. Since the linear regression model is not our primary focus here, we will provide only a brief summary of some key points.\n1. Non-linearity of the Data  \nFIGURE 3.9. Plots of residuals versus predicted (or fitted) values for the Auto data set. In each plot, the red line is a smooth fit to the residuals, intended to make it easier to identify a trend. Left: A linear regression of mpg on horsepower . A strong pattern in the residuals indicates non-linearity in the data. Right: A linear regression of mpg on horsepower and horsepower 2 . There is little pattern in the residuals.  \n<!-- image -->  \nThe linear regression model assumes that there is a straight-line relationship between the predictors and the response. If the true relationship is far from linear, then virtually all of the conclusions that we draw from the fit are suspect. In addition, the prediction accuracy of the model can be significantly reduced.  \nResidual plots are a useful graphical tool for identifying non-linearity. Given a simple linear regression model, we can plot the residuals, e i =  \ny i -ˆ y i , versus the predictor x i . In the case of a multiple regression model, since there are multiple predictors, we instead plot the residuals versus the predicted (or fitted ) values ˆ y i . Ideally, the residual plot will show no discernible pattern. The presence of a pattern may indicate a problem with some aspect of the linear model.  \nIf the residual plot indicates that there are non-linear associations in the data, then a simple approach is to use non-linear transformations of the predictors, such as log X , √ X , and X 2 , in the regression model. In the later chapters of this book, we will discuss other more advanced non-linear approaches for addressing this issue.  \nThe left panel of Figure 3.9 displays a residual plot from the linear regression of mpg onto horsepower on the Auto data set that was illustrated in Figure 3.8. The red line is a smooth fit to the residuals, which is displayed in order to make it easier to identify any trends. The residuals exhibit a clear U-shape, which provides a strong indication of non-linearity in the data. In contrast, the right-hand panel of Figure 3.9 displays the residual plot that results from the model (3.36), which contains a quadratic term. There appears to be little pattern in the residuals, suggesting that the quadratic term improves the fit to the data.\n2. Correlation of Error Terms  \nAn important assumption of the linear regression model is that the error terms, /epsilon1 1  /epsilon1 2         /epsilon1 n , are uncorrelated. What does this mean? For instance, if the errors are uncorrelated, then the fact that /epsilon1 i is positive provides little or no information about the sign of /epsilon1 i +1 . The standard errors that are computed for the estimated regression coefficients or the fitted values are based on the assumption of uncorrelated error terms. If in fact there is correlation among the error terms, then the estimated standard errors will tend to underestimate the true standard errors. As a result, confidence and prediction intervals will be narrower than they should be. For example, a 95 % confidence interval may in reality have a much lower probability than 0   95 of containing the true value of the parameter. In addition, p -values associated with the model will be lower than they should be; this could cause us to erroneously conclude that a parameter is statistically significant. In short, if the error terms are correlated, we may have an unwarranted sense of confidence in our model.  \nWhy might correlations among the error terms occur? Such correlations frequently occur in the context of time series data, which consists of observations for which measurements are obtained at discrete points in time. In many cases, observations that are obtained at adjacent time points will have positively correlated errors. In order to determine if this is the case for a given data set, we can plot the residuals from our model as a function of time. If the errors are uncorrelated, then there should be no discernible pat-  \nAs an extreme example, suppose we accidentally doubled our data, leading to observations and error terms identical in pairs. If we ignored this, our standard error calculations would be as if we had a sample of size 2 n , when in fact we have only n samples. Our estimated parameters would be the same for the 2 n samples as for the n samples, but the confidence intervals would be narrower by a factor of √ 2 !  \nfitted time series  \nFIGURE 3.10. Plots of residuals from simulated time series data sets generated with differing levels of correlation ρ between error terms for adjacent time points.  \n<!-- image -->  \ntern. On the other hand, if the error terms are positively correlated, then we may see tracking in the residuals-that is, adjacent residuals may have similar values. Figure 3.10 provides an illustration. In the top panel, we see the residuals from a linear regression fit to data generated with uncorrelated errors. There is no evidence of a time-related trend in the residuals. In contrast, the residuals in the bottom panel are from a data set in which adjacent errors had a correlation of 0   9 . Now there is a clear pattern in the residuals-adjacent residuals tend to take on similar values. Finally, the center panel illustrates a more moderate case in which the residuals had a correlation of 0   5 . There is still evidence of tracking, but the pattern is less clear.  \nMany methods have been developed to properly take account of correlations in the error terms in time series data. Correlation among the error terms can also occur outside of time series data. For instance, consider a study in which individuals' heights are predicted from their weights. The assumption of uncorrelated errors could be violated if some of the individuals in the study are members of the same family, eat the same diet, or have been exposed to the same environmental factors. In general, the assumption of uncorrelated errors is extremely important for linear regression as well as for other statistical methods, and good experimental design is crucial in order to mitigate the risk of such correlations.  \ntracking  \nFIGURE 3.11. Residual plots. In each plot, the red line is a smooth fit to the residuals, intended to make it easier to identify a trend. The blue lines track the outer quantiles of the residuals, and emphasize patterns. Left: The funnel shape indicates heteroscedasticity. Right: The response has been log transformed, and there is now no evidence of heteroscedasticity.  \n<!-- image -->\n3. Non-constant Variance of Error Terms  \nAnother important assumption of the linear regression model is that the error terms have a constant variance, Var ( /epsilon1 i ) = σ 2 . The standard errors, confidence intervals, and hypothesis tests associated with the linear model rely upon this assumption.  \nSometimes we have a good idea of the variance of each response. For example, the i th response could be an average of n i raw observations. If each of these raw observations is uncorrelated with variance σ 2 , then their average has variance σ 2 i = σ 2 glyph[triangleleft]n i . In this case a simple remedy is to fit our model by weighted least squares , with weights proportional to the inverse variances-i.e. w i = n i in this case. Most linear regression software allows for observation weights.  \nUnfortunately, it is often the case that the variances of the error terms are non-constant. For instance, the variances of the error terms may increase with the value of the response. One can identify non-constant variances in the errors, or heteroscedasticity , from the presence of a funnel shape in the residual plot. An example is shown in the left-hand panel of Figure 3.11, in which the magnitude of the residuals tends to increase with the fitted values. When faced with this problem, one possible solution is to transform the response Y using a concave function such as log Y or √ Y . Such a transformation results in a greater amount of shrinkage of the larger responses, leading to a reduction in heteroscedasticity. The right-hand panel of Figure 3.11 displays the residual plot after transforming the response using log Y . The residuals now appear to have constant variance, though there is some evidence of a slight non-linear relationship in the data.\n4. Outliers  \nAn outlier is a point for which y i is far from the value predicted by the heteroscedasticity  \nweighted least squares outlier  \nFIGURE 3.12. Left: The least squares regression line is shown in red, and the regression line after removing the outlier is shown in blue. Center: The residual plot clearly identifies the outlier. Right: The outlier has a studentized residual of 6 ; typically we expect values between -3 and 3 .  \n<!-- image -->  \nmodel. Outliers can arise for a variety of reasons, such as incorrect recording of an observation during data collection.  \nResidual plots can be used to identify outliers. In this example, the outlier is clearly visible in the residual plot illustrated in the center panel of Figure 3.12. But in practice, it can be difficult to decide how large a residual needs to be before we consider the point to be an outlier. To address this problem, instead of plotting the residuals, we can plot the studentized residuals , computed by dividing each residual e i by its estimated standard error. Observations whose studentized residuals are greater than 3 in absolute value are possible outliers. In the right-hand panel of Figure 3.12, the outlier's studentized residual exceeds 6 , while all other observations have studentized residuals between -2 and 2 .  \nThe red point (observation 20) in the left-hand panel of Figure 3.12 illustrates a typical outlier. The red solid line is the least squares regression fit, while the blue dashed line is the least squares fit after removal of the outlier. In this case, removing the outlier has little effect on the least squares line: it leads to almost no change in the slope, and a miniscule reduction in the intercept. It is typical for an outlier that does not have an unusual predictor value to have little effect on the least squares fit. However, even if an outlier does not have much effect on the least squares fit, it can cause other problems. For instance, in this example, the RSE is 1   09 when the outlier is included in the regression, but it is only 0   77 when the outlier is removed. Since the RSE is used to compute all confidence intervals and p -values, such a dramatic increase caused by a single data point can have implications for the interpretation of the fit. Similarly, inclusion of the outlier causes the R 2 to decline from 0   892 to 0   805 .  \nIf we believe that an outlier has occurred due to an error in data collection or recording, then one solution is to simply remove the observation. However, care should be taken, since an outlier may instead indicate a deficiency with the model, such as a missing predictor.\n5. High Leverage Points  \nWe just saw that outliers are observations for which the response y i is unusual given the predictor x i . In contrast, observations with high leverage have an unusual value for x i . For example, observation 41 in the left-hand studentized residual  \nhigh leverage  \nFIGURE 3.13. Left: Observation 41 is a high leverage point, while 20 is not. The red line is the fit to all the data, and the blue line is the fit with observation 41 removed. Center: The red observation is not unusual in terms of its X 1 value or its X 2 value, but still falls outside the bulk of the data, and hence has high leverage. Right: Observation 41 has a high leverage and a high residual.  \n<!-- image -->  \npanel of Figure 3.13 has high leverage, in that the predictor value for this observation is large relative to the other observations. (Note that the data displayed in Figure 3.13 are the same as the data displayed in Figure 3.12, but with the addition of a single high leverage observation.) The red solid line is the least squares fit to the data, while the blue dashed line is the fit produced when observation 41 is removed. Comparing the left-hand panels of Figures 3.12 and 3.13, we observe that removing the high leverage observation has a much more substantial impact on the least squares line than removing the outlier. In fact, high leverage observations tend to have a sizable impact on the estimated regression line. It is cause for concern if the least squares line is heavily affected by just a couple of observations, because any problems with these points may invalidate the entire fit. For this reason, it is important to identify high leverage observations.  \nIn order to quantify an observation's leverage, we compute the leverage statistic . A large value of this statistic indicates an observation with high leverage. For a simple linear regression,  \nIn a simple linear regression, high leverage observations are fairly easy to identify, since we can simply look for observations for which the predictor value is outside of the normal range of the observations. But in a multiple linear regression with many predictors, it is possible to have an observation that is well within the range of each individual predictor's values, but that is unusual in terms of the full set of predictors. An example is shown in the center panel of Figure 3.13, for a data set with two predictors, X 1 and X 2 . Most of the observations' predictor values fall within the blue dashed ellipse, but the red observation is well outside of this range. But neither its value for X 1 nor its value for X 2 is unusual. So if we examine just X 1 or just X 2 , we will fail to notice this high leverage point. This problem is more pronounced in multiple regression settings with more than two predictors, because then there is no simple way to plot all dimensions of the data simultaneously.  \n$$h _ { i } = \\frac { 1 } { n } + \\frac { ( x _ { i } - \\bar { x } ) ^ { 2 } } { \\sum _ { i ^ { \\prime } = 1 } ^ { n } ( x _ { i ^ { \\prime } } - \\bar { x } ) ^ { 2 } } .$$  \nleverage statistic  \nFIGURE 3.14. Scatterplots of the observations from the Credit data set. Left: A plot of age versus limit . These two variables are not collinear. Right: A plot of rating versus limit . There is high collinearity.  \n<!-- image -->  \nIt is clear from this equation that h i increases with the distance of x i from ¯ x . There is a simple extension of h i to the case of multiple predictors, though we do not provide the formula here. The leverage statistic h i is always between 1 glyph[triangleleft]n and 1 , and the average leverage for all the observations is always equal to ( p +1) glyph[triangleleft]n . So if a given observation has a leverage statistic that greatly exceeds ( p +1) glyph[triangleleft]n , then we may suspect that the corresponding point has high leverage.  \nThe right-hand panel of Figure 3.13 provides a plot of the studentized residuals versus h i for the data in the left-hand panel of Figure 3.13. Observation 41 stands out as having a very high leverage statistic as well as a high studentized residual. In other words, it is an outlier as well as a high leverage observation. This is a particularly dangerous combination! This plot also reveals the reason that observation 20 had relatively little effect on the least squares fit in Figure 3.12: it has low leverage.\n6. Collinearity  \nCollinearity refers to the situation in which two or more predictor variables are closely related to one another. The concept of collinearity is illustrated in Figure 3.14 using the Credit data set. In the left-hand panel of Figure 3.14, the two predictors limit and age appear to have no obvious relationship. In contrast, in the right-hand panel of Figure 3.14, the predictors limit and rating are very highly correlated with each other, and we say that they are collinear . The presence of collinearity can pose problems in the regression context, since it can be difficult to separate out the individual effects of collinear variables on the response. In other words, since limit and rating tend to increase or decrease together, it can be difficult to determine how each one separately is associated with the response, balance .  \nFigure 3.15 illustrates some of the difficulties that can result from collinearity. The left-hand panel of Figure 3.15 is a contour plot of the RSS (3.22) associated with different possible coefficient estimates for the regression of balance on limit and age . Each ellipse represents a set of coefficients collinearity  \nFIGURE 3.15. Contour plots for the RSS values as a function of the parameters β for various regressions involving the Credit data set. In each plot, the black dots represent the coefficient values corresponding to the minimum RSS. Left: A contour plot of RSS for the regression of balance onto age and limit . The minimum value is well defined. Right: A contour plot of RSS for the regression of balance onto rating and limit . Because of the collinearity, there are many pairs ( β Limit  β Rating ) with a similar value for RSS.  \n<!-- image -->  \nthat correspond to the same RSS, with ellipses nearest to the center taking on the lowest values of RSS. The black dots and associated dashed lines represent the coefficient estimates that result in the smallest possible RSS-in other words, these are the least squares estimates. The axes for limit and age have been scaled so that the plot includes possible coefficient estimates that are up to four standard errors on either side of the least squares estimates. Thus the plot includes all plausible values for the coefficients. For example, we see that the true limit coefficient is almost certainly somewhere between 0   15 and 0   20 .  \nSince collinearity reduces the accuracy of the estimates of the regression coefficients, it causes the standard error for ˆ β j to grow. Recall that the t -statistic for each predictor is calculated by dividing ˆ β j by its standard  \nIn contrast, the right-hand panel of Figure 3.15 displays contour plots of the RSS associated with possible coefficient estimates for the regression of balance onto limit and rating , which we know to be highly collinear. Now the contours run along a narrow valley; there is a broad range of values for the coefficient estimates that result in equal values for RSS. Hence a small change in the data could cause the pair of coefficient values that yield the smallest RSS-that is, the least squares estimates-to move anywhere along this valley. This results in a great deal of uncertainty in the coefficient estimates. Notice that the scale for the limit coefficient now runs from roughly -0   2 to 0   2 ; this is an eight-fold increase over the plausible range of the limit coefficient in the regression with age . Interestingly, even though the limit and rating coefficients now have much more individual uncertainty, they will almost certainly lie somewhere in this contour valley. For example, we would not expect the true value of the limit and rating coefficients to be -0   1 and 1 respectively, even though such a value is plausible for each coefficient individually.  \nTABLE 3.11. The results for two multiple regression models involving the Credit data set are shown. Model 1 is a regression of balance on age and limit , and Model 2 a regression of balance on rating and limit . The standard error of ˆ β limit increases 12-fold in the second regression, due to collinearity.  \n|         |           | Coefficient   |   Std. error | t -statistic   | p -value                      |\n|---------|-----------|---------------|--------------|----------------|-------------------------------|\n| Model 1 | Intercept | - 173.411     |       43.828 | - 3.957        | < 0   0001 |\n|         | age       | - 2.292       |        0.672 | - 3.407        | 0   0007   |\n|         | limit     | 0.173         |        0.005 | 34.496         | < 0   0001 |\n|         | Intercept | - 377.537     |       45.254 | - 8.343        | < 0   0001 |\n| Model 2 | rating    | 2.202         |        0.952 | 2.312          | 0.0213                        |\n|         | limit     | 0.025         |        0.064 | 0.384          | 0.7012                        |  \nerror. Consequently, collinearity results in a decline in the t -statistic. As a result, in the presence of collinearity, we may fail to reject H 0 : β j = 0 . This means that the power of the hypothesis test-the probability of correctly detecting a non-zero coefficient-is reduced by collinearity.  \nA simple way to detect collinearity is to look at the correlation matrix of the predictors. An element of this matrix that is large in absolute value indicates a pair of highly correlated variables, and therefore a collinearity problem in the data. Unfortunately, not all collinearity problems can be detected by inspection of the correlation matrix: it is possible for collinearity to exist between three or more variables even if no pair of variables has a particularly high correlation. We call this situation multicollinearity . Instead of inspecting the correlation matrix, a better way to assess multicollinearity is to compute the variance inflation factor (VIF). The VIF is the ratio of the variance of ˆ β j when fitting the full model divided by the variance of ˆ β j if fit on its own. The smallest possible value for VIF is 1 , which indicates the complete absence of collinearity. Typically in practice there is a small amount of collinearity among the predictors. As a rule of thumb, a VIF value that exceeds 5 or 10 indicates a problematic amount of collinearity. The VIF for each variable can be computed using the formula  \nTable 3.11 compares the coefficient estimates obtained from two separate multiple regression models. The first is a regression of balance on age and limit , and the second is a regression of balance on rating and limit . In the first regression, both age and limit are highly significant with very small p -values. In the second, the collinearity between limit and rating has caused the standard error for the limit coefficient estimate to increase by a factor of 12 and the p -value to increase to 0   701 . In other words, the importance of the limit variable has been masked due to the presence of collinearity. To avoid such a situation, it is desirable to identify and address potential collinearity problems while fitting the model.  \n$$V I F ( \\hat { \\beta } _ { j } ) = \\frac { 1 } { 1 - R _ { X _ { j } | X _ { - j } } ^ { 2 } } ,$$  \nwhere R 2 X j ♣ X -j is the R 2 from a regression of X j onto all of the other predictors. If R 2 X j ♣ X -j is close to one, then collinearity is present, and so the VIF will be large.  \npower multicollinearity variance inflation factor  \nIn the Credit data, a regression of balance on age , rating , and limit indicates that the predictors have VIF values of 1.01, 160.67, and 160.59. As we suspected, there is considerable collinearity in the data!  \nWhen faced with the problem of collinearity, there are two simple solutions. The first is to drop one of the problematic variables from the regression. This can usually be done without much compromise to the regression fit, since the presence of collinearity implies that the information that this variable provides about the response is redundant in the presence of the other variables. For instance, if we regress balance onto age and limit , without the rating predictor, then the resulting VIF values are close to the minimum possible value of 1 , and the R 2 drops from 0   754 to 0   75 . So dropping rating from the set of predictors has effectively solved the collinearity problem without compromising the fit. The second solution is to combine the collinear variables together into a single predictor. For instance, we might take the average of standardized versions of limit and rating in order to create a new variable that measures credit worthiness ."", '### 6.4.4 Interpreting Results in High Dimensions  \nWhen we perform the lasso, ridge regression, or other regression procedures in the high-dimensional setting, we must be quite cautious in the way that we report the results obtained. In Chapter 3, we learned about multicollinearity , the concept that the variables in a regression might be correlated with each other. In the high-dimensional setting, the multicollinearity problem is extreme: any variable in the model can be written as a linear combination of all of the other variables in the model. Essentially, this means that we can never know exactly which variables (if any) truly are predictive of the outcome, and we can never identify the best coefficients for use in the regression. At most, we can hope to assign large regression coefficients to variables that are correlated with the variables that truly are predictive of the outcome.  \ncurse of dimensionality  \nFor instance, suppose that we are trying to predict blood pressure on the basis of half a million SNPs, and that forward stepwise selection indicates that 17 of those SNPs lead to a good predictive model on the training data. It would be incorrect to conclude that these 17 SNPs predict blood pressure more effectively than the other SNPs not included in the model. There are likely to be many sets of 17 SNPs that would predict blood pressure just as well as the selected model. If we were to obtain an independent data set and perform forward stepwise selection on that data set, we would likely obtain a model containing a different, and perhaps even non-overlapping, set of SNPs. This does not detract from the value of the model obtainedfor instance, the model might turn out to be very effective in predicting blood pressure on an independent set of patients, and might be clinically useful for physicians. But we must be careful not to overstate the results obtained, and to make it clear that what we have identified is simply one of many possible models for predicting blood pressure, and that it must be further validated on independent data sets.  \nIt is also important to be particularly careful in reporting errors and measures of model fit in the high-dimensional setting. We have seen that when p &gt; n , it is easy to obtain a useless model that has zero residuals. Therefore, one should never use sum of squared errors, p-values, R 2 statistics, or other traditional measures of model fit on the training data as evidence of a good model fit in the high-dimensional setting. For instance, as we saw in Figure 6.23, one can easily obtain a model with R 2 = 1 when p &gt; n . Reporting this fact might mislead others into thinking that a statistically valid and useful model has been obtained, whereas in fact this provides absolutely no evidence of a compelling model. It is important to instead report results on an independent test set, or cross-validation errors. For instance, the MSE or R 2 on an independent test set is a valid measure of model fit, but the MSE on the training set certainly is not.']","To detect collinearity in a multiple regression model, you can:

1. **Inspect the Correlation Matrix**: Look at the correlation matrix of the predictors. A large absolute value in an element of this matrix indicates a pair of highly correlated variables, suggesting a collinearity problem. However, this method might not detect multicollinearity involving more than two variables.

2. **Compute the Variance Inflation Factor (VIF)**: The VIF is a more reliable method to assess multicollinearity. It is calculated as:

   $$VIF(\hat{\beta}_j) = \frac{1}{1 - R_{X_j | X_{-j}}^2},$$

   where \(R^2_{X_j | X_{-j}}\) is the \(R^2\) from a regression of \(X_j\) onto all other predictors. A VIF value exceeding 5 or 10 indicates a problematic amount of collinearity.

To address collinearity, you can:

1. **Drop One of the Problematic Variables**: Removing one of the collinear variables can often be done without much compromise to the regression fit, as the information it provides is redundant in the presence of the other variables.

2. **Combine Collinear Variables**: Create a single predictor by combining the collinear variables. For example, you might take the average of standardized versions of the collinear variables to form a new variable that captures the shared information.",Collinearity can be detected by examining the correlation matrix of the predictors or by calculating the Variance Inflation Factor (VIF). Two ways to address it are to either drop one of the problematic variables or to combine the collinear variables into a single predictor.,1.0,,0.9999999999,1.0
¿Qué es una variable ficticia ('dummy variable') y cuándo se utiliza en la regresión lineal?,"[""## 3.3 Other Considerations in the Regression Model  \n### 3.3.1 Qualitative Predictors  \nIn our discussion so far, we have assumed that all variables in our linear regression model are quantitative . But in practice, this is not necessarily the case; often some predictors are qualitative .  \nFor example, the Credit data set displayed in Figure 3.6 records variables for a number of credit card holders. The response is balance (average credit card debt for each individual) and there are several quantitative predictors: age , cards (number of credit cards), education (years of education), income (in thousands of dollars), limit (credit limit), and rating (credit rating). Each panel of Figure 3.6 is a scatterplot for a pair of variables whose identities are given by the corresponding row and column labels. For example, the scatterplot directly to the right of the word 'Balance' depicts balance versus age , while the plot directly to the right of 'Age' corresponds to age versus cards . In addition to these quantitative variables, we also have four qualitative variables: own (house ownership), student (student status), status (marital status), and region (East, West or South).\nPredictors with Only Two Levels  \nSuppose that we wish to investigate differences in credit card balance between those who own a house and those who don't, ignoring the other variables for the moment. If a qualitative predictor (also known as a factor ) only has two levels , or possible values, then incorporating it into a regression model is very simple. We simply create an indicator or dummy variable that takes on two possible numerical values. 10 For example, based on the own variable, we can create a new variable that takes the form  \n$$x _ { i } = \\begin{cases} 1 & \\text {if } \\text {ith person owns a house} \\\\ 0 & \\text {if } \\text {ith person does not own a house} , \\end{cases} ( 3 . 2 6 )$$  \nand use this variable as a predictor in the regression equation. This results in the model  \n$$y _ { i } = \\beta _ { 0 } + \\beta _ { 1 } x _ { i } + \\epsilon _ { i } = \\begin{cases} \\beta _ { 0 } + \\beta _ { 1 } + \\epsilon _ { i } & \\text {if } i t h \\, \\text { person owns a house} \\\\ \\beta _ { 0 } + \\epsilon _ { i } & \\text {if } i t h \\, \\text { person does not} . \\end{cases}$$  \nNow β 0 can be interpreted as the average credit card balance among those who do not own, β 0 + β 1 as the average credit card balance among those who do own their house, and β 1 as the average difference in credit card balance between owners and non-owners.  \nTable 3.7 displays the coefficient estimates and other information associated with the model (3.27). The average credit card debt for non-owners is estimated to be $509   80 , whereas owners are estimated to carry $19   73 in additional debt for a total of $509   80 + $19   73 = $529   53 . However, we  \n10 In the machine learning community, the creation of dummy variables to handle qualitative predictors is known as 'one-hot encoding'.  \nfactor level dummy variable  \nFIGURE 3.6. The Credit data set contains information about balance , age , cards , education , income , limit , and rating for a number of potential customers.  \n<!-- image -->  \nnotice that the p -value for the dummy variable is very high. This indicates that there is no statistical evidence of a difference in average credit card balance based on house ownership.  \n$$x _ { i } = \\begin{cases} 1 & \\text {if } i t h \\text { person owns a house} \\\\ - 1 & \\text {if } i t h \\text { person does not own a house} \\end{cases}$$  \nThe decision to code owners as 1 and non-owners as 0 in (3.27) is arbitrary, and has no effect on the regression fit, but does alter the interpretation of the coefficients. If we had coded non-owners as 1 and owners as 0 , then the estimates for β 0 and β 1 would have been 529   53 and -19   73 , respectively, leading once again to a prediction of credit card debt of $529   53 -$19   73 = $509   80 for non-owners and a prediction of $529   53 for owners. Alternatively, instead of a 0 glyph[triangleleft] 1 coding scheme, we could create a dummy variable and use this variable in the regression equation. This results in the model  \n$$a n d \\, \\text {use this variable in the $eq$ing equation.} \\, \\text {Results in the $model$} \\\\ y _ { i } = \\beta _ { 0 } + \\beta _ { 1 } x _ { i } + \\epsilon _ { i } = \\begin{cases} \\beta _ { 0 } + \\beta _ { 1 } + \\epsilon _ { i } & \\text {if $ith$ person owns a house} \\\\ \\beta _ { 0 } - \\beta _ { 1 } + \\epsilon _ { i } & \\text {if $ith$ person does not own a house} . \\end{cases}$$  \nTABLE 3.7. Least squares coefficient estimates associated with the regression of balance onto own in the Credit data set. The linear model is given in (3.27). That is, ownership is encoded as a dummy variable, as in (3.26).  \n|           |   Coefficient |   Std. error |   t -statistic | p -value                      |\n|-----------|---------------|--------------|----------------|-------------------------------|\n| Intercept |        509.8  |        33.13 |         15.389 | < 0   0001 |\n| own[Yes]  |         19.73 |        46.05 |          0.429 | 0.6690                        |  \nNow β 0 can be interpreted as the overall average credit card balance (ignoring the house ownership effect), and β 1 is the amount by which house owners and non-owners have credit card balances that are above and below the average, respectively. 11 In this example, the estimate for β 0 is $519   665 , halfway between the non-owner and owner averages of $509   80 and $529   53 . The estimate for β 1 is $9   865 , which is half of $19   73 , the average difference between owners and non-owners. It is important to note that the final predictions for the credit balances of owners and non-owners will be identical regardless of the coding scheme used. The only difference is in the way that the coefficients are interpreted.\nQualitative Predictors with More than Two Levels  \nWhen a qualitative predictor has more than two levels, a single dummy variable cannot represent all possible values. In this situation, we can create additional dummy variables. For example, for the region variable we create two dummy variables. The first could be  \n$$x _ { i 1 } = \\begin{cases} 1 & \\text {if } i t h \\, p \\, { \\mathrm { 1 } } \\, \\text { is from the South} \\\\ 0 & \\text {if } i t h \\, p \\, { \\mathrm { 1 } } \\, \\text { is not from the South} , \\end{cases}$$  \nand the second could be  \n$$x _ { i 2 } = \\begin{cases} 1 & \\text {if } \\text {th person is from the West} \\\\ 0 & \\text {if } \\text {th person is not from the West} . \\end{cases}$$  \nThen both of these variables can be used in the regression equation, in order to obtain the model  \n$$order \\, to \\, \\text {obtain} \\, \\text { the model } \\\\ y _ { i } = \\beta _ { 0 } + \\beta _ { 1 } x _ { i } + \\beta _ { 2 } x _ { i } + \\epsilon _ { i } = \\begin{cases} \\beta _ { 0 } + \\beta _ { 1 } + \\epsilon _ { i } & \\text {if } i \\text {th person is from the South} \\\\ \\beta _ { 0 } + \\beta _ { 2 } + \\epsilon _ { i } & \\text {if } i \\text {th person is from the West} \\\\ \\beta _ { 0 } + \\epsilon _ { i } & \\text {if } i \\text {th person is from the East} \\end{cases} \\\\ \\text {Now $\\beta_{0}$ can be interpreted as the average credit card balance for individuals}$$  \nNow β 0 can be interpreted as the average credit card balance for individuals from the East, β 1 can be interpreted as the difference in the average balance between people from the South versus the East, and β 2 can be interpreted as the difference in the average balance between those from the West versus the East. There will always be one fewer dummy variable than the number of levels. The level with no dummy variable-East in this example-is known as the baseline .  \n11 Technically β 0 is half the sum of the average debt for house owners and the average debt for non-house owners. Hence, β 0 is exactly equal to the overall average only if the two groups have an equal number of members.  \nbaseline  \nTABLE 3.8. Least squares coefficient estimates associated with the regression of balance onto region in the Credit data set. The linear model is given in ( 3.30 ). That is, region is encoded via two dummy variables (3.28) and (3.29).  \n|               | Coefficient   |   Std. error | t -statistic   | p -value                      |\n|---------------|---------------|--------------|----------------|-------------------------------|\n| Intercept     | 531.00        |        46.32 | 11.464         | < 0   0001 |\n| region[South] | - 12.50       |        56.68 | - 0.221        | 0.8260                        |\n| region[West]  | 18.69         |        65.02 | 0.287          | 0.7740                        |  \n-  \n-  \nFrom Table 3.8, we see that the estimated balance for the baseline, East, is $531   00 . It is estimated that those in the South will have $18   69 less debt than those in the East, and that those in the West will have $12   50 less debt than those in the East. However, the p -values associated with the coefficient estimates for the two dummy variables are very large, suggesting no statistical evidence of a real difference in average credit card balance between South and East or between West and East. 12 Once again, the level selected as the baseline category is arbitrary, and the final predictions for each group will be the same regardless of this choice. However, the coefficients and their p -values do depend on the choice of dummy variable coding. Rather than rely on the individual coefficients, we can use an F -test to test H 0 : β 1 = β 2 = 0 ; this does not depend on the coding. This F -test has a p -value of 0   96 , indicating that we cannot reject the null hypothesis that there is no relationship between balance and region .  \nThere are many different ways of coding qualitative variables besides the dummy variable approach taken here. All of these approaches lead to equivalent model fits, but the coefficients are different and have different interpretations, and are designed to measure particular contrasts . This topic is beyond the scope of the book.  \nUsing this dummy variable approach presents no difficulties when incorporating both quantitative and qualitative predictors. For example, to regress balance on both a quantitative variable such as income and a qualitative variable such as student , we must simply create a dummy variable for student and then fit a multiple regression model using income and the dummy variable as predictors for credit card balance."", '### 4.3.3 Making Predictions  \nOnce the coefficients have been estimated, we can compute the probability of default for any given credit card balance. For example, using the coefficient estimates given in Table 4.1, we predict that the default probability likelihood function  \nTABLE 4.1. For the Default data, estimated coefficients of the logistic regression model that predicts the probability of default using balance . A one-unit increase in balance is associated with an increase in the log odds of default by 0   0055 units.  \n|           | Coefficient   |   Std. error | z -statistic   | p -value   |\n|-----------|---------------|--------------|----------------|------------|\n| Intercept | - 10.6513     |       0.3612 | - 29.5         | < 0.0001   |\n| balance   | 0.0055        |       0.0002 | 24.9           | < 0.0001   |  \nTABLE 4.2. For the Default data, estimated coefficients of the logistic regression model that predicts the probability of default using student status. Student status is encoded as a dummy variable, with a value of 1 for a student and a value of 0 for a non-student, and represented by the variable student[Yes] in the table.  \n|              | Coefficient   |   Std. error | z -statistic   | p -value   |\n|--------------|---------------|--------------|----------------|------------|\n| Intercept    | - 3.5041      |       0.0707 | - 49.55        | < 0.0001   |\n| student[Yes] | 0.4049        |       0.115  | 3.52           | 0.0004     |  \nfor an individual with a balance of $1  000 is  \n$$\\hat { p } ( X ) = \\frac { e ^ { \\hat { \\beta } _ { 0 } + \\hat { \\beta } _ { 1 } X } } { 1 + e ^ { \\hat { \\beta } _ { 0 } + \\hat { \\beta } _ { 1 } X } } = \\frac { e ^ { - 1 0 6 5 1 3 + 0 . 0 0 5 5 \\times 1 , 0 0 0 0 } } { 1 + e ^ { - 1 0 . 6 5 1 3 + 0 . 0 0 5 5 \\times 1 , 0 0 0 } } = 0 . 0 0 5 7 6 ,$$  \nwhich is below 1 %. In contrast, the predicted probability of default for an individual with a balance of $2  000 is much higher, and equals 0   586 or 58   6 %.  \nOne can use qualitative predictors with the logistic regression model using the dummy variable approach from Section 3.3.1. As an example, the Default data set contains the qualitative variable student . To fit a model that uses student status as a predictor variable, we simply create a dummy variable that takes on a value of 1 for students and 0 for non-students. The logistic regression model that results from predicting probability of default from student status can be seen in Table 4.2. The coefficient associated with the dummy variable is positive, and the associated p -value is statistically significant. This indicates that students tend to have higher default probabilities than non-students:  \n$$\\text {Probabilities than non-students} \\colon & & e ^ { - 3 . 5 0 1 + 0 . 4 0 4 \\times 1 } \\\\ \\Pr ( \\text {default} = \\text {Yes} | \\text {student} = \\text {Yes} ) & = \\frac { e ^ { - 3 . 5 0 1 + 0 . 4 0 4 \\times 1 } } { 1 + e ^ { - 3 . 5 0 1 + 0 . 4 0 4 \\times 1 } } \\\\ \\widehat { \\Pr } ( \\text {default} = \\text {Yes} | \\text {student} = \\text {No} ) & = \\frac { e ^ { - 3 . 5 0 1 + 0 . 4 0 4 \\times 0 } } { 1 + e ^ { - 3 . 5 0 1 + 0 . 4 0 4 \\times 0 } } = 0 . 0 2 9 2 . \\\\$$', ""### 2.2.2 The Bias-Variance Trade-Off  \nThe U-shape observed in the test MSE curves (Figures 2.9-2.11) turns out to be the result of two competing properties of statistical learning methods.  \ncrossvalidation  \nFIGURE 2.11. Details are as in Figure 2.9, using a different f that is far from linear. In this setting, linear regression provides a very poor fit to the data.  \n<!-- image -->  \nThough the mathematical proof is beyond the scope of this book, it is possible to show that the expected test MSE, for a given value x 0 , can always be decomposed into the sum of three fundamental quantities: the variance of ˆ f ( x 0 ) , the squared bias of ˆ f ( x 0 ) and the variance of the error terms /epsilon1 . That is,  \n$$E \\left ( y _ { 0 } - \\hat { f } ( x _ { 0 } ) \\right ) ^ { 2 } = \\text {Var} ( \\hat { f } ( x _ { 0 } ) ) + \\left [ \\text {Bias} ( \\hat { f } ( x _ { 0 } ) ) \\right ] ^ { 2 } + \\text {Var} ( \\epsilon ) . \\\\$$  \nEquation 2.7 tells us that in order to minimize the expected test error, we need to select a statistical learning method that simultaneously achieves low variance and low bias . Note that variance is inherently a nonnegative quantity, and squared bias is also nonnegative. Hence, we see that the expected test MSE can never lie below Var ( /epsilon1 ) , the irreducible error from (2.3).  \nHere the notation E ( y 0 -ˆ f ( x 0 ) ) 2 defines the expected test MSE at x 0 , and refers to the average test MSE that we would obtain if we repeatedly estimated f using a large number of training sets, and tested each at x 0 . The overall expected test MSE can be computed by averaging E ( y 0 -ˆ f ( x 0 ) ) 2 over all possible values of x 0 in the test set.  \nWhat do we mean by the variance and bias of a statistical learning method? Variance refers to the amount by which ˆ f would change if we estimated it using a different training data set. Since the training data are used to fit the statistical learning method, different training data sets will result in a different ˆ f . But ideally the estimate for f should not vary too much between training sets. However, if a method has high variance then small changes in the training data can result in large changes in ˆ f . In general, more flexible statistical methods have higher variance. Consider the green and orange curves in Figure 2.9. The flexible green curve is following the observations very closely. It has high variance because changing any one of these data points may cause the estimate ˆ f to change considerably.  \nvariance bias expected test MSE  \nFIGURE 2.12. Squared bias (blue curve), variance (orange curve), Var ( /epsilon1 ) (dashed line), and test MSE (red curve) for the three data sets in Figures 2.9-2.11. The vertical dotted line indicates the flexibility level corresponding to the smallest test MSE.  \n<!-- image -->  \nIn contrast, the orange least squares line is relatively inflexible and has low variance, because moving any single observation will likely cause only a small shift in the position of the line.  \nAs a general rule, as we use more flexible methods, the variance will increase and the bias will decrease. The relative rate of change of these two quantities determines whether the test MSE increases or decreases. As we increase the flexibility of a class of methods, the bias tends to initially decrease faster than the variance increases. Consequently, the expected test MSE declines. However, at some point increasing flexibility has little impact on the bias but starts to significantly increase the variance. When this happens the test MSE increases. Note that we observed this pattern of decreasing test MSE followed by increasing test MSE in the right-hand panels of Figures 2.9-2.11.  \nOn the other hand, bias refers to the error that is introduced by approximating a real-life problem, which may be extremely complicated, by a much simpler model. For example, linear regression assumes that there is a linear relationship between Y and X 1  X 2         X p . It is unlikely that any real-life problem truly has such a simple linear relationship, and so performing linear regression will undoubtedly result in some bias in the estimate of f . In Figure 2.11, the true f is substantially non-linear, so no matter how many training observations we are given, it will not be possible to produce an accurate estimate using linear regression. In other words, linear regression results in high bias in this example. However, in Figure 2.10 the true f is very close to linear, and so given enough data, it should be possible for linear regression to produce an accurate estimate. Generally, more flexible methods result in less bias.  \nThe three plots in Figure 2.12 illustrate Equation 2.7 for the examples in Figures 2.9-2.11. In each case the blue solid curve represents the squared bias, for different levels of flexibility, while the orange curve corresponds to the variance. The horizontal dashed line represents Var ( /epsilon1 ) , the irreducible error. Finally, the red curve, corresponding to the test set MSE, is the sum  \nof these three quantities. In all three cases, the variance increases and the bias decreases as the method's flexibility increases. However, the flexibility level corresponding to the optimal test MSE differs considerably among the three data sets, because the squared bias and variance change at different rates in each of the data sets. In the left-hand panel of Figure 2.12, the bias initially decreases rapidly, resulting in an initial sharp decrease in the expected test MSE. On the other hand, in the center panel of Figure 2.12 the true f is close to linear, so there is only a small decrease in bias as flexibility increases, and the test MSE only declines slightly before increasing rapidly as the variance increases. Finally, in the right-hand panel of Figure 2.12, as flexibility increases, there is a dramatic decline in bias because the true f is very non-linear. There is also very little increase in variance as flexibility increases. Consequently, the test MSE declines substantially before experiencing a small increase as model flexibility increases.  \nIn a real-life situation in which f is unobserved, it is generally not possible to explicitly compute the test MSE, bias, or variance for a statistical learning method. Nevertheless, one should always keep the bias-variance trade-off in mind. In this book we explore methods that are extremely flexible and hence can essentially eliminate bias. However, this does not guarantee that they will outperform a much simpler method such as linear regression. To take an extreme example, suppose that the true f is linear. In this situation linear regression will have no bias, making it very hard for a more flexible method to compete. In contrast, if the true f is highly non-linear and we have an ample number of training observations, then we may do better using a highly flexible approach, as in Figure 2.11. In Chapter 5 we discuss cross-validation, which is a way to estimate the test MSE using the training data.  \nThe relationship between bias, variance, and test set MSE given in Equation 2.7 and displayed in Figure 2.12 is referred to as the bias-variance trade-off . Good test set performance of a statistical learning method requires low variance as well as low squared bias. This is referred to as a trade-off because it is easy to obtain a method with extremely low bias but high variance (for instance, by drawing a curve that passes through every single training observation) or a method with very low variance but high bias (by fitting a horizontal line to the data). The challenge lies in finding a method for which both the variance and the squared bias are low. This trade-off is one of the most important recurring themes in this book."", ""### 2.1.5 Regression Versus Classification Problems  \nVariables can be characterized as either quantitative or qualitative (also known as categorical ). Quantitative variables take on numerical values. Examples include a person's age, height, or income, the value of a house, and the price of a stock. In contrast, qualitative variables take on values in one of K different classes , or categories. Examples of qualitative variables include a person's marital status (married or not), the brand of product purchased (brand A, B, or C), whether a person defaults on a debt (yes or no), or a cancer diagnosis (Acute Myelogenous Leukemia, Acute Lymphoblastic Leukemia, or No Leukemia). We tend to refer to problems with a quantitative response as regression problems, while those involving a qualitative response are often referred to as classification problems. However, the distinction is not always that crisp. Least squares linear regression (Chapter 3) is used with a quantitative response, whereas logistic regression (Chapter 4) is typically used with a qualitative (two-class, or binary ) response. Thus, despite its name, logistic regression is a classification method. But since it estimates class probabilities, it can be thought of as a regression method as well. Some statistical methods, such as K -nearest neighbors (Chapters 2 and 4) and boosting (Chapter 8), can be used in the case of either quantitative or qualitative responses.  \nWe tend to select statistical learning methods on the basis of whether the response is quantitative or qualitative; i.e. we might use linear regression when quantitative and logistic regression when qualitative. However, whether the predictors are qualitative or quantitative is generally considered less important. Most of the statistical learning methods discussed in this book can be applied regardless of the predictor variable type, provided that any qualitative predictors are properly coded before the analysis is performed. This is discussed in Chapter 3.""]","Una variable ficticia, también conocida como variable indicadora o 'dummy variable', es una variable que se utiliza para incorporar predictores cualitativos en un modelo de regresión lineal. Estas variables toman valores numéricos, típicamente 0 y 1, para representar las diferentes categorías de un predictor cualitativo.

Se utiliza una variable ficticia cuando se desea incluir un predictor cualitativo en un modelo de regresión. Por ejemplo, si se tiene un predictor cualitativo con dos niveles, como la propiedad de una casa (propietario o no propietario), se puede crear una variable ficticia que tome el valor de 1 si la persona es propietaria de una casa y 0 si no lo es. Esto permite que el modelo de regresión lineal pueda manejar variables cualitativas de manera similar a las cuantitativas.

En el caso de predictores cualitativos con más de dos niveles, se crean múltiples variables ficticias para representar todas las categorías menos una, que se utiliza como categoría de referencia o base. Por ejemplo, si se tiene una variable de región con tres niveles (Este, Oeste, Sur), se pueden crear dos variables ficticias: una para el Sur y otra para el Oeste, dejando al Este como la categoría de referencia. Esto permite al modelo estimar diferencias en la variable de respuesta entre las diferentes categorías del predictor cualitativo.","Una variable ficticia (dummy variable) es una variable que toma valores de 0 o 1 para indicar la ausencia o presencia de alguna categoría o atributo. Se utiliza para incorporar predictores cualitativos en un modelo de regresión lineal, permitiendo analizar el efecto de diferentes categorías en la variable de respuesta.",1.0,,0.9166666666361111,1.0
What is the primary difference between Linear Discriminant Analysis (LDA) and Quadratic Discriminant Analysis (QDA)?,"[""### 4.4.3 Quadratic Discriminant Analysis  \nAs we have discussed, LDA assumes that the observations within each class are drawn from a multivariate Gaussian distribution with a class-specific mean vector and a covariance matrix that is common to all K classes. Quadratic discriminant analysis (QDA) provides an alternative approach. Like LDA, the QDA classifier results from assuming that the observations from each class are drawn from a Gaussian distribution, and plugging estimates for the parameters into Bayes' theorem in order to perform prediction. However, unlike LDA, QDA assumes that each class has its own covariance matrix. That is, it assumes that an observation from the k th class is of the form X ∼ N ( θ k  Σ k ) , where Σ k is a covariance matrix for the k th class. Under this assumption, the Bayes classifier assigns an observation X = x to the class for which  \n$$\\begin{array} { r l } { v a t i n g \\, X = x + t \\, \\text {the class for which} } \\\\ { \\delta _ { k } ( x ) \\ = \\ - \\frac { 1 } { 2 } ( x - \\mu _ { k } ) ^ { T } \\Sigma _ { k } ^ { - 1 } ( x - \\mu _ { k } ) - \\frac { 1 } { 2 } \\log | \\Sigma _ { k } | + \\log \\pi _ { k } } \\\\ { = \\ - \\frac { 1 } { 2 } x ^ { T } \\Sigma _ { k } ^ { - 1 } x + x ^ { T } \\Sigma _ { k } ^ { - 1 } \\mu _ { k } - \\frac { 1 } { 2 } \\mu _ { k } ^ { T } \\Sigma _ { k } ^ { - 1 } \\mu _ { k } - \\frac { 1 } { 2 } \\log | \\Sigma _ { k } | + \\log \\pi _ { k } } \\\\ { i \\, \\text {segment} \\, S _ { k } + \\text {a} \\, \\Delta _ { k } \\, \\log i \\, \\text {ex} \\, \\Pi _ { k } = \\Delta _ { k } \\log i \\, \\Pi _ { k } + \\Pi _ { k } \\, \\Pi _ { k } } \\end{array}$$  \nis largest. So the QDA classifier involves plugging estimates for Σ k , θ k , and π k into (4.28), and then assigning an observation X = x to the class for which this quantity is largest. Unlike in (4.24), the quantity x appears as a quadratic function in (4.28). This is where QDA gets its name.  \nWhy does it matter whether or not we assume that the K classes share a common covariance matrix? In other words, why would one prefer LDA to quadratic discriminant analysis  \n<!-- image -->  \n/negationslash  \nFIGURE 4.9. Left: The Bayes (purple dashed), LDA (black dotted), and QDA (green solid) decision boundaries for a two-class problem with Σ 1 = Σ 2 . The shading indicates the QDA decision rule. Since the Bayes decision boundary is linear, it is more accurately approximated by LDA than by QDA. Right: Details are as given in the left-hand panel, except that Σ 1 = Σ 2 . Since the Bayes decision boundary is non-linear, it is more accurately approximated by QDA than by LDA.  \nQDA, or vice-versa? The answer lies in the bias-variance trade-off. When there are p predictors, then estimating a covariance matrix requires estimating p ( p +1) glyph[triangleleft] 2 parameters. QDA estimates a separate covariance matrix for each class, for a total of Kp ( p +1) glyph[triangleleft] 2 parameters. With 50 predictors this is some multiple of 1,275, which is a lot of parameters. By instead assuming that the K classes share a common covariance matrix, the LDA model becomes linear in x , which means there are Kp linear coefficients to estimate. Consequently, LDA is a much less flexible classifier than QDA, and so has substantially lower variance. This can potentially lead to improved prediction performance. But there is a trade-off: if LDA's assumption that the K classes share a common covariance matrix is badly off, then LDA can suffer from high bias. Roughly speaking, LDA tends to be a better bet than QDA if there are relatively few training observations and so reducing variance is crucial. In contrast, QDA is recommended if the training set is very large, so that the variance of the classifier is not a major concern, or if the assumption of a common covariance matrix for the K classes is clearly untenable.  \nFigure 4.9 illustrates the performances of LDA and QDA in two scenarios. In the left-hand panel, the two Gaussian classes have a common correlation of 0   7 between X 1 and X 2 . As a result, the Bayes decision boundary is linear and is accurately approximated by the LDA decision boundary. The QDA decision boundary is inferior, because it suffers from higher variance without a corresponding decrease in bias. In contrast, the right-hand panel displays a situation in which the orange class has a correlation of 0   7 between the variables and the blue class has a correlation of -0   7 . Now the Bayes decision boundary is quadratic, and so QDA more accurately approximates this boundary than does LDA.""]","The primary difference between Linear Discriminant Analysis (LDA) and Quadratic Discriminant Analysis (QDA) lies in their assumptions about the covariance matrices of the classes:

- **LDA** assumes that the observations within each class are drawn from a multivariate Gaussian distribution with a class-specific mean vector and a covariance matrix that is common to all classes. This results in a linear decision boundary.

- **QDA**, on the other hand, assumes that each class has its own covariance matrix. This allows for a quadratic decision boundary, as the observations from each class are drawn from a Gaussian distribution with a class-specific mean vector and a class-specific covariance matrix.

The choice between LDA and QDA depends on the bias-variance trade-off and the size of the training dataset. LDA is less flexible and has lower variance, making it preferable when there are relatively few training observations. QDA is more flexible and is recommended when the training set is large or when the assumption of a common covariance matrix for the classes is clearly untenable.","The primary difference between LDA and QDA is the assumption about the covariance matrix of the predictors. LDA assumes that all classes share a common covariance matrix, resulting in a linear decision boundary. QDA, on the other hand, allows each class to have its own covariance matrix, leading to a quadratic decision boundary.",0.8823529411764706,,0.9999999999,1.0
What is the 'naive' assumption made by the Naive Bayes classifier?,"[""### 4.4.4 Naive Bayes  \nIn previous sections, we used Bayes' theorem (4.15) to develop the LDA and QDA classifiers. Here, we use Bayes' theorem to motivate the popular naive Bayes classifier.  \nHowever, estimating f 1 ( x )         f K ( x ) is more subtle. Recall that f k ( x ) is the p -dimensional density function for an observation in the k th class, for k = 1         K . In general, estimating a p -dimensional density function is challenging. In LDA, we make a very strong assumption that greatly simplifies the task: we assume that f k is the density function for a multivariate normal random variable with class-specific mean θ k , and shared covariance matrix Σ . By contrast, in QDA, we assume that f k is the density function for a multivariate normal random variable with class-specific mean θ k , and class-specific covariance matrix Σ k . By making these very strong assumptions, we are able to replace the very challenging problem of estimating K p -dimensional density functions with the much simpler problem of estimating K p -dimensional mean vectors and one (in the case of LDA) or K (in the case of QDA) ( p × p ) -dimensional covariance matrices.  \nRecall that Bayes' theorem (4.15) provides an expression for the posterior probability p k ( x ) = Pr( Y = k ♣ X = x ) in terms of π 1         π K and f 1 ( x )         f K ( x ) . To use (4.15) in practice, we need estimates for π 1         π K and f 1 ( x )         f K ( x ) . As we saw in previous sections, estimating the prior probabilities π 1         π K is typically straightforward: for instance, we can estimate ˆ π k as the proportion of training observations belonging to the k th class, for k = 1         K .  \nThe naive Bayes classifier takes a different tack for estimating f 1 ( x )         f K ( x ) . Instead of assuming that these functions belong to a particular family of distributions (e.g. multivariate normal), we instead make a single assumption:  \nWithin the k th class, the p predictors are independent.  \nStated mathematically, this assumption means that for k = 1         K ,  \n$$f _ { k } ( x ) = f _ { k 1 } ( x _ { 1 } ) \\times f _ { k 2 } ( x _ { 2 } ) \\times \\cdots \\times f _ { k p } ( x _ { p } ) , \\\\ \\intertext { f } \\colon \\intertext { x } \\colon \\intertext { 1 } 1 \\colon \\intertext { 2 } \\colon \\intertext { 3 } \\colon \\intertext { 4 } \\colon \\intertext { 5 } \\colon \\intertext { 6 } \\colon \\intertext { 7 } \\colon \\intertext { 8 } \\colon \\intertext { 9 } \\colon \\intertext { 1 } \\colon \\intertext { 2 } \\colon \\intertext { 3 } \\colon \\intertext { 4 } \\colon \\intertext { 5 } \\colon \\intertext { 6 } \\colon \\intertext { 7 } \\colon \\intertext { 8 } \\colon \\intertext { 9 } \\colon \\intertext { 1 } \\colon \\intertext { 2 } \\colon \\intertext { 3 } \\colon \\intertext { 4 } \\colon \\intertext { 5 } \\colon \\intertext { 6 } \\colon \\intertext { 7 } \\colon \\intertext { 8 } \\colon \\intertext { 9 } \\colon \\intertext { 1 } \\colon \\intertext { 2 } \\colon \\intertext { 3 } \\colon \\intertext { 4 } \\colon \\intertext { 5 } \\colon \\intertext { 6 } \\colon \\intertext { 7 } \\colon \\intertext { 8 } \\colon \\intertext { 9 } \\colon \\intertext { 1 } \\colon \\intertext { 2 } \\colon \\intertext { 3 } \\colon \\intertext { 4 } \\colon \\intertext { 5 } \\colon \\intertext { 6 } \\colon \\intertext { 7 } \\colon \\intertext { 8 } \\colon \\intertext { 9 } \\colon \\intertext { 1 } \\colon \\intertext { 2 } \\colon \\intertext { 3 } \\colon \\intertext { 4 } \\colon \\intertext { 5 } \\colon \\intertext { 6 } \\colon \\intertext { 7 } \\colon \\intertext { 8 } \\colon \\intertext { 9 } \\colon \\intertext { 1 } \\colon \\intertext { 2 } \\colon \\intertext { 3 } \\colon \\intertext { 4 } \\colon \\intertext { 5 } \\colon \\intertext { 6 } \\colon \\intertext { 7 } \\colon \\intertext { 8 } \\colon \\intertext { 9 } \\colon \\intertext { 1 } \\colon \\intertext { 2 } \\colon \\intertext { 3 } \\colon \\intertext { 4 } \\colon \\intertext { 5 } \\colon \\intertext { 6 } \\colon \\intertext { 7 } \\colon \\intertext { 8 } \\colon \\intertext { 9 } \\colon \\intertext { 1 } \\colon \\intertext { 2 } \\colon \\intertext { 3 } \\colon \\intertext { 4 } \\colon \\intertext { 5 } \\colon \\intertext { 6 } \\colon \\intertext { 7 } \\colon \\intertext { 8 } \\colon \\intertext { 9 } \\colon \\intertext { 1 } \\colon \\intertext { 2 } \\colon \\intertext { 3 } \\colon \\intertext { 4 } \\colon \\intertext { 5 } \\colon \\intertext { 6 } \\colon \\intertext { 7 } \\colon \\intertext { 8 } \\colon \\intertext { 9 } \\colon \\intertext { 1 } \\colon \\intertext { 2 } \\colon \\intertext { 3 } \\colon \\intertext { 4 } \\colon \\intertext { 5 } \\colon \\intertext { 6 } \\colon \\intertext { 7 } \\colon \\intertext { 8 } \\colon \\intertext { 9 } \\colon \\intertext { 1 } \\colon \\intertext { 2 } \\colon \\intertext { 3 } \\colon \\intertext { 4 } \\colon \\intertext { 5 } \\colon \\intertext { 6 } \\colon \\intertext { 7 } \\colon \\intertext { 8 } \\colon \\intertext { 9 } \\colon \\intertext { 1 } \\colon \\intertext { 2 } \\colon \\intertext { 3 } \\colon \\intertext { 4 } \\colon \\intertext { 5 } \\colon \\intertext { 6 } \\colon \\intertext { 7 } \\colon \\intertext { 8 } \\colon \\intertext { 9 } \\colon \\intertext { 1 } \\colon \\intertext { 2 } \\colon \\intertext { 3 } \\colon \\intertext { 4 } \\colon \\intertext { 5 } \\colon \\intertext { 6 } \\colon \\intertext { 7 } \\colon \\intertext { 8 } \\colon \\intertext { 9 } \\colon \\intertext { 1 } \\colon \\intertext { 2 } \\colon \\intertext { 3 } \\colon \\intertext { 4 } \\colon \\intertext { 5 } \\colon \\intertext { 6 } \\colon \\intertext { 7 } \\colon \\intertext { 8 } \\colon \\intertext { 9 } \\colon \\intertext { 1 } \\colon \\intertext { 2 } \\colon \\intertext { 3 } \\colon \\intertext { 4 } \\colon \\intertext { 5 } \\colon \\intertext { 6 } \\colon \\intertext { 7 } \\colon \\intertext { 8 } \\colon \\intertext { 9 } \\colon \\intertext { 1 } \\colon \\intertext { 2 } \\colon \\intertext { 3 } \\colon \\intertext { 4 } \\colon \\intertext { 5 } \\colon \\intertext { 6 } \\colon \\intertext { 7 } \\colon \\intertext { 8 } \\colon \\intertext { 9 } \\colon \\intertext { 1 } \\colon \\intertext { 2 } \\colon \\intertext { 3 } \\colon \\intertext { 4 } \\colon \\intertext { 5 } \\colon \\intertext { 6 } \\colon \\intertext { 7 } \\colon \\intertext { 8 } \\colon \\intertext { 9 } \\colon \\intertext { 1 } \\colon \\intertext { 2 } \\colon \\intertext { 3 } \\colon \\intertext { 4 } \\colon \\intertext { 5 } \\colon \\intertext { 6 } \\colon \\intertext { 7 } \\colon \\intertext { 8 } \\colon \\intertext { 9 } \\colon \\intertext { 1 } \\colon \\intertext { 2 } \\colon \\intertext { 3 } \\colon \\intertext { 4 } \\colon \\intertext { 5 } \\colon \\intertext { 6 } \\colon \\intertext { 7 } \\colon \\intertext { 8 } \\colon \\intertext { 9 } \\colon \\intertext { 1 } \\colon \\intertext { 2 } \\colon \\intertext { 3 } \\colon \\intertext { 4 } \\colon \\intertext { 5 } \\colon \\intertext { 6 } \\colon \\intertext { 7 } \\colon \\intertext { 8 } \\colon \\intertext { 9 } \\colon \\intertext { 1 } \\colon \\intertext { 2 } \\colon \\intertext { 3 } \\colon \\intertext { 4 } \\colon \\intertext { 5 } \\colon \\intertext { 6 } \\colon \\intertext { 7 } \\colon \\intertext { 8 } \\colon \\intertext { 9 } \\colon \\intertext { 1 } \\colon \\intertext { 2 } \\colon \\intertext { 3 } \\colon \\intertext { 4 } \\colon \\intertext { 5 } \\colon \\intertext { 6 } \\colon \\intertext { 7 } \\colon \\intertext { 8 } \\colon \\intertext { 9 } \\colon \\intertext { 1 } \\colon \\intertext { 2 } \\colon \\intertext { 3 } \\colon \\intertext { 4 } \\colon \\intertext { 5 } \\colon \\intertext { 6 } \\colon \\intertext { 7 } \\colon \\intertext { 8 } \\colon \\intertext { 9 } \\colon \\intertext { 1 } \\colon \\intertext { 2 } \\colon \\intertext { 3 } \\colon \\intertext { 4 } \\colon \\intertext { 5 } \\colon \\intertext { 6 } \\colon \\intertext { 7 } \\colon \\intertext { 8 } \\colon \\intertext { 9 } \\colon \\intertext { 1 } \\colon \\intertext { 2 } \\colon \\intertext { 3 } \\colon \\intertext { 4 } \\colon \\intertext { 5 } \\colon \\intertext { 6 } \\colon \\intertext { 7 } \\colon \\intertext { 8 } \\colon \\intertext { 9 } \\colon \\intertext { 1 } \\colon \\intertext { 2 } \\colon \\intertext { 3 } \\colon \\intertext { 4 } \\colon \\intertext { 5 } \\colon \\intertext { 6 } \\colon \\intertext { 7 } \\colon \\intertext { 8 } \\colon \\intertext { 9 } \\colon \\intertext { 1 } \\colon \\intertext { 2 } \\colon \\intertext { 3 } \\colon \\intertext { 4 } \\colon \\intertext { 5 } \\colon \\intertext { 6 } \\colon \\intertext { 7 } \\colon \\intertext { 8 } \\colon \\intertext { 9 } \\colon \\intertext { 1 } \\colon \\intertext { 2 } \\colon \\intertext { 3 } \\colon \\intertext { 4 } \\colon \\intertext { 5 } \\colon \\intertext { 6 } \\colon \\intertext { 7 } \\colon \\intertext { 8 } \\colon \\intertext { 9 } \\colon \\intertext { 1 } \\colon \\intertext { 2 } \\colon \\intertext { 3 } \\colon \\intertext { 4 } \\colon \\intertext { 5 } \\colon \\intertext { 6 } \\colon \\intertext { 7 } \\colon \\intertext { 8 } \\colon \\intertext { 9 } \\colon \\intertext { 1 } \\colon \\intertext { 2 } \\colon \\intertext { 3 } \\colon \\intertext { 4 } \\colon \\intertext { 5 } \\colon \\intertext { 6 } \\colon \\intertext { 7 } \\colon \\intertext { 8 } \\colon \\intertext { 9 } \\colon \\intertext { 1 } \\colon \\intertext { 2 } \\colon \\intertext { 3 } \\colon \\intertext { 4 } \\colon \\intertext { 5 } \\colon \\intertext { 6 } \\colon \\intertext { 7 } \\colon \\intertext { 8 } \\colon \\intertext { 9 } \\colon \\intertext { 1 } \\colon \\intertext { 2 } \\colon \\intertext { 3 } \\colon \\intertext { 4 } \\colon \\intertext { 5 } \\colon \\intertext { 6 } \\colon \\intertext { 7 } \\colon \\intertext { 8 } \\colon \\intertext { 9 } \\colon \\intertext { 1 } \\colon \\intertext { 2 } \\colon \\intertext { 3 } \\colon \\intertext { 4 } \\colon \\intertext { 5 } \\colon \\intertext { 6 } \\colon \\intertext { 7 } \\colon \\intertext { 8 } \\colon \\intertext { 9 } \\colon \\intertext { 1 } \\colon \\intertext { 2 } \\colon \\intertext { 3 } \\colon \\intertext { 4 } \\colon \\intertext { 5 } \\colon \\intertext { 6 } \\colon \\intertext { 7 } \\colon \\intertext { 8 } \\colon \\intertext { 9 } \\colon \\intertext { 1 } \\colon \\intertext { 2 } \\colon \\intertext { 3 } \\colon \\intertext { 4 } \\colon \\intertext { 5 } \\colon \\intertext { 6 } \\colon \\intertext { 7 } \\colon \\intertext { 8 } \\colon \\intertext { 9 } \\colon \\intertext { 1 } \\colon \\intertext { 2 } \\colon \\intertext { 3 } \\colon \\intertext { 4 } \\colon \\intertext { 5 } \\colon \\intertext { 6 } \\colon \\intertext { 7 } \\colon \\intertext { 8 } \\colon \\intertext { 9 } \\colon \\intertext { 1 } \\colon \\intertext { 2 } \\colon \\intertext { 3 } \\colon \\intertext { 4 } \\colon \\intertext { 5 } \\colon \\intertext { 6 } \\colon \\intertext { 7 } \\colon \\intertext { 8 } \\colon \\intertext { 9 } \\colon \\intertext { 1 } \\colon \\intertext { 2 } \\colon \\intertext { 3 } \\colon \\intertext { 4 } \\colon \\intertext { 5 } \\colon \\intertext { 6 } \\colon \\intertext { 7 } \\colon \\intertext { 8 } \\colon \\intertext { 9 } \\colon \\intertext { 1 } \\colon \\intertext { 2 } \\colon \\intertext { 3 } \\colon \\intertext { 4 } \\colon \\intertext { 5 } \\colon \\intertext { 6 } \\colon \\intertext { 7 } \\colon \\intertext { 8 } \\colon \\intertext { 9 } \\colon \\intertext { 1 } \\colon \\intertext { 2 } \\colon \\intertext { 3 } \\colon \\intertext { 4 } \\colon \\intertext { 5 } \\colon \\intertext { 6 } \\colon \\intertext { 7 } \\colon \\intertext { 8 } \\colon \\intertext { 9 } \\colon \\intertext { 1 } \\colon \\intertext { 2 } \\colon \\intertext { 3 } \\colon \\intertext { 4 } \\colon \\intertext { 5 } \\colon \\intertext { 6 } \\colon \\intertext { 7 } \\colon \\intertext { 8 } \\colon \\intertext { 9 } \\colon \\intertext { 1 } \\colon \\intertext { 2 } \\colon \\intertext { 3 } \\colon \\intertext { 4 } \\colon \\intertext { 5 } \\colon \\intertext { 6 } \\colon \\intertext { 7 } \\colon \\intertext { 8 } \\colon \\intertext { 9 } \\colon \\intertext { 1 } \\colon \\intertext { 2 } \\colon \\intertext { 3 } \\colon \\intertext { 4 } \\colon \\intertext { 5 } \\colon \\intertext { 6 } \\colon \\intertext { 7 } \\colon \\intertext { 8 } \\colon \\intertext { 9 } \\colon \\intertext { 1 } \\colon \\intertext { 2 } \\colon \\intertext { 3 } \\colon \\intertext { 4 } \\colon \\intertext { 5 } \\colon \\intertext { 6 } \\colon \\intertext { 7 } \\colon \\intertext { 8 } \\colon \\intertext { 9 } \\colon \\intertext { 1 } \\colon \\intertext { 2 } \\colon \\intertext { 3 } \\colon \\intertext { 4 } \\colon \\intertext { 5 } \\colon \\intertext { 6 } \\colon \\intertext { 7 } \\colon \\intertext { 8 } \\colon \\intertext { 9 } \\colon \\intertext { 1 } \\colon \\intertext { 2 } \\colon \\intertext { 3 } \\colon \\intertext { 4 } \\colon \\intertext { 5 } \\colon \\intertext { 6 } \\colon \\intertext { 7 } \\colon \\intertext { 8 } \\colon \\intertext { 9 } \\colon \\intertext { 1 } \\colon \\intertext { 2 } \\colon \\intertext { 3 } \\colon \\intertext { 4 } \\colon \\intertext { 5 } \\colon \\intertext { 6 } \\colon \\intertext { 7 } \\colon \\intertext { 8 } \\colon \\intertext { 9 } \\colon \\intertext { 1 } \\colon \\intertext { 2 } \\colon \\intertext { 3 } \\colon \\intertext { 4 } \\colon \\intertext { 5 } \\colon \\intertext { 6 } \\colon \\intertext { 7 } \\colon \\intertext { 8 } \\colon \\intertext { 9 } \\colon \\intertext { 1 } \\colon \\intertext { 2 } \\colon \\intertext { 3 } \\colon \\intertext { 4 } \\colon \\intertext { 5 } \\colon \\intertext { 6 } \\colon \\intertext { 7 } \\colon \\intertext { 8 } \\colon \\intertext { 9 } \\colon \\intertext { 1 } \\colon \\intertext { 2 } \\colon \\intertext { 3 } \\colon \\intertext { 4 } \\colon \\intertext { 5 } \\colon \\intertext { 6 } \\colon \\intertext { 7 } \\colon \\intertext { 8 } \\colon \\intertext { 9 } \\colon \\intertext { 1 } \\colon \\intertext { 2 } \\colon \\intertext { 3 } \\colon \\intertext { 4 } \\colon \\intertext { 5 } \\colon \\intertext { 6 } \\colon \\intertext { 7 } \\colon \\intertext { 8 } \\colon \\intertext { 9 } \\colon \\intertext { 1 } \\colon \\intertext { 2 } \\colon \\intertext { 3 } \\colon \\intertext { 4 } \\colon \\intertext { 5 } \\colon \\intertext { 6 } \\colon \\intertext { 7 } \\colon \\intertext { 8 } \\colon \\intertext { 9 } \\colon \\intertext { 1 } \\colon \\intertext { 2 } \\colon \\intertext { 3 } \\colon \\intertext { 4 } \\colon \\intertext { 5 } \\colon \\intertext { 6 } \\colon \\intertext { 7 } \\colon \\intertext { 8 } \\colon \\intertext { 9 } \\colon \\intertext { 1 } \\colon \\intertext { 2 } \\colon \\intertext { 3 } \\colon \\intertext { 4 } \\colon \\intertext { 5 } \\colon \\intertext { 6 } \\colon \\intertext { 7 } \\colon \\intertext { 8 } \\colon \\intertext { 9 } \\colon \\intertext { 1 } \\colon \\intertext { 2 } \\colon \\intertext { 3 } \\colon \\intertext { 4 } \\colon \\intertext { 5 } \\colon \\intertext { 6 } \\colon \\intertext { 7 } \\colon \\intertext { 8 } \\colon \\intertext { 9 } \\colon \\intertext { 1 } \\colon \\intertext { 2 } \\colon \\intertext { 3 } \\colon \\intertext { 4 } \\colon \\intertext { 5 } \\colon \\intertext { 6 } \\colon \\intertext { 7 } \\colon \\intertext { 8 } \\colon \\intertext { 9 } \\colon \\intertext \\colon \\intertext { 1 } \\colon \\intertext { 2 } \\colon \\intertext { 3 } \\colon \\intertext { 4 } \\colon \\intertext { 5 } \\colon \\intertext { 6 } \\colon \\intertext { 7 } \\colon \\intertext { 8 } \\colon \\intertext { 9 } \\colon \\intertext { 1 } \\colon \\intertext { 2 } \\colon \\intertext { 3 } \\colon \\intertext { 4 } \\colon \\intertext { 5 } \\colon \\intertext { 6 } \\colon \\intertext { 7 } \\colon \\intertext { 8 } \\colon \\intertext { 9 } \\colon \\intertext { 1 } \\colon \\intertext { 2 } \\colon \\intertext { 3 } \\colon \\intertext { 4 } \\colon \\intertext { 5 } \\colon \\intertext { 6 } \\colon \\intertext { 7 } \\colon \\intertext { 8 } \\colon \\intertext { 9 } \\colon \\intertext { 1 } \\colon \\intertext { 2 } \\colon \\intertext { 3 } \\colon \\intertext { 4 } \\colon \\intertext { 5 } \\colon \\intertext { 6 } \\colon \\intertext { 7 } \\colon \\intertext { 8 } \\colon \\intertext { 9 } \\colon \\intertext { 1 } \\colon \\intertext { 2 } \\colon \\intertext { 3 } \\colon \\intertext { 4 } \\colon \\intertext { 5 } \\colon \\intertext { 6 } \\colon \\intertext { 7 } \\colon \\intertext { 8 } \\colon \\intertext { 9 } \\colon \\intertext { 1 } \\colon \\intertext { 2 } \\colon \\intertext { 3 } \\colon \\intertext { 4 } \\colon \\intertext { 5 } \\colon \\intertext { 6 } \\colon \\intertext { 7 } \\colon \\intertext { 8 } \\colon \\intertext { 9 } \\colon \\intertext { 1 } \\colon \\intertext { 2 } \\colon \\intertext { 3 } \\colon \\intertext { 4 } \\colon \\intertext { 5 } \\colon \\intertext { 6 } \\colon \\intertext { 7 } \\colon \\intertext { 8 } \\colon \\intertext { 9 } \\colon \\intertext { 1 } \\colon \\intertext { 2 } \\colon \\intertext { 3 } \\colon \\intertext { 4 } \\colon \\intertext { 5 } \\colon \\intertext { 6 } \\colon \\intertext { 7 } \\colon \\intertext { 8 } \\colon \\intertext { 9 } \\colon \\intertext { 1 } \\colon \\intertext { 2 } \\colon \\intertext { 3 } \\colon \\intertext { 4 } \\colon \\intertext { 5 } \\colon \\intertext { 6 } \\colon \\intertext { 7 } \\colon \\intertext { 8 } \\colon \\intertext { 9 } \\colon \\intertext { 1 } \\colon \\intertext { 2 } \\colon \\intertext { 3 } \\colon \\intertext { 4 } \\colon \\intertext { 5 } \\colon \\intertext { 6 } \\colon \\intertext { 7 } \\colon \\intertext { 8 } \\colon \\intertext { 9 } \\colon \\intertext { 1 } \\colon \\intertext { 2 } \\colon \\intertext { 3 } \\colon \\intertext { 4 } \\colon \\intertext { 5 } \\colon \\intertext { 6 } \\colon \\intertext { 7 } \\colon \\intertext { 8 } \\colon \\intertext { 9 } \\colon \\intertext { 1 } \\colon \\intertext { 2 } \\colon \\intertext { 3 } \\colon \\intertext { 4 } \\colon \\intertext { 5 } \\colon \\intertext { 6 } \\colon \\intertext { 7 } \\colon \\intertext { 8 } \\colon \\intertext { 9 } \\colon \\intertext { 1 } \\colon \\intertext { 2 } \\colon \\intertext { 3 } \\colon \\intertext { 4 } \\colon \\intertext { 5 } \\colon \\intertext { 6 } \\colon \\intertext { 7 } \\colon \\intertext { 8 } \\colon \\intertext { 9 } \\colon \\intertext { 1 } \\colon \\intertext { 2 } \\colon \\intertext { 3 } \\colon \\intertext { 4 } \\colon \\intertext { 5 } \\colon \\intertext { 6 } \\colon \\intertext { 7 } \\colon \\intertext { 8 } \\colon \\intertext { 9 } \\colon \\intertext { 1 } \\colon \\intertext { 2 } \\colon \\intertext { 3 } \\colon \\intertext { 4 } \\colon \\intertext { 5 } \\colon \\intertext { 6 } \\colon \\intertext { 7 } \\colon \\intertext { 8 } \\colon \\intertext { 9 } \\colon \\intertext { 1 } \\colon \\intertext { 2 } \\colon \\intertext { 3 } \\colon \\intertext { 4 } \\colon \\intertext { 5 } \\colon \\intertext { 6 } \\colon \\intertext { 7 } \\colon \\intertext { 8 } \\colon \\intertext { 9 } \\colon \\intertext { 1 } \\colon \\intertext { 2 } \\colon \\intertext { 3 } \\colon \\intertext { 4 } \\colon \\intertext { 5 } \\colon \\intertext { 6 } \\colon \\intertext { 7 } \\colon \\intertext { 8 } \\colon \\intertext { 9 } \\colon \\intertext { 1 } \\colon \\intertext { 2 } \\colon \\intertext { 3 } \\colon \\intertext { 4 } \\colon \\intertext { 5 } \\colon \\intertext { 6 } \\colon \\intertext { 7 } \\colon \\intertext { 9 } \\colon \\intertext { 1 } \\colon \\intertext { 2 } \\colon \\intertext { 3 } \\colon \\intertext { 4 } \\colon \\intertext { 5 } \\colon \\intertext { 6 } \\colon \\intertext { 7 } \\colon \\intertext { 8 } \\colon \\intertext { 9 } \\colon \\intertext { 1 } \\colon \\intertext { 2 } \\colon \\intertext { 3 } \\colon \\intertext { 4 } \\colon \\intertext { 5 } \\colon \\intertext { 6 } \\colon \\intertext { 7 } \\colon \\intertext { 8 } \\colon \\intertext { 9 } \\colon \\intertext { 1 } \\colon \\intertext { 2 } \\colon \\intertext { 3 } \\colon \\intertext { 4 } \\colon \\intertext { 5 } \\colon \\intertext { 6 } \\colon \\intertext { 7 } \\colon \\intertext { 9 } \\colon \\intertext { 1 } \\colon \\intertext { 2 } \\colon \\intertext { 3 } \\colon \\intertext { 4 } \\colon \\intertext { 5 } \\colon \\intertext { 6 } \\colon \\intertext { 7 } \\colon \\intertext { 9 } \\colon \\intertext { 1 } \\colon \\intertext { 2 } \\colon \\intertext { 3 } \\colon \\intertext { 4 } \\colon \\intertext { 5 } \\colon \\intertext { 6 } \\colon \\intertext { 7 } \\colon \\intertext { 9 } \\colon \\intertext { 1 } \\colon \\intertext { 2 } \\colon \\intertext { 3 } \\colon \\intertext { 4 } \\colon \\intertext { 5 } \\colon \\intertext { 6 } \\colon \\intertext { 7 } \\colon \\intertext { 9 } \\colon \\intertext { 1 } \\colon \\intertext { 2 } \\colon \\intertext { 3 } \\colon \\intertext { 4 } \\colon \\intertext { 5 } \\colon \\intertext { 6 } \\colon \\intertext { 7 } \\colon \\intertext { 9 } \\colon \\intertext { 1 } \\colon \\intertext { 2 } \\colon \\intertext { 3 } \\colon \\intertext { 4 } \\colon \\intertext { 5 } \\colon \\intertext { 6 } \\colon \\intertext { 7 } \\colon \\intertext { 9 } \\colon \\intertext { 1 } \\colon \\intertext { 2 } \\colon \\intertext { 3 } \\colon \\intertext { 4 } \\colon \\intertext { 5 } \\colon \\intertext { 6 } \\colon \\intertext { 7 } \\colon \\intertext { 9 } \\colon \\intertext { 1 } \\colon \\intertext { 2 } \\colon \\intertext { 3 } \\colon \\intertext { 4 } \\colon \\intertext { 5 } \\colon \\intertext { 6 } \\colon \\intertext { 7 } \\colon \\intertext { 9 } \\colon \\intertext { 1 } \\colon \\intertext { 2 } \\colon \\intertext { 3 } \\colon \\intertext { 4 } \\colon \\intertext { 5 } \\colon \\intertext { 6 } \\colon \\intertext { 7 } \\colon \\intertext { 9 } \\colon \\intertext { 1 } \\colon \\intertext { 2 } \\colon \\intertext { 3 } \\colon \\intertext { 4 } \\colon \\intertext { 5 } \\colon \\intertext { 6 } \\colon \\intertext { 7 } \\colon \\intertext { 9 } \\colon \\intertext { 1 } \\colon \\intertext { 2 } \\colon \\intertext { 3 } \\colon \\intertext { 4 } \\colon \\intertext { 5 } \\colon \\intertext { 6 } \\colon \\intertext { 7 } \\colon \\intertext { 9 } \\colon \\intertext { 1 } \\colon \\intertext { 2 } \\colon \\intertext { 3 } \\colon \\intertext { 4 } \\colon \\intertext { 5 } \\colon \\intertext { 6 } \\colon \\intertext { 7 } \\colon \\intertext { 9 } \\colon \\intertext { 1 } \\colon \\intertext { 2 } \\colon \\intertext { 3 } \\colon \\intertext { 4 } \\colon \\intertext { 5 } \\colon \\intertext { 6 } \\colon \\intertext { 7 } \\colon \\intertext { 9 } \\colon \\intertext { 1 } \\colon \\intertext { 2 } \\colon \\intertext { 3 } \\colon \\intertext { 4 } \\colon \\intertext { 5 } \\colon \\intertext { 6 } \\colon \\intertext { $$  \nWhy is this assumption so powerful? Essentially, estimating a p -dimensional density function is challenging because we must consider not only the marginal distribution of each predictor - that is, the distribution of each predictor on its own - but also the joint distribution of the predictors - that is, the association between the different predictors. In the case of a multivariate normal distribution, the association between the different predictors is summarized by the off-diagonal elements of the covariance matrix. However, in general, this association can be very hard to characterize, and exceedingly challenging to estimate. But by assuming that the p covariates are independent within each class, we completely eliminate the need to worry about the association between the p predictors, because we have simply assumed that there is no association between the predictors!  \nwhere f kj is the density function of the j th predictor among observations in the k th class.  \nDo we really believe the naive Bayes assumption that the p covariates are independent within each class? In most settings, we do not. But even though this modeling assumption is made for convenience, it often leads to naive Bayes  \nmarginal distribution joint distribution  \npretty decent results, especially in settings where n is not large enough relative to p for us to effectively estimate the joint distribution of the predictors within each class. In fact, since estimating a joint distribution requires such a huge amount of data, naive Bayes is a good choice in a wide range of settings. Essentially, the naive Bayes assumption introduces some bias, but reduces variance, leading to a classifier that works quite well in practice as a result of the bias-variance trade-off.  \nOnce we have made the naive Bayes assumption, we can plug (4.29) into (4.15) to obtain an expression for the posterior probability,  \n$$\\Pr ( Y = k | X = x ) & = \\frac { \\pi _ { k } \\times f _ { k 1 } ( x _ { 1 } ) \\times f _ { k 2 } ( x _ { 2 } ) \\times \\dots \\times f _ { k p } ( x _ { p } ) } { \\sum _ { l = 1 } ^ { K } \\pi _ { l } \\times f _ { l 1 } ( x _ { 1 } ) \\times f _ { l 2 } ( x _ { 2 } ) \\times \\dots \\times f _ { l p } ( x _ { p } ) } \\quad ( 4 . 3 0 ) \\\\ \\text {for } k & = 1 , \\dots , K .$$  \nTo estimate the one-dimensional density function f kj using training data x 1 j         x nj , we have a few options.  \nfor k = 1         K .  \n- If X j is quantitative, then we can assume that X j ♣ Y = k ∼ N ( θ jk  σ 2 jk ) . In other words, we assume that within each class, the j th predictor is drawn from a (univariate) normal distribution. While this may sound a bit like QDA, there is one key difference, in that here we are assuming that the predictors are independent; this amounts to QDA with an additional assumption that the class-specific covariance matrix is diagonal.\n- If X j is quantitative, then another option is to use a non-parametric estimate for f kj . A very simple way to do this is by making a histogram for the observations of the j th predictor within each class. Then we can estimate f kj ( x j ) as the fraction of the training observations in the k th class that belong to the same histogram bin as x j . Alternatively, we can use a kernel density estimator , which is essentially a smoothed version of a histogram.\n- If X j is qualitative, then we can simply count the proportion of training observations for the j th predictor corresponding to each class. For instance, suppose that X j ∈ ¶ 1  2  3 ♦ , and we have 100 observations in the k th class. Suppose that the j th predictor takes on values of 1 , 2 , and 3 in 32, 55, and 13 of those observations, respectively. Then we can estimate f kj as  \n$$\\hat { f } _ { k j } ( x _ { j } ) = \\begin{cases} 0 . 3 2 & \\text {if } x _ { j } = 1 \\\\ 0 . 5 5 & \\text {if } x _ { j } = 2 \\\\ 0 . 1 3 & \\text {if } x _ { j } = 3 . \\end{cases}$$  \nWe now consider the naive Bayes classifier in a toy example with p = 3 predictors and K = 2 classes. The first two predictors are quantitative, and the third predictor is qualitative with three levels. Suppose further that ˆ π 1 = ˆ π 2 = 0   5 . The estimated density functions ˆ f kj for k = 1  2 and j = 1  2  3 are displayed in Figure 4.10. Now suppose that we wish to classify a new observation, x ∗ = (0   4  1   5  1) T . It turns out that in this kernel density estimator  \nFIGURE 4.10. In the toy example in Section 4.4.4, we generate data with p = 3 predictors and K = 2 classes. The first two predictors are quantitative, and the third predictor is qualitative with three levels. In each class, the estimated density for each of the three predictors is displayed. If the prior probabilities for the two classes are equal, then the observation x ∗ = (0   4  1   5  1) T has a 94   4% posterior probability of belonging to the first class.  \n<!-- image -->  \nTABLE 4.8. Comparison of the naive Bayes predictions to the true default status for the 10  000 training observations in the Default data set, when we predict default for any observation for which P ( Y = default ♣ X = x ) &gt; 0   5 .  \n|                |       | True default status   | True default status   | True default status   |\n|----------------|-------|-----------------------|-----------------------|-----------------------|\n|                |       | No                    | Yes                   | Total                 |\n| Predicted      | No    | 9621                  | 244                   | 9865                  |\n| default status | Yes   | 46                    | 89                    | 135                   |\n|                | Total | 9667                  | 333                   | 10000                 |  \nexample, ˆ f 11 (0   4) = 0   368 , ˆ f 12 (1   5) = 0   484 , ˆ f 13 (1) = 0   226 , and ˆ f 21 (0   4) = 0   030 , ˆ f 22 (1   5) = 0   130 , ˆ f 23 (1) = 0   616 . Plugging these estimates into (4.30) results in posterior probability estimates of Pr( Y = 1 ♣ X = x ∗ ) = 0   944 and Pr( Y = 2 ♣ X = x ∗ ) = 0   056 .  \nTable 4.8 provides the confusion matrix resulting from applying the naive Bayes classifier to the Default data set, where we predict a default if the posterior probability of a default - that is, P ( Y = default ♣ X = x ) -exceeds 0   5 . Comparing this to the results for LDA in Table 4.4, our findings are mixed. While LDA has a slightly lower overall error rate, naive Bayes  \nTABLE 4.9. Comparison of the naive Bayes predictions to the true default status for the 10  000 training observations in the Default data set, when we predict default for any observation for which P ( Y = default ♣ X = x ) &gt; 0   2 .  \n|                |       | True default status   | True default status   | True default status   |\n|----------------|-------|-----------------------|-----------------------|-----------------------|\n|                |       | No                    | Yes                   | Total                 |\n| Predicted      | No    | 9339                  | 130                   | 9469                  |\n| default status | Yes   | 328                   | 203                   | 531                   |\n|                | Total | 9667                  | 333                   | 10000                 |  \ncorrectly predicts a higher fraction of the true defaulters. In this implementation of naive Bayes, we have assumed that each quantitative predictor is drawn from a Gaussian distribution (and, of course, that within each class, each predictor is independent).  \nIn this example, it should not be too surprising that naive Bayes does not convincingly outperform LDA: this data set has n = 10  000 and p = 2 , and so the reduction in variance resulting from the naive Bayes assumption is not necessarily worthwhile. We expect to see a greater pay-off to using naive Bayes relative to LDA or QDA in instances where p is larger or n is smaller, so that reducing the variance is very important.  \nJust as with LDA, we can easily adjust the probability threshold for predicting a default. For example, Table 4.9 provides the confusion matrix resulting from predicting a default if P ( Y = default ♣ X = x ) &gt; 0   2 . Again, the results are mixed relative to LDA with the same threshold (Table 4.5). Naive Bayes has a higher error rate, but correctly predicts almost two-thirds of the true defaults."", ""### 2.2.3 The Classification Setting  \nThus far, our discussion of model accuracy has been focused on the regression setting. But many of the concepts that we have encountered, such as the bias-variance trade-off, transfer over to the classification setting with only some modifications due to the fact that y i is no longer quantitative. Suppose that we seek to estimate f on the basis of training observations ¶ ( x 1  y 1 )         ( x n  y n ) ♦ , where now y 1         y n are qualitative. The most common approach for quantifying the accuracy of our estimate ˆ f is the training error rate , the proportion of mistakes that are made if we apply bias-variance trade-off  \nerror rate  \nour estimate ˆ f to the training observations:  \n/negationslash  \n$$\\frac { 1 } { n } \\sum _ { i = 1 } ^ { n } I ( y _ { i } \\neq \\hat { y } _ { i } ) .$$  \n/negationslash  \n/negationslash  \n/negationslash  \nEquation 2.8 is referred to as the training error rate because it is computed based on the data that was used to train our classifier. As in the regression setting, we are most interested in the error rates that result from applying our classifier to test observations that were not used in training. The test error rate associated with a set of test observations of the form ( x 0  y 0 ) is given by  \nHere ˆ y i is the predicted class label for the i th observation using ˆ f . And I ( y i = ˆ y i ) is an indicator variable that equals 1 if y i = ˆ y i and zero if y i = ˆ y i . If I ( y i = ˆ y i ) = 0 then the i th observation was classified correctly by our classification method; otherwise it was misclassified. Hence Equation 2.8 computes the fraction of incorrect classifications.  \n$$\\ A v e \\left ( I ( y _ { 0 } \\neq \\hat { y } _ { 0 } ) \\right ) ,$$  \nwhere ˆ y 0 is the predicted class label that results from applying the classifier to the test observation with predictor x 0 . A good classifier is one for which the test error (2.9) is smallest.\nThe Bayes Classifier  \nIt is possible to show (though the proof is outside of the scope of this book) that the test error rate given in (2.9) is minimized, on average, by a very simple classifier that assigns each observation to the most likely class, given its predictor values . In other words, we should simply assign a test observation with predictor vector x 0 to the class j for which  \n$$\\Pr ( Y = j | X = x _ { 0 } )$$  \nis largest. Note that (2.10) is a conditional probability : it is the probability that Y = j , given the observed predictor vector x 0 . This very simple classifier is called the Bayes classifier . In a two-class problem where there are only two possible response values, say class 1 or class 2 , the Bayes classifier corresponds to predicting class one if Pr( Y = 1 ♣ X = x 0 ) &gt; 0   5 , and class two otherwise.  \nFigure 2.13 provides an example using a simulated data set in a twodimensional space consisting of predictors X 1 and X 2 . The orange and blue circles correspond to training observations that belong to two different classes. For each value of X 1 and X 2 , there is a different probability of the response being orange or blue. Since this is simulated data, we know how the data were generated and we can calculate the conditional probabilities for each value of X 1 and X 2 . The orange shaded region reflects the set of points for which Pr( Y = orange ♣ X ) is greater than 50 % , while the blue shaded region indicates the set of points for which the probability is below 50 % . The purple dashed line represents the points where the probability is exactly 50 % . This is called the Bayes decision boundary . The Bayes classifier's prediction is determined by the Bayes decision boundary; an observation that falls on the orange side of the boundary will be assigned  \n/negationslash indicator variable  \ntraining error test error  \nconditional probability Bayes classifier  \nBayes decision boundary  \nFIGURE 2.13. A simulated data set consisting of 100 observations in each of two groups, indicated in blue and in orange. The purple dashed line represents the Bayes decision boundary. The orange background grid indicates the region in which a test observation will be assigned to the orange class, and the blue background grid indicates the region in which a test observation will be assigned to the blue class.  \n<!-- image -->  \nto the orange class, and similarly an observation on the blue side of the boundary will be assigned to the blue class.  \nThe Bayes classifier produces the lowest possible test error rate, called the Bayes error rate . Since the Bayes classifier will always choose the class for which (2.10) is largest, the error rate will be 1 -max j Pr( Y = j ♣ X = x 0 ) at X = x 0 . In general, the overall Bayes error rate is given by  \n$$1 - E \\left ( \\max _ { j } \\Pr ( Y = j | X ) \\right ) ,$$  \nwhere the expectation averages the probability over all possible values of X . For our simulated data, the Bayes error rate is 0   133 . It is greater than zero, because the classes overlap in the true population, which implies that max j Pr( Y = j ♣ X = x 0 ) &lt; 1 for some values of x 0 . The Bayes error rate is analogous to the irreducible error, discussed earlier.\nK -Nearest Neighbors  \nIn theory we would always like to predict qualitative responses using the Bayes classifier. But for real data, we do not know the conditional distribution of Y given X , and so computing the Bayes classifier is impossible. Therefore, the Bayes classifier serves as an unattainable gold standard against which to compare other methods. Many approaches attempt to estimate the conditional distribution of Y given X , and then classify a given observation to the class with highest estimated probability. One such method is the K -nearest neighbors (KNN) classifier. Given a positive in-  \nBayes error rate  \nK -nearest neighbors  \nteger K and a test observation x 0 , the KNN classifier first identifies the K points in the training data that are closest to x 0 , represented by N 0 . It then estimates the conditional probability for class j as the fraction of points in N 0 whose response values equal j :  \n$$\\Pr ( Y = j | X = x _ { 0 } ) = \\frac { 1 } { K } \\sum _ { i \\in \\mathcal { N } _ { ( 0 ) } } I ( y _ { i } = j ) .$$  \nFinally, KNN classifies the test observation x 0 to the class with the largest probability from (2.12).  \nDespite the fact that it is a very simple approach, KNN can often produce classifiers that are surprisingly close to the optimal Bayes classifier. Figure 2.15 displays the KNN decision boundary, using K = 10 , when applied to the larger simulated data set from Figure 2.13. Notice that even though the true distribution is not known by the KNN classifier, the KNN decision boundary is very close to that of the Bayes classifier. The test error rate using KNN is 0   1363 , which is close to the Bayes error rate of 0   1304 .  \nFigure 2.14 provides an illustrative example of the KNN approach. In the left-hand panel, we have plotted a small training data set consisting of six blue and six orange observations. Our goal is to make a prediction for the point labeled by the black cross. Suppose that we choose K = 3 . Then KNN will first identify the three observations that are closest to the cross. This neighborhood is shown as a circle. It consists of two blue points and one orange point, resulting in estimated probabilities of 2 glyph[triangleleft] 3 for the blue class and 1 glyph[triangleleft] 3 for the orange class. Hence KNN will predict that the black cross belongs to the blue class. In the right-hand panel of Figure 2.14 we have applied the KNN approach with K = 3 at all of the possible values for X 1 and X 2 , and have drawn in the corresponding KNN decision boundary.  \nThe choice of K has a drastic effect on the KNN classifier obtained. Figure 2.16 displays two KNN fits to the simulated data from Figure 2.13, using K = 1 and K = 100 . When K = 1 , the decision boundary is overly flexible and finds patterns in the data that don't correspond to the Bayes decision boundary. This corresponds to a classifier that has low bias but very high variance. As K grows, the method becomes less flexible and produces a decision boundary that is close to linear. This corresponds to a low-variance but high-bias classifier. On this simulated data set, neither K = 1 nor K = 100 give good predictions: they have test error rates of 0   1695 and 0   1925 , respectively.  \nJust as in the regression setting, there is not a strong relationship between the training error rate and the test error rate. With K = 1 , the KNN training error rate is 0 , but the test error rate may be quite high. In general, as we use more flexible classification methods, the training error rate will decline but the test error rate may not. In Figure 2.17, we have plotted the KNN test and training errors as a function of 1 glyph[triangleleft]K . As 1 glyph[triangleleft]K increases, the method becomes more flexible. As in the regression setting, the training error rate consistently declines as the flexibility increases. However, the test error exhibits a characteristic U-shape, declining at first (with a minimum at approximately K = 10 ) before increasing again when the method becomes excessively flexible and overfits.  \nFIGURE 2.14. The KNN approach, using K = 3 , is illustrated in a simple situation with six blue observations and six orange observations. Left: a test observation at which a predicted class label is desired is shown as a black cross. The three closest points to the test observation are identified, and it is predicted that the test observation belongs to the most commonly-occurring class, in this case blue. Right: The KNN decision boundary for this example is shown in black. The blue grid indicates the region in which a test observation will be assigned to the blue class, and the orange grid indicates the region in which it will be assigned to the orange class.  \n<!-- image -->  \nKNN: K=10  \nFIGURE 2.15. The black curve indicates the KNN decision boundary on the data from Figure 2.13, using K = 10 . The Bayes decision boundary is shown as a purple dashed line. The KNN and Bayes decision boundaries are very similar.  \n<!-- image -->  \nIn both the regression and classification settings, choosing the correct level of flexibility is critical to the success of any statistical learning method. The bias-variance tradeoff, and the resulting U-shape in the test error, can make this a difficult task. In Chapter 5, we return to this topic and discuss  \nFIGURE 2.16. A comparison of the KNN decision boundaries (solid black curves) obtained using K = 1 and K = 100 on the data from Figure 2.13. With K = 1 , the decision boundary is overly flexible, while with K = 100 it is not sufficiently flexible. The Bayes decision boundary is shown as a purple dashed line.  \n<!-- image -->  \nFIGURE 2.17. The KNN training error rate (blue, 200 observations) and test error rate (orange, 5,000 observations) on the data from Figure 2.13, as the level of flexibility (assessed using 1 glyph[triangleleft]K on the log scale) increases, or equivalently as the number of neighbors K decreases. The black dashed line indicates the Bayes error rate. The jumpiness of the curves is due to the small size of the training data set.  \n<!-- image -->  \nvarious methods for estimating test error rates and thereby choosing the optimal level of flexibility for a given statistical learning method."", ""## 4.5 A Comparison of Classification Methods  \n### 4.5.1 An Analytical Comparison  \nWenow perform an analytical (or mathematical) comparison of LDA, QDA, naive Bayes, and logistic regression. We consider these approaches in a setting with K classes, so that we assign an observation to the class that maximizes Pr( Y = k ♣ X = x ) . Equivalently, we can set K as the baseline class and assign an observation to the class that maximizes  \n$$\\log \\left ( \\frac { \\Pr ( Y = k | X = x ) } { \\Pr ( Y = K | X = x ) } \\right )$$  \nfor k = 1         K . Examining the specific form of (4.31) for each method provides a clear understanding of their similarities and differences.  \nFirst, for LDA, we can make use of Bayes' theorem (4.15) as well as the assumption that the predictors within each class are drawn from a multivariate normal density (4.23) with class-specific mean and shared co-  \nvariance matrix in order to show that  \n$$variance matrix in order to show that \\\\ \\log \\left ( \\frac { \\Pr ( Y = k | X = x ) } { \\Pr ( Y = K | X = x ) } \\right ) & \\ = \\ \\log \\left ( \\frac { \\pi _ { k } f _ { k } ( x ) } { \\pi _ { K } f _ { K } ( x ) } \\right ) \\\\ & = \\ \\log \\left ( - \\left ( \\pi _ { K } \\exp \\left ( - \\frac { 1 } { 2 } ( x - \\mu _ { K } ) ^ { T } \\Sigma ^ { - 1 } ( x - \\mu _ { K } ) \\right ) \\right ) \\\\ & = \\ \\log \\left ( \\frac { \\pi _ { K } } { \\pi _ { K } } \\right ) - \\frac { 1 } { 2 } ( x - \\mu _ { k } ) ^ { T } \\Sigma ^ { - 1 } ( x - \\mu _ { k } ) \\\\ & + \\frac { 1 } { 2 } ( x - \\mu _ { K } ) ^ { T } \\Sigma ^ { - 1 } ( x - \\mu _ { K } ) \\\\ & = \\ \\log \\left ( \\frac { \\pi _ { K } } { \\pi _ { K } } \\right ) - \\frac { 1 } { 2 } ( \\mu _ { k } + \\mu _ { K } ) ^ { T } \\Sigma ^ { - 1 } ( \\mu _ { k } - \\mu _ { K } ) \\\\ & + x ^ { T } \\Sigma ^ { - 1 } ( \\mu _ { k } - \\mu _ { K } ) \\\\ & = \\ a _ { k } + \\sum _ { j = 1 } ^ { p } b _ { k j } x _ { j } , \\\\ \\intertext { w h e r a } \\text {where } a _ { k } = \\log \\left ( \\frac { \\pi _ { K } } { \\pi _ { K } } \\right ) - \\frac { 1 } { 2 } ( \\mu _ { k } + \\mu _ { K } ) ^ { T } \\Sigma ^ { - 1 } ( \\mu _ { k } - \\mu _ { K } ) \\text { and } b _ { k j } \\text { is the } j h \\\\ \\text {component of } \\mathbf f ^ { - 1 } ( \\mu _ { k } - \\mu _ { K } ) , \\text { hence } \\text {LDA} , \\text { like logistic regression} \\text { assumes }$$  \nwhere a k = log ( π k π K ) -1 2 ( θ k + θ K ) T Σ -1 ( θ k -θ K ) and b kj is the j th component of Σ -1 ( θ k -θ K ) . Hence LDA, like logistic regression, assumes that the log odds of the posterior probabilities is linear in x .  \nUsing similar calculations, in the QDA setting (4.31) becomes  \n$$\\log \\left ( \\frac { \\Pr ( Y = k | X = x ) } { \\Pr ( Y = K | X = x ) } \\right ) = a _ { k } + \\sum _ { j = 1 } ^ { p } b _ { k j } x _ { j } + \\sum _ { j = 1 } ^ { p } \\sum _ { l = 1 } ^ { p } c _ { k j l } x _ { j } x _ { l } ,$$  \nwhere a k  b kj , and c kjl are functions of π k  π K  θ k  θ K  Σ k and Σ K . Again, as the name suggests, QDA assumes that the log odds of the posterior probabilities is quadratic in x .  \nFinally, we examine (4.31) in the naive Bayes setting. Recall that in this setting, f k ( x ) is modeled as a product of p one-dimensional functions f kj ( x j ) for j = 1         p . Hence,  \n$$\\text {this setting, } f _ { k } ( x _ { j } ) & \\text { is modeled as a product of $p$ one-dimensional functions} \\\\ f _ { k j } ( x _ { j } ) & \\text { for $j=1,\\dots,p$. Hence,} \\\\ & \\log \\left ( \\frac { \\Pr ( Y = k | X = x ) } { \\Pr ( Y = K | X = x ) } \\right ) \\quad \\equiv \\quad \\log \\left ( \\frac { \\pi _ { k } f _ { k } ( x ) } { \\pi _ { K } f _ { K } ( x ) } \\right ) \\\\ & \\quad = \\quad \\log \\left ( \\frac { \\pi _ { k } \\prod _ { j = 1 } ^ { p } f _ { k j } ( x _ { j } ) } { \\pi _ { K } \\prod _ { j = 1 } ^ { p } f _ { K j } ( x _ { j } ) } \\right ) \\\\ & \\quad = \\quad \\log \\left ( \\frac { \\pi _ { k } } { \\pi _ { K } } \\right ) + \\sum _ { j = 1 } ^ { p } \\log \\left ( \\frac { f _ { k j } ( x _ { j } ) } { f _ { K j } ( x _ { j } ) } \\right ) \\\\ & \\quad = \\quad a _ { k } + \\sum _ { j = 1 } ^ { p } g _ { k j } ( x _ { j } ) , \\\\ \\intertext { where } \\ a _ { k } & = \\log \\left ( \\frac { \\pi _ { k } } { \\pi _ { K } } \\right ) \\text { and } g _ { k j } ( x _ { j } ) = \\log \\left ( \\frac { f _ { k j } ( x _ { j } ) } { f _ { K j } ( x _ { j } ) } \\right ) . \\text { Hence, the right-hand}$$  \nwhere a k = log ( π k π K ) and g kj ( x j ) = log ( f kj ( x j ) f Kj ( x j ) ) . Hence, the right-hand side of (4.34) takes the form of a generalized additive model , a topic that is discussed further in Chapter 7.  \nInspection of (4.32), (4.33), and (4.34) yields the following observations about LDA, QDA, and naive Bayes:  \n- LDA is a special case of QDA with c kjl = 0 for all j = 1         p , l = 1         p , and k = 1         K . (Of course, this is not surprising, since LDAis simply a restricted version of QDA with Σ 1 = · · · = Σ K = Σ .)\n- Any classifier with a linear decision boundary is a special case of naive Bayes with g kj ( x j ) = b kj x j . In particular, this means that LDA is a special case of naive Bayes! This is not at all obvious from the descriptions of LDA and naive Bayes earlier in this chapter, since each method makes very different assumptions: LDA assumes that the features are normally distributed with a common within-class covariance matrix, and naive Bayes instead assumes independence of the features.\n- If we model f kj ( x j ) in the naive Bayes classifier using a one-dimensional Gaussian distribution N ( θ kj  σ 2 j ) , then we end up with g kj ( x j ) = b kj x j where b kj = ( θ kj -θ Kj ) glyph[triangleleft] σ 2 j . In this case, naive Bayes is actually a special case of LDA with Σ restricted to be a diagonal matrix with j th diagonal element equal to σ 2 j .  \n/negationslash  \n- Neither QDA nor naive Bayes is a special case of the other. Naive Bayes can produce a more flexible fit, since any choice can be made for g kj ( x j ) . However, it is restricted to a purely additive fit, in the sense that in (4.34), a function of x j is added to a function of x l , for j = l ; however, these terms are never multiplied. By contrast, QDA includes multiplicative terms of the form c kjl x j x l . Therefore, QDA has the potential to be more accurate in settings where interactions among the predictors are important in discriminating between classes.  \nNone of these methods uniformly dominates the others: in any setting, the choice of method will depend on the true distribution of the predictors in each of the K classes, as well as other considerations, such as the values of n and p . The latter ties into the bias-variance trade-off.  \nHow does logistic regression tie into this story? Recall from (4.12) that multinomial logistic regression takes the form  \n$$\\log \\left ( \\frac { \\Pr ( Y = k | X = x ) } { \\Pr ( Y = K | X = x ) } \\right ) = \\beta _ { k 0 } + \\sum _ { j = 1 } ^ { p } \\beta _ { k j } x _ { j } .$$  \nThis is identical to the linear form of LDA (4.32): in both cases, log ( Pr( Y = k ♣ X = x ) Pr( Y = K ♣ X = x ) ) is a linear function of the predictors. In LDA, the coefficients in this linear function are functions of estimates for π k , π K , θ k , θ K , and Σ obtained by assuming that X 1         X p follow a normal distribution within each class. By contrast, in logistic regression, the coefficients are chosen to maximize the likelihood function (4.5). Thus, we expect LDA to outperform logistic regression when the normality assumption (approximately) holds, and we expect logistic regression to perform better when it does not.  \nWe close with a brief discussion of K -nearest neighbors (KNN), introduced in Chapter 2. Recall that KNN takes a completely different approach from the classifiers seen in this chapter. In order to make a prediction for an observation X = x , the training observations that are closest to x are identified. Then X is assigned to the class to which the plurality of these observations belong. Hence KNN is a completely non-parametric approach: no assumptions are made about the shape of the decision boundary. We make the following observations about KNN:  \n- Because KNN is completely non-parametric, we can expect this approach to dominate LDA and logistic regression when the decision boundary is highly non-linear, provided that n is very large and p is small.\n- In order to provide accurate classification, KNN requires a lot of observations relative to the number of predictors-that is, n much larger than p . This has to do with the fact that KNN is non-parametric, and thus tends to reduce the bias while incurring a lot of variance.\n- In settings where the decision boundary is non-linear but n is only modest, or p is not very small, then QDA may be preferred to KNN. This is because QDA can provide a non-linear decision boundary while taking advantage of a parametric form, which means that it requires a smaller sample size for accurate classification, relative to KNN.\n- Unlike logistic regression, KNN does not tell us which predictors are important: we don't get a table of coefficients as in Table 4.3.""]","The 'naive' assumption made by the Naive Bayes classifier is that within each class, the predictors are independent. This means that the joint distribution of the predictors is assumed to be the product of their individual distributions. This assumption simplifies the estimation of the p-dimensional density function by eliminating the need to consider the association between different predictors.","The Naive Bayes classifier makes the 'naive' assumption that within each class, the predictor variables are independent of each other. This simplification allows for easier estimation of the joint probability distribution.",1.0,,0.8333333332916666,1.0
How does logistic regression handle binary classification problems?,"[""## 4.4 Generative Models for Classification  \nLogistic regression involves directly modeling Pr( Y = k ♣ X = x ) using the logistic function, given by (4.7) for the case of two response classes. In statistical jargon, we model the conditional distribution of the response Y , given the predictor(s) X . We now consider an alternative and less direct approach to estimating these probabilities. In this new approach, we model the distribution of the predictors X separately in each of the response classes (i.e. for each value of Y ). We then use Bayes' theorem to flip these around into estimates for Pr( Y = k ♣ X = x ) . When the distribution of X within each class is assumed to be normal, it turns out that the model is very similar in form to logistic regression.  \nWhy do we need another method, when we have logistic regression? There are several reasons:  \n- When there is substantial separation between the two classes, the parameter estimates for the logistic regression model are surprisingly unstable. The methods that we consider in this section do not suffer from this problem.\n- If the distribution of the predictors X is approximately normal in each of the classes and the sample size is small, then the approaches in this section may be more accurate than logistic regression.\n- The methods in this section can be naturally extended to the case of more than two response classes. (In the case of more than two response classes, we can also use multinomial logistic regression from Section 4.3.5.)  \nSuppose that we wish to classify an observation into one of K classes, where K ≥ 2 . In other words, the qualitative response variable Y can take on K possible distinct and unordered values. Let π k represent the overall or prior probability that a randomly chosen observation comes from the k th class. Let f k ( X ) ≡ Pr( X ♣ Y = k ) 1 denote the density function of X for an observation that comes from the k th class. In other words, f k ( x ) is relatively large if there is a high probability that an observation in the k th class has X ≈ x , and f k ( x ) is small if it is very unlikely that an observation in the k th class has X ≈ x . Then Bayes' theorem states that  \n1 Technically, this definition is only correct if X is a qualitative random variable. If X is quantitative, then f k ( x ) dx corresponds to the probability of X falling in a small region dx around x .  \nprior density function  \nBayes' theorem  \n$$\\Pr ( Y = k | X = x ) = \\frac { \\pi _ { k } f _ { k } ( x ) } { \\sum _ { l = 1 } ^ { K } \\pi _ { l } f _ { l } ( x ) } . \\\\ \\intertext { c l e w h o r e a r i l e n t a t i o n , w e w l l u s e t h e a b r e v i a t i o n p _ { k } ( x ) = } | X \\, - \\, x \\rangle \\colon \\text { this is the probability } \\, \\text { that } \\, \\text { an observation }$$  \nEquation 4.15 suggests that instead of directly computing the posterior probability p k ( x ) as in Section 4.3.1, we can simply plug in estimates of π k and f k ( x ) into (4.15). In general, estimating π k is easy if we have a random sample from the population: we simply compute the fraction of the training observations that belong to the k th class. However, estimating the density function f k ( x ) is much more challenging. As we will see, to estimate f k ( x ) , we will typically have to make some simplifying assumptions.  \nIn accordance with our earlier notation, we will use the abbreviation p k ( x ) = Pr( Y = k ♣ X = x ) ; this is the posterior probability that an observation X = x belongs to the k th class. That is, it is the probability that the observation belongs to the k th class, given the predictor value for that observation.  \nWe know from Chapter 2 that the Bayes classifier, which classifies an observation x to the class for which p k ( x ) is largest, has the lowest possible error rate out of all classifiers. (Of course, this is only true if all of the terms in (4.15) are correctly specified.) Therefore, if we can find a way to estimate f k ( x ) , then we can plug it into (4.15) in order to approximate the Bayes classifier.  \nIn the following sections, we discuss three classifiers that use different estimates of f k ( x ) in (4.15) to approximate the Bayes classifier: linear discriminant analysis, quadratic discriminant analysis, and naive Bayes ."", '### 4.3.1 The Logistic Model  \nHow should we model the relationship between p ( X ) = Pr( Y = 1 ♣ X ) and X ? (For convenience we are using the generic 0/1 coding for the response.) In Section 4.2 we considered using a linear regression model to represent these probabilities:  \n$$p ( X ) = \\beta _ { 0 } + \\beta _ { 1 } X .$$  \nIf we use this approach to predict default = Yes using balance , then we obtain the model shown in the left-hand panel of Figure 4.2. Here we see the problem with this approach: for balances close to zero we predict a negative probability of default; if we were to predict for very large balances, we would get values bigger than 1. These predictions are not sensible, since of course the true probability of default, regardless of credit card balance, must fall between 0 and 1 . This problem is not unique to the credit default data. Any time a straight line is fit to a binary response that is coded as 0 or 1 , in principle we can always predict p ( X ) &lt; 0 for some values of X and p ( X ) &gt; 1 for others (unless the range of X is limited).  \nTo avoid this problem, we must model p ( X ) using a function that gives outputs between 0 and 1 for all values of X . Many functions meet this description. In logistic regression, we use the logistic function ,  \n$$p ( X ) = \\frac { e ^ { \\beta _ { 0 } + \\beta _ { 1 } X } } { 1 + e ^ { \\beta _ { 0 } + \\beta _ { 1 } X } } .$$  \nTo fit the model (4.2), we use a method called maximum likelihood , which we discuss in the next section. The right-hand panel of Figure 4.2 illustrates the fit of the logistic regression model to the Default data. Notice that for logistic function  \nmaximum likelihood  \nlow balances we now predict the probability of default as close to, but never below, zero. Likewise, for high balances we predict a default probability close to, but never above, one. The logistic function will always produce an S-shaped curve of this form, and so regardless of the value of X , we will obtain a sensible prediction. We also see that the logistic model is better able to capture the range of probabilities than is the linear regression model in the left-hand plot. The average fitted probability in both cases is 0.0333 (averaged over the training data), which is the same as the overall proportion of defaulters in the data set.  \nAfter a bit of manipulation of (4.2), we find that  \n$$\\frac { p ( X ) } { 1 - p ( X ) } = e ^ { \\beta _ { 0 } + \\beta _ { 1 } X } . \\\\ [ / ] 1 = p ( X ) ] \\text { is called the } \\sigma d d _ { s } \\text { and } \\text {can take on an } \\alpha \\text { value}$$  \nBy taking the logarithm of both sides of (4.3), we arrive at  \nThe quantity p ( X ) glyph[triangleleft] [1 -p ( X )] is called the odds , and can take on any value between 0 and ∞ . Values of the odds close to 0 and ∞ indicate very low and very high probabilities of default, respectively. For example, on average 1 in 5 people with an odds of 1 glyph[triangleleft] 4 will default, since p ( X ) = 0   2 implies an odds of 0   2 1 -0   2 = 1 glyph[triangleleft] 4 . Likewise, on average nine out of every ten people with an odds of 9 will default, since p ( X ) = 0   9 implies an odds of 0   9 1 -0   9 = 9 . Odds are traditionally used instead of probabilities in horse-racing, since they relate more naturally to the correct betting strategy.  \n$$\\log \\left ( \\frac { \\ p ( X ) } { 1 - p ( X ) } \\right ) & = \\beta _ { 0 } + \\beta _ { 1 } X . \\\\ \\text {side is called the } \\log \\alpha \\, \\underset { \\ } o d d s \\, \\text { or } \\, \\log i t ; \\text { We see that the logistic }$$  \nRecall from Chapter 3 that in a linear regression model, β 1 gives the average change in Y associated with a one-unit increase in X . By contrast, in a logistic regression model, increasing X by one unit changes the log odds by β 1 (4.4). Equivalently, it multiplies the odds by e β 1 (4.3). However, because the relationship between p ( X ) and X in (4.2) is not a straight line, β 1 does not correspond to the change in p ( X ) associated with a one-unit increase in X . The amount that p ( X ) changes due to a one-unit change in X depends on the current value of X . But regardless of the value of X , if β 1 is positive then increasing X will be associated with increasing p ( X ) , and if β 1 is negative then increasing X will be associated with decreasing p ( X ) . The fact that there is not a straight-line relationship between p ( X ) and X , and the fact that the rate of change in p ( X ) per unit change in X depends on the current value of X , can also be seen by inspection of the right-hand panel of Figure 4.2.  \nThe left-hand side is called the log odds or logit . We see that the logistic regression model (4.2) has a logit that is linear in X .', '## 4.3 Logistic Regression  \nConsider again the Default data set, where the response default falls into one of two categories, Yes or No . Rather than modeling this response Y directly, logistic regression models the probability that Y belongs to a particular category.  \nFor the Default data, logistic regression models the probability of default. For example, the probability of default given balance can be written as  \n$$\\Pr ( \\text {default} = \\text {Yes} | \\text {balance} ) .$$  \nThe values of Pr( default = Yes ♣ balance ) , which we abbreviate p ( balance ) , will range between 0 and 1. Then for any given value of balance , a prediction can be made for default . For example, one might predict default = Yes binary  \nFIGURE 4.2. Classification using the Default data. Left: Estimated probability of default using linear regression. Some estimated probabilities are negative! The orange ticks indicate the 0/1 values coded for default ( No or Yes ). Right: Predicted probabilities of default using logistic regression. All probabilities lie between 0 and 1 .  \n<!-- image -->  \nfor any individual for whom p ( balance ) &gt; 0   5 . Alternatively, if a company wishes to be conservative in predicting individuals who are at risk for default, then they may choose to use a lower threshold, such as p ( balance ) &gt; 0   1 .', ""## 4.2 Why Not Linear Regression?  \nWe have stated that linear regression is not appropriate in the case of a qualitative response. Why not?  \nSuppose that we are trying to predict the medical condition of a patient in the emergency room on the basis of her symptoms. In this simplified example, there are three possible diagnoses: stroke , drug overdose , and  \nFIGURE 4.1. The Default data set. Left: The annual incomes and monthly credit card balances of a number of individuals. The individuals who defaulted on their credit card payments are shown in orange, and those who did not are shown in blue. Center: Boxplots of balance as a function of default status. Right: Boxplots of income as a function of default status.  \n<!-- image -->  \nepileptic seizure . We could consider encoding these values as a quantitative response variable, Y , as follows:  \n$$Y = \\begin{cases} 1 & \\text {if stroke;} \\\\ 2 & \\text {if drug overdose;} \\\\ 3 & \\text {if epileptic seeizure.} \\end{cases} \\\\ \\text {, least squares could be used to fit a linear matrix} \\, .$$  \nUsing this coding, least squares could be used to fit a linear regression model to predict Y on the basis of a set of predictors X 1         X p . Unfortunately, this coding implies an ordering on the outcomes, putting drug overdose in between stroke and epileptic seizure , and insisting that the difference between stroke and drug overdose is the same as the difference between drug overdose and epileptic seizure . In practice there is no particular reason that this needs to be the case. For instance, one could choose an equally reasonable coding,  \n$$Y = \\begin{cases} 1 & \\text {if elliptic seeirize} ; \\\\ 2 & \\text {if stroke} ; \\\\ 3 & \\text {if drug overdose} , \\end{cases} \\\\ \\text {only a totally different relationship among} \\\\ \\text {these codings would produce fundamentally}$$  \nIf the response variable's values did take on a natural ordering, such as mild , moderate , and severe , and we felt the gap between mild and moderate was similar to the gap between moderate and severe, then a 1, 2, 3 coding would be reasonable. Unfortunately, in general there is no natural way to which would imply a totally different relationship among the three conditions. Each of these codings would produce fundamentally different linear models that would ultimately lead to different sets of predictions on test observations.  \nconvert a qualitative response variable with more than two levels into a quantitative response that is ready for linear regression.  \nFor a binary (two level) qualitative response, the situation is better. For instance, perhaps there are only two possibilities for the patient's medical condition: stroke and drug overdose . We could then potentially use the dummy variable approach from Section 3.3.1 to code the response as follows:  \n$$Y = \\begin{cases} 0 & \\text {if stroke;} \\\\ 1 & \\text {if drug overdose.} \\end{cases}$$  \nWe could then fit a linear regression to this binary response, and predict drug overdose if ˆ Y &gt; 0   5 and stroke otherwise. In the binary case it is not hard to show that even if we flip the above coding, linear regression will produce the same final predictions.  \nTo summarize, there are at least two reasons not to perform classification using a regression method: (a) a regression method cannot accommodate a qualitative response with more than two classes; (b) a regression method will not provide meaningful estimates of Pr( Y ♣ X ) , even with just two classes. Thus, it is preferable to use a classification method that is truly suited for qualitative response values. In the next section, we present logistic regression, which is well-suited for the case of a binary qualitative response; in later sections we will cover classification methods that are appropriate when the qualitative response has two or more classes.  \nFor a binary response with a 0/1 coding as above, regression by least squares is not completely unreasonable: it can be shown that the X ˆ β obtained using linear regression is in fact an estimate of Pr( drug overdose ♣ X ) in this special case. However, if we use linear regression, some of our estimates might be outside the [0  1] interval (see Figure 4.2), making them hard to interpret as probabilities! Nevertheless, the predictions provide an ordering and can be interpreted as crude probability estimates. Curiously, it turns out that the classifications that we get if we use linear regression to predict a binary response will be the same as for the linear discriminant analysis (LDA) procedure we discuss in Section 4.4.""]","Logistic regression handles binary classification problems by modeling the probability that a given observation belongs to a particular category. For a binary response, logistic regression models the probability of the response variable \( Y \) being 1 (or ""Yes"") given the predictor(s) \( X \). This probability is denoted as \( p(X) = \Pr(Y = 1 \mid X) \).

The logistic regression model uses the logistic function to ensure that the predicted probabilities are between 0 and 1 for all values of \( X \). The logistic function is given by:

\[ 
p(X) = \frac{e^{\beta_0 + \beta_1 X}}{1 + e^{\beta_0 + \beta_1 X}} 
\]

Here, \( \beta_0 \) and \( \beta_1 \) are the parameters of the model that are estimated using a method called maximum likelihood. This approach ensures that the predicted probabilities are sensible, as they fall within the range of 0 to 1.

The logistic regression model can be expressed in terms of odds and log odds (logit). The odds are defined as:

\[ 
\frac{p(X)}{1 - p(X)} = e^{\beta_0 + \beta_1 X} 
\]

Taking the logarithm of both sides gives the log odds or logit:

\[ 
\log \left( \frac{p(X)}{1 - p(X)} \right) = \beta_0 + \beta_1 X 
\]

In this model, increasing \( X \) by one unit changes the log odds by \( \beta_1 \), and it multiplies the odds by \( e^{\beta_1} \). The logistic regression model is well-suited for binary classification problems because it provides meaningful estimates of the probability that \( Y \) belongs to a particular category, given \( X \).","Logistic regression models the probability that the response variable belongs to a particular category by using the logistic function. This function takes a linear combination of the predictors and transforms it into a value between 0 and 1, which represents the probability of the outcome.",1.0,,0.999999999975,1.0
¿Qué es la curva ROC y qué representa el área bajo la curva (AUC)?,"[""## 11.7 Additional Topics  \n### 11.7.1 Area Under the Curve for Survival Analysis  \nIn Chapter 4, we introduced the area under the ROC curve - often referred to as the 'AUC' - as a way to quantify the performance of a two-class classifier. Define the score for the i th observation to be the classifier's estimate of Pr( Y = 1 ♣ X = x i ) . It turns out that if we consider all pairs consisting of one observation in Class 1 and one observation in Class 2, then the AUC is the fraction of pairs for which the score for the observation in Class 1 exceeds the score for the observation in Class 2.  \nThis suggests a way to generalize the notion of AUC to survival analysis. We calculate an estimated risk score, ˆ η i = ˆ β 1 x i 1 + · · · + ˆ β p x ip , for i = 1         n , using the Cox model coefficients. If ˆ η i ′ &gt; ˆ η i , then the model predicts that the i ′ th observation has a larger hazard than the i th observation, and thus that the survival time t i will be greater than t i ′ . Thus, it is tempting to try to generalize AUC by computing the proportion of observations for which t i &gt; t i ′ and ˆ η i ′ &gt; ˆ η i . However, things are not quite so easy, because recall that we do not observe t 1         t n ; instead, we observe  \nFIGURE 11.8. For the Publication data introduced in Section 11.5.4, we compute tertiles of 'risk' in the test set using coefficients estimated on the training set. There is clear separation between the resulting survival curves.  \n<!-- image -->  \nthe (possibly-censored) times y 1         y n , as well as the censoring indicators δ 1         δ n .  \nTherefore, Harrell's concordance index (or C -index ) computes the proportion of observation pairs for which ˆ η i ′ &gt; ˆ η i and y i &gt; y i ′ :  \n$$C = \\frac { \\sum _ { i , i ^ { \\prime } \\colon y _ { i } > y _ { i ^ { \\prime } } } I ( \\hat { \\eta } _ { i ^ { \\prime } } > \\hat { \\eta } _ { i } ) \\delta _ { i ^ { \\prime } } } { \\sum _ { i , i ^ { \\prime } \\colon y _ { i } > y _ { i ^ { \\prime } } } \\delta _ { i ^ { \\prime } } } , \\\\ \\text {for variable } I ( \\hat { \\eta } _ { i ^ { \\prime } } > \\hat { \\eta } _ { i } ) \\text { equals one if } \\hat { \\eta } _ { i ^ { \\prime } }$$  \nWe fit a Cox proportional hazards model on the training set of the Publication data, and computed the C -index on the test set. This yielded C = 0   733 . Roughly speaking, given two random papers from the test set, the model can predict with 73.3% accuracy which will be published first.  \nwhere the indicator variable I (ˆ η i ′ &gt; ˆ η i ) equals one if ˆ η i ′ &gt; ˆ η i , and equals zero otherwise. The numerator and denominator are multiplied by the status indicator δ i ′ , since if the i ′ th observation is uncensored (i.e. if δ i ′ = 1 ), then y i &gt; y i ′ implies that t i &gt; t i ′ . By contrast, if δ i ′ = 0 , then y i &gt; y i ′ does not imply that t i &gt; t i ′ ."", ""### 4.4.2 Linear Discriminant Analysis for p &gt; 1  \nWe now extend the LDA classifier to the case of multiple predictors. To do this, we will assume that X = ( X 1  X 2         X p ) is drawn from a multivariate Gaussian (or multivariate normal) distribution, with a class-specific mean vector and a common covariance matrix. We begin with a brief review of this distribution.  \n$$\\text {is defined as} \\\\ f ( x ) = \\frac { 1 } { ( 2 \\pi ) ^ { p / 2 } | \\Sigma | ^ { 1 / 2 } } \\exp \\left ( - \\frac { 1 } { 2 } ( x - \\mu ) ^ { T } \\Sigma ^ { - 1 } ( x - \\mu ) \\right ) . \\\\ \\text {In the case of } \\eta \\geq 1 \\, \\underset { p } { \\underset { p } { \\underset { p } { \\underset { p } { \\underset { p } { \\underset { p } { \\underset { p } { \\underset { p } { \\underset { p } { \\underset { p } { \\underset { p } { \\underset { p } { \\underset { p } { \\underset { p } { \\underset { p } { \\underset { p } { \\underset { p } { \\underset { p } { \\underset { p } { \\underset { p } { \\underset { p } { \\underset { p } { \\underset { p } { \\underset { p } { \\underset { p } { \\underset { p } { \\underset { p } { \\underset { p } { \\underset { p } { \\underset { p } { \\underset { p } { \\underset { p } { \\underset { p } { \\underset { p } { \\underset { p } { \\underset { p } { \\underset { p } { \\underset { p } { \\underset { p } { \\underset { p } { \\underset { p } { \\underset { p } { \\underset { p } { \\underset { p } { \\underset { p } { \\underset { p } { \\underset { p } { \\underset { p } { \\underset { p } { \\underset { p } { \\underset { p } { \\underset { p } { \\underset { p } { \\underset { p } { \\underset { p } { \\underset { p } { \\underset { p } { \\underset { p } { \\underset { p } { \\underset { p } { \\underset { p } { \\underset { p } { \\underset { p } { \\underset { p } { \\underset { p } { \\underset { p } { \\underset { p } { \\underset { p } { \\underset { p } { \\underset { p } { \\underset { p } { \\underset { p } { \\underset { p } { \\underset { p } { \\underset { p } { \\underset { p } { \\underset { p } { \\underset { p } { \\underset { p } { \\underset { p } { \\underset { p } { \\underset { p } { \\underset { p } { \\underset { p } { \\underset { p } { \\underset { p } { \\underset { p } { \\underset { p } { \\underset { p } { \\underset { p } { \\underset { p } { \\underset { p } { \\underset { p } { \\underset { p } { \\underset { p } { \\underset { p } { \\underset { p } { \\underset { p } { \\underset { p } { \\underset { p } { \\underset { p } { \\underset { p } { \\underset { p } { \\underset { p } { \\underset { p } { \\underset { p } { \\underset { p } { \\underset { p } { \\underset { p } { \\underset { p } { \\underset { p } { \\underset { p } { \\underset { p } { \\underset { p } { \\underset { p } { \\underset { p } { \\underset { p } { \\underset { p } { \\underset { p } { \\underset { p } { \\underset { p } { \\underset { p } { \\underset { p } { \\underset { p } { \\underset { p } { \\underset { p } { \\underset { p } { \\underset { p } { \\underset { p } { \\underset { p } { \\underset { p } { \\underset { p } { \\underset { p } { \\underset { p } { \\underset { p } { \\underset { p } { \\underset { p } { \\underset { p } { \\underset { p } { \\underset { p } { \\underset { p } { \\underset { p } { \\underset { p } { \\underset { p } { \\underset { p } { \\underset { p } { \\underset { p } { \\underset { p } { \\underset { p } { \\underset { p } { \\underset { p } { \\underset { p } { \\underset { p } { \\underset { p } { \\underset { p } { \\underset { p } { \\underset { p } { \\underset { p } { \\underset { p } { \\underset { p } { \\underset { p } { \\underset { p } { \\underset { p } { \\underset { p } { \\underset { p } { \\underset { p } { \\underset { p } { \\underset { p } { \\underset { p } { \\underset { p } { \\underset { p } { \\underset { p } { \\underset { p } { \\underset { p } { \\underset { p } { \\underset { p } { \\underset { p } { \\underset { p } { \\underset { p } { \\underset { p } { \\underset { p } { \\underset { p } { \\underset { p } { \\underset { p } { \\underset { p } { \\underset { p } { \\underset { p } { \\underset { p } { \\underset { p } { \\underset { p } { \\underset { p } { \\underset { p } { \\underset { p } { \\underset { p } { \\underset { p } { \\underset { p } { \\underset { p } { \\underset { p } { \\underset { p } { \\underset { p } { \\underset { p } { \\underset { p } { \\underset { p } { \\underset { p } { \\underset { p } { \\underset { p } { \\underset { p } { \\underset { p } { \\underset { p } { \\underset { p } { \\underset { p } { \\underset { p } { \\underset { p } { \\underset { p } { \\underset { p } { \\underset { p } { \\underset { p } { \\underset { p } { \\underset { p } { \\underset { p } { \\underset { p } { \\underset { p } { \\underset { p } { \\underset { p } { \\underset { p } { \\underset { p } { \\underset { p } { \\underset { p } { \\underset { p } { \\underset { p } { \\underset { p } { \\underset { p } { \\underset { p } { \\underset { p } { \\underset { p } { \\underset { p } { \\underset { p } { \\underset { p } { \\underset { p } { \\underset { p } { \\underset { p } { \\underset { p } { \\underset { p } { \\underset { p } { \\underset { p } { \\underset { p } { \\underset { p } { \\underset { p } { \\underset { p } { \\underset { p } { \\underset { p } { \\underset { p } { \\underset { p } { \\underset { p } { \\underset { p } { \\underset { p } { \\underset { p } { \\underset { p } { \\underset { p } { \\underset { p } { \\underset { p } { \\underset { p } { \\underset { p } { \\underset { p } { \\underset { p } { \\underset { p } { \\underset { p } { \\underset { p } { \\underset { p } { \\underset { p } { \\underset { p } { \\underset { p } { \\underset { p } { \\underset { p } { \\underset { p } { \\underset { p } { \\underset { p } { \\underset { p } { \\underset { p } { \\underset { p } { \\underset { p } { \\underset { p } { \\underset { p } { \\underset { p } { \\underset { p } { \\underset { p } { \\underset { p } { \\underset { p } { \\underset { p } { \\underset { p } { \\underset { p } { \\underset { p } { \\underset { p } { \\underset { p } { \\underset { p } { \\underset { p } { \\underset { p } { \\underset { p } { \\underset { p } { \\underset { p } { \\underset { p } { \\underset { p } { \\underset { p } { \\underset { p } { \\underset { p } { \\underset { p } { \\underset { p } { \\underset { p } { \\underset { p } { \\underset { p } { \\underset { p } { \\underset { p } { \\underset { p } { \\underset { p } { \\underset { p } { \\underset { p } { \\underset { p } { \\underset { p } { \\underset { p } { \\underset { p } { \\underset { p } { \\underset { p } { \\underset { p } { \\underset { p } { \\underset { p } { \\underset { p } { \\underset { p } { \\underset { p } { \\underset { p } { \\underset { p } { \\underset { p } { \\underset { p } { \\underset { p } { \\underset { p } { \\underset { p } { \\underset { p } { \\underset { p } { \\underset { p } { \\underset { p } { \\underset { p } { \\underset { p } { \\underset { p } { \\underset { p } { \\underset { p } { \\underset { p } { \\underset { p } { \\underset { p } { \\underset { p } { \\underset { p } { \\underset { p } { \\underset { p } { \\underset { p } { \\underset { p } { \\underset { p } { \\underset { p } { \\underset { p } { \\underset { p } { \\underset { p } { \\underset { p } { \\underset { p } { \\underset { p } { \\underset { p } { \\underset { p } { \\underset { p } { \\underset { p } { \\underset { p } { \\underset { p } { \\underset { p } { \\underset { p } { \\underset { p } { \\underset { p } { \\underset { p } { \\underset { p } { \\underset { p } { \\underset { p } { \\underset { p } { \\underset { p } { \\underset { p } { \\underset { p } { \\underset { p } { \\underset { p } { \\underset { p } { \\underset { p } { \\underset { p } { \\underset { p } { \\underset { p } { \\underset { p } { \\underset { p } { \\underset { p } { \\underset { p } { \\underset { p } { \\underset { p } { \\underset { p } { \\underset { p } { \\underset { p } { \\underset { p } { \\underset { p } { \\underset { p } { \\underset { p } { \\underset { p } { \\underset { p } { \\underset { p } { \\underset { p } { \\underset { p } { \\underset { p } { \\underset { p } { \\underset { p } { \\underset { p } { \\underset { p } { \\underset { p } { \\underset { p } { \\underset { p } { \\underset { p } { \\underset { p } { \\underset { p } { \\underset { p } { \\underset { p } { \\underset { p } { \\underset { p } { \\underset { p } { \\underset { p } { \\underset { p } { \\underset { p } { \\underset { p } { \\underset { p } { \\underset { p } { \\underset { p } { \\underset { p } { \\underset { p } { \\underset { p } { \\underset { p } { \\underset { p } { \\underset { p } { \\underset { p } { \\underset { p } { \\underset { p } { \\underset { p } { \\underset { p } { \\underset { p } { \\underset { p } { \\underset { p } { \\underset { p } { \\underset { p } { \\underset { p } { \\underset { p } { \\underset { p } { \\underset { p } { \\underset { p } { \\underset { p } { \\underset { p } { \\underset { p } { \\underset { p } { \\underset { p } { \\underset { p } { \\underset { p } { \\underset { p } { \\underset { p } { \\underset { p } { \\underset { p } { \\underset { p } { \\underset { p } { \\underset { p } { \\underset { p } { \\underset { p } { \\underset { p } { \\underset { p } { \\underset { p } { \\underset { p } { \\underset { p } { \\underset { p } { \\underset { p } { \\underset { p } { \\underset { p } { \\underset { p } { \\underset { p } { \\underset { p } { \\underset { p } { \\underset { p } { \\underset { p } { \\underset { p } { \\underset { p } { \\underset { p } { \\underset { p } { \\underset { p } { \\underset { p } { \\underset { p } { \\underset { p } { \\underset { p } { \\underset { p } { \\underset { p } { \\underset { p } { \\underset { p } { \\underset { p } { \\underset { p } { \\underset { p } { \\underset { p } { \\underset { p } { \\underset { p } { \\underset { p } { \\underset { p } { \\underset { p } { \\underset { p } { \\underset { p } { \\underset { p } { \\underset { p } { \\underset { p } { \\underset { p } { \\underset { p } { \\underset { p } { \\underset { p } { \\underset { p } { \\underset { p } { \\underset { p } { \\underset { p } { \\underset { p } { \\underset { p } { \\underset { p } { \\underset { p } { \\underset { p } { \\underset { p } { \\underset { p } { \\underset { p } { \\underset { p } { \\underset { p } { \\underset { p } { \\underset { p } { \\underset { p } { \\underset { p } { \\underset { p } { \\underset { p } { \\underset { p } { \\underset { p } { \\underset { p } { \\underset { p } { \\underset { p } { \\underset { p } { \\underset { p } { \\underset { p } { \\underset { p } { \\underset { p } { \\underset { p } { \\underset { p } { \\underset { p } { \\underset { p } { \\underset { p } { \\underset { p } { \\underset { p } { \\underset { p } { \\underset { p } { \\underset { p } { \\underset { p } { \\underset { p } { \\underset { p } { \\underset { p } { \\underset { p } { \\underset { p } { \\underset { p } { \\underset { p } { \\underset { p } { \\underset { p } { \\underset { p } { \\underset { p } { \\underset { p } { \\underset { p } { \\underset { p } { \\underset { p } { \\underset { p } { \\underset { p } { \\underset { p } { \\underset { p } { \\underset { p } { \\underset { p } { \\underset { p } { \\underset { p } { \\underset { p } { \\underset { p } { \\underset { p } { \\underset { p } { \\underset { p } { \\underset { p } { \\underset { p } { \\underset { p } { \\underset { p } { \\underset { p } { \\underset { p } { \\underset { p } { \\underset { p } { \\underset { p } { \\underset { p } { \\underset { p } { \\underset { p } { \\underset { p } { \\underset { p } { \\underset { p } { \\underset { p } { \\underset { p } { \\underset { p } { \\underset { p } { \\underset { p } { \\underset { p } { \\underset { p } { \\underset { p } { \\underset { p } { \\underset { p } { \\underset { p } { \\underset { p } { \\underset { p } { \\underset { p } { \\underset { p } { \\underset { p } { \\underset { p } { \\underset { p } { \\underset { p } { \\underset { p } { \\underset { p } { \\underset { p } { \\underset { p } { \\underset { p } { \\underset { p } { \\underset { p } { \\underset { p } { \\underset { p } { \\underset { p } { \\underset { p } { \\underset { p } { \\underset { p } { \\underset { p } { \\underset { p } { \\underset { p } { \\underset { p } { \\underset { p } { \\underset { p } { \\underset { p } { \\underset { p } { \\underset { p } { \\underset { p } { \\underset { p } { \\underset { p } { \\underset { p } { \\underset { p } { \\underset { p } { \\underset { p } { \\underset { p } { \\underset { p } { \\underset { p } { \\underset { p } { \\underset { p } { \\underset { p } { \\underset { p } { \\underset { p } { \\underset { p } { \\underset { p } { \\underset { p } { \\underset { p } { \\unders \\cdot } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } }$$  \nThe multivariate Gaussian distribution assumes that each individual predictor follows a one-dimensional normal distribution, as in (4.16), with some correlation between each pair of predictors. Two examples of multivariate Gaussian distributions with p = 2 are shown in Figure 4.5. The height of the surface at any particular point represents the probability that both X 1 and X 2 fall in a small region around that point. In either panel, if the surface is cut along the X 1 axis or along the X 2 axis, the resulting cross-section will have the shape of a one-dimensional normal distribution. The left-hand panel of Figure 4.5 illustrates an example in which Var ( X 1 ) = Var ( X 2 ) and Cor ( X 1  X 2 ) = 0 ; this surface has a characteristic bell shape . However, the bell shape will be distorted if the predictors are correlated or have unequal variances, as is illustrated in the right-hand panel of Figure 4.5. In this situation, the base of the bell will have an elliptical, rather than circular, shape. To indicate that a p -dimensional random variable X has a multivariate Gaussian distribution, we write X ∼ N ( θ Σ ) . Here E( X ) = θ is the mean of X (a vector with p components), and Cov ( X ) = Σ is the p × p covariance matrix of X . Formally, the multivariate Gaussian density is defined as  \nIn the case of p &gt; 1 predictors, the LDA classifier assumes that the observations in the k th class are drawn from a multivariate Gaussian distribution N ( θ k  Σ ) , where θ k is a class-specific mean vector, and Σ is a covariance matrix that is common to all K classes. Plugging the density function for the k th class, f k ( X = x ) , into (4.15) and performing a little bit of algebra reveals that the Bayes classifier assigns an observation X = x multivariate Gaussian  \nFIGURE 4.6. An example with three classes. The observations from each class are drawn from a multivariate Gaussian distribution with p = 2 , with a class-specific mean vector and a common covariance matrix. Left: Ellipses that contain 95 % of the probability for each of the three classes are shown. The dashed lines are the Bayes decision boundaries. Right: 20 observations were generated from each class, and the corresponding LDA decision boundaries are indicated using solid black lines. The Bayes decision boundaries are once again shown as dashed lines.  \n<!-- image -->  \nto the class for which  \n$$\\delta _ { k } ( x ) = x ^ { T } \\Sigma ^ { - 1 } \\mu _ { k } - \\frac { 1 } { 2 } \\mu _ { k } ^ { T } \\Sigma ^ { - 1 } \\mu _ { k } + \\log \\pi _ { k }$$  \nis largest. This is the vector/matrix version of (4.18).  \nAn example is shown in the left-hand panel of Figure 4.6. Three equallysized Gaussian classes are shown with class-specific mean vectors and a common covariance matrix. The three ellipses represent regions that contain 95 % of the probability for each of the three classes. The dashed lines are the Bayes decision boundaries. In other words, they represent the set of values x for which δ k ( x ) = δ /lscript ( x ) ; i.e.  \n$$x ^ { T } \\Sigma ^ { - 1 } \\mu _ { k } - \\frac { 1 } { 2 } \\mu _ { k } ^ { T } \\Sigma ^ { - 1 } \\mu _ { k } = x ^ { T } \\Sigma ^ { - 1 } \\mu _ { l } - \\frac { 1 } { 2 } \\mu _ { l } ^ { T } \\Sigma ^ { - 1 } \\mu _ { l }$$  \n/negationslash  \nOnce again, we need to estimate the unknown parameters θ 1         θ K , π 1         π K , and Σ ; the formulas are similar to those used in the onedimensional case, given in (4.20). To assign a new observation X = x , LDA plugs these estimates into (4.24) to obtain quantities ˆ δ k ( x ) , and classifies to the class for which ˆ δ k ( x ) is largest. Note that in (4.24) δ k ( x ) is a linear function of x ; that is, the LDA decision rule depends on x only for k = l . (The log π k term from (4.24) has disappeared because each of the three classes has the same number of training observations; i.e. π k is the same for each class.) Note that there are three lines representing the Bayes decision boundaries because there are three pairs of classes among the three classes. That is, one Bayes decision boundary separates class 1 from class 2, one separates class 1 from class 3, and one separates class 2 from class 3. These three Bayes decision boundaries divide the predictor space into three regions. The Bayes classifier will classify an observation according to the region in which it is located.  \nTABLE 4.4. A confusion matrix compares the LDA predictions to the true default statuses for the 10  000 training observations in the Default data set. Elements on the diagonal of the matrix represent individuals whose default statuses were correctly predicted, while off-diagonal elements represent individuals that were misclassified. LDA made incorrect predictions for 23 individuals who did not default and for 252 individuals who did default.  \n|                |       | True default status   | True default status   | True default status   |\n|----------------|-------|-----------------------|-----------------------|-----------------------|\n|                |       | No                    | Yes                   | Total                 |\n| Predicted      | No    | 9644                  | 252                   | 9896                  |\n| default status | Yes   | 23                    | 81                    | 104                   |\n|                | Total | 9667                  | 333                   | 10000                 |  \nthrough a linear combination of its elements. As previously discussed, this is the reason for the word linear in LDA.  \nWe can perform LDA on the Default data in order to predict whether or not an individual will default on the basis of credit card balance and student status. 4 The LDA model fit to the 10  000 training samples results in a training error rate of 2   75 %. This sounds like a low error rate, but two caveats must be noted.  \nIn the right-hand panel of Figure 4.6, 20 observations drawn from each of the three classes are displayed, and the resulting LDA decision boundaries are shown as solid black lines. Overall, the LDA decision boundaries are pretty close to the Bayes decision boundaries, shown again as dashed lines. The test error rates for the Bayes and LDA classifiers are 0   0746 and 0   0770 , respectively. This indicates that LDA is performing well on this data.  \n- First of all, training error rates will usually be lower than test error rates, which are the real quantity of interest. In other words, we might expect this classifier to perform worse if we use it to predict whether or not a new set of individuals will default. The reason is that we specifically adjust the parameters of our model to do well on the training data. The higher the ratio of parameters p to number of samples n , the more we expect this overfitting to play a role. For these data we don't expect this to be a problem, since p = 2 and n = 10  000 .\n- Second, since only 3   33 % of the individuals in the training sample defaulted, a simple but useless classifier that always predicts that an individual will not default, regardless of his or her credit card balance and student status, will result in an error rate of 3   33 %. In other words, the trivial null classifier will achieve an error rate that is only a bit higher than the LDA training set error rate.  \nIn practice, a binary classifier such as this one can make two types of errors: it can incorrectly assign an individual who defaults to the no default category, or it can incorrectly assign an individual who does not default to  \n4 The careful reader will notice that student status is qualitative - thus, the normality assumption made by LDA is clearly violated in this example! However, LDA is often remarkably robust to model violations, as this example shows. Naive Bayes, discussed in Section 4.4.4, provides an alternative to LDA that does not assume normally distributed predictors.  \noverfitting null  \nthe default category. It is often of interest to determine which of these two types of errors are being made. A confusion matrix , shown for the Default data in Table 4.4, is a convenient way to display this information. The table reveals that LDA predicted that a total of 104 people would default. Of these people, 81 actually defaulted and 23 did not. Hence only 23 out of 9  667 of the individuals who did not default were incorrectly labeled. This looks like a pretty low error rate! However, of the 333 individuals who defaulted, 252 (or 75   7 %) were missed by LDA. So while the overall error rate is low, the error rate among individuals who defaulted is very high. From the perspective of a credit card company that is trying to identify high-risk individuals, an error rate of 252 glyph[triangleleft] 333 = 75   7 % among individuals who default may well be unacceptable.  \nWhy does LDA do such a poor job of classifying the customers who default? In other words, why does it have such low sensitivity? As we have seen, LDA is trying to approximate the Bayes classifier, which has the lowest total error rate out of all classifiers. That is, the Bayes classifier will yield the smallest possible total number of misclassified observations, regardless of the class from which the errors stem. Some misclassifications will result from incorrectly assigning a customer who does not default to the default class, and others will result from incorrectly assigning a customer who defaults to the non-default class. In contrast, a credit card company might particularly wish to avoid incorrectly classifying an individual who will default, whereas incorrectly classifying an individual who will not default, though still to be avoided, is less problematic. We will now see that it is possible to modify LDA in order to develop a classifier that better meets the credit card company's needs.  \nClass-specific performance is also important in medicine and biology, where the terms sensitivity and specificity characterize the performance of a classifier or screening test. In this case the sensitivity is the percentage of true defaulters that are identified; it equals 24.3 %. The specificity is the percentage of non-defaulters that are correctly identified; it equals (1 -23 glyph[triangleleft] 9667) = 99   8 %.  \nThe Bayes classifier works by assigning an observation to the class for which the posterior probability p k ( X ) is greatest. In the two-class case, this amounts to assigning an observation to the default class if  \n$$\\Pr ( \\text {default} = \\text {Yes} | X = x ) & > 0 . 5 . \\\\$$  \nThus, the Bayes classifier, and by extension LDA, uses a threshold of 50 % for the posterior probability of default in order to assign an observation to the default class. However, if we are concerned about incorrectly predicting the default status for individuals who default, then we can consider lowering this threshold. For instance, we might label any customer with a posterior probability of default above 20 % to the default class. In other words, instead of assigning an observation to the default class if (4.26) holds, we could instead assign an observation to this class if  \n$$\\Pr ( \\text {default} = \\text {Yes} | X = x ) & > 0 . 2 . \\\\$$  \nThe error rates that result from taking this approach are shown in Table 4.5. Now LDA predicts that 430 individuals will default. Of the 333 individuals who default, LDA correctly predicts all but 138 , or 41   4 %. This is a vast confusion matrix  \nsensitivity specificity  \nTABLE 4.5. A confusion matrix compares the LDA predictions to the true default statuses for the 10  000 training observations in the Default data set, using a modified threshold value that predicts default for any individuals whose posterior default probability exceeds 20 %.  \n|                |       | True default status   | True default status   | True default status   |\n|----------------|-------|-----------------------|-----------------------|-----------------------|\n|                |       | No                    | Yes                   | Total                 |\n| Predicted      | No    | 9432                  | 138                   | 9570                  |\n| default status | Yes   | 235                   | 195                   | 430                   |\n|                | Total | 9667                  | 333                   | 10000                 |  \nFIGURE 4.7. For the Default data set, error rates are shown as a function of the threshold value for the posterior probability that is used to perform the assignment. The black solid line displays the overall error rate. The blue dashed line represents the fraction of defaulting customers that are incorrectly classified, and the orange dotted line indicates the fraction of errors among the non-defaulting customers.  \n<!-- image -->  \nimprovement over the error rate of 75   7 % that resulted from using the threshold of 50 %. However, this improvement comes at a cost: now 235 individuals who do not default are incorrectly classified. As a result, the overall error rate has increased slightly to 3   73 %. But a credit card company may consider this slight increase in the total error rate to be a small price to pay for more accurate identification of individuals who do indeed default.  \nThe ROC curve is a popular graphic for simultaneously displaying the two types of errors for all possible thresholds. The name 'ROC' is historic, and comes from communications theory. It is an acronym for receiver operating characteristics . Figure 4.8 displays the ROC curve for the LDA classifier on the training data. The overall performance of a classifier, sum-  \nFigure 4.7 illustrates the trade-off that results from modifying the threshold value for the posterior probability of default. Various error rates are shown as a function of the threshold value. Using a threshold of 0   5 , as in (4.26), minimizes the overall error rate, shown as a black solid line. This is to be expected, since the Bayes classifier uses a threshold of 0   5 and is known to have the lowest overall error rate. But when a threshold of 0   5 is used, the error rate among the individuals who default is quite high (blue dashed line). As the threshold is reduced, the error rate among individuals who default decreases steadily, but the error rate among the individuals who do not default increases. How can we decide which threshold value is best? Such a decision must be based on domain knowledge , such as detailed information about the costs associated with default.  \nROC curve\nROC Curve  \nFIGURE 4.8. A ROC curve for the LDA classifier on the Default data. It traces out two types of error as we vary the threshold value for the posterior probability of default. The actual thresholds are not shown. The true positive rate is the sensitivity: the fraction of defaulters that are correctly identified, using a given threshold value. The false positive rate is 1-specificity: the fraction of non-defaulters that we classify incorrectly as defaulters, using that same threshold value. The ideal ROC curve hugs the top left corner, indicating a high true positive rate and a low false positive rate. The dotted line represents the 'no information' classifier; this is what we would expect if student status and credit card balance are not associated with probability of default.  \n<!-- image -->  \nmarized over all possible thresholds, is given by the area under the (ROC) curve (AUC). An ideal ROC curve will hug the top left corner, so the larger the AUC the better the classifier. For this data the AUC is 0   95 , which is close to the maximum of 1   0 , so would be considered very good. We expect a classifier that performs no better than chance to have an AUC of 0.5 (when evaluated on an independent test set not used in model training). ROC curves are useful for comparing different classifiers, since they take into account all possible thresholds. It turns out that the ROC curve for the logistic regression model of Section 4.3.4 fit to these data is virtually indistinguishable from this one for the LDA model, so we do not display it here.  \nAs we have seen above, varying the classifier threshold changes its true positive and false positive rate. These are also called the sensitivity and one minus the specificity of our classifier. Since there is an almost bewildering array of terms used in this context, we now give a summary. Table 4.6 shows the possible results when applying a classifier (or diagnostic test) to a population. To make the connection with the epidemiology literature, we think of '+' as the 'disease' that we are trying to detect, and ' -' as the 'non-disease' state. To make the connection to the classical hypothesis testing literature, we think of ' -' as the null hypothesis and '+' as the area under the (ROC) curve  \nsensitivity specificity  \nTABLE 4.6. Possible results when applying a classifier or diagnostic test to a population.  \n|                 |               | True class      | True class      |       |\n|-----------------|---------------|-----------------|-----------------|-------|\n|                 |               | - or Null       | + or Non-null   | Total |\n| Predicted class | - or Null     | True Neg. (TN)  | False Neg. (FN) | N ∗   |\n|                 | + or Non-null | False Pos. (FP) | True Pos. (TP)  | P ∗   |\n|                 | Total         | N               | P               |       |  \nTABLE 4.7. Important measures for classification and diagnostic testing, derived from quantities in Table 4.6.  \n| Name             | Definition                 | Synonyms                                      |\n|------------------|----------------------------|-----------------------------------------------|\n| False Pos. rate  | FP glyph[triangleleft] N   | Type I error, 1 - Specificity                 |\n| True Pos. rate   | TP glyph[triangleleft] P   | 1 - Type II error, power, sensitivity, recall |\n| Pos. Pred. value | TP glyph[triangleleft] P ∗ | Precision, 1 - false discovery proportion     |\n| Neg. Pred. value | TN glyph[triangleleft] N ∗ |                                               |  \nalternative (non-null) hypothesis. In the context of the Default data, '+' indicates an individual who defaults, and ' -' indicates one who does not.  \nTable 4.7 lists many of the popular performance measures that are used in this context. The denominators for the false positive and true positive rates are the actual population counts in each class. In contrast, the denominators for the positive predictive value and the negative predictive value are the total predicted counts for each class."", '### 9.6.3 ROC Curves  \nSVMs and support vector classifiers output class labels for each observation. However, it is also possible to obtain fitted values for each observation, which are the numerical scores used to obtain the class labels. For instance, in the case of a support vector classifier, the fitted value for an observation X = ( X 1  X 2         X p ) T takes the form ˆ β 0 + ˆ β 1 X 1 + ˆ β 2 X 2 +       + ˆ β p X p . For an SVM with a non-linear kernel, the equation that yields the fitted value is given in (9.23). The sign of the fitted value determines on which side of the decision boundary the observation lies. Therefore, the relationship between the fitted value and the class prediction for a given observation is simple: if the fitted value exceeds zero then the observation is assigned to one class, and if it is less than zero then it is assigned to the other. By changing this threshold from zero to some positive value, we skew the classifications in favor of one class versus the other. By considering a range of these thresholds, positive and negative, we produce the ingredients for a ROC plot. We can access these values by calling the decision\\_function() method of a fitted SVM estimator.  \nThe function ROCCurveDisplay.from\\_estimator() (which we have abbreviated to roc\\_curve() ) will produce a plot of a ROC curve. It takes a fitted estimator as its first argument, followed by a model matrix X and labels y . The argument name is used in the legend, while color is used for the color of the line. Results are plotted on our axis object ax .  \n```\nIn[26]: fig, ax = subplots(figsize=(8,8)) roc_curve(best_svm, X_train, y_train, name=\'Training\', color=\'r\', ax=ax);\n```  \nIn this example, the SVM appears to provide accurate predictions. By increasing γ we can produce a more flexible fit and generate further improvements in accuracy.  \n```\nIn[27]: svm_flex = SVC(kernel=""rbf"", gamma=50,\n```  \n.function\\_ decision()  \nroc\\_curve()  \n```\nC=1) svm_flex.fit(X_train, y_train) fig, ax = subplots(figsize=(8,8)) roc_curve(svm_flex, X_train, y_train, name=\'Training $\\gamma=50$\', color=\'r\', ax=ax);\n```  \nHowever, these ROC curves are all on the training data. We are really more interested in the level of prediction accuracy on the test data. When we compute the ROC curves on the test data, the model with γ = 0   5 appears to provide the most accurate results.  \n```\nIn[28]: roc_curve(svm_flex, X_test, y_test, name=\'Test $\\gamma=50$\', color=\'b\', ax=ax) fig;\n```\nLet\'s look at our tuned SVM.  \n```\nIn[29]: fig, ax = subplots(figsize=(8,8)) for (X_, y_, c, name) in zip( (X_train, X_test), (y_train, y_test), (\'r\', \'b\'), (\'CV tuned on training\', \'CV tuned on test\')): roc_curve(best_svm, X_, y_, name=name, ax=ax, color=c)\n```']","La curva ROC (Receiver Operating Characteristic) es una representación gráfica que se utiliza para evaluar el rendimiento de un clasificador binario. Esta curva traza dos tipos de errores a medida que se varía el umbral para la probabilidad posterior de la clase positiva. En el eje y se representa la tasa de verdaderos positivos (sensibilidad), que es la fracción de positivos reales que son correctamente identificados por el clasificador. En el eje x se representa la tasa de falsos positivos (1 - especificidad), que es la fracción de negativos reales que son incorrectamente clasificados como positivos.

El área bajo la curva ROC (AUC, por sus siglas en inglés) es una medida del rendimiento global del clasificador. Un AUC de 1.0 indica un clasificador perfecto, mientras que un AUC de 0.5 sugiere un rendimiento no mejor que el azar. Un AUC más grande indica un mejor rendimiento del clasificador, ya que significa que la curva ROC está más cerca de la esquina superior izquierda del gráfico, lo que representa una alta tasa de verdaderos positivos y una baja tasa de falsos positivos.","La curva ROC (Receiver Operating Characteristic) es una gráfica que muestra el rendimiento de un modelo de clasificación binaria para todos los posibles umbrales de clasificación. El área bajo la curva (AUC) representa una medida agregada del rendimiento del clasificador; un valor de AUC cercano a 1 indica un modelo muy bueno, mientras que un valor de 0.5 indica un rendimiento similar al azar.",1.0,,0.99999999995,1.0
What is the difference between a forward stepwise selection and a backward stepwise selection method?,"[""### 6.1.2 Stepwise Selection  \nFor computational reasons, best subset selection cannot be applied with very large p . Best subset selection may also suffer from statistical problems when p is large. The larger the search space, the higher the chance of finding models that look good on the training data, even though they might not have any predictive power on future data. Thus an enormous search space can lead to overfitting and high variance of the coefficient estimates.  \nFor both of these reasons, stepwise methods, which explore a far more restricted set of models, are attractive alternatives to best subset selection.\nForward Stepwise Selection  \nForward stepwise selection is a computationally efficient alternative to best subset selection. While the best subset selection procedure considers all 2 p possible models containing subsets of the p predictors, forward stepwise considers a much smaller set of models. Forward stepwise selection begins with a model containing no predictors, and then adds predictors to the model, one-at-a-time, until all of the predictors are in the model. In particular, at each step the variable that gives the greatest additional improvement to the fit is added to the model. More formally, the forward stepwise selection procedure is given in Algorithm 6.2.  \nUnlike best subset selection, which involved fitting 2 p models, forward stepwise selection involves fitting one null model, along with p -k models in the k th iteration, for k = 0         p -1 . This amounts to a total of 1 + ∑ p -1 k =0 ( p -k ) = 1+ p ( p +1) glyph[triangleleft] 2 models. This is a substantial difference: when forward stepwise selection  \nTABLE 6.1. The first four selected models for best subset selection and forward stepwise selection on the Credit data set. The first three models are identical but the fourth models differ.  \n| # Variables        | Best subset                                                                     | Forward stepwise                                                                   |\n|--------------------|---------------------------------------------------------------------------------|------------------------------------------------------------------------------------|\n| One Two Three Four | rating rating , income rating , income , student cards , income student , limit | rating rating , income rating , income , student rating , income , student , limit |  \np = 20 , best subset selection requires fitting 1  048  576 models, whereas forward stepwise selection requires fitting only 211 models. 2  \nForward stepwise selection's computational advantage over best subset selection is clear. Though forward stepwise tends to do well in practice, it is not guaranteed to find the best possible model out of all 2 p models containing subsets of the p predictors. For instance, suppose that in a given data set with p = 3 predictors, the best possible one-variable model contains X 1 , and the best possible two-variable model instead contains X 2 and X 3 . Then forward stepwise selection will fail to select the best possible two-variable model, because M 1 will contain X 1 , so M 2 must also contain X 1 together with one additional variable.  \nIn Step 2(b) of Algorithm 6.2, we must identify the best model from among those p -k that augment M k with one additional predictor. We can do this by simply choosing the model with the lowest RSS or the highest R 2 . However, in Step 3, we must identify the best model among a set of models with different numbers of variables. This is more challenging, and is discussed in Section 6.1.3.  \nTable 6.1, which shows the first four selected models for best subset and forward stepwise selection on the Credit data set, illustrates this phenomenon. Both best subset selection and forward stepwise selection choose rating for the best one-variable model and then include income and student for the two- and three-variable models. However, best subset selection replaces rating by cards in the four-variable model, while forward stepwise selection must maintain rating in its four-variable model. In this example, Figure 6.1 indicates that there is not much difference between the threeand four-variable models in terms of RSS, so either of the four-variable models will likely be adequate.  \nForward stepwise selection can be applied even in the high-dimensional setting where n &lt; p ; however, in this case, it is possible to construct submodels M 0         M n -1 only, since each submodel is fit using least squares, which will not yield a unique solution if p ≥ n .\nBackward Stepwise Selection  \nLike forward stepwise selection, backward stepwise selection provides an  \n2 Though forward stepwise selection considers p ( p + 1) glyph[triangleleft] 2 + 1 models, it performs a guided search over model space, and so the effective model space considered contains substantially more than p ( p +1) glyph[triangleleft] 2 + 1 models.  \nefficient alternative to best subset selection. However, unlike forward stepwise selection, it begins with the full least squares model containing all p predictors, and then iteratively removes the least useful predictor, one-ata-time. Details are given in Algorithm 6.3.\nAlgorithm 6.3 Backward stepwise selection  \n1. Let M p denote the full model, which contains all p predictors.\n2. For k = p p -1         1 :\n3. (a) Consider all k models that contain all but one of the predictors in M k , for a total of k -1 predictors.\n4. (b) Choose the best among these k models, and call it M k -1 . Here best is defined as having smallest RSS or highest R 2 .\n3. Select a single best model from among M 0         M p using the prediction error on a validation set, C p (AIC), BIC, or adjusted R 2 . Or use the cross-validation method.  \nLike forward stepwise selection, the backward selection approach searches through only 1+ p ( p +1) glyph[triangleleft] 2 models, and so can be applied in settings where p is too large to apply best subset selection. 3 Also like forward stepwise selection, backward stepwise selection is not guaranteed to yield the best model containing a subset of the p predictors.  \nBackward selection requires that the number of samples n is larger than the number of variables p (so that the full model can be fit). In contrast, forward stepwise can be used even when n &lt; p , and so is the only viable subset method when p is very large.\nHybrid Approaches  \nThe best subset, forward stepwise, and backward stepwise selection approaches generally give similar but not identical models. As another alternative, hybrid versions of forward and backward stepwise selection are available, in which variables are added to the model sequentially, in analogy to forward selection. However, after adding each new variable, the method may also remove any variables that no longer provide an improvement in the model fit. Such an approach attempts to more closely mimic best subset selection while retaining the computational advantages of forward and backward stepwise selection.""]","Forward stepwise selection and backward stepwise selection are both methods used to select a subset of predictors for a model, but they differ in their approach:

1. **Forward Stepwise Selection:**
   - **Starting Point:** Begins with a model containing no predictors.
   - **Process:** Adds predictors to the model one at a time. At each step, it adds the predictor that provides the greatest improvement to the model fit.
   - **Model Space:** Considers a smaller set of models compared to best subset selection, fitting one null model and then \( p-k \) models in the \( k \)-th iteration.
   - **Advantage:** Can be used even when the number of predictors \( p \) is greater than the number of observations \( n \) (i.e., \( n < p \)).

2. **Backward Stepwise Selection:**
   - **Starting Point:** Begins with the full model containing all predictors.
   - **Process:** Removes the least useful predictor one at a time. At each step, it removes the predictor that provides the least contribution to the model fit.
   - **Model Space:** Also considers a smaller set of models compared to best subset selection.
   - **Requirement:** Requires that the number of samples \( n \) is larger than the number of predictors \( p \) (i.e., \( n > p \)).

Both methods are computationally more efficient than best subset selection and can be used when \( p \) is large. However, neither is guaranteed to find the best possible model containing a subset of the predictors.","Forward stepwise selection starts with no predictors and adds them one at a time, based on which addition provides the best improvement to the model. Backward stepwise selection starts with all predictors and removes them one by one, eliminating the least significant predictor at each step.",0.9333333333333333,,0.9999999999,1.0
Explain the concept of 'shrinkage' or 'regularization' in the context of linear models like ridge regression and lasso.,"['## 6.2 Shrinkage Methods  \nThe subset selection methods described in Section 6.1 involve using least squares to fit a linear model that contains a subset of the predictors. As an alternative, we can fit a model containing all p predictors using a technique that constrains or regularizes the coefficient estimates, or equivalently, that shrinks the coefficient estimates towards zero. It may not be immediately obvious why such a constraint should improve the fit, but it turns out that shrinking the coefficient estimates can significantly reduce their variance. The two best-known techniques for shrinking the regression coefficients towards zero are ridge regression and the lasso .', ""### 6.2.2 The Lasso  \nRidge regression does have one obvious disadvantage. Unlike best subset, forward stepwise, and backward stepwise selection, which will generally select models that involve just a subset of the variables, ridge regression will include all p predictors in the final model. The penalty λ ∑ β 2 j in (6.5) will shrink all of the coefficients towards zero, but it will not set any of them exactly to zero (unless λ = ∞ ). This may not be a problem for prediction accuracy, but it can create a challenge in model interpretation in settings in which the number of variables p is quite large. For example, in the Credit data set, it appears that the most important variables are income , limit , rating , and student . So we might wish to build a model including just these predictors. However, ridge regression will always generate a model involving all ten predictors. Increasing the value of λ will tend to reduce the magnitudes of the coefficients, but will not result in exclusion of any of the variables.  \nThe lasso is a relatively recent alternative to ridge regression that overcomes this disadvantage. The lasso coefficients, ˆ β L λ , minimize the quantity  \n$$& \\quad \\text {Conjures this disagrandage.} \\, \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\$$  \nAs with ridge regression, the lasso shrinks the coefficient estimates towards zero. However, in the case of the lasso, the /lscript 1 penalty has the effect of forcing some of the coefficient estimates to be exactly equal to zero when the tuning parameter λ is sufficiently large. Hence, much like best subset selection, the lasso performs variable selection . As a result, models generated from the lasso are generally much easier to interpret than those produced by ridge regression. We say that the lasso yields sparse models-that is, models that involve only a subset of the variables. As in ridge regression, selecting a good value of λ for the lasso is critical; we defer this discussion to Section 6.2.3, where we use cross-validation.  \nComparing (6.7) to (6.5), we see that the lasso and ridge regression have similar formulations. The only difference is that the β 2 j term in the ridge regression penalty (6.5) has been replaced by ♣ β j ♣ in the lasso penalty (6.7). In statistical parlance, the lasso uses an /lscript 1 (pronounced 'ell 1') penalty instead of an /lscript 2 penalty. The /lscript 1 norm of a coefficient vector β is given by ‖ β ‖ 1 = ∑ ♣ β j ♣ .  \nlasso sparse  \nFIGURE 6.6. The standardized lasso coefficients on the Credit data set are shown as a function of λ and ‖ ˆ β L λ ‖ 1 glyph[triangleleft] ‖ ˆ β ‖ 1 .  \n<!-- image -->  \nAs an example, consider the coefficient plots in Figure 6.6, which are generated from applying the lasso to the Credit data set. When λ = 0 , then the lasso simply gives the least squares fit, and when λ becomes sufficiently large, the lasso gives the null model in which all coefficient estimates equal zero. However, in between these two extremes, the ridge regression and lasso models are quite different from each other. Moving from left to right in the right-hand panel of Figure 6.6, we observe that at first the lasso results in a model that contains only the rating predictor. Then student and limit enter the model almost simultaneously, shortly followed by income . Eventually, the remaining variables enter the model. Hence, depending on the value of λ , the lasso can produce a model involving any number of variables. In contrast, ridge regression will always include all of the variables in the model, although the magnitude of the coefficient estimates will depend on λ .\nAnother Formulation for Ridge Regression and the Lasso  \nOne can show that the lasso and ridge regression coefficient estimates solve the problems  \n$$\\min _ { \\beta } \\left \\{ \\sum _ { i = 1 } ^ { n } \\left ( y _ { i } - \\beta _ { 0 } - \\sum _ { j = 1 } ^ { p } \\beta _ { j } x _ { i j } \\right ) ^ { 2 } \\right \\} & \\quad \\text {subject to } \\quad \\sum _ { j = 1 } ^ { p } | \\beta _ { j } | \\leq s \\\\ \\intertext { a n d }$$  \nand  \n$$\\text { and } \\\\ \\minimize _ { \\beta } \\left \\{ \\sum _ { i = 1 } ^ { n } \\left ( y _ { i } - \\beta _ { 0 } - \\sum _ { j = 1 } ^ { p } \\beta _ { j } x _ { i j } \\right ) ^ { 2 } \\right \\} \\text { subject to } \\sum _ { j = 1 } ^ { p } \\beta _ { j } ^ { 2 } \\leq s , \\\\ \\text {respectively. In other words, for every value of } \\lambda , \\text { there is some } s \\text { such that }$$  \nrespectively. In other words, for every value of λ , there is some s such that the Equations (6.7) and (6.8) will give the same lasso coefficient estimates. Similarly, for every value of λ there is a corresponding s such that Equations (6.5) and (6.9) will give the same ridge regression coefficient estimates.  \nWhen p = 2 , then (6.8) indicates that the lasso coefficient estimates have the smallest RSS out of all points that lie within the diamond defined by ♣ β 1 ♣ + ♣ β 2 ♣ ≤ s . Similarly, the ridge regression estimates have the smallest RSS out of all points that lie within the circle defined by β 2 1 + β 2 2 ≤ s .  \nThe formulations (6.8) and (6.9) reveal a close connection between the lasso, ridge regression, and best subset selection. Consider the problem  \nWe can think of (6.8) as follows. When we perform the lasso we are trying to find the set of coefficient estimates that lead to the smallest RSS, subject to the constraint that there is a budget s for how large ∑ p j =1 ♣ β j ♣ can be. When s is extremely large, then this budget is not very restrictive, and so the coefficient estimates can be large. In fact, if s is large enough that the least squares solution falls within the budget, then (6.8) will simply yield the least squares solution. In contrast, if s is small, then ∑ p j =1 ♣ β j ♣ must be small in order to avoid violating the budget. Similarly, (6.9) indicates that when we perform ridge regression, we seek a set of coefficient estimates such that the RSS is as small as possible, subject to the requirement that ∑ p j =1 β 2 j not exceed the budget s .  \n/negationslash  \nHere I ( β j = 0) is an indicator variable: it takes on a value of 1 if β j = 0 , and equals zero otherwise. Then (6.10) amounts to finding a set of coefficient estimates such that RSS is as small as possible, subject to the constraint that no more than s coefficients can be nonzero. The problem (6.10) is equivalent to best subset selection. Unfortunately, solving (6.10) is computationally infeasible when p is large, since it requires considering all ( p s ) models containing s predictors. Therefore, we can interpret ridge regression and the lasso as computationally feasible alternatives to best subset selection that replace the intractable form of the budget in (6.10) with forms that are much easier to solve. Of course, the lasso is much more closely related to best subset selection, since the lasso performs feature selection for s sufficiently small in (6.8), while ridge regression does not.  \n$$\\L a s o , \\, \\text {ride regression, and best subset selection. Consider the problem} \\\\ \\minimize & \\left \\{ \\sum _ { i = 1 } ^ { n } \\left ( y _ { i } - \\beta _ { 0 } - \\sum _ { j = 1 } ^ { p } \\beta _ { j } x _ { i j } \\right ) ^ { 2 } \\right \\} \\text { subject to } \\sum _ { j = 1 } ^ { p } I ( \\beta _ { j } \\neq 0 ) \\leq s . \\\\ \\intertext { h e r I ( \\beta _ { j } \\neq 0 ) is an indicator variable \\colon t takes on a value of 1 if \\beta _ { j } \\neq 0 , and } & \\cosh \\min \\left ( 1 - \\beta _ { 0 } \\right ) ,$$  \n/negationslash\nThe Variable Selection Property of the Lasso  \nWhy is it that the lasso, unlike ridge regression, results in coefficient estimates that are exactly equal to zero? The formulations (6.8) and (6.9) can be used to shed light on the issue. Figure 6.7 illustrates the situation. The least squares solution is marked as ˆ β , while the blue diamond and circle represent the lasso and ridge regression constraints in (6.8) and (6.9), respectively. If s is sufficiently large, then the constraint regions will contain ˆ β , and so the ridge regression and lasso estimates will be the same as the least squares estimates. (Such a large value of s corresponds to λ = 0 in (6.5) and (6.7).) However, in Figure 6.7 the least squares estimates lie outside of the diamond and the circle, and so the least squares estimates are not the same as the lasso and ridge regression estimates.  \nEach of the ellipses centered around ˆ β represents a contour : this means that all of the points on a particular ellipse have the same RSS value. As  \n/negationslash contour  \nFIGURE 6.7. Contours of the error and constraint functions for the lasso (left) and ridge regression (right) . The solid blue areas are the constraint regions, ♣ β 1 ♣ + ♣ β 2 ♣ ≤ s and β 2 1 + β 2 2 ≤ s , while the red ellipses are the contours of the RSS.  \n<!-- image -->  \nthe ellipses expand away from the least squares coefficient estimates, the RSS increases. Equations (6.8) and (6.9) indicate that the lasso and ridge regression coefficient estimates are given by the first point at which an ellipse contacts the constraint region. Since ridge regression has a circular constraint with no sharp points, this intersection will not generally occur on an axis, and so the ridge regression coefficient estimates will be exclusively non-zero. However, the lasso constraint has corners at each of the axes, and so the ellipse will often intersect the constraint region at an axis. When this occurs, one of the coefficients will equal zero. In higher dimensions, many of the coefficient estimates may equal zero simultaneously. In Figure 6.7, the intersection occurs at β 1 = 0 , and so the resulting model will only include β 2 .  \nIn Figure 6.7, we considered the simple case of p = 2 . When p = 3 , then the constraint region for ridge regression becomes a sphere, and the constraint region for the lasso becomes a polyhedron. When p &gt; 3 , the constraint for ridge regression becomes a hypersphere, and the constraint for the lasso becomes a polytope. However, the key ideas depicted in Figure 6.7 still hold. In particular, the lasso leads to feature selection when p &gt; 2 due to the sharp corners of the polyhedron or polytope.\nComparing the Lasso and Ridge Regression  \nIt is clear that the lasso has a major advantage over ridge regression, in that it produces simpler and more interpretable models that involve only a subset of the predictors. However, which method leads to better prediction accuracy? Figure 6.8 displays the variance, squared bias, and test MSE of the lasso applied to the same simulated data as in Figure 6.5. Clearly the lasso leads to qualitatively similar behavior to ridge regression, in that as λ increases, the variance decreases and the bias increases. In the right-hand  \nFIGURE 6.8. Left: Plots of squared bias (black), variance (green), and test MSE (purple) for the lasso on a simulated data set. Right: Comparison of squared bias, variance, and test MSE between lasso (solid) and ridge (dotted). Both are plotted against their R 2 on the training data, as a common form of indexing. The crosses in both plots indicate the lasso model for which the MSE is smallest.  \n<!-- image -->  \npanel of Figure 6.8, the dotted lines represent the ridge regression fits. Here we plot both against their R 2 on the training data. This is another useful way to index models, and can be used to compare models with different types of regularization, as is the case here. In this example, the lasso and ridge regression result in almost identical biases. However, the variance of ridge regression is slightly lower than the variance of the lasso. Consequently, the minimum MSE of ridge regression is slightly smaller than that of the lasso.  \nThese two examples illustrate that neither ridge regression nor the lasso will universally dominate the other. In general, one might expect the lasso to perform better in a setting where a relatively small number of predictors have substantial coefficients, and the remaining predictors have coefficients that are very small or that equal zero. Ridge regression will perform better when the response is a function of many predictors, all with coefficients of roughly equal size. However, the number of predictors that is related to the response is never known a priori for real data sets. A technique such as cross-validation can be used in order to determine which approach is better on a particular data set.  \nHowever, the data in Figure 6.8 were generated in such a way that all 45 predictors were related to the response-that is, none of the true coefficients β 1         β 45 equaled zero. The lasso implicitly assumes that a number of the coefficients truly equal zero. Consequently, it is not surprising that ridge regression outperforms the lasso in terms of prediction error in this setting. Figure 6.9 illustrates a similar situation, except that now the response is a function of only 2 out of 45 predictors. Now the lasso tends to outperform ridge regression in terms of bias, variance, and MSE.  \nAs with ridge regression, when the least squares estimates have excessively high variance, the lasso solution can yield a reduction in variance at the expense of a small increase in bias, and consequently can generate more accurate predictions. Unlike ridge regression, the lasso performs variable selection, and hence results in models that are easier to interpret.  \nFIGURE 6.9. Left: Plots of squared bias (black), variance (green), and test MSE (purple) for the lasso. The simulated data is similar to that in Figure 6.8, except that now only two predictors are related to the response. Right: Comparison of squared bias, variance, and test MSE between lasso (solid) and ridge (dotted). Both are plotted against their R 2 on the training data, as a common form of indexing. The crosses in both plots indicate the lasso model for which the MSE is smallest.  \n<!-- image -->  \nThere are very efficient algorithms for fitting both ridge and lasso models; in both cases the entire coefficient paths can be computed with about the same amount of work as a single least squares fit. We will explore this further in the lab at the end of this chapter.\nA Simple Special Case for Ridge Regression and the Lasso  \nIn order to obtain a better intuition about the behavior of ridge regression and the lasso, consider a simple special case with n = p , and X a diagonal matrix with 1 's on the diagonal and 0 's in all off-diagonal elements. To simplify the problem further, assume also that we are performing regression without an intercept. With these assumptions, the usual least squares problem simplifies to finding β 1         β p that minimize  \n$$\\sum _ { j = 1 } ^ { p } ( y _ { j } - \\beta _ { j } ) ^ { 2 } .$$  \nIn this case, the least squares solution is given by  \n$$\\hat { \\beta } _ { j } = y _ { j } .$$  \nAnd in this setting, ridge regression amounts to finding β 1         β p such that  \n$$\\sum _ { j = 1 } ^ { p } ( y _ { j } - \\beta _ { j } ) ^ { 2 } + \\lambda \\sum _ { j = 1 } ^ { p } \\beta _ { j } ^ { 2 }$$  \nis minimized, and the lasso amounts to finding the coefficients such that  \n$$\\sum _ { j = 1 } ^ { p } ( y _ { j } - \\beta _ { j } ) ^ { 2 } + \\lambda \\sum _ { j = 1 } ^ { p } | \\beta _ { j } |$$  \nFIGURE 6.10. The ridge regression and lasso coefficient estimates for a simple setting with n = p and X a diagonal matrix with 1 's on the diagonal. Left: The ridge regression coefficient estimates are shrunken proportionally towards zero, relative to the least squares estimates. Right: The lasso coefficient estimates are soft-thresholded towards zero.  \n<!-- image -->  \nis minimized. One can show that in this setting, the ridge regression estimates take the form  \n$$\\hat { \\beta } _ { j } ^ { R } = y _ { j } / ( 1 + \\lambda ) ,$$  \nand the lasso estimates take the form  \n$$\\ s o \\text { estimates take the form} \\\\ \\hat { \\beta } _ { j } ^ { L } = \\begin{cases} y _ { j } - \\lambda / 2 & \\text {if } y _ { j } > \\lambda / 2 ; \\\\ y _ { j } + \\lambda / 2 & \\text {if } y _ { j } < - \\lambda / 2 ; \\end{cases} \\\\ \\hat { 0 } & \\text {if } | y _ { j } | \\leq \\lambda / 2 . \\\\ \\intertext { 1 . 0 \\, d i s l a y s } \\text {the situation. We can see that ridge regression and } \\\\ \\text {form two very different types of shrinkage. In ridge regression}$$  \nIn the case of a more general data matrix X , the story is a little more complicated than what is depicted in Figure 6.10, but the main ideas still hold approximately: ridge regression more or less shrinks every dimension of the data by the same proportion, whereas the lasso more or less shrinks all coefficients toward zero by a similar amount, and sufficiently small coefficients are shrunken all the way to zero.  \nFigure 6.10 displays the situation. We can see that ridge regression and the lasso perform two very different types of shrinkage. In ridge regression, each least squares coefficient estimate is shrunken by the same proportion. In contrast, the lasso shrinks each least squares coefficient towards zero by a constant amount, λ glyph[triangleleft] 2 ; the least squares coefficients that are less than λ glyph[triangleleft] 2 in absolute value are shrunken entirely to zero. The type of shrinkage performed by the lasso in this simple setting (6.15) is known as softthresholding . The fact that some lasso coefficients are shrunken entirely to zero explains why the lasso performs feature selection.\nBayesian Interpretation of Ridge Regression and the Lasso  \nWe now show that one can view ridge regression and the lasso through a Bayesian lens. A Bayesian viewpoint for regression assumes that the coefficient vector β has some prior distribution, say p ( β ) , where β = ( β 0  β 1         β p ) T . The likelihood of the data can be written as f ( Y ♣ X β ) , softthresholding  \n<!-- image -->  \nFIGURE 6.11. Left: Ridge regression is the posterior mode for β under a Gaussian prior. Right: The lasso is the posterior mode for β under a double-exponential prior.  \n<!-- image -->  \nwhere X = ( X 1         X p ) . Multiplying the prior distribution by the likelihood gives us (up to a proportionality constant) the posterior distribution , which takes the form  \n$$p ( \\beta | X , Y ) \\, \\infty \\, f ( Y | X , \\beta ) p ( \\beta | X ) = f ( Y | X , \\beta ) p ( \\beta ) ,$$  \nwhere the proportionality above follows from Bayes' theorem, and the equality above follows from the assumption that X is fixed.  \nWe assume the usual linear model,  \n$$Y = \\beta _ { 0 } + X _ { 1 } \\beta _ { 1 } + \\dots + X _ { p } \\beta _ { p } + \\epsilon ,$$  \nand suppose that the errors are independent and drawn from a normal distribution. Furthermore, assume that p ( β ) = ∏ p j =1 g ( β j ) , for some density function g . It turns out that ridge regression and the lasso follow naturally from two special cases of g :  \n- If g is a Gaussian distribution with mean zero and standard deviation a function of λ , then it follows that the posterior mode for β -that is, the most likely value for β , given the data-is given by the ridge regression solution. (In fact, the ridge regression solution is also the posterior mean.)\n- If g is a double-exponential (Laplace) distribution with mean zero and scale parameter a function of λ , then it follows that the posterior mode for β is the lasso solution. (However, the lasso solution is not the posterior mean, and in fact, the posterior mean does not yield a sparse coefficient vector.)  \nThe Gaussian and double-exponential priors are displayed in Figure 6.11. Therefore, from a Bayesian viewpoint, ridge regression and the lasso follow directly from assuming the usual linear model with normal errors, together with a simple prior distribution for β . Notice that the lasso prior is steeply peaked at zero, while the Gaussian is flatter and fatter at zero. Hence, the lasso expects a priori that many of the coefficients are (exactly) zero, while ridge assumes the coefficients are randomly distributed about zero.  \nposterior distribution posterior mode  \nFIGURE 6.12. Left: Cross-validation errors that result from applying ridge regression to the Credit data set with various values of λ . Right: The coefficient estimates as a function of λ . The vertical dashed lines indicate the value of λ selected by cross-validation.  \n<!-- image -->"", ""### 6.2.1 Ridge Regression  \nRecall from Chapter 3 that the least squares fitting procedure estimates β 0  β 1         β p using the values that minimize  \n$$R S S = \\sum _ { i = 1 } ^ { n } \\left ( y _ { i } - \\beta _ { 0 } - \\sum _ { j = 1 } ^ { p } \\beta _ { j } x _ { j } \\right ) ^ { 2 } .$$  \nRidge regression is very similar to least squares, except that the coefficients are estimated by minimizing a slightly different quantity. In particular, the ridge regression coefficient estimates ˆ β R are the values that minimize  \n$$\\sum _ { i = 1 } ^ { n } \\left ( y _ { i } - \\beta _ { 0 } - \\sum _ { j = 1 } ^ { p } \\beta _ { j } x _ { i j } \\right ) ^ { 2 } + \\lambda \\sum _ { j = 1 } ^ { p } \\beta _ { j } ^ { 2 } & = R S S + \\lambda \\sum _ { j = 1 } ^ { p } \\beta _ { j } ^ { 2 } , \\\\ \\intertext { w h e r $ \\lambda \\geq 0 $ i s a t u n i n g $ \\text { parameter} $ , $ \\text { to be determined separately} $ . }$$  \nwhere λ ≥ 0 is a tuning parameter , to be determined separately. Equation 6.5 trades off two different criteria. As with least squares, ridge regression seeks coefficient estimates that fit the data well, by making the RSS small. However, the second term, λ ∑ j β 2 j , called a shrinkage penalty , is small when β 1         β p are close to zero, and so it has the effect of shrinking the estimates of β j towards zero. The tuning parameter λ serves to control onestandarderror rule  \nridge regression tuning parameter  \nshrinkage penalty  \nFIGURE 6.4. The standardized ridge regression coefficients are displayed for the Credit data set, as a function of λ and ‖ ˆ β R λ ‖ 2 glyph[triangleleft] ‖ ˆ β ‖ 2 .  \n<!-- image -->  \nthe relative impact of these two terms on the regression coefficient estimates. When λ = 0 , the penalty term has no effect, and ridge regression will produce the least squares estimates. However, as λ →∞ , the impact of the shrinkage penalty grows, and the ridge regression coefficient estimates will approach zero. Unlike least squares, which generates only one set of coefficient estimates, ridge regression will produce a different set of coefficient estimates, ˆ β R λ , for each value of λ . Selecting a good value for λ is critical; we defer this discussion to Section 6.2.3, where we use cross-validation.\nAn Application to the Credit Data  \nNote that in (6.5), the shrinkage penalty is applied to β 1         β p , but not to the intercept β 0 . We want to shrink the estimated association of each variable with the response; however, we do not want to shrink the intercept, which is simply a measure of the mean value of the response when x i 1 = x i 2 =       = x ip = 0 . If we assume that the variables-that is, the columns of the data matrix X -have been centered to have mean zero before ridge regression is performed, then the estimated intercept will take the form ˆ β 0 = ¯ y = ∑ n i =1 y i glyph[triangleleft]n .  \nIn Figure 6.4, the ridge regression coefficient estimates for the Credit data set are displayed. In the left-hand panel, each curve corresponds to the ridge regression coefficient estimate for one of the ten variables, plotted as a function of λ . For example, the black solid line represents the ridge regression estimate for the income coefficient, as λ is varied. At the extreme left-hand side of the plot, λ is essentially zero, and so the corresponding ridge coefficient estimates are the same as the usual least squares estimates. But as λ increases, the ridge coefficient estimates shrink towards zero. When λ is extremely large, then all of the ridge coefficient estimates are basically zero; this corresponds to the null model that contains no predictors. In this plot, the income , limit , rating , and student variables are displayed in distinct colors, since these variables tend to have by far the largest coefficient estimates. While the ridge coefficient estimates tend to decrease in aggregate as λ increases, individual coefficients, such as rating and income , may occasionally increase as λ increases.  \nThe right-hand panel of Figure 6.4 displays the same ridge coefficient estimates as the left-hand panel, but instead of displaying λ on the x -axis, we now display ‖ ˆ β R λ ‖ 2 glyph[triangleleft] ‖ ˆ β ‖ 2 , where ˆ β denotes the vector of least squares coefficient estimates. The notation ‖ β ‖ 2 denotes the /lscript 2 norm (pronounced 'ell 2') of a vector, and is defined as ‖ β ‖ 2 = √ ∑ p j =1 β j 2 . It measures the distance of β from zero. As λ increases, the /lscript 2 norm of ˆ β R λ will always decrease, and so will ‖ ˆ β R λ ‖ 2 glyph[triangleleft] ‖ ˆ β ‖ 2 . The latter quantity ranges from 1 (when λ = 0 , in which case the ridge regression coefficient estimate is the same as the least squares estimate, and so their /lscript 2 norms are the same) to 0 (when λ = ∞ , in which case the ridge regression coefficient estimate is a vector of zeros, with /lscript 2 norm equal to zero). Therefore, we can think of the x -axis in the right-hand panel of Figure 6.4 as the amount that the ridge regression coefficient estimates have been shrunken towards zero; a small value indicates that they have been shrunken very close to zero.  \nThe standard least squares coefficient estimates discussed in Chapter 3 are scale equivariant : multiplying X j by a constant c simply leads to a scaling of the least squares coefficient estimates by a factor of 1 glyph[triangleleft]c . In other words, regardless of how the j th predictor is scaled, X j ˆ β j will remain the same. In contrast, the ridge regression coefficient estimates can change substantially when multiplying a given predictor by a constant. For instance, consider the income variable, which is measured in dollars. One could reasonably have measured income in thousands of dollars, which would result in a reduction in the observed values of income by a factor of 1,000. Now due to the sum of squared coefficients term in the ridge regression formulation (6.5), such a change in scale will not simply cause the ridge regression coefficient estimate for income to change by a factor of 1,000. In other words, X j ˆ β R j λ will depend not only on the value of λ , but also on the scaling of the j th predictor. In fact, the value of X j ˆ β R j λ may even depend on the scaling of the other predictors! Therefore, it is best to apply ridge regression after standardizing the predictors , using the formula  \n$$\\tilde { x } _ { i j } = \\frac { x _ { i j } } { \\sqrt { \\frac { 1 } { n } \\sum _ { i = 1 } ^ { n } ( x _ { i j } - \\bar { x } _ { j } ) ^ { 2 } } } , \\\\ \\text {are all on the same scale. In } ( 6 . 6 ) , \\, \\text { the denominator is the}$$  \nso that they are all on the same scale. In (6.6), the denominator is the estimated standard deviation of the j th predictor. Consequently, all of the standardized predictors will have a standard deviation of one. As a result the final fit will not depend on the scale on which the predictors are measured. In Figure 6.4, the y -axis displays the standardized ridge regression coefficient estimates-that is, the coefficient estimates that result from performing ridge regression using standardized predictors.\nWhy Does Ridge Regression Improve Over Least Squares?  \nRidge regression's advantage over least squares is rooted in the bias-variance trade-off . As λ increases, the flexibility of the ridge regression fit decreases, leading to decreased variance but increased bias. This is illustrated in the left-hand panel of Figure 6.5, using a simulated data set containing p = 45 predictors and n = 50 observations. The green curve in the left-hand panel  \n/lscript 2 norm scale equivariant  \nFIGURE 6.5. Squared bias (black), variance (green), and test mean squared error (purple) for the ridge regression predictions on a simulated data set, as a function of λ and ‖ ˆ β R λ ‖ 2 glyph[triangleleft] ‖ ˆ β ‖ 2 . The horizontal dashed lines indicate the minimum possible MSE. The purple crosses indicate the ridge regression models for which the MSE is smallest.  \n<!-- image -->  \nof Figure 6.5 displays the variance of the ridge regression predictions as a function of λ . At the least squares coefficient estimates, which correspond to ridge regression with λ = 0 , the variance is high but there is no bias. But as λ increases, the shrinkage of the ridge coefficient estimates leads to a substantial reduction in the variance of the predictions, at the expense of a slight increase in bias. Recall that the test mean squared error (MSE), plotted in purple, is closely related to the variance plus the squared bias. For values of λ up to about 10 , the variance decreases rapidly, with very little increase in bias, plotted in black. Consequently, the MSE drops considerably as λ increases from 0 to 10 . Beyond this point, the decrease in variance due to increasing λ slows, and the shrinkage on the coefficients causes them to be significantly underestimated, resulting in a large increase in the bias. The minimum MSE is achieved at approximately λ = 30 . Interestingly, because of its high variance, the MSE associated with the least squares fit, when λ = 0 , is almost as high as that of the null model for which all coefficient estimates are zero, when λ = ∞ . However, for an intermediate value of λ , the MSE is considerably lower.  \nIn general, in situations where the relationship between the response and the predictors is close to linear, the least squares estimates will have low bias but may have high variance. This means that a small change in the training data can cause a large change in the least squares coefficient estimates. In particular, when the number of variables p is almost as large as the number of observations n , as in the example in Figure 6.5, the least squares estimates will be extremely variable. And if p &gt; n , then the least squares estimates do not even have a unique solution, whereas ridge regression can still perform well by trading off a small increase in bias for a  \nThe right-hand panel of Figure 6.5 displays the same curves as the lefthand panel, this time plotted against the /lscript 2 norm of the ridge regression coefficient estimates divided by the /lscript 2 norm of the least squares estimates. Now as we move from left to right, the fits become more flexible, and so the bias decreases and the variance increases.  \nlarge decrease in variance. Hence, ridge regression works best in situations where the least squares estimates have high variance.  \nRidge regression also has substantial computational advantages over best subset selection, which requires searching through 2 p models. As we discussed previously, even for moderate values of p , such a search can be computationally infeasible. In contrast, for any fixed value of λ , ridge regression only fits a single model, and the model-fitting procedure can be performed quite quickly. In fact, one can show that the computations required to solve (6.5), simultaneously for all values of λ , are almost identical to those for fitting a model using least squares.""]","In the context of linear models, 'shrinkage' or 'regularization' refers to techniques used to constrain or regularize the coefficient estimates, effectively shrinking them towards zero. This is done to reduce the variance of the estimates, which can improve the model's prediction accuracy and interpretability, especially when dealing with a large number of predictors.

### Ridge Regression
- **Ridge regression** applies an \(\ell_2\) penalty, which is the sum of the squares of the coefficients. This penalty term is added to the residual sum of squares (RSS) that the model aims to minimize. The ridge regression coefficient estimates are those that minimize:
  \[
  \sum_{i=1}^{n} \left( y_i - \beta_0 - \sum_{j=1}^{p} \beta_j x_{ij} \right)^2 + \lambda \sum_{j=1}^{p} \beta_j^2
  \]
- The tuning parameter \(\lambda\) controls the strength of the penalty. As \(\lambda\) increases, the coefficients are shrunk more towards zero, reducing variance but potentially increasing bias.

### Lasso
- **Lasso** (Least Absolute Shrinkage and Selection Operator) uses an \(\ell_1\) penalty, which is the sum of the absolute values of the coefficients. The lasso coefficient estimates minimize:
  \[
  \sum_{i=1}^{n} \left( y_i - \beta_0 - \sum_{j=1}^{p} \beta_j x_{ij} \right)^2 + \lambda \sum_{j=1}^{p} |\beta_j|
  \]
- The \(\ell_1\) penalty has the effect of forcing some coefficients to be exactly zero when \(\lambda\) is sufficiently large, thus performing variable selection and resulting in sparse models.

### Comparison
- **Ridge regression** includes all predictors in the model, shrinking their coefficients but not setting any to zero, which can be a challenge for model interpretation when the number of predictors is large.
- **Lasso**, on the other hand, can produce simpler and more interpretable models by setting some coefficients to zero, effectively selecting a subset of predictors.

Both methods aim to improve prediction accuracy and model interpretability by addressing the bias-variance trade-off, with ridge regression being more suitable when many predictors have small but non-zero coefficients, and lasso being more effective when only a few predictors have substantial coefficients.","Shrinkage or regularization is a technique used to constrain or reduce the coefficient estimates of a linear model towards zero. This helps to reduce the model's variance and prevent overfitting, particularly when dealing with a large number of predictors.",1.0,,0.9999999999666667,1.0
What is the key difference between ridge regression and the lasso?,"[""### 6.2.2 The Lasso  \nRidge regression does have one obvious disadvantage. Unlike best subset, forward stepwise, and backward stepwise selection, which will generally select models that involve just a subset of the variables, ridge regression will include all p predictors in the final model. The penalty λ ∑ β 2 j in (6.5) will shrink all of the coefficients towards zero, but it will not set any of them exactly to zero (unless λ = ∞ ). This may not be a problem for prediction accuracy, but it can create a challenge in model interpretation in settings in which the number of variables p is quite large. For example, in the Credit data set, it appears that the most important variables are income , limit , rating , and student . So we might wish to build a model including just these predictors. However, ridge regression will always generate a model involving all ten predictors. Increasing the value of λ will tend to reduce the magnitudes of the coefficients, but will not result in exclusion of any of the variables.  \nThe lasso is a relatively recent alternative to ridge regression that overcomes this disadvantage. The lasso coefficients, ˆ β L λ , minimize the quantity  \n$$& \\quad \\text {Conjures this disagrandage.} \\, \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\$$  \nAs with ridge regression, the lasso shrinks the coefficient estimates towards zero. However, in the case of the lasso, the /lscript 1 penalty has the effect of forcing some of the coefficient estimates to be exactly equal to zero when the tuning parameter λ is sufficiently large. Hence, much like best subset selection, the lasso performs variable selection . As a result, models generated from the lasso are generally much easier to interpret than those produced by ridge regression. We say that the lasso yields sparse models-that is, models that involve only a subset of the variables. As in ridge regression, selecting a good value of λ for the lasso is critical; we defer this discussion to Section 6.2.3, where we use cross-validation.  \nComparing (6.7) to (6.5), we see that the lasso and ridge regression have similar formulations. The only difference is that the β 2 j term in the ridge regression penalty (6.5) has been replaced by ♣ β j ♣ in the lasso penalty (6.7). In statistical parlance, the lasso uses an /lscript 1 (pronounced 'ell 1') penalty instead of an /lscript 2 penalty. The /lscript 1 norm of a coefficient vector β is given by ‖ β ‖ 1 = ∑ ♣ β j ♣ .  \nlasso sparse  \nFIGURE 6.6. The standardized lasso coefficients on the Credit data set are shown as a function of λ and ‖ ˆ β L λ ‖ 1 glyph[triangleleft] ‖ ˆ β ‖ 1 .  \n<!-- image -->  \nAs an example, consider the coefficient plots in Figure 6.6, which are generated from applying the lasso to the Credit data set. When λ = 0 , then the lasso simply gives the least squares fit, and when λ becomes sufficiently large, the lasso gives the null model in which all coefficient estimates equal zero. However, in between these two extremes, the ridge regression and lasso models are quite different from each other. Moving from left to right in the right-hand panel of Figure 6.6, we observe that at first the lasso results in a model that contains only the rating predictor. Then student and limit enter the model almost simultaneously, shortly followed by income . Eventually, the remaining variables enter the model. Hence, depending on the value of λ , the lasso can produce a model involving any number of variables. In contrast, ridge regression will always include all of the variables in the model, although the magnitude of the coefficient estimates will depend on λ .\nAnother Formulation for Ridge Regression and the Lasso  \nOne can show that the lasso and ridge regression coefficient estimates solve the problems  \n$$\\min _ { \\beta } \\left \\{ \\sum _ { i = 1 } ^ { n } \\left ( y _ { i } - \\beta _ { 0 } - \\sum _ { j = 1 } ^ { p } \\beta _ { j } x _ { i j } \\right ) ^ { 2 } \\right \\} & \\quad \\text {subject to } \\quad \\sum _ { j = 1 } ^ { p } | \\beta _ { j } | \\leq s \\\\ \\intertext { a n d }$$  \nand  \n$$\\text { and } \\\\ \\minimize _ { \\beta } \\left \\{ \\sum _ { i = 1 } ^ { n } \\left ( y _ { i } - \\beta _ { 0 } - \\sum _ { j = 1 } ^ { p } \\beta _ { j } x _ { i j } \\right ) ^ { 2 } \\right \\} \\text { subject to } \\sum _ { j = 1 } ^ { p } \\beta _ { j } ^ { 2 } \\leq s , \\\\ \\text {respectively. In other words, for every value of } \\lambda , \\text { there is some } s \\text { such that }$$  \nrespectively. In other words, for every value of λ , there is some s such that the Equations (6.7) and (6.8) will give the same lasso coefficient estimates. Similarly, for every value of λ there is a corresponding s such that Equations (6.5) and (6.9) will give the same ridge regression coefficient estimates.  \nWhen p = 2 , then (6.8) indicates that the lasso coefficient estimates have the smallest RSS out of all points that lie within the diamond defined by ♣ β 1 ♣ + ♣ β 2 ♣ ≤ s . Similarly, the ridge regression estimates have the smallest RSS out of all points that lie within the circle defined by β 2 1 + β 2 2 ≤ s .  \nThe formulations (6.8) and (6.9) reveal a close connection between the lasso, ridge regression, and best subset selection. Consider the problem  \nWe can think of (6.8) as follows. When we perform the lasso we are trying to find the set of coefficient estimates that lead to the smallest RSS, subject to the constraint that there is a budget s for how large ∑ p j =1 ♣ β j ♣ can be. When s is extremely large, then this budget is not very restrictive, and so the coefficient estimates can be large. In fact, if s is large enough that the least squares solution falls within the budget, then (6.8) will simply yield the least squares solution. In contrast, if s is small, then ∑ p j =1 ♣ β j ♣ must be small in order to avoid violating the budget. Similarly, (6.9) indicates that when we perform ridge regression, we seek a set of coefficient estimates such that the RSS is as small as possible, subject to the requirement that ∑ p j =1 β 2 j not exceed the budget s .  \n/negationslash  \nHere I ( β j = 0) is an indicator variable: it takes on a value of 1 if β j = 0 , and equals zero otherwise. Then (6.10) amounts to finding a set of coefficient estimates such that RSS is as small as possible, subject to the constraint that no more than s coefficients can be nonzero. The problem (6.10) is equivalent to best subset selection. Unfortunately, solving (6.10) is computationally infeasible when p is large, since it requires considering all ( p s ) models containing s predictors. Therefore, we can interpret ridge regression and the lasso as computationally feasible alternatives to best subset selection that replace the intractable form of the budget in (6.10) with forms that are much easier to solve. Of course, the lasso is much more closely related to best subset selection, since the lasso performs feature selection for s sufficiently small in (6.8), while ridge regression does not.  \n$$\\L a s o , \\, \\text {ride regression, and best subset selection. Consider the problem} \\\\ \\minimize & \\left \\{ \\sum _ { i = 1 } ^ { n } \\left ( y _ { i } - \\beta _ { 0 } - \\sum _ { j = 1 } ^ { p } \\beta _ { j } x _ { i j } \\right ) ^ { 2 } \\right \\} \\text { subject to } \\sum _ { j = 1 } ^ { p } I ( \\beta _ { j } \\neq 0 ) \\leq s . \\\\ \\intertext { h e r I ( \\beta _ { j } \\neq 0 ) is an indicator variable \\colon t takes on a value of 1 if \\beta _ { j } \\neq 0 , and } & \\cosh \\min \\left ( 1 - \\beta _ { 0 } \\right ) ,$$  \n/negationslash\nThe Variable Selection Property of the Lasso  \nWhy is it that the lasso, unlike ridge regression, results in coefficient estimates that are exactly equal to zero? The formulations (6.8) and (6.9) can be used to shed light on the issue. Figure 6.7 illustrates the situation. The least squares solution is marked as ˆ β , while the blue diamond and circle represent the lasso and ridge regression constraints in (6.8) and (6.9), respectively. If s is sufficiently large, then the constraint regions will contain ˆ β , and so the ridge regression and lasso estimates will be the same as the least squares estimates. (Such a large value of s corresponds to λ = 0 in (6.5) and (6.7).) However, in Figure 6.7 the least squares estimates lie outside of the diamond and the circle, and so the least squares estimates are not the same as the lasso and ridge regression estimates.  \nEach of the ellipses centered around ˆ β represents a contour : this means that all of the points on a particular ellipse have the same RSS value. As  \n/negationslash contour  \nFIGURE 6.7. Contours of the error and constraint functions for the lasso (left) and ridge regression (right) . The solid blue areas are the constraint regions, ♣ β 1 ♣ + ♣ β 2 ♣ ≤ s and β 2 1 + β 2 2 ≤ s , while the red ellipses are the contours of the RSS.  \n<!-- image -->  \nthe ellipses expand away from the least squares coefficient estimates, the RSS increases. Equations (6.8) and (6.9) indicate that the lasso and ridge regression coefficient estimates are given by the first point at which an ellipse contacts the constraint region. Since ridge regression has a circular constraint with no sharp points, this intersection will not generally occur on an axis, and so the ridge regression coefficient estimates will be exclusively non-zero. However, the lasso constraint has corners at each of the axes, and so the ellipse will often intersect the constraint region at an axis. When this occurs, one of the coefficients will equal zero. In higher dimensions, many of the coefficient estimates may equal zero simultaneously. In Figure 6.7, the intersection occurs at β 1 = 0 , and so the resulting model will only include β 2 .  \nIn Figure 6.7, we considered the simple case of p = 2 . When p = 3 , then the constraint region for ridge regression becomes a sphere, and the constraint region for the lasso becomes a polyhedron. When p &gt; 3 , the constraint for ridge regression becomes a hypersphere, and the constraint for the lasso becomes a polytope. However, the key ideas depicted in Figure 6.7 still hold. In particular, the lasso leads to feature selection when p &gt; 2 due to the sharp corners of the polyhedron or polytope.\nComparing the Lasso and Ridge Regression  \nIt is clear that the lasso has a major advantage over ridge regression, in that it produces simpler and more interpretable models that involve only a subset of the predictors. However, which method leads to better prediction accuracy? Figure 6.8 displays the variance, squared bias, and test MSE of the lasso applied to the same simulated data as in Figure 6.5. Clearly the lasso leads to qualitatively similar behavior to ridge regression, in that as λ increases, the variance decreases and the bias increases. In the right-hand  \nFIGURE 6.8. Left: Plots of squared bias (black), variance (green), and test MSE (purple) for the lasso on a simulated data set. Right: Comparison of squared bias, variance, and test MSE between lasso (solid) and ridge (dotted). Both are plotted against their R 2 on the training data, as a common form of indexing. The crosses in both plots indicate the lasso model for which the MSE is smallest.  \n<!-- image -->  \npanel of Figure 6.8, the dotted lines represent the ridge regression fits. Here we plot both against their R 2 on the training data. This is another useful way to index models, and can be used to compare models with different types of regularization, as is the case here. In this example, the lasso and ridge regression result in almost identical biases. However, the variance of ridge regression is slightly lower than the variance of the lasso. Consequently, the minimum MSE of ridge regression is slightly smaller than that of the lasso.  \nThese two examples illustrate that neither ridge regression nor the lasso will universally dominate the other. In general, one might expect the lasso to perform better in a setting where a relatively small number of predictors have substantial coefficients, and the remaining predictors have coefficients that are very small or that equal zero. Ridge regression will perform better when the response is a function of many predictors, all with coefficients of roughly equal size. However, the number of predictors that is related to the response is never known a priori for real data sets. A technique such as cross-validation can be used in order to determine which approach is better on a particular data set.  \nHowever, the data in Figure 6.8 were generated in such a way that all 45 predictors were related to the response-that is, none of the true coefficients β 1         β 45 equaled zero. The lasso implicitly assumes that a number of the coefficients truly equal zero. Consequently, it is not surprising that ridge regression outperforms the lasso in terms of prediction error in this setting. Figure 6.9 illustrates a similar situation, except that now the response is a function of only 2 out of 45 predictors. Now the lasso tends to outperform ridge regression in terms of bias, variance, and MSE.  \nAs with ridge regression, when the least squares estimates have excessively high variance, the lasso solution can yield a reduction in variance at the expense of a small increase in bias, and consequently can generate more accurate predictions. Unlike ridge regression, the lasso performs variable selection, and hence results in models that are easier to interpret.  \nFIGURE 6.9. Left: Plots of squared bias (black), variance (green), and test MSE (purple) for the lasso. The simulated data is similar to that in Figure 6.8, except that now only two predictors are related to the response. Right: Comparison of squared bias, variance, and test MSE between lasso (solid) and ridge (dotted). Both are plotted against their R 2 on the training data, as a common form of indexing. The crosses in both plots indicate the lasso model for which the MSE is smallest.  \n<!-- image -->  \nThere are very efficient algorithms for fitting both ridge and lasso models; in both cases the entire coefficient paths can be computed with about the same amount of work as a single least squares fit. We will explore this further in the lab at the end of this chapter.\nA Simple Special Case for Ridge Regression and the Lasso  \nIn order to obtain a better intuition about the behavior of ridge regression and the lasso, consider a simple special case with n = p , and X a diagonal matrix with 1 's on the diagonal and 0 's in all off-diagonal elements. To simplify the problem further, assume also that we are performing regression without an intercept. With these assumptions, the usual least squares problem simplifies to finding β 1         β p that minimize  \n$$\\sum _ { j = 1 } ^ { p } ( y _ { j } - \\beta _ { j } ) ^ { 2 } .$$  \nIn this case, the least squares solution is given by  \n$$\\hat { \\beta } _ { j } = y _ { j } .$$  \nAnd in this setting, ridge regression amounts to finding β 1         β p such that  \n$$\\sum _ { j = 1 } ^ { p } ( y _ { j } - \\beta _ { j } ) ^ { 2 } + \\lambda \\sum _ { j = 1 } ^ { p } \\beta _ { j } ^ { 2 }$$  \nis minimized, and the lasso amounts to finding the coefficients such that  \n$$\\sum _ { j = 1 } ^ { p } ( y _ { j } - \\beta _ { j } ) ^ { 2 } + \\lambda \\sum _ { j = 1 } ^ { p } | \\beta _ { j } |$$  \nFIGURE 6.10. The ridge regression and lasso coefficient estimates for a simple setting with n = p and X a diagonal matrix with 1 's on the diagonal. Left: The ridge regression coefficient estimates are shrunken proportionally towards zero, relative to the least squares estimates. Right: The lasso coefficient estimates are soft-thresholded towards zero.  \n<!-- image -->  \nis minimized. One can show that in this setting, the ridge regression estimates take the form  \n$$\\hat { \\beta } _ { j } ^ { R } = y _ { j } / ( 1 + \\lambda ) ,$$  \nand the lasso estimates take the form  \n$$\\ s o \\text { estimates take the form} \\\\ \\hat { \\beta } _ { j } ^ { L } = \\begin{cases} y _ { j } - \\lambda / 2 & \\text {if } y _ { j } > \\lambda / 2 ; \\\\ y _ { j } + \\lambda / 2 & \\text {if } y _ { j } < - \\lambda / 2 ; \\end{cases} \\\\ \\hat { 0 } & \\text {if } | y _ { j } | \\leq \\lambda / 2 . \\\\ \\intertext { 1 . 0 \\, d i s l a y s } \\text {the situation. We can see that ridge regression and } \\\\ \\text {form two very different types of shrinkage. In ridge regression}$$  \nIn the case of a more general data matrix X , the story is a little more complicated than what is depicted in Figure 6.10, but the main ideas still hold approximately: ridge regression more or less shrinks every dimension of the data by the same proportion, whereas the lasso more or less shrinks all coefficients toward zero by a similar amount, and sufficiently small coefficients are shrunken all the way to zero.  \nFigure 6.10 displays the situation. We can see that ridge regression and the lasso perform two very different types of shrinkage. In ridge regression, each least squares coefficient estimate is shrunken by the same proportion. In contrast, the lasso shrinks each least squares coefficient towards zero by a constant amount, λ glyph[triangleleft] 2 ; the least squares coefficients that are less than λ glyph[triangleleft] 2 in absolute value are shrunken entirely to zero. The type of shrinkage performed by the lasso in this simple setting (6.15) is known as softthresholding . The fact that some lasso coefficients are shrunken entirely to zero explains why the lasso performs feature selection.\nBayesian Interpretation of Ridge Regression and the Lasso  \nWe now show that one can view ridge regression and the lasso through a Bayesian lens. A Bayesian viewpoint for regression assumes that the coefficient vector β has some prior distribution, say p ( β ) , where β = ( β 0  β 1         β p ) T . The likelihood of the data can be written as f ( Y ♣ X β ) , softthresholding  \n<!-- image -->  \nFIGURE 6.11. Left: Ridge regression is the posterior mode for β under a Gaussian prior. Right: The lasso is the posterior mode for β under a double-exponential prior.  \n<!-- image -->  \nwhere X = ( X 1         X p ) . Multiplying the prior distribution by the likelihood gives us (up to a proportionality constant) the posterior distribution , which takes the form  \n$$p ( \\beta | X , Y ) \\, \\infty \\, f ( Y | X , \\beta ) p ( \\beta | X ) = f ( Y | X , \\beta ) p ( \\beta ) ,$$  \nwhere the proportionality above follows from Bayes' theorem, and the equality above follows from the assumption that X is fixed.  \nWe assume the usual linear model,  \n$$Y = \\beta _ { 0 } + X _ { 1 } \\beta _ { 1 } + \\dots + X _ { p } \\beta _ { p } + \\epsilon ,$$  \nand suppose that the errors are independent and drawn from a normal distribution. Furthermore, assume that p ( β ) = ∏ p j =1 g ( β j ) , for some density function g . It turns out that ridge regression and the lasso follow naturally from two special cases of g :  \n- If g is a Gaussian distribution with mean zero and standard deviation a function of λ , then it follows that the posterior mode for β -that is, the most likely value for β , given the data-is given by the ridge regression solution. (In fact, the ridge regression solution is also the posterior mean.)\n- If g is a double-exponential (Laplace) distribution with mean zero and scale parameter a function of λ , then it follows that the posterior mode for β is the lasso solution. (However, the lasso solution is not the posterior mean, and in fact, the posterior mean does not yield a sparse coefficient vector.)  \nThe Gaussian and double-exponential priors are displayed in Figure 6.11. Therefore, from a Bayesian viewpoint, ridge regression and the lasso follow directly from assuming the usual linear model with normal errors, together with a simple prior distribution for β . Notice that the lasso prior is steeply peaked at zero, while the Gaussian is flatter and fatter at zero. Hence, the lasso expects a priori that many of the coefficients are (exactly) zero, while ridge assumes the coefficients are randomly distributed about zero.  \nposterior distribution posterior mode  \nFIGURE 6.12. Left: Cross-validation errors that result from applying ridge regression to the Credit data set with various values of λ . Right: The coefficient estimates as a function of λ . The vertical dashed lines indicate the value of λ selected by cross-validation.  \n<!-- image -->""]","The key difference between ridge regression and the lasso lies in their penalty terms and their effects on the model coefficients:

- **Ridge Regression** uses an \( \ell_2 \) penalty, which is the sum of the squares of the coefficients (\( \sum \beta_j^2 \)). This penalty shrinks the coefficients towards zero but does not set any of them exactly to zero. As a result, ridge regression includes all predictors in the final model, which can be a disadvantage when model interpretability is important.

- **The Lasso** uses an \( \ell_1 \) penalty, which is the sum of the absolute values of the coefficients (\( \sum |\beta_j| \)). This penalty not only shrinks the coefficients towards zero but can also set some of them exactly to zero when the tuning parameter \( \lambda \) is sufficiently large. This results in variable selection, producing sparser models that are generally easier to interpret.

In summary, while both methods shrink coefficients, the lasso has the additional property of performing variable selection by setting some coefficients to zero, unlike ridge regression.","The key difference is that ridge regression uses an L2 penalty, which shrinks the coefficients towards zero but never sets them exactly to zero. The lasso, on the other hand, uses an L1 penalty, which can force some coefficient estimates to be exactly zero, effectively performing variable selection.",1.0,,0.9999999999,1.0
¿Qué es el 'Principio de jerarquía' en la selección de modelos?,"['### 7.5.2 Choosing the Smoothing Parameter λ  \nWe have seen that a smoothing spline is simply a natural cubic spline with knots at every unique value of x i . It might seem that a smoothing spline will have far too many degrees of freedom, since a knot at each data point allows a great deal of flexibility. But the tuning parameter λ controls the roughness of the smoothing spline, and hence the effective degrees of freedom . It is possible to show that as λ increases from 0 to ∞ , the effective degrees of freedom, which we write df λ , decrease from n to 2 .  \nIn the context of smoothing splines, why do we discuss effective degrees of freedom instead of degrees of freedom? Usually degrees of freedom refer to the number of free parameters, such as the number of coefficients fit in a polynomial or cubic spline. Although a smoothing spline has n parameters and hence n nominal degrees of freedom, these n parameters are heavily constrained or shrunk down. Hence df λ is a measure of the flexibility of the smoothing spline-the higher it is, the more flexible (and the lower-bias but higher-variance) the smoothing spline. The definition of effective degrees of effective degrees of freedom  \nfreedom is somewhat technical. We can write  \n$$\\hat { g } _ { \\lambda } = S _ { \\lambda } y ,$$  \nwhere ˆ g λ is the solution to (7.11) for a particular choice of λ -that is, it is an n -vector containing the fitted values of the smoothing spline at the training points x 1         x n . Equation 7.12 indicates that the vector of fitted values when applying a smoothing spline to the data can be written as a n × n matrix S λ (for which there is a formula) times the response vector y . Then the effective degrees of freedom is defined to be  \n$$d f _ { \\lambda } = \\sum _ { i = 1 } ^ { n } \\{ S _ { \\lambda } \\} _ { i i } , \\\\$$  \nthe sum of the diagonal elements of the matrix S λ .  \nIn fitting a smoothing spline, we do not need to select the number or location of the knots-there will be a knot at each training observation, x 1         x n . Instead, we have another problem: we need to choose the value of λ . It should come as no surprise that one possible solution to this problem is cross-validation. In other words, we can find the value of λ that makes the cross-validated RSS as small as possible. It turns out that the leaveone-out cross-validation error (LOOCV) can be computed very efficiently for smoothing splines, with essentially the same cost as computing a single fit, using the following formula:  \n$$R S _ { c v } ( \\lambda ) = \\sum _ { i = 1 } ^ { n } ( y _ { i } - \\hat { g } _ { \\lambda } ^ { ( - i ) } ( x _ { i } ) ) ^ { 2 } = \\sum _ { i = 1 } ^ { n } \\left [ \\frac { y _ { i } - \\hat { g } _ { \\lambda } ( x _ { i } ) } { 1 - \\{ S _ { \\lambda } \\} _ { i i } } \\right ] ^ { 2 } .$$  \nThe notation ˆ g ( -i ) λ ( x i ) indicates the fitted value for this smoothing spline evaluated at x i , where the fit uses all of the training observations except for the i th observation ( x i  y i ) . In contrast, ˆ g λ ( x i ) indicates the smoothing spline function fit to all of the training observations and evaluated at x i . This remarkable formula says that we can compute each of these leaveone-out fits using only ˆ g λ , the original fit to all of the data! 5 We have a very similar formula (5.2) on page 205 in Chapter 5 for least squares linear regression. Using (5.2), we can very quickly perform LOOCV for the regression splines discussed earlier in this chapter, as well as for least squares regression using arbitrary basis functions.  \nFigure 7.8 shows the results from fitting a smoothing spline to the Wage data. The red curve indicates the fit obtained from pre-specifying that we would like a smoothing spline with 16 effective degrees of freedom. The blue curve is the smoothing spline obtained when λ is chosen using LOOCV; in this case, the value of λ chosen results in 6   8 effective degrees of freedom (computed using (7.13)). For this data, there is little discernible difference between the two smoothing splines, beyond the fact that the one with 16 degrees of freedom seems slightly wigglier. Since there is little difference between the two fits, the smoothing spline fit with 6   8 degrees of freedom  \n5 The exact formulas for computing ˆ g ( x i ) and S λ are very technical; however, efficient algorithms are available for computing these quantities.\nSmoothing Spline  \nFIGURE 7.8. Smoothing spline fits to the Wage data. The red curve results from specifying 16 effective degrees of freedom. For the blue curve, λ was found automatically by leave-one-out cross-validation, which resulted in 6   8 effective degrees of freedom.  \n<!-- image -->  \nis preferable, since in general simpler models are better unless the data provides evidence in support of a more complex model.', ""## 10.6 When to Use Deep Learning  \nThe performance of deep learning in this chapter has been rather impressive. It nailed the digit classification problem, and deep CNNs have really revolutionized image classification. We see daily reports of new success stories for deep learning. Many of these are related to image classification tasks, such as machine diagnosis of mammograms or digital X-ray images, ophthalmology eye scans, annotations of MRI scans, and so on. Likewise there are numerous successes of RNNs in speech and language translation, forecasting, and document modeling. The question that then begs an answer is: should we discard all our older tools, and use deep learning on every problem with data? To address this question, we revisit our Hitters dataset from Chapter 6.  \nThis is a regression problem, where the goal is to predict the Salary of a baseball player in 1987 using his performance statistics from 1986. After removing players with missing responses, we are left with 263 players and 19 variables. We randomly split the data into a training set of 176 players (two thirds), and a test set of 87 players (one third). We used three methods for fitting a regression model to these data.  \n- A linear model was used to fit the training data, and make predictions on the test data. The model has 20 parameters.\n- The same linear model was fit with lasso regularization. The tuning parameter was selected by 10-fold cross-validation on the training data. It selected a model with 12 variables having nonzero coefficients.\n- A neural network with one hidden layer consisting of 64 ReLU units was fit to the data. This model has 1,345 parameters. 20  \n20 The model was fit by stochastic gradient descent with a batch size of 32 for 1,000 epochs, and 10% dropout regularization. The test error performance flattened out and started to slowly increase after 1,000 epochs. These fitting details are discussed in Section 10.7.  \nbidirectional  \nSeq2Seq  \nTABLE 10.2. Prediction results on the Hitters test data for linear models fit by ordinary least squares and lasso, compared to a neural network fit by stochastic gradient descent with dropout regularization.  \n| Model             |   # Parameters |   Mean Abs. Error |   Test Set R 2 |\n|-------------------|----------------|-------------------|----------------|\n| Linear Regression |             20 |             254.7 |           0.56 |\n| Lasso             |             12 |             252.3 |           0.51 |\n| Neural Network    |           1345 |             257.4 |           0.54 |  \nTABLE 10.3. Least squares coefficient estimates associated with the regression of Salary on four variables chosen by lasso on the Hitters data set. This model achieved the best performance on the test data, with a mean absolute error of 224.8. The results reported here were obtained from a regression on the test data, which was not used in fitting the lasso model.  \n|           |   Coefficient |   Std. error |   t -statistic | p -value                      |\n|-----------|---------------|--------------|----------------|-------------------------------|\n| Intercept |      -226.67  |        86.26 |          -2.63 | 0.0103                        |\n| Hits      |         3.06  |         1.02 |           3    | 0.0036                        |\n| Walks     |         0.181 |         2.04 |           0.09 | 0.9294                        |\n| CRuns     |         0.859 |         0.12 |           7.09 | < 0   0001 |\n| PutOuts   |         0.465 |         0.13 |           3.6  | 0.0005                        |  \nTable 10.2 compares the results. We see similar performance for all three models. We report the mean absolute error on the test data, as well as the test R 2 for each method, which are all respectable (see Exercise 5). We spent a fair bit of time fiddling with the configuration parameters of the neural network to achieve these results. It is possible that if we were to spend more time, and got the form and amount of regularization just right, that we might be able to match or even outperform linear regression and the lasso. But with great ease we obtained linear models that work well. Linear models are much easier to present and understand than the neural network, which is essentially a black box. The lasso selected 12 of the 19 variables in making its prediction. So in cases like this we are much better off following the Occam's razor principle: when faced with several methods that give roughly equivalent performance, pick the simplest.  \nWe have a number of very powerful tools at our disposal, including neural networks, random forests and boosting, support vector machines and generalized additive models, to name a few. And then we have linear models, and simple variants of these. When faced with new data modeling and prediction problems, it's tempting to always go for the trendy new methods. Often they give extremely impressive results, especially when the datasets are very large and can support the fitting of high-dimensional nonlinear models. However, if we can produce models with the simpler tools that  \nAfter a bit more exploration with the lasso model, we identified an even simpler model with four variables. We then refit the linear model with these four variables to the training data (the so-called relaxed lasso ), and achieved a test mean absolute error of 224.8, the overall winner! It is tempting to present the summary table from this fit, so we can see coefficients and pvalues; however, since the model was selected on the training data, there would be selection bias . Instead, we refit the model on the test data, which was not used in the selection. Table 10.3 shows the results.  \nOccam's razor  \nperform as well, they are likely to be easier to fit and understand, and potentially less fragile than the more complex approaches. Wherever possible, it makes sense to try the simpler models as well, and then make a choice based on the performance/complexity tradeoff.  \nTypically we expect deep learning to be an attractive choice when the sample size of the training set is extremely large, and when interpretability of the model is not a high priority."", '# Contents  \nPreface\nvii\n1\nIntroduction\n1\n2\nStatistical Learning\n15\n2.1\nWhat Is Statistical Learning? . . . . . . . . . . . . . . . . .\n15\n2.1.1\nWhy Estimate f? . . . . . . . . . . . . . . . . . . .\n17\n2.1.2\nHow Do We Estimate f? . . . . . . . . . . . . . . .\n20\n2.1.3\nThe Trade-Off Between Prediction Accuracy\nand Model Interpretability . . . . . . . . . . . . . .\n23\n2.1.4\nSupervised Versus Unsupervised Learning . . . . .\n25\n2.1.5\nRegression Versus Classification Problems . . . . .\n27\n2.2\nAssessing Model Accuracy\n. . . . . . . . . . . . . . . . . .\n27\n2.2.1\nMeasuring the Quality of Fit\n. . . . . . . . . . . .\n28\n2.2.2\nThe Bias-Variance Trade-Off . . . . . . . . . . . . .\n31\n2.2.3\nThe Classification Setting\n. . . . . . . . . . . . . .\n34\n2.3\nLab: Introduction to Python . . . . . . . . . . . . . . . . .\n40\n2.3.1\nGetting Started . . . . . . . . . . . . . . . . . . . .\n40\n2.3.2\nBasic Commands . . . . . . . . . . . . . . . . . . .\n40\n2.3.3\nIntroduction to Numerical Python\n. . . . . . . . .\n42\n2.3.4\nGraphics . . . . . . . . . . . . . . . . . . . . . . . .\n48\n2.3.5\nSequences and Slice Notation\n. . . . . . . . . . . .\n51\n2.3.6\nIndexing Data . . . . . . . . . . . . . . . . . . . . .\n51\n2.3.7\nLoading Data . . . . . . . . . . . . . . . . . . . . .\n55\n2.3.8\nFor Loops . . . . . . . . . . . . . . . . . . . . . . .\n59\n2.3.9\nAdditional Graphical and Numerical Summaries . .\n61\n2.4\nExercises . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n63\n3\nLinear Regression\n69\n3.1\nSimple Linear Regression . . . . . . . . . . . . . . . . . . .\n70\n3.1.1\nEstimating the Coefficients\n. . . . . . . . . . . . .\n71\n3.1.2\nAssessing the Accuracy of the Coefficient\nEstimates . . . . . . . . . . . . . . . . . . . . . . .\n72\n3.1.3\nAssessing the Accuracy of the Model . . . . . . . .\n77\n3.2\nMultiple Linear Regression . . . . . . . . . . . . . . . . . .\n80\n3.2.1\nEstimating the Regression Coefficients . . . . . . .\n81\nix  \n<!-- FIN DEL BLOQUE DE FALLBACK -->  \n4  \n|                    | 3.2.2                                                                                  | Some Important Questions 83                                                             |\n|--------------------|----------------------------------------------------------------------------------------|-----------------------------------------------------------------------------------------|\n| 3.3                | . . . . . . . . . . . . . . Other Considerations in the Regression Model . . . . . . . | 91                                                                                      |\n|                    | 3.3.1                                                                                  | Qualitative Predictors . . . . . . . . . . . . . . . . 91                               |\n|                    | 3.3.2                                                                                  | Extensions of the Linear Model . . . . . . . . . . . 94                                 |\n|                    | 3.3.3                                                                                  | Potential Problems . . . . . . . . 100                                                  |\n| 3.4                | . . . . . . . . . . The Marketing Plan . . . . . . . . . . . . . . . .                 | . . . . . . 109                                                                         |\n| 3.5                | Comparison of Linear Regression with K -Nearest                                        | Comparison of Linear Regression with K -Nearest                                         |\n|                    | Neighbors . . . . . . . . . . . . . . . . . . . . . . . . . . . . 111                  | Neighbors . . . . . . . . . . . . . . . . . . . . . . . . . . . . 111                   |\n| 3.6                | Lab: Linear Regression . . . . . . . . . . . . . . . . . . . .                         | 116                                                                                     |\n|                    | 3.6.1                                                                                  | Importing packages . . . . . . . . . . . . . . . . . . 116                              |\n|                    | 3.6.2                                                                                  | Simple Linear Regression . . . . . . . . . . . . . . . 117                              |\n|                    | 3.6.3                                                                                  | Multiple Linear Regression . . . . . . . . . . . . . . 122 . . . . . . . .              |\n|                    | 3.6.4                                                                                  | Multivariate Goodness of Fit . . . . 123                                                |\n|                    | 3.6.5                                                                                  | Interaction Terms . . . . . . . . . . . . . . . . . . . 124                             |\n|                    | 3.6.6                                                                                  | Non-linear Transformations of the Predictors . . . 125                                  |\n|                    | 3.6.7                                                                                  | Qualitative Predictors . . . . . . . . . . . . . . . . 126                              |\n| 3.7                | Exercises . . . . . . . . . . . . . . . . . . . . . . . . . . . . 127                  | Exercises . . . . . . . . . . . . . . . . . . . . . . . . . . . . 127                   |\n| Classification 135 | Classification 135                                                                     | Classification 135                                                                      |\n| 4.1                | An Overview of Classification . . . . . . . . . . . . . . . . . 135                    | An Overview of Classification . . . . . . . . . . . . . . . . . 135                     |\n| 4.2                | Why Not Linear Regression? . . . . . . . . . . . . . . . . . 136 . 138                 | Why Not Linear Regression? . . . . . . . . . . . . . . . . . 136 . 138                  |\n| 4.3                | Logistic Regression . . . . . . . . . . . . . . . . . . . . . . . . . . . . .          | Logistic Regression . . . . . . . . . . . . . . . . . . . . . . . . . . . . .           |\n|                    | 4.3.1                                                                                  | The Logistic Model . . . . . . . . . . 139                                              |\n|                    | 4.3.2                                                                                  | Estimating the Regression Coefficients . . . . . . . 140                                |\n|                    | 4.3.3                                                                                  | Making Predictions . . . . . . . . . . . . . . . . . . 141                              |\n|                    | 4.3.4                                                                                  | Multiple Logistic Regression . . . . . . . . . . . . . 142                              |\n|                    | 4.3.5                                                                                  | Multinomial Logistic Regression . . . . . . . . . . . 144                               |\n| 4.4                | Generative Models for Classification . . . . . . . . . . . . . 146 . . . . . . . 147   | Generative Models for Classification . . . . . . . . . . . . . 146 . . . . . . . 147    |\n|                    | 4.4.1                                                                                  | Linear Discriminant Analysis for p = 1                                                  |\n|                    | 4.4.2                                                                                  | Linear Discriminant Analysis for p > 1 . . . . . . . 150                                |\n|                    | 4.4.3                                                                                  | Quadratic Discriminant Analysis . . . . . . . . . . 156 . . . . . . . . . . . . . . . . |\n|                    | 4.4.4                                                                                  | Naive Bayes . . . . . . 158                                                             |\n| 4.5                | A Comparison of Classification Methods . . . . . . . . . . . . . . . . . .             | . . 161                                                                                 |\n|                    | 4.5.1                                                                                  | An Analytical Comparison . . . . 161                                                    |\n|                    | 4.5.2                                                                                  | An Empirical Comparison . . . . . . . . . . . . . . 164                                 |\n| 4.6                | Generalized Linear Models . . . . . . . . . . . . . . . . .                            | . 167                                                                                   |\n|                    | 4.6.1                                                                                  | Linear Regression on the Bikeshare Data . . . . . . 167                                 |\n|                    | 4.6.2                                                                                  | Poisson Regression on the Bikeshare Data . . . . . 169                                  |\n|                    | 4.6.3                                                                                  | Generalized Linear Models in Greater Generality . 172                                   |\n| 4.7                | Lab: Logistic Regression, LDA, QDA, and KNN . .                                        | . . . . 173                                                                             |\n|                    | 4.7.1                                                                                  | The Stock Market Data . . . . . . . . . . . . . . . 173                                 |\n|                    | 4.7.2                                                                                  | Logistic Regression . . . . . . . . . . . . . . . . . . 174                             |\n|                    | 4.7.3                                                                                  | Linear Discriminant Analysis . . . . . . . . . . . . 179                                |\n|                    | 4.7.4                                                                                  | Quadratic Discriminant Analysis . . . . . . 181                                         |\n|                    | 4.7.5                                                                                  | . . . . Naive Bayes . . . . . . . . . . . . . . . . . . . . . . 182                     |\n|                    | 4.7.6                                                                                  | K-Nearest Neighbors . . . . . . . . . . . . . . . . . 183                               |\n|                    | 4.7.7                                                                                  | Linear and Poisson Regression on the Bikeshare Data188                                  |\n| 4.8                | Exercises . . . . . . . . . . . . . . . . . . . . . . . . . . .                        | . 193                                                                                   |  \n| 5 Resampling Methods   | 5 Resampling Methods                      | 5 Resampling Methods                                                                                                 | 5 Resampling Methods                                                                                                      | 201         |\n|------------------------|-------------------------------------------|----------------------------------------------------------------------------------------------------------------------|---------------------------------------------------------------------------------------------------------------------------|-------------|\n|                        | 5.1                                       | Cross-Validation . . . .                                                                                             | . . . . . . . . . . . . . . . . . . .                                                                                     | 202         |\n|                        |                                           | 5.1.1 The Validation Set                                                                                             | Approach . . . . . . . . . . .                                                                                            | 202         |\n|                        |                                           | 5.1.2                                                                                                                | Leave-One-Out Cross-Validation . . . . . . . . .                                                                          | 204         |\n|                        |                                           | 5.1.3                                                                                                                | k -Fold Cross-Validation . . . . . . . . . .                                                                              | 206         |\n|                        |                                           | 5.1.4                                                                                                                | . . . . Bias-Variance Trade-Off for k -Fold Cross-Validation . . . . . . . . . . . . . . . . . .                          | 208         |\n|                        |                                           | 5.1.5                                                                                                                | Cross-Validation on Classification Problems . . .                                                                         | 209         |\n|                        | 5.2                                       | The Bootstrap . . . . .                                                                                              | . . . . . . . . . . . . . . . . . . .                                                                                     | 212         |\n|                        | 5.3                                       | Lab: Cross-Validation and the Bootstrap                                                                              | . . . . . . . . .                                                                                                         | 215         |\n|                        |                                           | 5.3.1                                                                                                                | The Validation Set Approach . . . . . . . . . . .                                                                         | 216         |\n|                        |                                           | 5.3.2                                                                                                                | Cross-Validation . . . . . . . . . . . . . . . . . .                                                                      | 217         |\n|                        |                                           | 5.3.3                                                                                                                | The Bootstrap . . . . . . . . . . . . . . . . . . .                                                                       | 220         |\n|                        | 5.4                                       | Exercises . . . . . . . . . . . . . . . . . . . . . . . . . . .                                                      | Exercises . . . . . . . . . . . . . . . . . . . . . . . . . . .                                                           | 224         |\n| 6                      | Linear Model Selection and Regularization | Linear Model Selection and Regularization                                                                            | Linear Model Selection and Regularization                                                                                 | 229         |\n|                        | 6.1                                       | Subset Selection . . . . . . .                                                                                       | . . . . . . . . . . . . . . . .                                                                                           | 231         |\n|                        |                                           | 6.1.1                                                                                                                | Best Subset Selection . . . . . . . . . . . . . . . .                                                                     | 231         |\n|                        |                                           | 6.1.2                                                                                                                | Stepwise Selection . . . . . . . . . . . . . . . . .                                                                      | 233         |\n|                        |                                           | 6.1.3                                                                                                                | Choosing the Optimal Model                                                                                                | 235         |\n|                        | 6.2                                       | . . . . . . . . . . . Shrinkage Methods . . . . . . . . . . . . . . . . . . . . .                                    | . . . . . . . . . . . Shrinkage Methods . . . . . . . . . . . . . . . . . . . . .                                         | 240         |\n|                        |                                           | 6.2.1                                                                                                                | Ridge Regression . . . . . . . . . . . . . . . . . . . .                                                                  | 240         |\n|                        |                                           | 6.2.2                                                                                                                | The Lasso . . . . . . . . . . . . . . . . . . . .                                                                         | 244         |\n|                        |                                           | 6.2.3                                                                                                                | Selecting the Tuning Parameter . . . . . . .                                                                              | 252         |\n|                        | 6.3                                       | . . . . . . . . .                                                                                                    | . . . . . . .                                                                                                             | 253         |\n|                        |                                           | Dimension Reduction Methods . .                                                                                      | Dimension Reduction Methods . .                                                                                           |             |\n|                        |                                           | 6.3.1                                                                                                                | Principal Components Regression . . . . . . . . .                                                                         | 254         |\n|                        |                                           | 6.3.2                                                                                                                | Partial Least Squares . . . . . . . . . . . . . . . .                                                                     | 260         |\n|                        | 6.4                                       | 6.4.1 High-Dimensional                                                                                               | Considerations in High Dimensions . . . . . . . . . . . . Data . . . . . . . . . . . . . . .                              | 262 262     |\n|                        |                                           | 6.4.2                                                                                                                | What Goes Wrong in High Dimensions? . . . . .                                                                             | 263         |\n|                        |                                           | 6.4.3                                                                                                                | . . . . . . . . . .                                                                                                       |             |\n|                        |                                           |                                                                                                                      | Regression in High Dimensions                                                                                             | 265         |\n|                        |                                           | 6.4.4                                                                                                                | Interpreting Results in High Dimensions                                                                                   | 266         |\n|                        | 6.5                                       | . . . . . Lab: Linear Models and Regularization Methods . . . . . 6.5.1                                              | . . . . . . . . . . .                                                                                                     | 267 268     |\n|                        |                                           | 6.5.2                                                                                                                | Subset Selection Methods . . Ridge Regression and the Lasso . . . . . . . . .                                             | 273         |\n|                        |                                           | .                                                                                                                    | . . . . . .                                                                                                               | 283         |\n|                        |                                           | 6.5.3                                                                                                                | PCR and PLS Regression . . . . . . . . . . . . .                                                                          | 280         |\n| 7                      | 6.6                                       | Exercises . . . . . . . . . . . . . . . . . . . . . Moving Beyond Linearity                                          | Exercises . . . . . . . . . . . . . . . . . . . . . Moving Beyond Linearity                                               | 289 290     |\n|                        | 7.1 7.2                                   | Polynomial Regression .                                                                                              | . . . . . . . . . . . . . . . . . . .                                                                                     |             |\n|                        |                                           | Step Functions . . . . . . . . . . . . . . . . . . . . . . . . Basis . . . . . . .                                   | Step Functions . . . . . . . . . . . . . . . . . . . . . . . . Basis . . . . . . .                                        | 292         |\n|                        | 7.3                                       | . . .                                                                                                                | . . .                                                                                                                     | 293         |\n|                        | 7.4                                       | Functions . . . . . . . . . . . . . Regression Splines . . . . . . . . . . . . 7.4.1 Piecewise Polynomials . . . . . | . . . . . . . . . .                                                                                                       | 294         |\n|                        |                                           | 7.4.2 7.4.3                                                                                                          | . . . . . . . . . . Constraints and Splines . . . . . . . . . . . . . . The Spline Basis Representation . . . . . . . . . | 294 296 296 |\n|                        |                                           | 7.4.4                                                                                                                | Choosing the Number and Locations                                                                                         | 297         |\n|                        |                                           |                                                                                                                      | of the Knots . . . . . . . . . . . . . . . . . . . . .                                                                    |             |\n|                        |                                           | 7.4.5                                                                                                                | Comparison to Polynomial Regression . . . . . .                                                                           | 299         |\nxii Contents  \n|     | 7.5 Smoothing Splines . . . . . . . . . . . . . . . . . .                                                     | 7.5 Smoothing Splines . . . . . . . . . . . . . . . . . .                                                     |\n|-----|---------------------------------------------------------------------------------------------------------------|---------------------------------------------------------------------------------------------------------------|\n|     | 7.5.1                                                                                                         | An Overview of Smoothing Splines . . . .                                                                      |\n|     | 7.5.2                                                                                                         | Smoothing Parameter λ . .                                                                                     |\n|     |                                                                                                               | Choosing the                                                                                                  |\n| 7.7 | 7.6 Local                                                                                                     | Regression . . . . . . . . . . . . . . . . . . . Generalized Additive Models . . . . . . . . . . . .          |\n|     | 7.7.1                                                                                                         | GAMs for Regression Problems . . . . . .                                                                      |\n|     | 7.7.2                                                                                                         | GAMs for Classification Problems . . . . .                                                                    |\n| 7.8 | Lab:                                                                                                          | Non-Linear Modeling                                                                                           |\n|     | 7.8.1                                                                                                         | . . . . . . . . . . . . . Polynomial Regression and Step Functions                                            |\n|     | 7.8.2                                                                                                         | Splines . . . . . . . . . . . . . . . . . . . .                                                               |\n|     | 7.8.3                                                                                                         | Smoothing Splines and GAMs . . . . . . . . . .                                                                |\n|     | 7.8.4                                                                                                         | Local Regression . . . . . . .                                                                                |\n|     |                                                                                                               | . . . . . . . . . . . .                                                                                       |\n|     | 7.9 Exercises                                                                                                 | . . . . . .                                                                                                   |\n|     | . . . . . . . . . . .                                                                                         | . . . . . . . . . . .                                                                                         |\n| 8   | Tree-Based Methods                                                                                            | Tree-Based Methods                                                                                            |\n|     | 8.1 The Basics                                                                                                | of Decision Trees . . . . . . . . . . . . . . . . . .                                                         |\n|     | 8.1.1                                                                                                         | Regression Trees . . . . . . . .                                                                              |\n|     | 8.1.2                                                                                                         | Classification Trees . . . . . . . . . . . . .                                                                |\n|     | 8.1.3                                                                                                         | Trees Versus Linear Models . . . . . . . .                                                                    |\n|     | 8.1.4                                                                                                         | Advantages and Disadvantages of Trees . .                                                                     |\n| 8.2 | Bagging, Random Forests, Boosting, and Bayesian Additive Regression . . . . . . . . . . . . . . . . . . . . . | Bagging, Random Forests, Boosting, and Bayesian Additive Regression . . . . . . . . . . . . . . . . . . . . . |\n|     |                                                                                                               | Trees                                                                                                         |\n|     | 8.2.1 .                                                                                                       | Bagging . . . . . . . . . . . . . . . . . .                                                                   |\n|     | 8.2.2                                                                                                         | Random Forests . . . . . . . . . . . . . . .                                                                  |\n|     | 8.2.3 8.2.4                                                                                                   | Boosting . . . . . . . . . . . . . . . . . . . Bayesian Additive Regression Trees . . . .                     |\n| 8.3 | Lab: Tree-Based Methods . . . Fitting Classification Trees                                                    | . . . . . . . . . . . . . . . . . . . .                                                                       |\n|     | Vector 9.1 Maximal                                                                                            | Machines . . . . . .                                                                                          |\n|     | 8.3.1 8.3.5 Bayesian Exercises . . . . .                                                                      | Additive Regression Trees . . . . . . . . . . . . . . . . . . . . . .                                         |\n|     | 8.3.2                                                                                                         | Fitting Regression Trees . . . . . . . . . .                                                                  |\n| 8.4 | 8.3.3 8.3.4                                                                                                   | Bagging and Random Forests . . . . . . . Boosting . . . . . . . . . . . . . . . . . . .                       |\n| 9   |                                                                                                               |                                                                                                               |\n|     | Support                                                                                                       | Margin Classifier . . . . . . .                                                                               |\n|     | 9.1.1                                                                                                         | What Is a Hyperplane? . . . . . . . . . . .                                                                   |\n|     | 9.1.2 9.1.3                                                                                                   | Classification Using a Separating Hyperplane The Maximal Margin Classifier . . . . .                          |\n|     |                                                                                                               | . Classifier                                                                                                  |\n|     |                                                                                                               | Margin                                                                                                        |\n|     | . .                                                                                                           | . . . . . .                                                                                                   |\n|     | 9.1.4                                                                                                         | . . . . . . . .                                                                                               |\n|     |                                                                                                               | Construction of the Maximal The Non-separable Case . . . . . .                                                |\n|     | 9.1.5                                                                                                         |                                                                                                               |\n| 9.2 | Support Vector                                                                                                | Classifiers . .                                                                                               |\n|     | 9.2.1                                                                                                         | Overview of the Support                                                                                       |\n|     | 9.2.2                                                                                                         | of the Support Vector Classifier . .                                                                          |\n| 9.3 | Details Support Vector Machines . . . . . . . . . . . .                                                       | .                                                                                                             |\n|     |                                                                                                               | Vector Classifier .                                                                                           |\n|     | 9.3.1                                                                                                         | Classification with Non-Linear Decision                                                                       |\n|     |                                                                                                               | Boundaries . . . . . . . . . . . . . . . . .                                                                  |\n|     | 9.3.2                                                                                                         | The Support Vector Machine . . . . . . .                                                                      |  \n|                                                                                                                                       | 9.3.3                                                                                                                                 | An Application to the Heart                                                                                                           | Disease                                                                                                                               |                                                                                                                                       | Data .                                                                                                                                | . .                                                                                                                                   | . .                                                                                                                                   | 382                                                                   | 382                                                                   | 382                                                                   |\n|---------------------------------------------------------------------------------------------------------------------------------------|---------------------------------------------------------------------------------------------------------------------------------------|---------------------------------------------------------------------------------------------------------------------------------------|---------------------------------------------------------------------------------------------------------------------------------------|---------------------------------------------------------------------------------------------------------------------------------------|---------------------------------------------------------------------------------------------------------------------------------------|---------------------------------------------------------------------------------------------------------------------------------------|---------------------------------------------------------------------------------------------------------------------------------------|-----------------------------------------------------------------------|-----------------------------------------------------------------------|-----------------------------------------------------------------------|\n| 9.4                                                                                                                                   | SVMs                                                                                                                                  | with More than Two Classes                                                                                                            | . .                                                                                                                                   | . . .                                                                                                                                 | . . . . .                                                                                                                             | . .                                                                                                                                   | . .                                                                                                                                   | 383                                                                   | 383                                                                   | 383                                                                   |\n|                                                                                                                                       | 9.4.1                                                                                                                                 | One-Versus-One Classification                                                                                                         | . .                                                                                                                                   | . . .                                                                                                                                 | . . .                                                                                                                                 | .                                                                                                                                     | . .                                                                                                                                   | 384                                                                   | 384                                                                   | 384                                                                   |\n|                                                                                                                                       | 9.4.2                                                                                                                                 | One-Versus-All Classification                                                                                                         | . . .                                                                                                                                 | . .                                                                                                                                   | . . . .                                                                                                                               | .                                                                                                                                     | . .                                                                                                                                   | 384                                                                   | 384                                                                   | 384                                                                   |\n| 9.5                                                                                                                                   | Relationship                                                                                                                          | to Logistic Regression                                                                                                                | . .                                                                                                                                   | . . . .                                                                                                                               | . . . .                                                                                                                               | .                                                                                                                                     | . .                                                                                                                                   | 384                                                                   | 384                                                                   | 384                                                                   |\n| 9.6                                                                                                                                   | Lab: Support Vector                                                                                                                   | Machines                                                                                                                              | . . . . .                                                                                                                             | . . .                                                                                                                                 | . . . . .                                                                                                                             | .                                                                                                                                     | . .                                                                                                                                   | 387                                                                   | 387                                                                   | 387                                                                   |\n|                                                                                                                                       | 9.6.1                                                                                                                                 | Support Vector Classifier                                                                                                             | . . . .                                                                                                                               | . . .                                                                                                                                 | . . . . .                                                                                                                             | .                                                                                                                                     | . .                                                                                                                                   | 387                                                                   | 387                                                                   | 387                                                                   |\n|                                                                                                                                       | 9.6.2                                                                                                                                 | Support Vector Machine                                                                                                                | . . . .                                                                                                                               | . .                                                                                                                                   | . . . . . .                                                                                                                           | .                                                                                                                                     | . .                                                                                                                                   | 390                                                                   | 390                                                                   | 390                                                                   |\n|                                                                                                                                       | 9.6.3                                                                                                                                 | ROC Curves . . .                                                                                                                      | . . . . .                                                                                                                             | . . .                                                                                                                                 | . . . . . .                                                                                                                           | . . .                                                                                                                                 | . .                                                                                                                                   | 392                                                                   | 392                                                                   | 392                                                                   |\n|                                                                                                                                       | 9.6.4                                                                                                                                 | SVM with Multiple Classes                                                                                                             | . . .                                                                                                                                 | . .                                                                                                                                   | . . . . .                                                                                                                             | .                                                                                                                                     | . .                                                                                                                                   | 393                                                                   | 393                                                                   | 393                                                                   |\n|                                                                                                                                       | 9.6.5                                                                                                                                 | Application to Gene Expression                                                                                                        | Data                                                                                                                                  | . .                                                                                                                                   | . . .                                                                                                                                 | .                                                                                                                                     | . .                                                                                                                                   | 394                                                                   | 394                                                                   | 394                                                                   |\n| 9.7                                                                                                                                   | Exercises . . . . . . . . . . . . . . . . . . . . . . . . . . . . 395                                                                 | Exercises . . . . . . . . . . . . . . . . . . . . . . . . . . . . 395                                                                 | Exercises . . . . . . . . . . . . . . . . . . . . . . . . . . . . 395                                                                 | Exercises . . . . . . . . . . . . . . . . . . . . . . . . . . . . 395                                                                 | Exercises . . . . . . . . . . . . . . . . . . . . . . . . . . . . 395                                                                 | Exercises . . . . . . . . . . . . . . . . . . . . . . . . . . . . 395                                                                 | Exercises . . . . . . . . . . . . . . . . . . . . . . . . . . . . 395                                                                 | Exercises . . . . . . . . . . . . . . . . . . . . . . . . . . . . 395 | Exercises . . . . . . . . . . . . . . . . . . . . . . . . . . . . 395 | Exercises . . . . . . . . . . . . . . . . . . . . . . . . . . . . 395 |\n| 10 Deep Learning                                                                                                                      | 10 Deep Learning                                                                                                                      | 10 Deep Learning                                                                                                                      | 10 Deep Learning                                                                                                                      | 10 Deep Learning                                                                                                                      | 10 Deep Learning                                                                                                                      | 10 Deep Learning                                                                                                                      | 10 Deep Learning                                                                                                                      | 399                                                                   | 399                                                                   | 399                                                                   |\n| 10.1 Single Layer Neural Networks . . . . . . . . . . . . . . . .                                                                     | 10.1 Single Layer Neural Networks . . . . . . . . . . . . . . . .                                                                     | 10.1 Single Layer Neural Networks . . . . . . . . . . . . . . . .                                                                     | 10.1 Single Layer Neural Networks . . . . . . . . . . . . . . . .                                                                     | 10.1 Single Layer Neural Networks . . . . . . . . . . . . . . . .                                                                     | 10.1 Single Layer Neural Networks . . . . . . . . . . . . . . . .                                                                     | 10.1 Single Layer Neural Networks . . . . . . . . . . . . . . . .                                                                     | 10.1 Single Layer Neural Networks . . . . . . . . . . . . . . . .                                                                     | 400                                                                   | 400                                                                   | 400                                                                   |\n| 10.2 Multilayer Neural Networks . . . . . . . . . . . . . . . . . .                                                                   | 10.2 Multilayer Neural Networks . . . . . . . . . . . . . . . . . .                                                                   | 10.2 Multilayer Neural Networks . . . . . . . . . . . . . . . . . .                                                                   | 10.2 Multilayer Neural Networks . . . . . . . . . . . . . . . . . .                                                                   | 10.2 Multilayer Neural Networks . . . . . . . . . . . . . . . . . .                                                                   | 10.2 Multilayer Neural Networks . . . . . . . . . . . . . . . . . .                                                                   | 10.2 Multilayer Neural Networks . . . . . . . . . . . . . . . . . .                                                                   | 10.2 Multilayer Neural Networks . . . . . . . . . . . . . . . . . .                                                                   | 402                                                                   | 402                                                                   | 402                                                                   |\n| 10.3 Convolutional Neural Networks . . . . . . . . . . . . . . . .                                                                    | 10.3 Convolutional Neural Networks . . . . . . . . . . . . . . . .                                                                    | 10.3 Convolutional Neural Networks . . . . . . . . . . . . . . . .                                                                    | 10.3 Convolutional Neural Networks . . . . . . . . . . . . . . . .                                                                    | 10.3 Convolutional Neural Networks . . . . . . . . . . . . . . . .                                                                    | 10.3 Convolutional Neural Networks . . . . . . . . . . . . . . . .                                                                    | 10.3 Convolutional Neural Networks . . . . . . . . . . . . . . . .                                                                    | 10.3 Convolutional Neural Networks . . . . . . . . . . . . . . . .                                                                    | 406                                                                   | 406                                                                   | 406                                                                   |\n|                                                                                                                                       | Convolution Layers . . . . . . . . . . . . . . . . . . . . .                                                                          | Convolution Layers . . . . . . . . . . . . . . . . . . . . .                                                                          | Convolution Layers . . . . . . . . . . . . . . . . . . . . .                                                                          | Convolution Layers . . . . . . . . . . . . . . . . . . . . .                                                                          | Convolution Layers . . . . . . . . . . . . . . . . . . . . .                                                                          | Convolution Layers . . . . . . . . . . . . . . . . . . . . .                                                                          | Convolution Layers . . . . . . . . . . . . . . . . . . . . .                                                                          | 407                                                                   | 407                                                                   | 407                                                                   |\n|                                                                                                                                       | 10.3.1                                                                                                                                |                                                                                                                                       |                                                                                                                                       |                                                                                                                                       |                                                                                                                                       |                                                                                                                                       |                                                                                                                                       | 410                                                                   | 410                                                                   | 410                                                                   |\n|                                                                                                                                       | 10.3.2                                                                                                                                | Pooling Layers                                                                                                                        | . . . . . . . . . . . . . . .                                                                                                         |                                                                                                                                       | .                                                                                                                                     |                                                                                                                                       | .                                                                                                                                     |                                                                       |                                                                       |                                                                       |\n|                                                                                                                                       | 10.3.3                                                                                                                                | Architecture of a Convolutional                                                                                                       | Neural                                                                                                                                |                                                                                                                                       | Network                                                                                                                               |                                                                                                                                       | . .                                                                                                                                   | 410                                                                   | 410                                                                   | 410                                                                   |\n|                                                                                                                                       | 10.3.4                                                                                                                                | Data Augmentation .                                                                                                                   | . . . . . .                                                                                                                           | . .                                                                                                                                   | . . . . . .                                                                                                                           | .                                                                                                                                     | . .                                                                                                                                   | 411                                                                   | 411                                                                   | 411                                                                   |\n|                                                                                                                                       | 10.3.5                                                                                                                                | Results Using a Pretrained                                                                                                            | Classifier .                                                                                                                          | .                                                                                                                                     | . . .                                                                                                                                 | .                                                                                                                                     | . .                                                                                                                                   | 412                                                                   | 412                                                                   | 412                                                                   |\n| 10.4                                                                                                                                  | Document                                                                                                                              | Classification . . . .                                                                                                                |                                                                                                                                       |                                                                                                                                       |                                                                                                                                       | .                                                                                                                                     | .                                                                                                                                     | 413                                                                   | 413                                                                   | 413                                                                   |\n|                                                                                                                                       | . . . . . . . . . . . . . .                                                                                                           | . . . . . . . . . . . . . .                                                                                                           | . . . . . . . . . . . . . .                                                                                                           | . . . . . . . . . . . . . .                                                                                                           | . . . . . . . . . . . . . .                                                                                                           | . . . . . . . . . . . . . .                                                                                                           | . . . . . . . . . . . . . .                                                                                                           | 416                                                                   | 416                                                                   | 416                                                                   |\n| 10.5                                                                                                                                  | Recurrent Neural Networks . . . . . . . . . . . . . . . . . . .                                                                       | Recurrent Neural Networks . . . . . . . . . . . . . . . . . . .                                                                       | Recurrent Neural Networks . . . . . . . . . . . . . . . . . . .                                                                       | Recurrent Neural Networks . . . . . . . . . . . . . . . . . . .                                                                       | Recurrent Neural Networks . . . . . . . . . . . . . . . . . . .                                                                       | Recurrent Neural Networks . . . . . . . . . . . . . . . . . . .                                                                       | Recurrent Neural Networks . . . . . . . . . . . . . . . . . . .                                                                       | 418                                                                   | 418                                                                   | 418                                                                   |\n|                                                                                                                                       | Sequential Models for Document Classification . . . . .                                                                               | Sequential Models for Document Classification . . . . .                                                                               | Sequential Models for Document Classification . . . . .                                                                               | Sequential Models for Document Classification . . . . .                                                                               | Sequential Models for Document Classification . . . . .                                                                               | Sequential Models for Document Classification . . . . .                                                                               | Sequential Models for Document Classification . . . . .                                                                               |                                                                       |                                                                       |                                                                       |\n|                                                                                                                                       | 10.5.1                                                                                                                                |                                                                                                                                       |                                                                                                                                       |                                                                                                                                       |                                                                                                                                       | . .                                                                                                                                   | .                                                                                                                                     | 420                                                                   | 420                                                                   | 420                                                                   |\n|                                                                                                                                       | 10.5.2                                                                                                                                | Time Series Forecasting                                                                                                               | . . . . . . . . . . .                                                                                                                 |                                                                                                                                       | .                                                                                                                                     | .                                                                                                                                     |                                                                                                                                       | 424                                                                   | 424                                                                   | 424                                                                   |\n|                                                                                                                                       | 10.5.3                                                                                                                                | Summary of RNNs .                                                                                                                     | . . . . . . . . . . . . .                                                                                                             | . . .                                                                                                                                 | . . . . . . .                                                                                                                         | . .                                                                                                                                   | . . . .                                                                                                                               | 425                                                                   | 425                                                                   | 425                                                                   |\n| 10.6 10.7                                                                                                                             | When to Fitting                                                                                                                       | Use Deep Learning a Neural Network .                                                                                                  | . .                                                                                                                                   | .                                                                                                                                     | .                                                                                                                                     | .                                                                                                                                     | .                                                                                                                                     | 427                                                                   | 427                                                                   | 427                                                                   |\n|                                                                                                                                       | . . . . . . . . . . . . . . . . . . . . . . . . . . . . .                                                                             | . . . . . . . . . . . . . . . . . . . . . . . . . . . . .                                                                             | . . . . . . . . . . . . . . . . . . . . . . . . . . . . .                                                                             | . . . . . . . . . . . . . . . . . . . . . . . . . . . . .                                                                             | . . . . . . . . . . . . . . . . . . . . . . . . . . . . .                                                                             | . . . . . . . . . . . . . . . . . . . . . . . . . . . . .                                                                             | . . . . . . . . . . . . . . . . . . . . . . . . . . . . .                                                                             | 428                                                                   | 428                                                                   | 428                                                                   |\n|                                                                                                                                       | 10.7.1                                                                                                                                | Backpropagation                                                                                                                       | . . Stochastic Gradient                                                                                                               |                                                                                                                                       |                                                                                                                                       | Descent .                                                                                                                             | . .                                                                                                                                   | 429                                                                   | 429                                                                   | 429                                                                   |\n|                                                                                                                                       | 10.7.2                                                                                                                                | Regularization and Dropout Learning .                                                                                                 | . . . . . . .                                                                                                                         | . .                                                                                                                                   | . . . .                                                                                                                               | . .                                                                                                                                   | . .                                                                                                                                   | 431                                                                   | 431                                                                   | 431                                                                   |\n|                                                                                                                                       | 10.7.3                                                                                                                                |                                                                                                                                       | .                                                                                                                                     | .                                                                                                                                     | .                                                                                                                                     | .                                                                                                                                     |                                                                                                                                       |                                                                       |                                                                       |                                                                       |\n|                                                                                                                                       | 10.7.4                                                                                                                                | Network Tuning .                                                                                                                      | . . . . . .                                                                                                                           | .                                                                                                                                     | . . . .                                                                                                                               | . .                                                                                                                                   | . .                                                                                                                                   | 431                                                                   | 431                                                                   | 431                                                                   |\n| 10.8                                                                                                                                  | Interpolation and Double Descent . . . . . . . . . . . . .                                                                            | Interpolation and Double Descent . . . . . . . . . . . . .                                                                            | Interpolation and Double Descent . . . . . . . . . . . . .                                                                            | Interpolation and Double Descent . . . . . . . . . . . . .                                                                            | Interpolation and Double Descent . . . . . . . . . . . . .                                                                            | Interpolation and Double Descent . . . . . . . . . . . . .                                                                            | .                                                                                                                                     | 432                                                                   | 432                                                                   | 432                                                                   |\n| 10.9                                                                                                                                  | Lab: Deep Learning . . . . . . . . . . . . . . . . . . . . .                                                                          | Lab: Deep Learning . . . . . . . . . . . . . . . . . . . . .                                                                          | Lab: Deep Learning . . . . . . . . . . . . . . . . . . . . .                                                                          | Lab: Deep Learning . . . . . . . . . . . . . . . . . . . . .                                                                          | Lab: Deep Learning . . . . . . . . . . . . . . . . . . . . .                                                                          | Lab: Deep Learning . . . . . . . . . . . . . . . . . . . . .                                                                          | Lab: Deep Learning . . . . . . . . . . . . . . . . . . . . .                                                                          | 435                                                                   | 435                                                                   | 435                                                                   |\n|                                                                                                                                       | . on Hitters . . .                                                                                                                    | . on Hitters . . .                                                                                                                    | . on Hitters . . .                                                                                                                    | . on Hitters . . .                                                                                                                    | . on Hitters . . .                                                                                                                    | . on Hitters . . .                                                                                                                    | . on Hitters . . .                                                                                                                    |                                                                       |                                                                       |                                                                       |\n|                                                                                                                                       | 10.9.1                                                                                                                                | Single Layer Network                                                                                                                  | Data                                                                                                                                  | .                                                                                                                                     | .                                                                                                                                     | .                                                                                                                                     | .                                                                                                                                     | 437                                                                   | 437                                                                   | 437                                                                   |\n|                                                                                                                                       | 10.9.2                                                                                                                                | Multilayer                                                                                                                            | Network on the MNIST                                                                                                                  | Digit                                                                                                                                 | Data                                                                                                                                  | . .                                                                                                                                   | . .                                                                                                                                   | 444                                                                   | 444                                                                   | 444                                                                   |\n|                                                                                                                                       | 10.9.3                                                                                                                                | Convolutional                                                                                                                         | Neural Networks . . .                                                                                                                 | .                                                                                                                                     | . .                                                                                                                                   | .                                                                                                                                     | . .                                                                                                                                   | 448                                                                   | 448                                                                   | 448                                                                   |\n|                                                                                                                                       | 10.9.4                                                                                                                                | Using Pretrained CNN Models                                                                                                           | . . . . .                                                                                                                             | .                                                                                                                                     | . .                                                                                                                                   | .                                                                                                                                     | .                                                                                                                                     | 452                                                                   | 452                                                                   | 452                                                                   |\n|                                                                                                                                       |                                                                                                                                       | Classification                                                                                                                        | . .                                                                                                                                   | . .                                                                                                                                   | .                                                                                                                                     | .                                                                                                                                     | . .                                                                                                                                   | 454                                                                   | 454                                                                   | 454                                                                   |\n|                                                                                                                                       | 10.9.5 10.9.6 Recurrent                                                                                                               | IMDB Document Neural                                                                                                                  | . . Networks                                                                                                                          | . . .                                                                                                                                 | . . . . . . . . .                                                                                                                     | .                                                                                                                                     | . . .                                                                                                                                 | 458                                                                   | 458                                                                   | 458                                                                   |\n| 10.10 Exercises . . . . . . . . . . . . . . . . . . . . . . . . . . . . 11                                                            | 10.10 Exercises . . . . . . . . . . . . . . . . . . . . . . . . . . . . 11                                                            | 10.10 Exercises . . . . . . . . . . . . . . . . . . . . . . . . . . . . 11                                                            | 10.10 Exercises . . . . . . . . . . . . . . . . . . . . . . . . . . . . 11                                                            | 10.10 Exercises . . . . . . . . . . . . . . . . . . . . . . . . . . . . 11                                                            | 10.10 Exercises . . . . . . . . . . . . . . . . . . . . . . . . . . . . 11                                                            | 10.10 Exercises . . . . . . . . . . . . . . . . . . . . . . . . . . . . 11                                                            | 10.10 Exercises . . . . . . . . . . . . . . . . . . . . . . . . . . . . 11                                                            | 469                                                                   | 469                                                                   | 469                                                                   |\n| Survival Analysis and Censored Data                                                                                                   | Survival Analysis and Censored Data                                                                                                   | Survival Analysis and Censored Data                                                                                                   | Survival Analysis and Censored Data                                                                                                   | Survival Analysis and Censored Data                                                                                                   | Survival Analysis and Censored Data                                                                                                   | Survival Analysis and Censored Data                                                                                                   | Survival Analysis and Censored Data                                                                                                   |                                                                       |                                                                       |                                                                       |\n|                                                                                                                                       |                                                                                                                                       |                                                                                                                                       |                                                                                                                                       |                                                                                                                                       |                                                                                                                                       |                                                                                                                                       |                                                                                                                                       | 470                                                                   | 470                                                                   | 470                                                                   |\n| 11.1 Survival and Censoring Times . . . . . . . . . . . . . . . . 11.2 A Closer Look at Censoring . . . . . . . . . . . . . . . . . . | 11.1 Survival and Censoring Times . . . . . . . . . . . . . . . . 11.2 A Closer Look at Censoring . . . . . . . . . . . . . . . . . . | 11.1 Survival and Censoring Times . . . . . . . . . . . . . . . . 11.2 A Closer Look at Censoring . . . . . . . . . . . . . . . . . . | 11.1 Survival and Censoring Times . . . . . . . . . . . . . . . . 11.2 A Closer Look at Censoring . . . . . . . . . . . . . . . . . . | 11.1 Survival and Censoring Times . . . . . . . . . . . . . . . . 11.2 A Closer Look at Censoring . . . . . . . . . . . . . . . . . . | 11.1 Survival and Censoring Times . . . . . . . . . . . . . . . . 11.2 A Closer Look at Censoring . . . . . . . . . . . . . . . . . . | 11.1 Survival and Censoring Times . . . . . . . . . . . . . . . . 11.2 A Closer Look at Censoring . . . . . . . . . . . . . . . . . . | 11.1 Survival and Censoring Times . . . . . . . . . . . . . . . . 11.2 A Closer Look at Censoring . . . . . . . . . . . . . . . . . . | 470                                                                   | 470                                                                   | 470                                                                   |\n| The                                                                                                                                   | The                                                                                                                                   | The                                                                                                                                   | The                                                                                                                                   | The                                                                                                                                   |                                                                                                                                       | 11.3                                                                                                                                  |                                                                                                                                       | Kaplan-Meier Survival Curve . . . . . . . . . . . . . .               | 472                                                                   |                                                                       |\n| .                                                                                                                                     | .                                                                                                                                     | .                                                                                                                                     | .                                                                                                                                     |                                                                                                                                       | . .                                                                                                                                   | .                                                                                                                                     | .                                                                                                                                     | 474                                                                   | 474                                                                   | 474                                                                   |\n| 11.4 The Log-Rank Test . . . . . . . . . . . . . . . . 11.5 Regression Models With a Survival Response . .                            | 11.4 The Log-Rank Test . . . . . . . . . . . . . . . . 11.5 Regression Models With a Survival Response . .                            | 11.4 The Log-Rank Test . . . . . . . . . . . . . . . . 11.5 Regression Models With a Survival Response . .                            | 11.4 The Log-Rank Test . . . . . . . . . . . . . . . . 11.5 Regression Models With a Survival Response . .                            |                                                                                                                                       | . . . .                                                                                                                               | . .                                                                                                                                   | . 476                                                                                                                                 | . 476                                                                 | . 476                                                                 | . 476                                                                 |  \n|              | 11.5.1                                                                                                                        | The Hazard Function . . . . . . . . . . . . . . . . . 476                                                                                         |\n|--------------|-------------------------------------------------------------------------------------------------------------------------------|---------------------------------------------------------------------------------------------------------------------------------------------------|\n|              | 11.5.2                                                                                                                        | Proportional Hazards . . . . . . . . . . . . . . . . . 478                                                                                        |\n|              | 11.5.3                                                                                                                        | Example: Brain Cancer Data . . . . . . . . . . . . 482                                                                                            |\n|              | 11.5.4                                                                                                                        | Example: Publication Data . . . . . . . . . . . . . 482                                                                                           |\n| 11.6         | Shrinkage for the Cox Model . . . . . . . . . . . . . . . . . 484 486                                                         | Shrinkage for the Cox Model . . . . . . . . . . . . . . . . . 484 486                                                                             |\n| 11.7         | Additional Topics . . . . . . . . . . . . . . . . . . . . . . .                                                               | Additional Topics . . . . . . . . . . . . . . . . . . . . . . .                                                                                   |\n|              | 11.7.1                                                                                                                        | Area Under the Curve for Survival Analysis . . . . 486                                                                                            |\n|              | 11.7.2                                                                                                                        | Choice of Time Scale . . . . . . . . . . . . . . . . . 487                                                                                        |\n|              | 11.7.3                                                                                                                        | Time-Dependent Covariates . . . . . . . . . . . . . 488                                                                                           |\n|              | 11.7.4                                                                                                                        | Checking the Proportional Hazards Assumption . . 488                                                                                              |\n|              | 11.7.5                                                                                                                        | Survival Trees . . . . . . . . . . . . . . . . . . . . . 488                                                                                      |\n| 11.8         | Lab: Survival Analysis . . . . . . . . . . . . . . . . . . . . . 489                                                          | Lab: Survival Analysis . . . . . . . . . . . . . . . . . . . . . 489                                                                              |\n|              |                                                                                                                               | Brain Cancer Data . . . . . . . . . . . . . . . . . 489                                                                                           |\n|              | 11.8.1                                                                                                                        | .                                                                                                                                                 |\n|              | 11.8.2                                                                                                                        | Publication Data . . . . . . . . . . . . . . . . . . . 493                                                                                        |\n| 11.9         | 11.8.3                                                                                                                        | Call Center Data . . . . . . . . . . . . . . . . . . . 494 . .                                                                                    |\n|              | Exercises                                                                                                                     | . . . . . . . . . . . . . . . . . . . . . . . . . . 498                                                                                           |\n| Unsupervised |                                                                                                                               |                                                                                                                                                   |\n| 12           | Learning 503                                                                                                                  | Learning 503                                                                                                                                      |\n| 12.1 12.2    | The Challenge of Unsupervised Learning                                                                                        | . . . . . . . . . . 503 . . . .                                                                                                                   |\n|              | Principal                                                                                                                     | Components Analysis . . . . . . . . . . . . 504                                                                                                   |\n|              | 12.2.1 12.2.2                                                                                                                 | What Are Principal Components? . . . . . . . . . . 505 Another Interpretation of Principal Components . 508                                       |\n|              | 12.2.3                                                                                                                        | The Proportion of Variance Explained . . . . . . . 510                                                                                            |\n|              | 12.2.4                                                                                                                        | More on PCA . . . . . . . . . . . . . . . . . . . . . 512                                                                                         |\n| 12.3         | 12.2.5 Other Uses for Principal Components . . . . . . . . 515 Missing Values and Matrix Completion . . . . . . . . . . . 515 | 12.2.5 Other Uses for Principal Components . . . . . . . . 515 Missing Values and Matrix Completion . . . . . . . . . . . 515                     |\n| 12.4         |                                                                                                                               | Methods . . . . . . . . . . . . . . . . . . . . . . 520                                                                                           |\n|              | Clustering 12.4.1                                                                                                             | K -Means Clustering . . . . . . . . . . . . . . . . . 521 .                                                                                       |\n| 12.5         | 12.4.2 12.4.3                                                                                                                 | Hierarchical Clustering . . . . . . . . . . . . . . . 525 Practical Issues in Clustering . . . . . . . . . . . . 532                              |\n|              | Lab: Unsupervised Learning . . . . . . . . . . . . . . . . . 535                                                              | Lab: Unsupervised Learning . . . . . . . . . . . . . . . . . 535                                                                                  |\n|              | 12.5.1                                                                                                                        | Principal Components Analysis . . . . . . . . . . 535                                                                                             |\n|              | 12.5.2                                                                                                                        | . Matrix Completion . . . . . . . . . . . . . . . . . . 539                                                                                       |\n|              | 12.5.3 12.5.4                                                                                                                 | Clustering . . . . . . . . . . . . . . . . . . . . . . . 542 NCI60 Data Example . . . . . . . . . . . . . . . . . 546                             |\n| 13           | Multiple Testing 557 . . . . . . . . . . . 558                                                                                | Multiple Testing 557 . . . . . . . . . . . 558                                                                                                    |\n| 13.1         | A Quick                                                                                                                       | Review of Hypothesis Testing                                                                                                                      |\n|              |                                                                                                                               | Testing a Hypothesis . . . . . . . . . . . . . 558                                                                                                |\n|              | 13.1.1                                                                                                                        | .                                                                                                                                                 |\n| 13.2 13.3    | 13.1.2                                                                                                                        | . . . Type I and Type II Errors . . . . . . . . . . . 562                                                                                         |\n|              | The The                                                                                                                       | . . . Challenge of Multiple Testing . . . . . . . . . . . . . . 563                                                                               |\n| 13.4         | 13.3.1 13.3.2 Approaches to Control                                                                                           | Family-Wise Error Rate . . . . . . . . . . . . . . . . . 565 What is the Family-Wise Error Rate? . . . . . . . 565 the Family-Wise Error Rate 567 |  \n| 13.5   | A Re-Sampling Approach to p -Values and False Discovery                                |\n|--------|----------------------------------------------------------------------------------------|\n|        | Rates . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 577                  |\n|        | 13.5.1 A Re-Sampling Approach to the p -Value . . . . . . 578                          |\n|        | 13.5.2 A Re-Sampling Approach to the False Discovery Rate579                           |\n|        | 13.5.3 When Are Re-Sampling Approaches Useful? . . . . 581 . . . . . . . . . . . . 583 |\n| 13.6   | Lab: Multiple Testing . . . . . . . . .                                                |\n|        | 13.6.1 Review of Hypothesis Tests . . . . . . . . . . . . . 583                        |\n|        | 13.6.2 Family-Wise Error Rate . . . . . . . . . . . . . . . 585                        |\n|        | 13.6.3 False Discovery Rate . . . . . . . . . . . . . . . . . 588                      |\n|        | 13.6.4 A Re-Sampling Approach . . . . . . . . . . . . . . 590                          |\n| 13.7   | Exercises . . . . . . . . . . . . . . . . . . . . . . . . . . . . 593                  |\n| Index  | Index                                                                                  |', '### 2.1.3 The Trade-Off Between Prediction Accuracy and Model Interpretability  \nOf the many methods that we examine in this book, some are less flexible, or more restrictive, in the sense that they can produce just a relatively small range of shapes to estimate f . For example, linear regression is a relatively inflexible approach, because it can only generate linear functions such as the lines shown in Figure 2.1 or the plane shown in Figure 2.4. Other methods, such as the thin plate splines shown in Figures 2.5 and 2.6, are considerably more flexible because they can generate a much wider range of possible shapes to estimate f .  \nFIGURE 2.7. A representation of the tradeoff between flexibility and interpretability, using different statistical learning methods. In general, as the flexibility of a method increases, its interpretability decreases.  \n<!-- image -->  \nOne might reasonably ask the following question: why would we ever choose to use a more restrictive method instead of a very flexible approach? There are several reasons that we might prefer a more restrictive model. If we are mainly interested in inference, then restrictive models are much more interpretable. For instance, when inference is the goal, the linear model may be a good choice since it will be quite easy to understand the relationship between Y and X 1  X 2         X p . In contrast, very flexible approaches, such as the splines discussed in Chapter 7 and displayed in Figures 2.5 and 2.6, and the boosting methods discussed in Chapter 8, can lead to such complicated estimates of f that it is difficult to understand how any individual predictor is associated with the response.  \nFigure 2.7 provides an illustration of the trade-off between flexibility and interpretability for some of the methods that we cover in this book. Least squares linear regression, discussed in Chapter 3, is relatively inflexible but is quite interpretable. The lasso , discussed in Chapter 6, relies upon the linear model (2.4) but uses an alternative fitting procedure for estimating the coefficients β 0  β 1         β p . The new procedure is more restrictive in estimating the coefficients, and sets a number of them to exactly zero. Hence in this sense the lasso is a less flexible approach than linear regression. It is also more interpretable than linear regression, because in the final model the response variable will only be related to a small subset of the predictors-namely, those with nonzero coefficient estimates. Generalized additive models (GAMs), discussed in Chapter 7, instead extend the linear model (2.4) to allow for certain non-linear relationships. Consequently, GAMs are more flexible than linear regression. They are also somewhat less interpretable than linear regression, because the relationship between each predictor and the response is now modeled using a curve. Finally, fully non-linear methods such as bagging , boosting , support vector machines with non-linear kernels, and neural networks (deep learning), discussed in Chapters 8, 9, and 10, are highly flexible approaches that are harder to interpret.  \nlasso generalized additive model  \nbagging boosting support vector machine  \nWe have established that when inference is the goal, there are clear advantages to using simple and relatively inflexible statistical learning methods. In some settings, however, we are only interested in prediction, and the interpretability of the predictive model is simply not of interest. For instance, if we seek to develop an algorithm to predict the price of a stock, our sole requirement for the algorithm is that it predict accuratelyinterpretability is not a concern. In this setting, we might expect that it will be best to use the most flexible model available. Surprisingly, this is not always the case! We will often obtain more accurate predictions using a less flexible method. This phenomenon, which may seem counterintuitive at first glance, has to do with the potential for overfitting in highly flexible methods. We saw an example of overfitting in Figure 2.6. We will discuss this very important concept further in Section 2.2 and throughout this book.']","Lo siento, pero no tengo información específica sobre el 'Principio de jerarquía' en la selección de modelos en el contexto proporcionado. Si tienes otra pregunta o necesitas información sobre otro tema relacionado con el aprendizaje automático, estaré encantado de ayudarte.","El principio de jerarquía establece que si se incluye un término de interacción en un modelo, también se deben incluir los efectos principales correspondientes, incluso si sus coeficientes no son estadísticamente significativos. Esto se debe a que la interacción suele estar correlacionada con los efectos principales, y omitirlos puede cambiar el significado de la interacción.",0.3333333333333333,,0.0,1.0
What are regression splines and how do they differ from polynomial regression?,"['# 7 Moving Beyond Linearity  \n<!-- image -->  \nSo far in this book, we have mostly focused on linear models. Linear models are relatively simple to describe and implement, and have advantages over other approaches in terms of interpretation and inference. However, standard linear regression can have significant limitations in terms of predictive power. This is because the linearity assumption is almost always an approximation, and sometimes a poor one. In Chapter 6 we see that we can improve upon least squares using ridge regression, the lasso, principal components regression, and other techniques. In that setting, the improvement is obtained by reducing the complexity of the linear model, and hence the variance of the estimates. But we are still using a linear model, which can only be improved so far! In this chapter we relax the linearity assumption while still attempting to maintain as much interpretability as possible. We do this by examining very simple extensions of linear models like polynomial regression and step functions, as well as more sophisticated approaches such as splines, local regression, and generalized additive models.  \n- Polynomial regression extends the linear model by adding extra predictors, obtained by raising each of the original predictors to a power. For example, a cubic regression uses three variables, X , X 2 , and X 3 , as predictors. This approach provides a simple way to provide a nonlinear fit to data.\n- Step functions cut the range of a variable into K distinct regions in order to produce a qualitative variable. This has the effect of fitting a piecewise constant function.\n- Regression splines are more flexible than polynomials and step functions, and in fact are an extension of the two. They involve dividing the range of X into K distinct regions. Within each region, a polynomial function is fit to the data. However, these polynomials are  \nconstrained so that they join smoothly at the region boundaries, or knots . Provided that the interval is divided into enough regions, this can produce an extremely flexible fit.  \n- Smoothing splines are similar to regression splines, but arise in a slightly different situation. Smoothing splines result from minimizing a residual sum of squares criterion subject to a smoothness penalty.\n- Local regression is similar to splines, but differs in an important way. The regions are allowed to overlap, and indeed they do so in a very smooth way.\n- Generalized additive models allow us to extend the methods above to deal with multiple predictors.  \nIn Sections 7.1-7.6, we present a number of approaches for modeling the relationship between a response Y and a single predictor X in a flexible way. In Section 7.7, we show that these approaches can be seamlessly integrated in order to model a response Y as a function of several predictors X 1         X p .', '## 7.4 Regression Splines  \nNow we discuss a flexible class of basis functions that extends upon the polynomial regression and piecewise constant regression approaches that we have just seen.', '### 7.4.5 Comparison to Polynomial Regression  \nFigure 7.7 compares a natural cubic spline with 15 degrees of freedom to a degree15 polynomial on the Wage data set. The extra flexibility in the polynomial produces undesirable results at the boundaries, while the natural cubic spline still provides a reasonable fit to the data. Regression splines often give superior results to polynomial regression. This is because unlike polynomials, which must use a high degree (exponent in the highest monomial term, e.g. X 15 ) to produce flexible fits, splines introduce flexibility by increasing the number of knots but keeping the degree fixed. Generally, this approach produces more stable estimates. Splines also allow us to place more knots, and hence flexibility, over regions where the function f seems to be changing rapidly, and fewer knots where f appears more stable.  \nFIGURE 7.7. On the Wage data set, a natural cubic spline with 15 degrees of freedom is compared to a degree15 polynomial. Polynomials can show wild behavior, especially near the tails.  \n<!-- image -->', '## 7.5 Smoothing Splines  \nIn the last section we discussed regression splines, which we create by specifying a set of knots, producing a sequence of basis functions, and then using least squares to estimate the spline coefficients. We now introduce a somewhat different approach that also produces a spline.']","Regression splines are a flexible class of basis functions that extend upon polynomial regression and piecewise constant regression. They involve dividing the range of the predictor variable \( X \) into \( K \) distinct regions. Within each region, a polynomial function is fit to the data. However, these polynomials are constrained to join smoothly at the region boundaries, known as knots. This approach allows for a very flexible fit, especially if the interval is divided into enough regions.

The key differences between regression splines and polynomial regression are:

1. **Flexibility and Stability**: Regression splines introduce flexibility by increasing the number of knots while keeping the degree of the polynomial fixed. This generally results in more stable estimates compared to polynomial regression, which requires a high degree polynomial to achieve flexibility.

2. **Behavior at Boundaries**: Polynomials can exhibit wild behavior, especially near the boundaries of the data. In contrast, regression splines provide a more reasonable fit at the boundaries due to their construction.

3. **Placement of Knots**: Splines allow for more knots, and hence more flexibility, in regions where the function seems to be changing rapidly, and fewer knots where the function appears more stable. This adaptability is not available in polynomial regression.

Overall, regression splines often give superior results to polynomial regression due to these advantages.","Regression splines are a method for fitting non-linear relationships by dividing the range of the predictor variable into distinct regions and fitting a separate polynomial in each region. Unlike polynomial regression, which fits a single high-degree polynomial over the entire range, splines are more flexible and stable, especially at the boundaries, because they use lower-degree polynomials in each segment.",1.0,,0.9999999999666667,1.0
What is the purpose of a smoothing spline?,"['## 7.5 Smoothing Splines  \nIn the last section we discussed regression splines, which we create by specifying a set of knots, producing a sequence of basis functions, and then using least squares to estimate the spline coefficients. We now introduce a somewhat different approach that also produces a spline.', ""### 7.5.1 An Overview of Smoothing Splines  \nIn fitting a smooth curve to a set of data, what we really want to do is find some function, say g ( x ) , that fits the observed data well: that is, we want RSS = ∑ n i =1 ( y i -g ( x i )) 2 to be small. However, there is a problem with this approach. If we don't put any constraints on g ( x i ) , then we can always make RSS zero simply by choosing g such that it interpolates all of the y i . Such a function would woefully overfit the data-it would be far too flexible. What we really want is a function g that makes RSS small, but that is also smooth .  \nHow might we ensure that g is smooth? There are a number of ways to do this. A natural approach is to find the function g that minimizes  \n$$\\sum _ { i = 1 } ^ { n } ( y _ { i } - g ( x _ { i } ) ) ^ { 2 } + \\lambda \\int g ^ { \\prime \\prime } ( t ) ^ { 2 } d t \\\\$$  \nwhere λ is a nonnegative tuning parameter . The function g that minimizes (7.11) is known as a smoothing spline .  \nWhat does (7.11) mean? Equation 7.11 takes the 'Loss+Penalty' formulation that we encounter in the context of ridge regression and the lasso in Chapter 6. The term ∑ n i =1 ( y i -g ( x i )) 2 is a loss function that encourages g to fit the data well, and the term λ ∫ g ′′ ( t ) 2 dt is a penalty term that penalizes the variability in g . The notation g ′′ ( t ) indicates the second derivative of the function g . The first derivative g ′ ( t ) measures the slope smoothing spline  \nloss function  \nof a function at t , and the second derivative corresponds to the amount by which the slope is changing. Hence, broadly speaking, the second derivative of a function is a measure of its roughness : it is large in absolute value if g ( t ) is very wiggly near t , and it is close to zero otherwise. (The second derivative of a straight line is zero; note that a line is perfectly smooth.) The ∫ notation is an integral , which we can think of as a summation over the range of t . In other words, ∫ g ′′ ( t ) 2 dt is simply a measure of the total change in the function g ′ ( t ) , over its entire range. If g is very smooth, then g ′ ( t ) will be close to constant and ∫ g ′′ ( t ) 2 dt will take on a small value. Conversely, if g is jumpy and variable then g ′ ( t ) will vary significantly and ∫ g ′′ ( t ) 2 dt will take on a large value. Therefore, in (7.11), λ ∫ g ′′ ( t ) 2 dt encourages g to be smooth. The larger the value of λ , the smoother g will be.  \nThe function g ( x ) that minimizes (7.11) can be shown to have some special properties: it is a piecewise cubic polynomial with knots at the unique values of x 1         x n , and continuous first and second derivatives at each knot. Furthermore, it is linear in the region outside of the extreme knots. In other words, the function g ( x ) that minimizes (7.11) is a natural cubic spline with knots at x 1         x n ! However, it is not the same natural cubic spline that one would get if one applied the basis function approach described in Section 7.4.3 with knots at x 1         x n -rather, it is a shrunken version of such a natural cubic spline, where the value of the tuning parameter λ in (7.11) controls the level of shrinkage.  \nWhen λ = 0 , then the penalty term in (7.11) has no effect, and so the function g will be very jumpy and will exactly interpolate the training observations. When λ → ∞ , g will be perfectly smooth-it will just be a straight line that passes as closely as possible to the training points. In fact, in this case, g will be the linear least squares line, since the loss function in (7.11) amounts to minimizing the residual sum of squares. For an intermediate value of λ , g will approximate the training observations but will be somewhat smooth. We see that λ controls the bias-variance trade-off of the smoothing spline."", '### 7.4.4 Choosing the Number and Locations of the Knots  \nWhen we fit a spline, where should we place the knots? The regression spline is most flexible in regions that contain a lot of knots, because in those regions the polynomial coefficients can change rapidly. Hence, one natural spline\nNatural Cubic Spline  \nFIGURE 7.5. A natural cubic spline function with four degrees of freedom is fit to the Wage data. Left: A spline is fit to wage (in thousands of dollars) as a function of age . Right: Logistic regression is used to model the binary event wage&gt;250 as a function of age . The fitted posterior probability of wage exceeding $250  000 is shown. The dashed lines denote the knot locations.  \n<!-- image -->  \noption is to place more knots in places where we feel the function might vary most rapidly, and to place fewer knots where it seems more stable. While this option can work well, in practice it is common to place knots in a uniform fashion. One way to do this is to specify the desired degrees of freedom, and then have the software automatically place the corresponding number of knots at uniform quantiles of the data.  \nHow many knots should we use, or equivalently how many degrees of freedom should our spline contain? One option is to try out different numbers of knots and see which produces the best looking curve. A somewhat more objective approach is to use cross-validation, as discussed in Chapters 5 and 6. With this method, we remove a portion of the data (say 10 %), fit a spline with a certain number of knots to the remaining data, and then use the spline to make predictions for the held-out portion. We repeat this process multiple times until each observation has been left out once, and  \nFigure 7.5 shows an example on the Wage data. As in Figure 7.4, we have fit a natural cubic spline with three knots, except this time the knot locations were chosen automatically as the 25th, 50th, and 75th percentiles of age . This was specified by requesting four degrees of freedom. The argument by which four degrees of freedom leads to three interior knots is somewhat technical. 4  \n4 There are actually five knots, including the two boundary knots. A cubic spline with five knots has nine degrees of freedom. But natural cubic splines have two additional natural constraints at each boundary to enforce linearity, resulting in 9 -4 = 5 degrees of freedom. Since this includes a constant, which is absorbed in the intercept, we count it as four degrees of freedom.  \nFIGURE 7.6. Ten-fold cross-validated mean squared errors for selecting the degrees of freedom when fitting splines to the Wage data. The response is wage and the predictor age . Left: A natural cubic spline. Right: A cubic spline.  \n<!-- image -->  \nthen compute the overall cross-validated RSS . This procedure can be repeated for different numbers of knots K . Then the value of K giving the smallest RSS is chosen.  \nIn Section 7.7 we fit additive spline models simultaneously on several variables at a time. This could potentially require the selection of degrees of freedom for each variable. In cases like this we typically adopt a more pragmatic approach and set the degrees of freedom to a fixed number, say four, for all terms.  \nFigure 7.6 shows ten-fold cross-validated mean squared errors for splines with various degrees of freedom fit to the Wage data. The left-hand panel corresponds to a natural cubic spline and the right-hand panel to a cubic spline. The two methods produce almost identical results, with clear evidence that a one-degree fit (a linear regression) is not adequate. Both curves flatten out quickly, and it seems that three degrees of freedom for the natural spline and four degrees of freedom for the cubic spline are quite adequate.', '### 7.4.3 The Spline Basis Representation  \nThe regression splines that we just saw in the previous section may have seemed somewhat complex: how can we fit a piecewise degreed polynomial under the constraint that it (and possibly its first d -1 derivatives) be continuous? It turns out that we can use the basis model (7.7) to represent a regression spline. A cubic spline with K knots can be modeled as  \n$$y _ { i } = \\beta _ { 0 } + \\beta _ { 1 } b _ { 1 } ( x _ { i } ) + \\beta _ { 2 } b _ { 2 } ( x _ { i } ) + \\dots + \\beta _ { K + 3 } b _ { K + 3 } ( x _ { i } ) + \\epsilon _ { i } ,$$  \nfor an appropriate choice of basis functions b 1  b 2         b K +3 . The model (7.9) can then be fit using least squares.  \nJust as there were several ways to represent polynomials, there are also many equivalent ways to represent cubic splines using different choices of basis functions in (7.9). The most direct way to represent a cubic spline using (7.9) is to start off with a basis for a cubic polynomial-namely, x x 2  and x 3 -and then add one truncated power basis function per knot.  \n3 Cubic splines are popular because most human eyes cannot detect the discontinuity at the knots.  \nderivative cubic spline  \nlinear spline truncated power basis  \nFIGURE 7.4. A cubic spline and a natural cubic spline, with three knots, fit to a subset of the Wage data. The dashed lines denote the knot locations.  \n<!-- image -->  \nA truncated power basis function is defined as  \n$$h ( x , \\xi ) = ( x - \\xi ) _ { + } ^ { 3 } = \\left \\{ \\begin{array} { c c } ( x - \\xi ) ^ { 3 } & \\text {if } x > \\xi \\\\ 0 & \\text {otherwise,} \\end{array}$$  \nwhere ξ is the knot. One can show that adding a term of the form β 4 h ( x ξ ) to the model (7.8) for a cubic polynomial will lead to a discontinuity in only the third derivative at ξ ; the function will remain continuous, with continuous first and second derivatives, at each of the knots.  \nUnfortunately, splines can have high variance at the outer range of the predictors-that is, when X takes on either a very small or very large value. Figure 7.4 shows a fit to the Wage data with three knots. We see that the confidence bands in the boundary region appear fairly wild. A natural spline is a regression spline with additional boundary constraints : the function is required to be linear at the boundary (in the region where X is smaller than the smallest knot, or larger than the largest knot). This additional constraint means that natural splines generally produce more stable estimates at the boundaries. In Figure 7.4, a natural cubic spline is also displayed as a red line. Note that the corresponding confidence intervals are narrower.  \nIn other words, in order to fit a cubic spline to a data set with K knots, we perform least squares regression with an intercept and 3+ K predictors, of the form XX 2  X 3  h ( X ξ 1 )  h ( X ξ 2 )         h ( X ξ K ) , where ξ 1         ξ K are the knots. This amounts to estimating a total of K + 4 regression coefficients; for this reason, fitting a cubic spline with K knots uses K +4 degrees of freedom.']","The purpose of a smoothing spline is to fit a smooth curve to a set of data points in a way that balances fitting the data well while maintaining smoothness. This is achieved by minimizing a combination of a loss function and a penalty term. The loss function, which is the residual sum of squares (RSS), encourages the function to fit the data closely. The penalty term, which involves the second derivative of the function, penalizes the roughness or variability of the function. The tuning parameter \(\lambda\) controls the trade-off between fitting the data and smoothness. A smoothing spline is particularly useful for avoiding overfitting by ensuring that the fitted curve is not too flexible and wiggly, while still capturing the underlying trend in the data.","A smoothing spline aims to find a function that fits the data well by minimizing the residual sum of squares, while simultaneously penalizing the roughness of the function. This is achieved by adding a penalty term based on the integral of the squared second derivative of the function, controlled by a tuning parameter lambda.",0.9090909090909091,,0.49999999995,1.0
How does local regression work?,"['## 7.6 Local Regression  \nLocal regression is a different approach for fitting flexible non-linear functions, which involves computing the fit at a target point x 0 using only the nearby training observations. Figure 7.9 illustrates the idea on some simulated data, with one target point near 0   4 , and another near the boundary at 0   05 . In this figure the blue line represents the function f ( x ) from which the data were generated, and the light orange line corresponds to the local regression estimate ˆ f ( x ) . Local regression is described in Algorithm 7.1.  \nIn order to perform local regression, there are a number of choices to be made, such as how to define the weighting function K , and whether to fit a linear, constant, or quadratic regression in Step 3. (Equation 7.14 corresponds to a linear regression.) While all of these choices make some difference, the most important choice is the span s , which is the proportion of points used to compute the local regression at x 0 , as defined in Step 1 above. The span plays a role like that of the tuning parameter λ in smooth-  \nNote that in Step 3 of Algorithm 7.1, the weights K i 0 will differ for each value of x 0 . In other words, in order to obtain the local regression fit at a new point, we need to fit a new weighted least squares regression model by minimizing (7.14) for a new set of weights. Local regression is sometimes referred to as a memory-based procedure, because like nearest-neighbors, we need all the training data each time we wish to compute a prediction. We will avoid getting into the technical details of local regression here-there are books written on the topic.  \nlocal regression\nLocal Regression  \nFIGURE 7.9. Local regression illustrated on some simulated data, where the blue curve represents f ( x ) from which the data were generated, and the light orange curve corresponds to the local regression estimate ˆ f ( x ) . The orange colored points are local to the target point x 0 , represented by the orange vertical line. The yellow bell-shape superimposed on the plot indicates weights assigned to each point, decreasing to zero with distance from the target point. The fit ˆ f ( x 0 ) at x 0 is obtained by fitting a weighted linear regression (orange line segment), and using the fitted value at x 0 (orange solid dot) as the estimate ˆ f ( x 0 ) .  \n<!-- image -->  \ning splines: it controls the flexibility of the non-linear fit. The smaller the value of s , the more local and wiggly will be our fit; alternatively, a very large value of s will lead to a global fit to the data using all of the training observations. We can again use cross-validation to choose s , or we can specify it directly. Figure 7.10 displays local linear regression fits on the Wage data, using two values of s : 0   7 and 0   2 . As expected, the fit obtained using s = 0   7 is smoother than that obtained using s = 0   2 .  \nThe idea of local regression can be generalized in many different ways. In a setting with multiple features X 1  X 2         X p , one very useful generalization involves fitting a multiple linear regression model that is global in some variables, but local in another, such as time. Such varying coefficient models are a useful way of adapting a model to the most recently gathered data. Local regression also generalizes very naturally when we want to fit models that are local in a pair of variables X 1 and X 2 , rather than one. We can simply use two-dimensional neighborhoods, and fit bivariate linear regression models using the observations that are near each target point in two-dimensional space. Theoretically the same approach can be implemented in higher dimensions, using linear regressions fit to p -dimensional neighborhoods. However, local regression can perform poorly if p is much larger than about 3 or 4 because there will generally be very few training observations close to x 0 . Nearest-neighbors regression, discussed in Chapter 3, suffers from a similar problem in high dimensions.  \nvarying coefficient model\nAlgorithm 7.1 Local Regression At X = x 0  \n1. Gather the fraction s = kglyph[triangleleft]n of training points whose x i are closest to x 0 .\n2. Assign a weight K i 0 = K ( x i  x 0 ) to each point in this neighborhood, so that the point furthest from x 0 has weight zero, and the closest has the highest weight. All but these k nearest neighbors get weight zero.\n3. Fit a weighted least squares regression of the y i on the x i using the aforementioned weights, by finding ˆ β 0 and ˆ β 1 that minimize  \n$$\\sum _ { i = 1 } ^ { n } K _ { i 0 } ( y _ { i } - \\beta _ { 0 } - \\beta _ { 1 } x _ { i } ) ^ { 2 } .$$  \n4. The fitted value at x 0 is given by ˆ f ( x 0 ) = ˆ β 0 + ˆ β 1 x 0 .\nLocal Linear Regression  \nFIGURE 7.10. Local linear fits to the Wage data. The span specifies the fraction of the data used to compute the fit at each target point.  \n<!-- image -->']","Local regression is a technique used to fit flexible non-linear functions by computing the fit at a target point \( x_0 \) using only the nearby training observations. Here's a step-by-step explanation of how it works, based on Algorithm 7.1:

1. **Select Nearby Points**: Determine the fraction \( s \) of training points whose \( x_i \) values are closest to the target point \( x_0 \). This fraction \( s \) is known as the span and controls the flexibility of the fit.

2. **Assign Weights**: Assign a weight \( K_{i0} = K(x_i, x_0) \) to each point in this neighborhood. The weight is highest for the point closest to \( x_0 \) and zero for the point furthest away. Points outside this neighborhood receive a weight of zero.

3. **Fit Weighted Least Squares Regression**: Perform a weighted least squares regression of the \( y_i \) values on the \( x_i \) values using the weights assigned in the previous step. This involves finding coefficients \( \hat{\beta}_0 \) and \( \hat{\beta}_1 \) that minimize the weighted sum of squared errors:
   \[
   \sum_{i=1}^{n} K_{i0} (y_i - \beta_0 - \beta_1 x_i)^2
   \]

4. **Compute the Fitted Value**: The fitted value at \( x_0 \) is given by:
   \[
   \hat{f}(x_0) = \hat{\beta}_0 + \hat{\beta}_1 x_0
   \]

Local regression is sometimes referred to as a memory-based procedure because it requires all the training data each time a prediction is made. The span \( s \) is a crucial parameter, similar to the tuning parameter in smoothing splines, as it controls the flexibility of the non-linear fit. A smaller \( s \) results in a more local and wiggly fit, while a larger \( s \) leads to a smoother, more global fit. Cross-validation can be used to choose the optimal span \( s \).","Local regression, or LOESS, is a non-parametric method that fits a separate regression model for each target point, using only the data points in its local neighborhood. It assigns weights to these neighboring points based on their distance from the target point, and then fits a weighted least squares regression. This results in a smooth and flexible curve that adapts to the local structure of the data.",1.0,,0.9999999999,0.6666666666666666
¿Qué es un modelo aditivo generalizado (GAM) y cuál es su principal ventaja?,"['## 7.7 Generalized Additive Models  \nIn Sections 7.1-7.6, we present a number of approaches for flexibly predicting a response Y on the basis of a single predictor X . These approaches can be seen as extensions of simple linear regression. Here we explore the problem of flexibly predicting Y on the basis of several predictors, X 1         X p . This amounts to an extension of multiple linear regression.  \nGeneralized additive models (GAMs) provide a general framework for extending a standard linear model by allowing non-linear functions of each of the variables, while maintaining additivity . Just like linear models, GAMs can be applied with both quantitative and qualitative responses. We first generalized additive model additivity  \nFIGURE 7.11. For the Wage data, plots of the relationship between each feature and the response, wage , in the fitted model (7.16). Each plot displays the fitted function and pointwise standard errors. The first two functions are natural splines in year and age , with four and five degrees of freedom, respectively. The third function is a step function, fit to the qualitative variable education .  \n<!-- image -->  \nexamine GAMs for a quantitative response in Section 7.7.1, and then for a qualitative response in Section 7.7.2.', '### 7.7.1 GAMs for Regression Problems  \nA natural way to extend the multiple linear regression model  \n$$y _ { i } = \\beta _ { 0 } + \\beta _ { 1 } x _ { i 1 } + \\beta _ { 2 } x _ { i 2 } + \\cdots + \\beta _ { p } x _ { i p } + \\epsilon _ { i }$$  \nin order to allow for non-linear relationships between each feature and the response is to replace each linear component β j x ij with a (smooth) nonlinear function f j ( x ij ) . We would then write the model as  \n$$y _ { i } \\ & = \\ \\beta _ { 0 } + \\sum _ { j = 1 } ^ { p } f _ { j } ( x _ { i j } ) + \\epsilon _ { i } \\\\ & = \\ \\beta _ { 0 } + f _ { 1 } ( x _ { i 1 } ) + f _ { 2 } ( x _ { i 2 } ) + \\dots + f _ { p } ( x _ { i p } ) + \\epsilon _ { i } . \\\\ \\intertext { s i n g a l l } \\ \\intertext { s i n g a l l } \\ \\intertext { s i n g a l l } \\ \\intertext { s i n g a l l } \\ \\intertext { s i n g a l l } \\ \\intertext { s i n g a l l } \\ \\intertext { s i n g a l l } \\ \\intertext { s i n g a l l } \\ \\intertext { s i n g a l l } \\ \\intertext { s i n g a l l } \\ \\intertext { s i n g a l l } \\ \\intertext { s i n g a l l } \\ \\intertext { s i n g a l l } \\ \\intertext { s i n g a l l } \\ \\intertext { s i n g a l l } \\ \\intertext { s i n g a l l } \\ \\intertext { s i n g a l l } \\ \\intertext { s i n g a l l } \\ \\intertext { s i n g a l l } \\ \\intertext { s i n g a l l } \\ \\intertext { s i n g a l l } \\ \\intertext { s i n g a l l } \\ \\intertext { s i n g a l l } \\ \\intertext { s i n g a l l } \\ \\intertext { s i n g a l l } \\ \\intertext { s i n g a l l } \\ \\intertext { s i n g a l l } \\ \\intertext { s i n g a l l } \\ \\intertext { s i n g a l l } \\ \\intertext { s i n g a l l } \\ \\intertext { s i n g a l l } \\ \\intertext { s i n g a l l } \\ \\intertext { s i n g a l l } \\ \\intertext { s i n g a l l } \\ \\intertext { s i n g a l l } \\ \\intertext { s i n g a l l } \\ \\intertext { s i n g a l l } \\ \\intertext { s i n g a l l } \\ \\intertext { s i n g a l l } \\ \\intertext { s i n g a l l } \\ \\intertext { s i n g a l l } \\ \\intertext { s i n g a l l } \\ \\intertext { s i n g a l l } \\ \\intertext { s i n g a l l } \\ \\intertext { s i n g a l l } \\ \\intertext { s i n g a l l } \\ \\intertext { s i n g a l l } \\ \\intertext { s i n g a l l } \\ \\intertext { s i n g a l l } \\ \\intertext { s i n g a l l } \\ \\intertext { s i n g a l l } \\ \\intertext { s i n g a l l } \\ \\intertext { s i n g a l l } \\ \\intertext { s i n g a l l } \\ \\intertext { s i n g a l l } \\ \\intertext { s i n g a l l } \\ \\intertext { s i n g a l l } \\ \\intertext { s i n g a l l } \\ \\intertext { s i n g a l l } \\ \\intertext { s i n g a l l } \\ \\intertext { s i n g a l l } \\ \\intertext { s i n g a l l } \\ \\intertext { s i n g a l l } \\ \\intertext { s i n g a l l } \\ \\intertext { s i n g a l l } \\ \\intertext { s i n g a l l } \\ \\intertext { s i n g a l l } \\ \\intertext { s i n g a l l } \\ \\intertext { s i n g a l l } \\ \\intertext { s i n g a l l } \\ \\intertext { s i n g a l l } \\ \\intertext { s i n g a l l } \\ \\intertext { s i n g a l l } \\ \\intertext { s i n g a l l } \\ \\intertext { s i n g a l l } \\ \\intertext { s i n g a l l } \\ \\intertext { s i n g a l l } \\ \\intertext { s i n g a l l } \\ \\intertext { s i n g a l l } \\ \\intertext { s i n g a l l } \\ \\intertext { s i n g a l l } \\ \\intertext { s i n g a l l } \\ \\intertext { s i n g a l l } \\ \\intertext { s i n g a l l } \\ \\intertext { s i n g a l l } \\ \\intertext { s i n g a l l } \\ \\intertext { s i n g a l l } \\ \\intertext { s i n g a l l } \\ \\intertext { s i n g a l l } \\ \\intertext { s i n g a l l } \\ \\intertext { s i n g a l l } \\ \\intertext { s i n g a l l } \\ \\intertext { s i n g a l l } \\ \\intertext { s i n g a l l } \\ \\intertext { s i n g a l l } \\ \\intertext { s i n g a l l } \\ \\intertext { s i n g a l l } \\ \\intertext { s i n g a l l } \\ \\intertext { s i n g a l l } \\ \\intertext { s i n g a l l } \\ \\intertext { s i n g a l l } \\ \\intertext { s i n g a l l } \\ \\intertext { s i n g a l l } \\ \\intertext { s i n g a l l } \\ \\intertext { s i n g a l l } \\ \\intertext { s i n g a l l } \\ \\intertext { s i n g a l l } \\ \\intertext { s i n g a l l } \\ \\intertext { s i n g a l l } \\ \\intertext { s i n g a l l } \\ \\intertext { s i n g a l l } \\ \\intertext { s i n g a l l } \\ \\intertext { s i n g a l l } \\ \\intertext { s i n g a l l } \\ \\intertext { s i n g a l l } \\ \\intertext { s i n g a l l } \\ \\intertext { s i n g a l l } \\ \\intertext { s i n g a l l } \\ \\intertext { s i n g a l l } \\ \\intertext { s i n g a l l } \\ \\intertext { s i n g a l l } \\ \\intertext { s i n g a l l } \\ \\intertext { s i n g a l l } \\ \\intertext { s i n g a l l } \\ \\intertext { s i n g a l l } \\ \\intertext { s i n g a l l } \\ \\intertext { s i n g a l l } \\ \\intertext { s i n g a l l } \\ \\intertext { s i n g a l l } \\ \\intertext { s i n g a l l } \\ \\intertext { s i n g a l l } \\ \\intertext { s i n g a l l } \\ \\intertext { s i n g a l l } \\ \\intertext { s i n g a l l } \\ \\intertext { s i n g a l l } \\ \\intertext { s i n g a l l } \\ \\intertext { s i n g a l l } \\ \\intertext { s i n g a l l } \\ \\intertext { s i n g a l l } \\ \\intertext { s i n g a l l } \\ \\intertext { s i n g a l l } \\ \\intertext { s i n g a l l } \\ \\intertext { s i n g a l l } \\ \\intertext { s i n g a l l } \\ \\intertext { s i n g a l l } \\ \\intertext { s i n g a l l } \\ \\intertext { s i n g a l l } \\ \\intertext { s i n g a l l } \\ \\intertext { s i n g a l l } \\ \\intertext { s i n g a l l } \\ \\intertext { s i n g a l l } \\ \\intertext { s i n g a l l } \\ \\intertext { s i n g a l l } \\ \\intertext { s i n g a l l } \\ \\intertext { s i n g a l l } \\ \\intertext { s i n g a l l } \\ \\intertext { s i n g a l l } \\ \\intertext { s i n g a l l } \\ \\intertext { s i n g a l l } \\ \\intertext { s i n g a l l } \\ \\intertext { s i n g a l l } \\ \\intertext { s i n g a l l } \\ \\intertext { s i n g a l l } \\ \\intertext { s i n g a l l } \\ \\intertext { s i n g a l l } \\ \\intertext { s i n g a l l } \\ \\intertext { s i n g a l l } \\ \\intertext { s i n g a l l } \\ \\intertext { s i n g a l l } \\ \\intertext { s i n g a l l } \\ \\intertext { s i n g a l l } \\ \\intertext { s i n g a l l } \\ \\intertext { s i n g a l l } \\ \\intertext { s i n g a l l } \\ \\intertext { s i n g a l l } \\ \\intertext { s i n g a l l } \\ \\intertext { s i n g a l l } \\ \\intertext { s i n g a l l } \\ \\intertext { s i n g a l l } \\ \\intertext { s i n g a l l } \\ \\intertext { s i n g a l l } \\ \\intertext { s i n g a l l } \\ \\intertext { s i n g a l l } \\ \\intertext { s i n g a l l } \\ \\intertext { s i n g a l l } \\ \\intertext { s i n g a l l } \\ \\intertext { s i n g a l l } \\ \\intertext { s i n g a l l } \\ \\intertext { s i n g a l l } \\ \\intertext { s i n g a l l } \\ \\intertext { s i n g a l l } \\ \\intertext { s i n g a l l } \\ \\intertext { s i n g a l l } \\ \\intertext { s i n g a l l } \\ \\intertext { s i n g a l l } \\ \\intertext { s i n g a l l } \\ \\intertext { s i n g a l l } \\ \\intertext { s i n g a l l } \\ \\intertext { s i n g a l l } \\ \\intertext { s i n g a l l } \\ \\intertext { s i n g a l l } \\ \\intertext { s i n g a l l } \\ \\intertext { s i n g a l l } \\ \\intertext { s i n g a l l } \\ \\intertext { s i n g a l l } \\ \\intertext { s i n g a l l } \\ \\intertext { s i n g a l l } \\ \\intertext { s i n g a l l } \\ \\intertext { s i n g a l l } \\ \\intertext { s i n g a l l } \\ \\intertext { s i n g a l l } \\ \\intertext { s i n g a l l } \\ \\intertext { s i n g a l l } \\ \\intertext { s i n g a l l } \\ \\intertext { s i n g a l l } \\ \\intertext { s i n g a l l } \\ \\intertext { s i n g a l l } \\ \\intertext { s i n g a l l } \\ \\intertext { s i n g a l l } \\ \\intertext { s i n g a l l } \\ \\intertext { s i n g a l l } \\ \\intertext { s i n g a l l } \\ \\intertext { s i n g a l l } \\ \\intertext { s i n g a l l } \\ \\intertext { s i n g a l l } \\ \\intertext { s i n g a l l } \\ \\intertext { s i n g a l l } \\ \\intertext { s i n g a l l } \\ \\intertext { s i n g a l l } \\ \\intertext { s i n g a l l } \\ \\intertext { s i n g a l l } \\ \\intertext { s i n g a l l } \\ \\intertext { s i n g a l l } \\ \\intertext { s i n g a l l } \\ \\intertext { s i n g a l l } \\ \\intertext { s i n g a l l } \\ \\intertext { s i n g a l l } \\ \\intertext { s i n g a l l } \\ \\intertext { s i n g a l l } \\ \\intertext { s i n g a l l } \\ \\intertext { s i n g a l l } \\ \\intertext { s i n g a l l } \\ \\intertext { s i n g a l l } \\ \\intertext { s i n g a l l } \\ \\intertext { s i n g a l l } \\ \\intertext { s i n g a l l } \\ \\intertext { s i n g a l l } \\ \\intertext { s i n g a l l } \\ \\intertext { s i n g a l l } \\ \\intertext { s i n g a l l } \\ \\intertext { s i n g a l l } \\ \\intertext { s i n g a l l } \\ \\intertext { s i n g a l l } \\ \\intertext { s i n g a l l } \\ \\intertext { s i n g a l l } \\ \\intertext { s i n g a l l } \\ \\intertext { s i n g a l l } \\ \\intertext { s i n g a l l } \\ \\intertext { s i n g a l l } \\ \\intertext { s i n g a l l } \\ \\intertext { s i n g a l l } \\ \\intertext { s i n g a l l } \\ \\intertext { s i n g a l l } \\ \\intertext { s i n g a l l } \\ \\intertext { s i n g a l l } \\ \\intertext { s i n g a l l } \\ \\intertext { s i n g a l l } \\ \\intertext { s i n g a l l } \\ \\intertext { s i n g a l l } \\ \\intertext { s i n g a l l } \\ \\intertext { s i n g a l l } \\ \\intertext { s i n g a l l } \\ \\intertext { s i n g a l l } \\ \\intertext { s i n g a l l } \\ \\intertext { s i n g a l l } \\ \\intertext { s i n g a l l } \\ \\intertext { s i n g a l l } \\ \\intertext { s i n g a l l } \\ \\intertext { s i n g a l l } \\ \\intertext { s i n g a l l } \\ \\intertext { s i n g a l l } \\ \\intertext { s i n g a l l } \\ \\intertext { s i n g a l l } \\ \\intertext { s i n g a l l } \\ \\intertext { s i n g a l l } \\ \\intertext { s i n g a l l } \\ \\intertext { s i n g a l l } \\ \\intertext { s i n g a l l } \\ \\intertext { s i n g a l l } \\ \\intertext { s i n g a l l } \\ \\intertext { s i n g a l l } \\ \\intertext { s i n g a l l } \\ \\intertext { s i n g a l l } \\ \\intertext { s i n g a l l } \\ \\intertext { s i n g a l l } \\ \\intertext { s i n g a l l } \\ \\intertext { s i n g a l l } \\ \\intertext { s i n g a l l } \\ \\intertext { s i n g a l l } \\ \\intertext { s i n g a l l } \\ \\intertext { s i n g a l l } \\ \\intertext { s i n g a l l } \\ \\intertext { s i n g a l l } \\ \\intertext { s i n g a l l } \\ \\intertext { s i n g a l l } \\ \\intertext { s i n g a l l } \\ \\intertext { s i n g a l l } \\ \\intertext { s i n g a l l } \\ \\intertext { s i n g a l l } \\ \\intertext { s i n g a l l } \\ \\intertext { s i n g a l l } \\ \\intertext { s i n g a l l } \\ \\intertext { s i n g a l l } \\ \\intertext { s i n g a l l } \\ \\intertext { s i n g a l l } \\ \\intertext { s i n g a l l } \\ \\intertext { s i n g a l l } \\ \\intertext { s i n g a l l } \\ \\intertext { s i n g a l l } \\ \\intertext { s i n g a l l } \\ \\intertext { s i n g a l l } \\ \\intertext { s i n g a l l } \\ \\intertext { s i n g a l l } \\ \\intertext { s i n g a l l } \\ \\intertext { s i n g a l l } \\ \\intertext { s i n g a l l } \\ \\intertext { s i n g a l l } \\ \\intertext { s i n g a l l } \\ \\intertext { s i n g a l l } \\ \\intertext { s i n g a l l } \\ \\intertext { s i n g a l l } \\ \\intertext { s i n g a l l } \\ \\intertext { s i n g a l l } \\ \\intertext { s i n g a l l } \\ \\intertext { s i n g a l l } \\ \\intertext { s i n g a l l } \\ \\intertext { s i n g a l l } \\ \\intertext { s i n g a l l } \\ \\intertext { s i n g a l l } \\ \\intertext { s i n g a l l } \\ \\intertext { s i n g a l l } \\ \\intertext { s i n g a l l } \\ \\intertext { s i n g a l l } \\ \\intertext { s i n g a l l } \\ \\intertext { s i n g a l l } \\ \\intertext { s i n g a l l } \\ \\intertext { s i n g a l l } \\ \\intertext { s i n g a l l } \\ \\intertext { s i n g a l l } \\ \\intertext { s i n g a l l } \\ \\intertext { s i n g a l l } \\ \\intertext { s i n g a l l } \\ \\intertext { s i n g a l l } \\ \\intertext { s i n g a l l } \\ \\intertext { s i n g a l l } \\ \\intertext { s i n g a l l } \\ \\intertext { s i n g a l l } \\ \\intertext { s i n g a l l } \\ \\intertext { s i n g a l l } \\ \\intertext { s i n g a l l } \\ \\intertext { s i n g a l l } \\ \\intertext { s i n g a l l } \\ \\intertext { s i n g a l l } \\ \\intertext { s i n g a l l } \\ \\intertext { s i n g a l l } \\ \\intertext { s i n g a l l } \\ \\intertext { s i n g a l l } \\ \\intertext { s i n g a l l } \\ \\intertext { s i n g a l l } \\ \\intertext { s i n g a l l } \\ \\intertext { s i n g a l l } \\ \\intertext { s i n g a l l } \\ \\intertext { s i n g a l l } \\ \\intertext { s i n g a l l } \\ \\intertext { s i n g a l l } \\ \\intertext { s i n g a l l } \\ \\intertext { s i n g a l l } \\ \\intertext { s i n g a l l } \\ \\intertext { s i n g a l l } \\ \\intertext { s i n g a l l } \\ \\intertext { s i n g a l l } \\ \\intertext { s i n g a l l } \\ \\intertext { s i n g a l l } \\ \\intertext { s i n g a l l } \\ \\intertext { s i n g a l l } \\ \\intertext { s i n g a l l } \\ \\intertext { s i n g a l l } \\ \\intertext { s i n g a l l } \\ \\intertext { s i n g a l l } \\ \\intertext { s i n g a l l } \\ \\intertext { s i n g a l l } \\ \\intertext { s i n g a l l } \\ \\intertext { s i n g a l l } \\ \\intertext { s i n g a l l } \\ \\intertext { s i n g a l l } \\ \\intertext { s i n g a l l } \\ \\intertext { s i n g a l l } \\ \\intertext { s i n g a l l } \\ \\intertext { s i n g a l l } \\ \\intertext { s i n g a l l } \\ \\intertext { s i n g a l l } \\ \\intertext { s i n g a l l } \\ \\intertext { s i n g a l l } \\ \\intertext { s i n g a l l } \\ \\intertext { s i n g a l l } \\ \\intertext { s i n g a l l } \\ \\intertext { s i n g a l l } \\ \\intertext { s i n g a l l } \\ \\intertext { s i n g a l l } \\ \\intertext { s i n g a l l } \\ \\intertext { s i n g a l l } \\ \\intertext { s i n g a l l } \\ \\intertext { s i n g a l l } \\ \\intertext { s i n g a l l } \\ \\intertext { s i n g a l l } \\ \\intertext { s i n g a l l } \\ \\intertext { s i n g a l l } \\ \\intertext { s i n g a l l } \\ \\intertext { s i n g a l l } \\ \\intertext { s i n g a l l } \\ \\intertext { s i n g a l l } \\ \\intertext { s i n g a l l } \\ \\intertext { s i n g a l l } \\ \\intertext { s i n g a l l } \\ \\intertext { s i n g a l l } \\ \\intertext { s i n g a l l } \\ \\intertext { s i n g a l l } \\ \\intertext { s i n g a l l } \\ \\intertext { s i n g a l l } \\ \\intertext { s i n g a l l } \\ \\intertext { s i n g a l l } \\ \\intertext { s i n g a l l } \\ \\intertext { s i n g a l l } \\ \\intertext { s i n g a l l } \\ \\intertext { s i n g a l l } \\ \\intertext { s i n g a l l } \\ \\intertext { s i n g a l l } \\ \\intertext { s i n g a l l } \\ \\intertext { s i n g a l l } \\ \\intertext { s i n g a l l } \\ \\intertext { s i n g a l l } \\ \\intertext { s i n g a l l } \\ \\intertext { s i n g a l l } \\ \\intertext { s i n g a l l } \\ \\intertext { s i n g a l l } \\ \\intertext { s i n g a l l } \\ \\intertext { s i n g a l l } \\ \\intertext { s i n g a l l } \\ \\intertext { s i n g a l l } \\ \\intertext { s i n g a l l } \\ \\intertext { s i n g a l l } \\ \\intertext { s i n g a l l } \\ \\intertext { s i n g a l l } \\ \\intertext { s i n g a l l } \\ \\intertext { s i n g a l l } \\ \\intertext { s i n g a l l } \\ \\intertext { s i n g a l l } \\ \\intertext { s i n g a l l } \\ \\intertext { s i n g a l l } \\ \\intertext { s i n g a l l } \\ \\intertext { s i n g a l l } \\ \\intertext { s i n g a l l } \\ \\intertext { s i n g a l l } \\ \\intertext { s i n g a l l } \\ \\intertext { s i n g a l l } \\ \\intertext { s i n g a l l } \\ \\intertext { s i n g a l l } \\ \\intertext { s i n g a l l } \\ \\intertext { s i n g a l l } \\ \\intertext { s i n g a l l } \\ \\intertext { s i n g a l l } \\ \\intertext { s i n g a l l } \\ \\intertext { s i n g a l l } \\ \\intertext { s i n g a l l } \\ \\intertext { s i n g a l l } \\ \\intertext { s i n g a l l } \\ \\intertext { s i n g a l l } \\ \\intertext { s i n g a l l } \\ \\intertext { s i n g a l l } \\ \\intertext { s i n g a l l } \\ \\intertext { s i n g a l l } \\ \\intertext { s i n g a l l } \\ \\intertext { s i n g a l l } \\ \\intertext { s i n g a l l } \\ \\intertext { s i n g a l l } \\ \\intertext { s i n g a l l } \\ \\intertext { s i n g a l l } \\ \\intertext { s i n g a l l } \\ \\intertext { s i n g a l l } \\ \\intertext { s i n g a l l } \\ \\intertext { s i n g a l l } \\ \\intertext { s i n g a l l } \\ \\intertext { s i n g a l l } \\ \\intertext { s i n g a l l } \\ \\intertext { s i n g a l l } \\ \\intertext { s i n g a l l } \\ \\intertext { s i n g a l l } \\ \\intertext { s i n g a l l } \\ \\intertext { s i n g a l l } \\ \\intertext { s i n g a l l } \\ \\intertext { s i n g a l l } \\ \\intertext { s i n g a l l } \\ \\intertext { s i n g a l l } \\ \\intertext { s i n g a l l } \\ \\intertext { s i n g a l l } \\ \\intertext { s i n g a l l } \\ \\intertext { s i n g a l l } \\ \\intertext { s i n g a l l } \\ \\intertext { s i n g a l l } \\ \\intertext { s i n g a l l } \\ \\intertext { s i n g a l l } \\ \\intertext { s i n g a l l } \\ \\intertext { s i n g a l l } \\ \\intertext { s i n g a l l } \\ \\intertext { s i n g a l l } \\ \\intertext { s i n g a l l } \\ \\intertext { s i n g a l l } \\ \\intertext { s i n g a l l } \\ \\intertext { s i n g a l l } \\ \\intertext { s i n g a l l } \\ \\intertext { s i n g a l l } \\ \\intertext { s i n g a l l } \\ \\intertext { s i n g a l l } \\ \\intertext { s i n g a l l } \\ \\intertext { s i n g a l l } \\ \\intertext { s i n g a l l } \\ \\intertext { s i n g a l l } \\ \\intertext { s i n g a l l } \\ \\intertext { s i n g a l l } \\ \\intertext { s i n g a l l } \\ \\intertext { s i n g a l l } \\ \\intertext { s i n g a l l } \\ \\intertext { s i n g a l l } \\ \\intertext { s i n g a l l } \\ \\intertext { s i n g a l l } \\ \\intertext { s i n g a l l } \\ \\intertext { s i n g a l l } \\ \\intertext { s i n g a l l } \\ \\intertext { s i n g a l l } \\ \\intertext { s i n g a l l } \\ \\intertext { s i n g a l l } \\ \\intertext { s i n g a l l } \\ \\intertext { s i n g a l l } \\ \\intertext { s i n g a l l } \\ \\intertext { s i n g a l l } \\ \\intertext { s i n g a l l } \\ \\intertext { s i n g a l l } \\ \\intertext { s i n g a l l } \\ \\intertext { s i n g a l l } \\ \\intertext { s i n g a l l } \\ \\intertext { s i n g a l l } \\ \\intertext { s i n g a l l } \\ \\intertext { s i n g a l l } \\ \\intertext { s i n g a l l } \\ \\intertext { s i n g a l l } \\ \\intertext { s i n g a l l } \\ \\intertext { s i n g a l l } \\ \\intertext { s i n g a l l } \\ \\intertext { s i n g a l l } \\ \\intertext { s i n g a l l } \\ \\intertext { s i n g a l l } \\ \\intertext { s i n g a l l } \\ \\intertext { s i n g a l l } \\ \\intertext { s i n g a l l } \\ \\intertext { s i n g a l l } \\ \\intertext { s i n g a l l } \\ \\intertext { s i n g a l l } \\ \\intertext { s i n g a l l } \\ \\intertext { s i n g a l l } \\ \\intertext { s i n g a l l } \\ \\intertext { s i n g a l l } \\ \\intertext { s i n g a l l } \\ \\intertext { s i n g a l l } \\ \\intertext { s i n g a l l } \\ \\intertext { s i n g a l l } \\ \\intertext { s i n g a l l } \\ \\intertext { s i n g a l l } \\ \\intertext { s i n g a l l } \\ \\intertext { s i n g a l l } \\ \\intertext { s i n g a l l } \\ \\intertext { s i n g a l l } \\ \\intertext { s i n g a l l } \\ \\intertext { s i n g a l l } \\ \\intertext { s i n g a l l } \\ \\intertext { s i n g a l l } \\ \\intertext { s i n g a l l } \\ \\intertext { s i n g a l l } \\ \\intertext { s i n g a l l } \\ \\intertext { s i n g a l l } \\ \\intertext { s i n g a l l } \\ \\intertext { s i n g a l l } \\ \\intertext { s i n g a l l } \\ \\intertext { s i n g a l l } \\ \\intertext { s i n g a l l$$  \nThis is an example of a GAM. It is called an additive model because we calculate a separate f j for each X j , and then add together all of their contributions.  \nIn Sections 7.1-7.6, we discuss many methods for fitting functions to a single variable. The beauty of GAMs is that we can use these methods as building blocks for fitting an additive model. In fact, for most of the methods that we have seen so far in this chapter, this can be done fairly trivially. Take, for example, natural splines, and consider the task of fitting the model  \n$$\\ w g { e } = \\beta _ { 0 } + f _ { 1 } ( y e a r ) + f _ { 2 } ( a g e ) + f _ { 3 } ( \\text {education} ) + \\epsilon$$  \non the Wage data. Here year and age are quantitative variables, while the variable education is qualitative with five levels: &lt;HS , HS , &lt;Coll , Coll , &gt;Coll , referring to the amount of high school or college education that an individual has completed. We fit the first two functions using natural splines. We  \nFIGURE 7.12. Details are as in Figure 7.11, but now f 1 and f 2 are smoothing splines with four and five degrees of freedom, respectively.  \n<!-- image -->  \nfit the third function using a separate constant for each level, via the usual dummy variable approach of Section 3.3.1.  \nFigure 7.11 can be easily interpreted. The left-hand panel indicates that holding age and education fixed, wage tends to increase slightly with year ; this may be due to inflation. The center panel indicates that holding education and year fixed, wage tends to be highest for intermediate values of age , and lowest for the very young and very old. The right-hand panel indicates that holding year and age fixed, wage tends to increase with education : the more educated a person is, the higher their salary, on average. All of these findings are intuitive.  \nFigure 7.11 shows the results of fitting the model (7.16) using least squares. This is easy to do, since as discussed in Section 7.4, natural splines can be constructed using an appropriately chosen set of basis functions. Hence the entire model is just a big regression onto spline basis variables and dummy variables, all packed into one big regression matrix.  \nFigure 7.12 shows a similar triple of plots, but this time f 1 and f 2 are smoothing splines with four and five degrees of freedom, respectively. Fitting a GAM with a smoothing spline is not quite as simple as fitting a GAM with a natural spline, since in the case of smoothing splines, least squares cannot be used. However, standard software such as the Python package pygam can be used to fit GAMs using smoothing splines, via an approach known as backfitting . This method fits a model involving multiple predictors by repeatedly updating the fit for each predictor in turn, holding the others fixed. The beauty of this approach is that each time we update a function, we simply apply the fitting method for that variable to a partial residual . 6  \nThe fitted functions in Figures 7.11 and 7.12 look rather similar. In most situations, the differences in the GAMs obtained using smoothing splines versus natural splines are small.  \n6 A partial residual for X 3 , for example, has the form r i = y i -f 1 ( x i 1 ) -f 2 ( x i 2 ) . If we know f 1 and f 2 , then we can fit f 3 by treating this residual as a response in a non-linear regression on X 3 .  \npygam backfitting  \nWe do not have to use splines as the building blocks for GAMs: we can just as well use local regression, polynomial regression, or any combination of the approaches seen earlier in this chapter in order to create a GAM. GAMs are investigated in further detail in the lab at the end of this chapter.\nPros and Cons of GAMs  \nBefore we move on, let us summarize the advantages and limitations of a GAM.  \n- ▲ GAMs allow us to fit a non-linear f j to each X j , so that we can automatically model non-linear relationships that standard linear regression will miss. This means that we do not need to manually try out many different transformations on each variable individually.\n- ▲ The non-linear fits can potentially make more accurate predictions for the response Y .\n- ▲ Because the model is additive, we can examine the effect of each X j on Y individually while holding all of the other variables fixed.\n- ▲ The smoothness of the function f j for the variable X j can be summarized via degrees of freedom.\n- ◆ The main limitation of GAMs is that the model is restricted to be additive. With many variables, important interactions can be missed. However, as with linear regression, we can manually add interaction terms to the GAM model by including additional predictors of the form X j × X k . In addition we can add low-dimensional interaction functions of the form f jk ( X j  X k ) into the model; such terms can be fit using two-dimensional smoothers such as local regression, or two-dimensional splines (not covered here).  \nFor fully general models, we have to look for even more flexible approaches such as random forests and boosting, described in Chapter 8. GAMs provide a useful compromise between linear and fully nonparametric models.', '### 2.1.3 The Trade-Off Between Prediction Accuracy and Model Interpretability  \nOf the many methods that we examine in this book, some are less flexible, or more restrictive, in the sense that they can produce just a relatively small range of shapes to estimate f . For example, linear regression is a relatively inflexible approach, because it can only generate linear functions such as the lines shown in Figure 2.1 or the plane shown in Figure 2.4. Other methods, such as the thin plate splines shown in Figures 2.5 and 2.6, are considerably more flexible because they can generate a much wider range of possible shapes to estimate f .  \nFIGURE 2.7. A representation of the tradeoff between flexibility and interpretability, using different statistical learning methods. In general, as the flexibility of a method increases, its interpretability decreases.  \n<!-- image -->  \nOne might reasonably ask the following question: why would we ever choose to use a more restrictive method instead of a very flexible approach? There are several reasons that we might prefer a more restrictive model. If we are mainly interested in inference, then restrictive models are much more interpretable. For instance, when inference is the goal, the linear model may be a good choice since it will be quite easy to understand the relationship between Y and X 1  X 2         X p . In contrast, very flexible approaches, such as the splines discussed in Chapter 7 and displayed in Figures 2.5 and 2.6, and the boosting methods discussed in Chapter 8, can lead to such complicated estimates of f that it is difficult to understand how any individual predictor is associated with the response.  \nFigure 2.7 provides an illustration of the trade-off between flexibility and interpretability for some of the methods that we cover in this book. Least squares linear regression, discussed in Chapter 3, is relatively inflexible but is quite interpretable. The lasso , discussed in Chapter 6, relies upon the linear model (2.4) but uses an alternative fitting procedure for estimating the coefficients β 0  β 1         β p . The new procedure is more restrictive in estimating the coefficients, and sets a number of them to exactly zero. Hence in this sense the lasso is a less flexible approach than linear regression. It is also more interpretable than linear regression, because in the final model the response variable will only be related to a small subset of the predictors-namely, those with nonzero coefficient estimates. Generalized additive models (GAMs), discussed in Chapter 7, instead extend the linear model (2.4) to allow for certain non-linear relationships. Consequently, GAMs are more flexible than linear regression. They are also somewhat less interpretable than linear regression, because the relationship between each predictor and the response is now modeled using a curve. Finally, fully non-linear methods such as bagging , boosting , support vector machines with non-linear kernels, and neural networks (deep learning), discussed in Chapters 8, 9, and 10, are highly flexible approaches that are harder to interpret.  \nlasso generalized additive model  \nbagging boosting support vector machine  \nWe have established that when inference is the goal, there are clear advantages to using simple and relatively inflexible statistical learning methods. In some settings, however, we are only interested in prediction, and the interpretability of the predictive model is simply not of interest. For instance, if we seek to develop an algorithm to predict the price of a stock, our sole requirement for the algorithm is that it predict accuratelyinterpretability is not a concern. In this setting, we might expect that it will be best to use the most flexible model available. Surprisingly, this is not always the case! We will often obtain more accurate predictions using a less flexible method. This phenomenon, which may seem counterintuitive at first glance, has to do with the potential for overfitting in highly flexible methods. We saw an example of overfitting in Figure 2.6. We will discuss this very important concept further in Section 2.2 and throughout this book.']","Un modelo aditivo generalizado (GAM) es un marco general que extiende un modelo lineal estándar al permitir funciones no lineales de cada una de las variables, manteniendo la aditividad. Esto significa que en lugar de tener una relación lineal simple entre cada predictor y la respuesta, se pueden utilizar funciones suaves no lineales para modelar estas relaciones. La fórmula general de un GAM para problemas de regresión es:

\[ y_i = \beta_0 + \sum_{j=1}^{p} f_j(x_{ij}) + \epsilon_i \]

donde \( f_j(x_{ij}) \) son funciones no lineales de los predictores \( x_{ij} \).

La principal ventaja de los GAMs es que permiten ajustar una función no lineal \( f_j \) a cada predictor \( X_j \), lo que permite modelar automáticamente relaciones no lineales que un modelo de regresión lineal estándar podría pasar por alto. Esto significa que no es necesario probar manualmente muchas transformaciones diferentes en cada variable individualmente, lo que puede resultar en predicciones más precisas para la respuesta \( Y \). Además, debido a que el modelo es aditivo, se puede examinar el efecto de cada predictor en \( Y \) individualmente mientras se mantienen fijos todos los demás predictores.","Un modelo aditivo generalizado (GAM) es una extensión de los modelos lineales que permite que la relación entre cada variable predictora y la variable de respuesta sea no lineal, mientras se mantiene la aditividad de los efectos. Su principal ventaja es la capacidad de modelar relaciones complejas y no lineales de manera flexible, al mismo tiempo que se puede examinar el efecto de cada predictor individualmente mientras se mantienen los demás constantes.",1.0,,0.9999999999666667,1.0
What is the key difference between bagging and boosting?,"['### 8.2.2 Random Forests  \nRandom forests provide an improvement over bagged trees by way of a small tweak that decorrelates the trees. As in bagging, we build a number of decision trees on bootstrapped training samples. But when building these decision trees, each time a split in a tree is considered, a random sample of m predictors is chosen as split candidates from the full set of p predictors. The split is allowed to use only one of those m predictors. A fresh sample of m predictors is taken at each split, and typically we choose m ≈ √ p -that is, the number of predictors considered at each split is approximately equal to the square root of the total number of predictors (4 out of the 13 for the Heart data).  \nIn other words, in building a random forest, at each split in the tree, the algorithm is not even allowed to consider a majority of the available predictors. This may sound crazy, but it has a clever rationale. Suppose that there is one very strong predictor in the data set, along with a number of other moderately strong predictors. Then in the collection of bagged trees, most or all of the trees will use this strong predictor in the top split. Consequently, all of the bagged trees will look quite similar to each other.  \nvariable importance random forest  \nHence the predictions from the bagged trees will be highly correlated. Unfortunately, averaging many highly correlated quantities does not lead to as large of a reduction in variance as averaging many uncorrelated quantities. In particular, this means that bagging will not lead to a substantial reduction in variance over a single tree in this setting.  \nThe main difference between bagging and random forests is the choice of predictor subset size m . For instance, if a random forest is built using m = p , then this amounts simply to bagging. On the Heart data, random forests using m = √ p leads to a reduction in both test error and OOB error over bagging (Figure 8.8).  \nRandom forests overcome this problem by forcing each split to consider only a subset of the predictors. Therefore, on average ( p -m ) glyph[triangleleft]p of the splits will not even consider the strong predictor, and so other predictors will have more of a chance. We can think of this process as decorrelating the trees, thereby making the average of the resulting trees less variable and hence more reliable.  \nUsing a small value of m in building a random forest will typically be helpful when we have a large number of correlated predictors. We applied random forests to a high-dimensional biological data set consisting of expression measurements of 4,718 genes measured on tissue samples from 349 patients. There are around 20,000 genes in humans, and individual genes have different levels of activity, or expression, in particular cells, tissues, and biological conditions. In this data set, each of the patient samples has a qualitative label with 15 different levels: either normal or 1 of 14 different types of cancer. Our goal was to use random forests to predict cancer type based on the 500 genes that have the largest variance in the training set. We randomly divided the observations into a training and a test set, and applied random forests to the training set for three different values of the number of splitting variables m . The results are shown in Figure 8.10. The error rate of a single tree is 45   7 % , and the null rate is 75   4 % . 4 We see that using 400 trees is sufficient to give good performance, and that the choice m = √ p gave a small improvement in test error over bagging ( m = p ) in this example. As with bagging, random forests will not overfit if we increase B , so in practice we use a value of B sufficiently large for the error rate to have settled down.', '### 8.2.3 Boosting  \nWe now discuss boosting , yet another approach for improving the predictions resulting from a decision tree. Like bagging, boosting is a general approach that can be applied to many statistical learning methods for regression or classification. Here we restrict our discussion of boosting to the context of decision trees.  \nRecall that bagging involves creating multiple copies of the original training data set using the bootstrap, fitting a separate decision tree to each copy, and then combining all of the trees in order to create a single predic-  \n4 The null rate results from simply classifying each observation to the dominant class overall, which is in this case the normal class.  \nboosting  \nFIGURE 8.10. Results from random forests for the 15-class gene expression data set with p = 500 predictors. The test error is displayed as a function of the number of trees. Each colored line corresponds to a different value of m , the number of predictors available for splitting at each interior tree node. Random forests ( m &lt; p ) lead to a slight improvement over bagging ( m = p ). A single classification tree has an error rate of 45.7 %.  \n<!-- image -->  \ntive model. Notably, each tree is built on a bootstrap data set, independent of the other trees. Boosting works in a similar way, except that the trees are grown sequentially : each tree is grown using information from previously grown trees. Boosting does not involve bootstrap sampling; instead each tree is fit on a modified version of the original data set.  \nWhat is the idea behind this procedure? Unlike fitting a single large decision tree to the data, which amounts to fitting the data hard and potentially overfitting, the boosting approach instead learns slowly . Given the current model, we fit a decision tree to the residuals from the model. That is, we fit a tree using the current residuals, rather than the outcome Y , as the response. We then add this new decision tree into the fitted function in order to update the residuals. Each of these trees can be rather small, with just a few terminal nodes, determined by the parameter d in the algorithm. By fitting small trees to the residuals, we slowly improve ˆ f in areas where it does not perform well. The shrinkage parameter λ slows the process down even further, allowing more and different shaped trees to attack the residuals. In general, statistical learning approaches that learn slowly tend to perform well. Note that in boosting, unlike in bagging, the construction of each tree depends strongly on the trees that have already been grown.  \nConsider first the regression setting. Like bagging, boosting involves combining a large number of decision trees, ˆ f 1         ˆ f B . Boosting is described in Algorithm 8.2.  \nWe have just described the process of boosting regression trees. Boosting classification trees proceeds in a similar but slightly more complex way, and the details are omitted here.\nAlgorithm 8.2 Boosting for Regression Trees  \n1. Set ˆ f ( x ) = 0 and r i = y i for all i in the training set.\n2. For b = 1  2         B , repeat:\n3. (a) Fit a tree ˆ f b with d splits ( d +1 terminal nodes) to the training data ( Xr ) .\n4. (b) Update ˆ f by adding in a shrunken version of the new tree:  \n$$\\hat { f } ( x ) \\leftarrow \\hat { f } ( x ) + \\lambda \\hat { f } ^ { b } ( x ) .$$  \n- (c) Update the residuals,  \n$$r _ { i } \\leftarrow r _ { i } - \\lambda \\hat { f } ^ { b } ( x _ { i } ) .$$  \n3. Output the boosted model,  \n$$\\hat { f } ( x ) = \\sum _ { b = 1 } ^ { B } \\lambda \\hat { f } ^ { b } ( x ) .$$\nBoosting has three tuning parameters:  \n1. The number of trees B . Unlike bagging and random forests, boosting can overfit if B is too large, although this overfitting tends to occur slowly if at all. We use cross-validation to select B .\n2. The shrinkage parameter λ , a small positive number. This controls the rate at which boosting learns. Typical values are 0   01 or 0   001 , and the right choice can depend on the problem. Very small λ can require using a very large value of B in order to achieve good performance.\n3. The number d of splits in each tree, which controls the complexity of the boosted ensemble. Often d = 1 works well, in which case each tree is a stump , consisting of a single split. In this case, the boosted ensemble is fitting an additive model, since each term involves only a single variable. More generally d is the interaction depth , and controls the interaction order of the boosted model, since d splits can involve at most d variables.  \nIn Figure 8.11, we applied boosting to the 15-class cancer gene expression data set, in order to develop a classifier that can distinguish the normal class from the 14 cancer classes. We display the test error as a function of the total number of trees and the interaction depth d . We see that simple stumps with an interaction depth of one perform well if enough of them are included. This model outperforms the depth-two model, and both outperform a random forest. This highlights one difference between boosting and random forests: in boosting, because the growth of a particular tree takes into account the other trees that have already been grown, smaller stump  \ninteraction depth  \nFIGURE 8.11. Results from performing boosting and random forests on the 15-class gene expression data set in order to predict cancer versus normal . The test error is displayed as a function of the number of trees. For the two boosted models, λ = 0   01 . Depth-1 trees slightly outperform depth-2 trees, and both outperform the random forest, although the standard errors are around 0.02, making none of these differences significant. The test error rate for a single tree is 24 %.  \n<!-- image -->  \ntrees are typically sufficient. Using smaller trees can aid in interpretability as well; for instance, using stumps leads to an additive model.', '### 8.2.1 Bagging  \nThe bootstrap, introduced in Chapter 5, is an extremely powerful idea. It is used in many situations in which it is hard or even impossible to directly compute the standard deviation of a quantity of interest. We see here that the bootstrap can be used in a completely different context, in order to improve statistical learning methods such as decision trees.  \nRecall that given a set of n independent observations Z 1         Z n , each with variance σ 2 , the variance of the mean ¯ Z of the observations is given by σ 2 glyph[triangleleft]n . In other words, averaging a set of observations reduces variance . Hence a natural way to reduce the variance and increase the test set accuracy of a statistical learning method is to take many training sets from the population, build a separate prediction model using each training set, and average the resulting predictions. In other words, we could calculate ˆ f 1 ( x )  ˆ f 2 ( x )         ˆ f B ( x ) using B separate training sets, and average them in order to obtain a single low-variance statistical learning model, given by  \nThe decision trees discussed in Section 8.1 suffer from high variance . This means that if we split the training data into two parts at random, and fit a decision tree to both halves, the results that we get could be quite different. In contrast, a procedure with low variance will yield similar results if applied repeatedly to distinct data sets; linear regression tends to have low variance, if the ratio of n to p is moderately large. Bootstrap aggregation , or bagging , is a general-purpose procedure for reducing the variance of a statistical learning method; we introduce it here because it is particularly useful and frequently used in the context of decision trees.  \n$$\\hat { f } _ { a v g } ( x ) = \\frac { 1 } { B } \\sum _ { b = 1 } ^ { B } \\hat { f } ^ { b } ( x ) . \\\\$$  \nOf course, this is not practical because we generally do not have access to multiple training sets. Instead, we can bootstrap, by taking repeated samples from the (single) training data set. In this approach we generate B different bootstrapped training data sets. We then train our method on the b th bootstrapped training set in order to get ˆ f ∗ b ( x ) , and finally average all the predictions, to obtain  \n$$\\hat { f } _ { b a g } ( x ) = \\frac { 1 } { B } \\sum _ { b = 1 } ^ { B } \\hat { f } ^ { * b } ( x ) .$$  \nensemble weak learners  \nbagging  \nFIGURE 8.8. Bagging and random forest results for the Heart data. The test error (black and orange) is shown as a function of B , the number of bootstrapped training sets used. Random forests were applied with m = √ p . The dashed line indicates the test error resulting from a single classification tree. The green and blue traces show the OOB error, which in this case is - by chance - considerably lower.  \n<!-- image -->\nThis is called bagging.  \nThus far, we have described the bagging procedure in the regression context, to predict a quantitative outcome Y . How can bagging be extended to a classification problem where Y is qualitative? In that situation, there are a few possible approaches, but the simplest is as follows. For a given test observation, we can record the class predicted by each of the B trees, and take a majority vote : the overall prediction is the most commonly occurring class among the B predictions.  \nWhile bagging can improve predictions for many regression methods, it is particularly useful for decision trees. To apply bagging to regression trees, we simply construct B regression trees using B bootstrapped training sets, and average the resulting predictions. These trees are grown deep, and are not pruned. Hence each individual tree has high variance, but low bias. Averaging these B trees reduces the variance. Bagging has been demonstrated to give impressive improvements in accuracy by combining together hundreds or even thousands of trees into a single procedure.  \nFigure 8.8 shows the results from bagging trees on the Heart data. The test error rate is shown as a function of B , the number of trees constructed using bootstrapped training data sets. We see that the bagging test error rate is slightly lower in this case than the test error rate obtained from a single tree. The number of trees B is not a critical parameter with bagging; using a very large value of B will not lead to overfitting. In practice we majority vote  \nuse a value of B sufficiently large that the error has settled down. Using B = 100 is sufficient to achieve good performance in this example.\nOut-of-Bag Error Estimation  \nIt turns out that there is a very straightforward way to estimate the test error of a bagged model, without the need to perform cross-validation or the validation set approach. Recall that the key to bagging is that trees are repeatedly fit to bootstrapped subsets of the observations. One can show that on average, each bagged tree makes use of around two-thirds of the observations. 3 The remaining one-third of the observations not used to fit a given bagged tree are referred to as the out-of-bag (OOB) observations. We can predict the response for the i th observation using each of the trees in which that observation was OOB. This will yield around Bglyph[triangleleft] 3 predictions for the i th observation. In order to obtain a single prediction for the i th observation, we can average these predicted responses (if regression is the goal) or can take a majority vote (if classification is the goal). This leads to a single OOB prediction for the i th observation. An OOB prediction can be obtained in this way for each of the n observations, from which the overall OOB MSE (for a regression problem) or classification error (for a classification problem) can be computed. The resulting OOB error is a valid estimate of the test error for the bagged model, since the response for each observation is predicted using only the trees that were not fit using that observation. Figure 8.8 displays the OOB error on the Heart data. It can be shown that with B sufficiently large, OOB error is virtually equivalent to leave-one-out cross-validation error. The OOB approach for estimating the test error is particularly convenient when performing bagging on large data sets for which cross-validation would be computationally onerous.\nVariable Importance Measures  \nAs we have discussed, bagging typically results in improved accuracy over prediction using a single tree. Unfortunately, however, it can be difficult to interpret the resulting model. Recall that one of the advantages of decision trees is the attractive and easily interpreted diagram that results, such as the one displayed in Figure 8.1. However, when we bag a large number of trees, it is no longer possible to represent the resulting statistical learning procedure using a single tree, and it is no longer clear which variables are most important to the procedure. Thus, bagging improves prediction accuracy at the expense of interpretability.  \nAlthough the collection of bagged trees is much more difficult to interpret than a single tree, one can obtain an overall summary of the importance of each predictor using the RSS (for bagging regression trees) or the Gini index (for bagging classification trees). In the case of bagging regression trees, we can record the total amount that the RSS (8.1) is decreased due to splits over a given predictor, averaged over all B trees. A large value indicates an important predictor. Similarly, in the context of bagging classification  \n3 This relates to Exercise 2 of Chapter 5.  \nout-of-bag  \nFIGURE 8.9. A variable importance plot for the Heart data. Variable importance is computed using the mean decrease in Gini index, and expressed relative to the maximum.  \n<!-- image -->  \ntrees, we can add up the total amount that the Gini index (8.6) is decreased by splits over a given predictor, averaged over all B trees.  \nA graphical representation of the variable importances in the Heart data is shown in Figure 8.9. We see the mean decrease in Gini index for each variable, relative to the largest. The variables with the largest mean decrease in Gini index are Thal , Ca , and ChestPain .']","The key difference between bagging and boosting lies in how the decision trees are constructed and combined:

- **Bagging (Bootstrap Aggregating):** In bagging, multiple copies of the original training data set are created using the bootstrap method. Each decision tree is built independently on these bootstrapped data sets. The final model is an average (for regression) or a majority vote (for classification) of all the trees. The trees are grown independently of each other.

- **Boosting:** Boosting involves building trees sequentially, where each tree is grown using information from the previously grown trees. Instead of using bootstrap samples, each tree is fit on a modified version of the original data set. Boosting focuses on fitting trees to the residuals of the current model, gradually improving the model by focusing on areas where it performs poorly. The construction of each tree in boosting depends strongly on the trees that have already been grown.

In summary, bagging builds trees independently and combines them, while boosting builds trees sequentially, with each tree learning from the errors of the previous ones.","Bagging involves creating multiple independent bootstrap samples of the training data and fitting a separate model to each, with the final prediction being the average of all models. Boosting, in contrast, builds models sequentially, where each new model is fitted to the residuals of the previous ones, thus focusing on the errors of the prior models.",1.0,,0.5833333333041666,1.0
How does a random forest improve upon a bagged tree model?,"['### 8.2.2 Random Forests  \nRandom forests provide an improvement over bagged trees by way of a small tweak that decorrelates the trees. As in bagging, we build a number of decision trees on bootstrapped training samples. But when building these decision trees, each time a split in a tree is considered, a random sample of m predictors is chosen as split candidates from the full set of p predictors. The split is allowed to use only one of those m predictors. A fresh sample of m predictors is taken at each split, and typically we choose m ≈ √ p -that is, the number of predictors considered at each split is approximately equal to the square root of the total number of predictors (4 out of the 13 for the Heart data).  \nIn other words, in building a random forest, at each split in the tree, the algorithm is not even allowed to consider a majority of the available predictors. This may sound crazy, but it has a clever rationale. Suppose that there is one very strong predictor in the data set, along with a number of other moderately strong predictors. Then in the collection of bagged trees, most or all of the trees will use this strong predictor in the top split. Consequently, all of the bagged trees will look quite similar to each other.  \nvariable importance random forest  \nHence the predictions from the bagged trees will be highly correlated. Unfortunately, averaging many highly correlated quantities does not lead to as large of a reduction in variance as averaging many uncorrelated quantities. In particular, this means that bagging will not lead to a substantial reduction in variance over a single tree in this setting.  \nThe main difference between bagging and random forests is the choice of predictor subset size m . For instance, if a random forest is built using m = p , then this amounts simply to bagging. On the Heart data, random forests using m = √ p leads to a reduction in both test error and OOB error over bagging (Figure 8.8).  \nRandom forests overcome this problem by forcing each split to consider only a subset of the predictors. Therefore, on average ( p -m ) glyph[triangleleft]p of the splits will not even consider the strong predictor, and so other predictors will have more of a chance. We can think of this process as decorrelating the trees, thereby making the average of the resulting trees less variable and hence more reliable.  \nUsing a small value of m in building a random forest will typically be helpful when we have a large number of correlated predictors. We applied random forests to a high-dimensional biological data set consisting of expression measurements of 4,718 genes measured on tissue samples from 349 patients. There are around 20,000 genes in humans, and individual genes have different levels of activity, or expression, in particular cells, tissues, and biological conditions. In this data set, each of the patient samples has a qualitative label with 15 different levels: either normal or 1 of 14 different types of cancer. Our goal was to use random forests to predict cancer type based on the 500 genes that have the largest variance in the training set. We randomly divided the observations into a training and a test set, and applied random forests to the training set for three different values of the number of splitting variables m . The results are shown in Figure 8.10. The error rate of a single tree is 45   7 % , and the null rate is 75   4 % . 4 We see that using 400 trees is sufficient to give good performance, and that the choice m = √ p gave a small improvement in test error over bagging ( m = p ) in this example. As with bagging, random forests will not overfit if we increase B , so in practice we use a value of B sufficiently large for the error rate to have settled down.', ""### 8.2.4 Bayesian Additive Regression Trees  \nFinally, we discuss Bayesian additive regression trees (BART), another ensemble method that uses decision trees as its building blocks. For simplicity, we present BART for regression (as opposed to classification).  \nBefore we introduce the BART algorithm, we define some notation. We let K denote the number of regression trees, and B the number of iterations for which the BART algorithm will be run. The notation ˆ f b k ( x ) represents the prediction at x for the k th regression tree used in the b th iteration. At the end of each iteration, the K trees from that iteration will be summed, i.e. ˆ f b ( x ) = ∑ K k =1 ˆ f b k ( x ) for b = 1         B .  \nRecall that bagging and random forests make predictions from an average of regression trees, each of which is built using a random sample of data and/or predictors. Each tree is built separately from the others. By contrast, boosting uses a weighted sum of trees, each of which is constructed by fitting a tree to the residual of the current fit. Thus, each new tree attempts to capture signal that is not yet accounted for by the current set of trees. BART is related to both approaches: each tree is constructed in a random manner as in bagging and random forests, and each tree tries to capture signal not yet accounted for by the current model, as in boosting. The main novelty in BART is the way in which new trees are generated.  \nIn the first iteration of the BART algorithm, all trees are initialized to have a single root node, with ˆ f 1 k ( x ) = 1 nK ∑ n i =1 y i , the mean of the response  \nBayesian additive regression trees  \nFIGURE 8.12. A schematic of perturbed trees from the BART algorithm. (a): The k th tree at the ( b -1) st iteration, ˆ f b -1 k ( X ) , is displayed. Panels (b)-(d) display three of many possibilities for ˆ f b k ( X ) , given the form of ˆ f b -1 k ( X ) . (b): One possibility is that ˆ f b k ( X ) has the same structure as ˆ f b -1 k ( X ) , but with different predictions at the terminal nodes. (c): Another possibility is that ˆ f b k ( X ) results from pruning ˆ f b -1 k ( X ) . (d): Alternatively, ˆ f b k ( X ) may have more terminal nodes than ˆ f b -1 k ( X ) .  \n<!-- image -->  \nvalues divided by the total number of trees. Thus, ˆ f 1 ( x ) = ∑ K k =1 ˆ f 1 k ( x ) = 1 n ∑ n i =1 y i .  \nIn subsequent iterations, BART updates each of the K trees, one at a time. In the b th iteration, to update the k th tree, we subtract from each response value the predictions from all but the k th tree, in order to obtain a partial residual  \n$$r _ { i } = y _ { i } - \\sum _ { k ^ { \\prime } < k } \\hat { f } _ { k ^ { \\prime } } ^ { b } ( x _ { i } ) - \\sum _ { k ^ { \\prime } > k } \\hat { f } _ { k ^ { \\prime \\prime } } ^ { b - 1 } ( x _ { i } )$$  \nfor the i th observation, i = 1         n . Rather than fitting a fresh tree to this partial residual, BART randomly chooses a perturbation to the tree from the previous iteration ( ˆ f b -1 k ) from a set of possible perturbations, favoring ones that improve the fit to the partial residual. There are two components to this perturbation:  \n1. Wemaychange the structure of the tree by adding or pruning branches.\n2. We may change the prediction in each terminal node of the tree.  \nFigure 8.12 illustrates examples of possible perturbations to a tree.  \nThe output of BART is a collection of prediction models,  \n$$\\hat { f } ^ { b } ( x ) = \\sum _ { k = 1 } ^ { K } \\hat { f } _ { k } ^ { b } ( x ) , \\text { for } b = 1 , 2 , \\dots , B .$$  \nAlgorithm 8.3 Bayesian Additive Regression Trees  \n1. Let ˆ f 1 1 ( x ) = ˆ f 1 2 ( x ) = · · · = ˆ f 1 K ( x ) = 1 nK ∑ n i =1 y i .\n3. For b = 2         B :\n2. Compute ˆ f 1 ( x ) = ∑ K k =1 ˆ f 1 k ( x ) = 1 n ∑ n i =1 y i .\n4. (a) For k = 1  2         K :\n- i. For i = 1         n , compute the current partial residual  \n$$r _ { i } = y _ { i } - \\sum _ { k ^ { \\prime } < k } \\hat { f } _ { k ^ { \\prime } } ^ { b } ( x _ { i } ) - \\sum _ { k ^ { \\prime } > k } \\hat { f } _ { k ^ { \\prime } } ^ { b - 1 } ( x _ { i } ) .$$  \n- ii. Fit a new tree, ˆ f b k ( x ) , to r i , by randomly perturbing the k th tree from the previous iteration, ˆ f b -1 k ( x ) . Perturbations that improve the fit are favored.\n4. Compute the mean after L burn-in samples,\n3. (b) Compute ˆ f b ( x ) = ∑ K k =1 ˆ f b k ( x ) .  \n$$\\hat { f } ( x ) = \\frac { 1 } { B - L } \\sum _ { b = L + 1 } ^ { B } \\hat { f } ^ { b } ( x ) .$$  \nWe typically throw away the first few of these prediction models, since models obtained in the earlier iterations - known as the burn-in period - tend not to provide very good results. We can let L denote the number of burn-in iterations; for instance, we might take L = 200 . Then, to obtain a single prediction, we simply take the average after the burn-in iterations, ˆ f ( x ) = 1 B -L ∑ B b = L +1 ˆ f b ( x ) . However, it is also possible to compute quantities other than the average: for instance, the percentiles of ˆ f L +1 ( x )         ˆ f B ( x ) provide a measure of uncertainty in the final prediction. The overall BART procedure is summarized in Algorithm 8.3.  \nFigure 8.13 shows the result of applying BART to the Heart data, using K = 200 trees, as the number of iterations is increased to 10  000 . During the initial iterations, the test and training errors jump around a bit. After this initial burn-in period, the error rates settle down. We note that there is only a small difference between the training error and the test error, indicating that the tree perturbation process largely avoids overfitting.  \nA key element of the BART approach is that in Step 3(a)ii., we do not fit a fresh tree to the current partial residual: instead, we try to improve the fit to the current partial residual by slightly modifying the tree obtained in the previous iteration (see Figure 8.12). Roughly speaking, this guards against overfitting since it limits how 'hard' we fit the data in each iteration. Furthermore, the individual trees are typically quite small. We limit the tree size in order to avoid overfitting the data, which would be more likely to occur if we grew very large trees.  \nburn-in  \nFIGURE 8.13. BART and boosting results for the Heart data. Both training and test errors are displayed. After a burn-in period of 100 iterations (shown in gray), the error rates for BART settle down. Boosting begins to overfit after a few hundred iterations.  \n<!-- image -->  \nThe training and test errors for boosting are also displayed in Figure 8.13. We see that the test error for boosting approaches that of BART, but then begins to increase as the number of iterations increases. Furthermore, the training error for boosting decreases as the number of iterations increases, indicating that boosting has overfit the data.  \nWhen we apply BART, we must select the number of trees K , the number of iterations B , and the number of burn-in iterations L . We typically choose large values for B and K , and a moderate value for L : for instance, K = 200 , B = 1  000 , and L = 100 is a reasonable choice. BART has been shown to have very impressive out-of-box performance - that is, it performs well with minimal tuning.  \nThough the details are outside of the scope of this book, it turns out that the BART method can be viewed as a Bayesian approach to fitting an ensemble of trees: each time we randomly perturb a tree in order to fit the residuals, we are in fact drawing a new tree from a posterior distribution. (Of course, this Bayesian connection is the motivation for BART's name.) Furthermore, Algorithm 8.3 can be viewed as a Markov chain Monte Carlo algorithm for fitting the BART model.""]","Random forests improve upon bagged tree models by introducing a process that decorrelates the trees. In bagging, multiple decision trees are built on bootstrapped training samples, and predictions are made by averaging these trees. However, if there is a strong predictor in the dataset, most of the bagged trees will use this predictor in the top split, leading to highly correlated trees. Averaging these correlated trees does not significantly reduce variance compared to averaging uncorrelated trees.

Random forests address this issue by considering only a random subset of predictors at each split in the tree. Specifically, at each split, a random sample of \( m \) predictors is chosen from the full set of \( p \) predictors, and the split is allowed to use only one of those \( m \) predictors. Typically, \( m \) is chosen to be approximately equal to the square root of the total number of predictors (\( m \approx \sqrt{p} \)). This approach ensures that not all trees will use the strong predictor at the top split, allowing other predictors to have more influence and resulting in less correlated trees. Consequently, the average of these less correlated trees is less variable and more reliable, leading to improved performance over bagged trees.","A random forest improves on bagging by introducing an additional layer of randomness. When building each tree in the forest, at each split, only a random subset of predictors is considered as split candidates. This decorrelates the trees, reducing the variance of the overall model and often leading to better predictive accuracy.",1.0,,0.99999999995,1.0
What is the role of the shrinkage parameter in boosting?,"['### 8.2.3 Boosting  \nWe now discuss boosting , yet another approach for improving the predictions resulting from a decision tree. Like bagging, boosting is a general approach that can be applied to many statistical learning methods for regression or classification. Here we restrict our discussion of boosting to the context of decision trees.  \nRecall that bagging involves creating multiple copies of the original training data set using the bootstrap, fitting a separate decision tree to each copy, and then combining all of the trees in order to create a single predic-  \n4 The null rate results from simply classifying each observation to the dominant class overall, which is in this case the normal class.  \nboosting  \nFIGURE 8.10. Results from random forests for the 15-class gene expression data set with p = 500 predictors. The test error is displayed as a function of the number of trees. Each colored line corresponds to a different value of m , the number of predictors available for splitting at each interior tree node. Random forests ( m &lt; p ) lead to a slight improvement over bagging ( m = p ). A single classification tree has an error rate of 45.7 %.  \n<!-- image -->  \ntive model. Notably, each tree is built on a bootstrap data set, independent of the other trees. Boosting works in a similar way, except that the trees are grown sequentially : each tree is grown using information from previously grown trees. Boosting does not involve bootstrap sampling; instead each tree is fit on a modified version of the original data set.  \nWhat is the idea behind this procedure? Unlike fitting a single large decision tree to the data, which amounts to fitting the data hard and potentially overfitting, the boosting approach instead learns slowly . Given the current model, we fit a decision tree to the residuals from the model. That is, we fit a tree using the current residuals, rather than the outcome Y , as the response. We then add this new decision tree into the fitted function in order to update the residuals. Each of these trees can be rather small, with just a few terminal nodes, determined by the parameter d in the algorithm. By fitting small trees to the residuals, we slowly improve ˆ f in areas where it does not perform well. The shrinkage parameter λ slows the process down even further, allowing more and different shaped trees to attack the residuals. In general, statistical learning approaches that learn slowly tend to perform well. Note that in boosting, unlike in bagging, the construction of each tree depends strongly on the trees that have already been grown.  \nConsider first the regression setting. Like bagging, boosting involves combining a large number of decision trees, ˆ f 1         ˆ f B . Boosting is described in Algorithm 8.2.  \nWe have just described the process of boosting regression trees. Boosting classification trees proceeds in a similar but slightly more complex way, and the details are omitted here.\nAlgorithm 8.2 Boosting for Regression Trees  \n1. Set ˆ f ( x ) = 0 and r i = y i for all i in the training set.\n2. For b = 1  2         B , repeat:\n3. (a) Fit a tree ˆ f b with d splits ( d +1 terminal nodes) to the training data ( Xr ) .\n4. (b) Update ˆ f by adding in a shrunken version of the new tree:  \n$$\\hat { f } ( x ) \\leftarrow \\hat { f } ( x ) + \\lambda \\hat { f } ^ { b } ( x ) .$$  \n- (c) Update the residuals,  \n$$r _ { i } \\leftarrow r _ { i } - \\lambda \\hat { f } ^ { b } ( x _ { i } ) .$$  \n3. Output the boosted model,  \n$$\\hat { f } ( x ) = \\sum _ { b = 1 } ^ { B } \\lambda \\hat { f } ^ { b } ( x ) .$$\nBoosting has three tuning parameters:  \n1. The number of trees B . Unlike bagging and random forests, boosting can overfit if B is too large, although this overfitting tends to occur slowly if at all. We use cross-validation to select B .\n2. The shrinkage parameter λ , a small positive number. This controls the rate at which boosting learns. Typical values are 0   01 or 0   001 , and the right choice can depend on the problem. Very small λ can require using a very large value of B in order to achieve good performance.\n3. The number d of splits in each tree, which controls the complexity of the boosted ensemble. Often d = 1 works well, in which case each tree is a stump , consisting of a single split. In this case, the boosted ensemble is fitting an additive model, since each term involves only a single variable. More generally d is the interaction depth , and controls the interaction order of the boosted model, since d splits can involve at most d variables.  \nIn Figure 8.11, we applied boosting to the 15-class cancer gene expression data set, in order to develop a classifier that can distinguish the normal class from the 14 cancer classes. We display the test error as a function of the total number of trees and the interaction depth d . We see that simple stumps with an interaction depth of one perform well if enough of them are included. This model outperforms the depth-two model, and both outperform a random forest. This highlights one difference between boosting and random forests: in boosting, because the growth of a particular tree takes into account the other trees that have already been grown, smaller stump  \ninteraction depth  \nFIGURE 8.11. Results from performing boosting and random forests on the 15-class gene expression data set in order to predict cancer versus normal . The test error is displayed as a function of the number of trees. For the two boosted models, λ = 0   01 . Depth-1 trees slightly outperform depth-2 trees, and both outperform the random forest, although the standard errors are around 0.02, making none of these differences significant. The test error rate for a single tree is 24 %.  \n<!-- image -->  \ntrees are typically sufficient. Using smaller trees can aid in interpretability as well; for instance, using stumps leads to an additive model.', '### Applied  \n7. In Section 8.3.3, we applied random forests to the Boston data using max\\_features = 6 and using n\\_estimators = 100 and n\\_estimators = 500 . Create a plot displaying the test error resulting from random forests on this data set for a more comprehensive range of values for max\\_features and n\\_estimators . You can model your plot after Figure 8.10. Describe the results obtained.\n8. In the lab, a classification tree was applied to the Carseats data set after converting Sales into a qualitative response variable. Now we will seek to predict Sales using regression trees and related approaches, treating the response as a quantitative variable.  \n- (a) Split the data set into a training set and a test set.\n- (b) Fit a regression tree to the training set. Plot the tree, and interpret the results. What test MSE do you obtain?\n- (c) Use cross-validation in order to determine the optimal level of tree complexity. Does pruning the tree improve the test MSE?\n- (d) Use the bagging approach in order to analyze this data. What test MSE do you obtain? Use the feature\\_importance\\_ values to determine which variables are most important.\n- (e) Use random forests to analyze this data. What test MSE do you obtain? Use the feature\\_importance\\_ values to determine which variables are most important. Describe the effect of m , the number of variables considered at each split, on the error rate obtained.\n- (f) Now analyze the data using BART, and report your results.\n9. This problem involves the OJ data set which is part of the ISLP package.\n- (a) Create a training set containing a random sample of 800 observations, and a test set containing the remaining observations.\n- (b) Fit a tree to the training data, with Purchase as the response and the other variables as predictors. What is the training error rate?\n- (c) Create a plot of the tree, and interpret the results. How many terminal nodes does the tree have?\n- (d) Use the export\\_tree() function to produce a text summary of the fitted tree. Pick one of the terminal nodes, and interpret the information displayed.\n- (e) Predict the response on the test data, and produce a confusion matrix comparing the test labels to the predicted test labels. What is the test error rate?\n- (f) Use cross-validation on the training set in order to determine the optimal tree size.\n- (g) Produce a plot with tree size on the x -axis and cross-validated classification error rate on the y -axis.\n- (h) Which tree size corresponds to the lowest cross-validated classification error rate?\n- (i) Produce a pruned tree corresponding to the optimal tree size obtained using cross-validation. If cross-validation does not lead to selection of a pruned tree, then create a pruned tree with five terminal nodes.\n- (j) Compare the training error rates between the pruned and unpruned trees. Which is higher?\n- (k) Compare the test error rates between the pruned and unpruned trees. Which is higher?  \n10. We now use boosting to predict Salary in the Hitters data set.\n2. (a) Remove the observations for whom the salary information is unknown, and then log-transform the salaries.\n3. (b) Create a training set consisting of the first 200 observations, and a test set consisting of the remaining observations.\n4. (c) Perform boosting on the training set with 1,000 trees for a range of values of the shrinkage parameter λ . Produce a plot with different shrinkage values on the x -axis and the corresponding training set MSE on the y -axis.\n5. (d) Produce a plot with different shrinkage values on the x -axis and the corresponding test set MSE on the y -axis.\n6. (e) Compare the test MSE of boosting to the test MSE that results from applying two of the regression approaches seen in Chapters 3 and 6.\n7. (f) Which variables appear to be the most important predictors in the boosted model?\n8. (g) Now apply bagging to the training set. What is the test set MSE for this approach?\n11. This question uses the Caravan data set.\n10. (a) Create a training set consisting of the first 1,000 observations, and a test set consisting of the remaining observations.\n11. (b) Fit a boosting model to the training set with Purchase as the response and the other variables as predictors. Use 1,000 trees, and a shrinkage value of 0   01 . Which predictors appear to be the most important?\n12. (c) Use the boosting model to predict the response on the test data. Predict that a person will make a purchase if the estimated probability of purchase is greater than 20 % . Form a confusion matrix. What fraction of the people predicted to make a purchase do in fact make one? How does this compare with the results obtained from applying KNN or logistic regression to this data set?\n12. Apply boosting, bagging, random forests, and BART to a data set of your choice. Be sure to fit the models on a training set and to evaluate their performance on a test set. How accurate are the results compared to simple methods like linear or logistic regression? Which of these approaches yields the best performance?', '## 6.2 Shrinkage Methods  \nThe subset selection methods described in Section 6.1 involve using least squares to fit a linear model that contains a subset of the predictors. As an alternative, we can fit a model containing all p predictors using a technique that constrains or regularizes the coefficient estimates, or equivalently, that shrinks the coefficient estimates towards zero. It may not be immediately obvious why such a constraint should improve the fit, but it turns out that shrinking the coefficient estimates can significantly reduce their variance. The two best-known techniques for shrinking the regression coefficients towards zero are ridge regression and the lasso .']","The shrinkage parameter \( \lambda \) in boosting controls the rate at which the boosting algorithm learns. It is a small positive number, typically set to values like 0.01 or 0.001. The choice of \( \lambda \) can depend on the specific problem being addressed. A very small \( \lambda \) can require using a very large number of trees \( B \) to achieve good performance. By slowing down the learning process, the shrinkage parameter allows for more and different shaped trees to address the residuals, which can lead to better model performance.","The shrinkage parameter, often denoted by lambda, in a boosting algorithm controls the learning rate. A smaller value for lambda slows down the learning process, requiring more trees to be added to the model but generally leading to improved performance by reducing the risk of overfitting.",0.8571428571428571,,0.99999999995,1.0
¿Qué es el sobreajuste (overfitting) en el contexto de los árboles de decisión y cómo se puede evitar?,"['### 8.2.3 Boosting  \nWe now discuss boosting , yet another approach for improving the predictions resulting from a decision tree. Like bagging, boosting is a general approach that can be applied to many statistical learning methods for regression or classification. Here we restrict our discussion of boosting to the context of decision trees.  \nRecall that bagging involves creating multiple copies of the original training data set using the bootstrap, fitting a separate decision tree to each copy, and then combining all of the trees in order to create a single predic-  \n4 The null rate results from simply classifying each observation to the dominant class overall, which is in this case the normal class.  \nboosting  \nFIGURE 8.10. Results from random forests for the 15-class gene expression data set with p = 500 predictors. The test error is displayed as a function of the number of trees. Each colored line corresponds to a different value of m , the number of predictors available for splitting at each interior tree node. Random forests ( m &lt; p ) lead to a slight improvement over bagging ( m = p ). A single classification tree has an error rate of 45.7 %.  \n<!-- image -->  \ntive model. Notably, each tree is built on a bootstrap data set, independent of the other trees. Boosting works in a similar way, except that the trees are grown sequentially : each tree is grown using information from previously grown trees. Boosting does not involve bootstrap sampling; instead each tree is fit on a modified version of the original data set.  \nWhat is the idea behind this procedure? Unlike fitting a single large decision tree to the data, which amounts to fitting the data hard and potentially overfitting, the boosting approach instead learns slowly . Given the current model, we fit a decision tree to the residuals from the model. That is, we fit a tree using the current residuals, rather than the outcome Y , as the response. We then add this new decision tree into the fitted function in order to update the residuals. Each of these trees can be rather small, with just a few terminal nodes, determined by the parameter d in the algorithm. By fitting small trees to the residuals, we slowly improve ˆ f in areas where it does not perform well. The shrinkage parameter λ slows the process down even further, allowing more and different shaped trees to attack the residuals. In general, statistical learning approaches that learn slowly tend to perform well. Note that in boosting, unlike in bagging, the construction of each tree depends strongly on the trees that have already been grown.  \nConsider first the regression setting. Like bagging, boosting involves combining a large number of decision trees, ˆ f 1         ˆ f B . Boosting is described in Algorithm 8.2.  \nWe have just described the process of boosting regression trees. Boosting classification trees proceeds in a similar but slightly more complex way, and the details are omitted here.\nAlgorithm 8.2 Boosting for Regression Trees  \n1. Set ˆ f ( x ) = 0 and r i = y i for all i in the training set.\n2. For b = 1  2         B , repeat:\n3. (a) Fit a tree ˆ f b with d splits ( d +1 terminal nodes) to the training data ( Xr ) .\n4. (b) Update ˆ f by adding in a shrunken version of the new tree:  \n$$\\hat { f } ( x ) \\leftarrow \\hat { f } ( x ) + \\lambda \\hat { f } ^ { b } ( x ) .$$  \n- (c) Update the residuals,  \n$$r _ { i } \\leftarrow r _ { i } - \\lambda \\hat { f } ^ { b } ( x _ { i } ) .$$  \n3. Output the boosted model,  \n$$\\hat { f } ( x ) = \\sum _ { b = 1 } ^ { B } \\lambda \\hat { f } ^ { b } ( x ) .$$\nBoosting has three tuning parameters:  \n1. The number of trees B . Unlike bagging and random forests, boosting can overfit if B is too large, although this overfitting tends to occur slowly if at all. We use cross-validation to select B .\n2. The shrinkage parameter λ , a small positive number. This controls the rate at which boosting learns. Typical values are 0   01 or 0   001 , and the right choice can depend on the problem. Very small λ can require using a very large value of B in order to achieve good performance.\n3. The number d of splits in each tree, which controls the complexity of the boosted ensemble. Often d = 1 works well, in which case each tree is a stump , consisting of a single split. In this case, the boosted ensemble is fitting an additive model, since each term involves only a single variable. More generally d is the interaction depth , and controls the interaction order of the boosted model, since d splits can involve at most d variables.  \nIn Figure 8.11, we applied boosting to the 15-class cancer gene expression data set, in order to develop a classifier that can distinguish the normal class from the 14 cancer classes. We display the test error as a function of the total number of trees and the interaction depth d . We see that simple stumps with an interaction depth of one perform well if enough of them are included. This model outperforms the depth-two model, and both outperform a random forest. This highlights one difference between boosting and random forests: in boosting, because the growth of a particular tree takes into account the other trees that have already been grown, smaller stump  \ninteraction depth  \nFIGURE 8.11. Results from performing boosting and random forests on the 15-class gene expression data set in order to predict cancer versus normal . The test error is displayed as a function of the number of trees. For the two boosted models, λ = 0   01 . Depth-1 trees slightly outperform depth-2 trees, and both outperform the random forest, although the standard errors are around 0.02, making none of these differences significant. The test error rate for a single tree is 24 %.  \n<!-- image -->  \ntrees are typically sufficient. Using smaller trees can aid in interpretability as well; for instance, using stumps leads to an additive model.', ""### 8.2.4 Bayesian Additive Regression Trees  \nFinally, we discuss Bayesian additive regression trees (BART), another ensemble method that uses decision trees as its building blocks. For simplicity, we present BART for regression (as opposed to classification).  \nBefore we introduce the BART algorithm, we define some notation. We let K denote the number of regression trees, and B the number of iterations for which the BART algorithm will be run. The notation ˆ f b k ( x ) represents the prediction at x for the k th regression tree used in the b th iteration. At the end of each iteration, the K trees from that iteration will be summed, i.e. ˆ f b ( x ) = ∑ K k =1 ˆ f b k ( x ) for b = 1         B .  \nRecall that bagging and random forests make predictions from an average of regression trees, each of which is built using a random sample of data and/or predictors. Each tree is built separately from the others. By contrast, boosting uses a weighted sum of trees, each of which is constructed by fitting a tree to the residual of the current fit. Thus, each new tree attempts to capture signal that is not yet accounted for by the current set of trees. BART is related to both approaches: each tree is constructed in a random manner as in bagging and random forests, and each tree tries to capture signal not yet accounted for by the current model, as in boosting. The main novelty in BART is the way in which new trees are generated.  \nIn the first iteration of the BART algorithm, all trees are initialized to have a single root node, with ˆ f 1 k ( x ) = 1 nK ∑ n i =1 y i , the mean of the response  \nBayesian additive regression trees  \nFIGURE 8.12. A schematic of perturbed trees from the BART algorithm. (a): The k th tree at the ( b -1) st iteration, ˆ f b -1 k ( X ) , is displayed. Panels (b)-(d) display three of many possibilities for ˆ f b k ( X ) , given the form of ˆ f b -1 k ( X ) . (b): One possibility is that ˆ f b k ( X ) has the same structure as ˆ f b -1 k ( X ) , but with different predictions at the terminal nodes. (c): Another possibility is that ˆ f b k ( X ) results from pruning ˆ f b -1 k ( X ) . (d): Alternatively, ˆ f b k ( X ) may have more terminal nodes than ˆ f b -1 k ( X ) .  \n<!-- image -->  \nvalues divided by the total number of trees. Thus, ˆ f 1 ( x ) = ∑ K k =1 ˆ f 1 k ( x ) = 1 n ∑ n i =1 y i .  \nIn subsequent iterations, BART updates each of the K trees, one at a time. In the b th iteration, to update the k th tree, we subtract from each response value the predictions from all but the k th tree, in order to obtain a partial residual  \n$$r _ { i } = y _ { i } - \\sum _ { k ^ { \\prime } < k } \\hat { f } _ { k ^ { \\prime } } ^ { b } ( x _ { i } ) - \\sum _ { k ^ { \\prime } > k } \\hat { f } _ { k ^ { \\prime \\prime } } ^ { b - 1 } ( x _ { i } )$$  \nfor the i th observation, i = 1         n . Rather than fitting a fresh tree to this partial residual, BART randomly chooses a perturbation to the tree from the previous iteration ( ˆ f b -1 k ) from a set of possible perturbations, favoring ones that improve the fit to the partial residual. There are two components to this perturbation:  \n1. Wemaychange the structure of the tree by adding or pruning branches.\n2. We may change the prediction in each terminal node of the tree.  \nFigure 8.12 illustrates examples of possible perturbations to a tree.  \nThe output of BART is a collection of prediction models,  \n$$\\hat { f } ^ { b } ( x ) = \\sum _ { k = 1 } ^ { K } \\hat { f } _ { k } ^ { b } ( x ) , \\text { for } b = 1 , 2 , \\dots , B .$$  \nAlgorithm 8.3 Bayesian Additive Regression Trees  \n1. Let ˆ f 1 1 ( x ) = ˆ f 1 2 ( x ) = · · · = ˆ f 1 K ( x ) = 1 nK ∑ n i =1 y i .\n3. For b = 2         B :\n2. Compute ˆ f 1 ( x ) = ∑ K k =1 ˆ f 1 k ( x ) = 1 n ∑ n i =1 y i .\n4. (a) For k = 1  2         K :\n- i. For i = 1         n , compute the current partial residual  \n$$r _ { i } = y _ { i } - \\sum _ { k ^ { \\prime } < k } \\hat { f } _ { k ^ { \\prime } } ^ { b } ( x _ { i } ) - \\sum _ { k ^ { \\prime } > k } \\hat { f } _ { k ^ { \\prime } } ^ { b - 1 } ( x _ { i } ) .$$  \n- ii. Fit a new tree, ˆ f b k ( x ) , to r i , by randomly perturbing the k th tree from the previous iteration, ˆ f b -1 k ( x ) . Perturbations that improve the fit are favored.\n4. Compute the mean after L burn-in samples,\n3. (b) Compute ˆ f b ( x ) = ∑ K k =1 ˆ f b k ( x ) .  \n$$\\hat { f } ( x ) = \\frac { 1 } { B - L } \\sum _ { b = L + 1 } ^ { B } \\hat { f } ^ { b } ( x ) .$$  \nWe typically throw away the first few of these prediction models, since models obtained in the earlier iterations - known as the burn-in period - tend not to provide very good results. We can let L denote the number of burn-in iterations; for instance, we might take L = 200 . Then, to obtain a single prediction, we simply take the average after the burn-in iterations, ˆ f ( x ) = 1 B -L ∑ B b = L +1 ˆ f b ( x ) . However, it is also possible to compute quantities other than the average: for instance, the percentiles of ˆ f L +1 ( x )         ˆ f B ( x ) provide a measure of uncertainty in the final prediction. The overall BART procedure is summarized in Algorithm 8.3.  \nFigure 8.13 shows the result of applying BART to the Heart data, using K = 200 trees, as the number of iterations is increased to 10  000 . During the initial iterations, the test and training errors jump around a bit. After this initial burn-in period, the error rates settle down. We note that there is only a small difference between the training error and the test error, indicating that the tree perturbation process largely avoids overfitting.  \nA key element of the BART approach is that in Step 3(a)ii., we do not fit a fresh tree to the current partial residual: instead, we try to improve the fit to the current partial residual by slightly modifying the tree obtained in the previous iteration (see Figure 8.12). Roughly speaking, this guards against overfitting since it limits how 'hard' we fit the data in each iteration. Furthermore, the individual trees are typically quite small. We limit the tree size in order to avoid overfitting the data, which would be more likely to occur if we grew very large trees.  \nburn-in  \nFIGURE 8.13. BART and boosting results for the Heart data. Both training and test errors are displayed. After a burn-in period of 100 iterations (shown in gray), the error rates for BART settle down. Boosting begins to overfit after a few hundred iterations.  \n<!-- image -->  \nThe training and test errors for boosting are also displayed in Figure 8.13. We see that the test error for boosting approaches that of BART, but then begins to increase as the number of iterations increases. Furthermore, the training error for boosting decreases as the number of iterations increases, indicating that boosting has overfit the data.  \nWhen we apply BART, we must select the number of trees K , the number of iterations B , and the number of burn-in iterations L . We typically choose large values for B and K , and a moderate value for L : for instance, K = 200 , B = 1  000 , and L = 100 is a reasonable choice. BART has been shown to have very impressive out-of-box performance - that is, it performs well with minimal tuning.  \nThough the details are outside of the scope of this book, it turns out that the BART method can be viewed as a Bayesian approach to fitting an ensemble of trees: each time we randomly perturb a tree in order to fit the residuals, we are in fact drawing a new tree from a posterior distribution. (Of course, this Bayesian connection is the motivation for BART's name.) Furthermore, Algorithm 8.3 can be viewed as a Markov chain Monte Carlo algorithm for fitting the BART model."", '### 8.2.1 Bagging  \nThe bootstrap, introduced in Chapter 5, is an extremely powerful idea. It is used in many situations in which it is hard or even impossible to directly compute the standard deviation of a quantity of interest. We see here that the bootstrap can be used in a completely different context, in order to improve statistical learning methods such as decision trees.  \nRecall that given a set of n independent observations Z 1         Z n , each with variance σ 2 , the variance of the mean ¯ Z of the observations is given by σ 2 glyph[triangleleft]n . In other words, averaging a set of observations reduces variance . Hence a natural way to reduce the variance and increase the test set accuracy of a statistical learning method is to take many training sets from the population, build a separate prediction model using each training set, and average the resulting predictions. In other words, we could calculate ˆ f 1 ( x )  ˆ f 2 ( x )         ˆ f B ( x ) using B separate training sets, and average them in order to obtain a single low-variance statistical learning model, given by  \nThe decision trees discussed in Section 8.1 suffer from high variance . This means that if we split the training data into two parts at random, and fit a decision tree to both halves, the results that we get could be quite different. In contrast, a procedure with low variance will yield similar results if applied repeatedly to distinct data sets; linear regression tends to have low variance, if the ratio of n to p is moderately large. Bootstrap aggregation , or bagging , is a general-purpose procedure for reducing the variance of a statistical learning method; we introduce it here because it is particularly useful and frequently used in the context of decision trees.  \n$$\\hat { f } _ { a v g } ( x ) = \\frac { 1 } { B } \\sum _ { b = 1 } ^ { B } \\hat { f } ^ { b } ( x ) . \\\\$$  \nOf course, this is not practical because we generally do not have access to multiple training sets. Instead, we can bootstrap, by taking repeated samples from the (single) training data set. In this approach we generate B different bootstrapped training data sets. We then train our method on the b th bootstrapped training set in order to get ˆ f ∗ b ( x ) , and finally average all the predictions, to obtain  \n$$\\hat { f } _ { b a g } ( x ) = \\frac { 1 } { B } \\sum _ { b = 1 } ^ { B } \\hat { f } ^ { * b } ( x ) .$$  \nensemble weak learners  \nbagging  \nFIGURE 8.8. Bagging and random forest results for the Heart data. The test error (black and orange) is shown as a function of B , the number of bootstrapped training sets used. Random forests were applied with m = √ p . The dashed line indicates the test error resulting from a single classification tree. The green and blue traces show the OOB error, which in this case is - by chance - considerably lower.  \n<!-- image -->\nThis is called bagging.  \nThus far, we have described the bagging procedure in the regression context, to predict a quantitative outcome Y . How can bagging be extended to a classification problem where Y is qualitative? In that situation, there are a few possible approaches, but the simplest is as follows. For a given test observation, we can record the class predicted by each of the B trees, and take a majority vote : the overall prediction is the most commonly occurring class among the B predictions.  \nWhile bagging can improve predictions for many regression methods, it is particularly useful for decision trees. To apply bagging to regression trees, we simply construct B regression trees using B bootstrapped training sets, and average the resulting predictions. These trees are grown deep, and are not pruned. Hence each individual tree has high variance, but low bias. Averaging these B trees reduces the variance. Bagging has been demonstrated to give impressive improvements in accuracy by combining together hundreds or even thousands of trees into a single procedure.  \nFigure 8.8 shows the results from bagging trees on the Heart data. The test error rate is shown as a function of B , the number of trees constructed using bootstrapped training data sets. We see that the bagging test error rate is slightly lower in this case than the test error rate obtained from a single tree. The number of trees B is not a critical parameter with bagging; using a very large value of B will not lead to overfitting. In practice we majority vote  \nuse a value of B sufficiently large that the error has settled down. Using B = 100 is sufficient to achieve good performance in this example.\nOut-of-Bag Error Estimation  \nIt turns out that there is a very straightforward way to estimate the test error of a bagged model, without the need to perform cross-validation or the validation set approach. Recall that the key to bagging is that trees are repeatedly fit to bootstrapped subsets of the observations. One can show that on average, each bagged tree makes use of around two-thirds of the observations. 3 The remaining one-third of the observations not used to fit a given bagged tree are referred to as the out-of-bag (OOB) observations. We can predict the response for the i th observation using each of the trees in which that observation was OOB. This will yield around Bglyph[triangleleft] 3 predictions for the i th observation. In order to obtain a single prediction for the i th observation, we can average these predicted responses (if regression is the goal) or can take a majority vote (if classification is the goal). This leads to a single OOB prediction for the i th observation. An OOB prediction can be obtained in this way for each of the n observations, from which the overall OOB MSE (for a regression problem) or classification error (for a classification problem) can be computed. The resulting OOB error is a valid estimate of the test error for the bagged model, since the response for each observation is predicted using only the trees that were not fit using that observation. Figure 8.8 displays the OOB error on the Heart data. It can be shown that with B sufficiently large, OOB error is virtually equivalent to leave-one-out cross-validation error. The OOB approach for estimating the test error is particularly convenient when performing bagging on large data sets for which cross-validation would be computationally onerous.\nVariable Importance Measures  \nAs we have discussed, bagging typically results in improved accuracy over prediction using a single tree. Unfortunately, however, it can be difficult to interpret the resulting model. Recall that one of the advantages of decision trees is the attractive and easily interpreted diagram that results, such as the one displayed in Figure 8.1. However, when we bag a large number of trees, it is no longer possible to represent the resulting statistical learning procedure using a single tree, and it is no longer clear which variables are most important to the procedure. Thus, bagging improves prediction accuracy at the expense of interpretability.  \nAlthough the collection of bagged trees is much more difficult to interpret than a single tree, one can obtain an overall summary of the importance of each predictor using the RSS (for bagging regression trees) or the Gini index (for bagging classification trees). In the case of bagging regression trees, we can record the total amount that the RSS (8.1) is decreased due to splits over a given predictor, averaged over all B trees. A large value indicates an important predictor. Similarly, in the context of bagging classification  \n3 This relates to Exercise 2 of Chapter 5.  \nout-of-bag  \nFIGURE 8.9. A variable importance plot for the Heart data. Variable importance is computed using the mean decrease in Gini index, and expressed relative to the maximum.  \n<!-- image -->  \ntrees, we can add up the total amount that the Gini index (8.6) is decreased by splits over a given predictor, averaged over all B trees.  \nA graphical representation of the variable importances in the Heart data is shown in Figure 8.9. We see the mean decrease in Gini index for each variable, relative to the largest. The variables with the largest mean decrease in Gini index are Thal , Ca , and ChestPain .']","El sobreajuste (overfitting) en el contexto de los árboles de decisión ocurre cuando un modelo se ajusta demasiado bien a los datos de entrenamiento, capturando no solo las relaciones subyacentes sino también el ruido y las fluctuaciones aleatorias. Esto resulta en un modelo que tiene un rendimiento excelente en el conjunto de entrenamiento pero que no generaliza bien a nuevos datos no vistos, es decir, tiene un rendimiento pobre en el conjunto de prueba.

Para evitar el sobreajuste en los árboles de decisión, se pueden utilizar varias técnicas:

1. **Poda de Árboles**: La poda implica recortar las ramas del árbol que tienen poca importancia o que no mejoran significativamente la precisión del modelo. Esto ayuda a simplificar el modelo y a mejorar su capacidad de generalización.

2. **Limitación de la Profundidad del Árbol**: Restringir la profundidad máxima del árbol puede prevenir que el modelo se vuelva demasiado complejo. Al limitar el número de niveles, se reduce la posibilidad de que el árbol capture el ruido en los datos de entrenamiento.

3. **Uso de Métodos de Ensamble**: Técnicas como el bagging y el boosting pueden ayudar a reducir el sobreajuste. El bagging, por ejemplo, promedia las predicciones de múltiples árboles de decisión, lo que reduce la varianza del modelo. El boosting, por otro lado, construye árboles de manera secuencial, enfocándose en los errores de los árboles anteriores, lo que puede mejorar la precisión sin sobreajustar.

4. **Regularización**: Aplicar técnicas de regularización que penalicen la complejidad del modelo puede ayudar a evitar el sobreajuste. Esto puede incluir la adición de términos de penalización en la función de costo que desincentiven la creación de árboles muy complejos.

Estas estrategias ayudan a crear modelos de árboles de decisión que son más robustos y que generalizan mejor a datos nuevos.","El sobreajuste en los árboles de decisión ocurre cuando el árbol es demasiado complejo y se ajusta a los datos de entrenamiento tan bien que captura el ruido en lugar de la señal subyacente. Esto resulta en un bajo rendimiento en datos nuevos. Para evitarlo, se puede podar el árbol, es decir, reducir su tamaño, o utilizar métodos de conjunto como bagging, random forests o boosting, que combinan múltiples árboles para mejorar la generalización.",0.631578947368421,,0.9999999999666667,1.0
What is the role of a kernel in a Support Vector Machine (SVM)?,"['### 9.3.2 The Support Vector Machine  \nThe support vector machine (SVM) is an extension of the support vector classifier that results from enlarging the feature space in a specific way, using kernels . We will now discuss this extension, the details of which are somewhat complex and beyond the scope of this book. However, the main idea is described in Section 9.3.1: we may want to enlarge our feature space in order to accommodate a non-linear boundary between the classes. The kernel approach that we describe here is simply an efficient computational approach for enacting this idea.  \nWe have not discussed exactly how the support vector classifier is computed because the details become somewhat technical. However, it turns out that the solution to the support vector classifier problem (9.12)-(9.15) involves only the inner products of the observations (as opposed to the observations themselves). The inner product of two r -vectors a and b is defined as 〈 a b 〉 = ∑ r i =1 a i b i . Thus the inner product of two observations x i , x i ′ is given by  \nIt can be shown that  \n$$\\langle x _ { i } , x _ { i ^ { \\prime } } \\rangle = \\sum _ { j = 1 } ^ { p } x _ { i j } x _ { i ^ { \\prime } j } .$$  \n- The linear support vector classifier can be represented as  \n$$f ( x ) = \\beta _ { 0 } + \\sum _ { i = 1 } ^ { n } \\alpha _ { i } \\langle x , x _ { i } \\rangle ,$$  \nsupport vector machine kernel  \nwhere there are n parameters α i  i = 1         n , one per training observation.  \n- To estimate the parameters α 1         α n and β 0 , all we need are the ( n 2 ) inner products 〈 x i  x i ′ 〉 between all pairs of training observations. (The notation ( n 2 ) means n ( n -1) glyph[triangleleft] 2 , and gives the number of pairs among a set of n items.)  \nNotice that in (9.18), in order to evaluate the function f ( x ) , we need to compute the inner product between the new point x and each of the training points x i . However, it turns out that α i is nonzero only for the support vectors in the solution-that is, if a training observation is not a support vector, then its α i equals zero. So if S is the collection of indices of these support points, we can rewrite any solution function of the form (9.18) as  \n$$f ( x ) = \\beta _ { 0 } + \\sum _ { i \\in \\mathcal { S } } \\alpha _ { i } \\langle x , x _ { i } \\rangle ,$$  \nwhich typically involves far fewer terms than in (9.18). 2  \nNow suppose that every time the inner product (9.17) appears in the representation (9.18), or in a calculation of the solution for the support vector classifier, we replace it with a generalization of the inner product of the form  \nTo summarize, in representing the linear classifier f ( x ) , and in computing its coefficients, all we need are inner products.  \n$$K ( x _ { i } , x _ { i ^ { \\prime } } ) ,$$  \nwhere K is some function that we will refer to as a kernel . A kernel is a function that quantifies the similarity of two observations. For instance, we could simply take  \n$$K ( x _ { i } , x _ { i ^ { \\prime } } ) = \\sum _ { j = 1 } ^ { p } x _ { i j } x _ { i ^ { \\prime } j } , \\\\ \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad$$  \nwhich would just give us back the support vector classifier. Equation 9.21 is known as a linear kernel because the support vector classifier is linear in the features; the linear kernel essentially quantifies the similarity of a pair of observations using Pearson (standard) correlation. But one could instead choose another form for (9.20). For instance, one could replace every instance of ∑ p j =1 x ij x i ′ j with the quantity  \n$$K ( x _ { i } , x _ { i ^ { \\prime } } ) = ( 1 + \\sum _ { j = 1 } ^ { p } x _ { i j } x _ { i j ^ { \\prime } j } ) ^ { d } .$$  \nThis is known as a polynomial kernel of degree d , where d is a positive integer. Using such a kernel with d &gt; 1 , instead of the standard linear kernel (9.21), in the support vector classifier algorithm leads to a much more flexible decision boundary. It essentially amounts to fitting a support vector  \n2 By expanding each of the inner products in (9.19), it is easy to see that f ( x ) is a linear function of the coordinates of x . Doing so also establishes the correspondence between the α i and the original parameters β j .  \nkernel polynomial kernel  \nFIGURE 9.9. Left: An SVM with a polynomial kernel of degree 3 is applied to the non-linear data from Figure 9.8, resulting in a far more appropriate decision rule. Right: An SVM with a radial kernel is applied. In this example, either kernel is capable of capturing the decision boundary.  \n<!-- image -->  \nclassifier in a higher-dimensional space involving polynomials of degree d , rather than in the original feature space. When the support vector classifier is combined with a non-linear kernel such as (9.22), the resulting classifier is known as a support vector machine. Note that in this case the (non-linear) function has the form  \n$$f ( x ) = \\beta _ { 0 } + \\sum _ { i \\in \\mathcal { S } } \\alpha _ { i } K ( x , x _ { i } ) . \\\\$$  \nThe left-hand panel of Figure 9.9 shows an example of an SVM with a polynomial kernel applied to the non-linear data from Figure 9.8. The fit is a substantial improvement over the linear support vector classifier. When d = 1 , then the SVM reduces to the support vector classifier seen earlier in this chapter.  \nThe polynomial kernel shown in (9.22) is one example of a possible non-linear kernel, but alternatives abound. Another popular choice is the radial kernel , which takes the form  \n$$K ( x _ { i } , x _ { i ^ { \\prime } } ) = \\exp ( - \\gamma \\sum _ { j = 1 } ^ { p } ( x _ { i j } - x _ { i ^ { \\prime } j } ) ^ { 2 } ) .$$  \nIn (9.24), γ is a positive constant. The right-hand panel of Figure 9.9 shows an example of an SVM with a radial kernel on this non-linear data; it also does a good job in separating the two classes.  \nHow does the radial kernel (9.24) actually work? If a given test observation x ∗ = ( x ∗ 1         x ∗ p ) T is far from a training observation x i in terms of Euclidean distance, then ∑ p j =1 ( x ∗ j -x ij ) 2 will be large, and so K ( x ∗  x i ) = exp( -γ ∑ p j =1 ( x ∗ j -x ij ) 2 ) will be tiny. This means that in (9.23), x i will play virtually no role in f ( x ∗ ) . Recall that the predicted class label for the test observation x ∗ is based on the sign of f ( x ∗ ) . In other words, training observations that are far from x ∗ will play essentially no role in the predicted class label for x ∗ . This means that the radial kernel has very local radial kernel  \nFIGURE 9.10. ROC curves for the Heart data training set. Left: The support vector classifier and LDA are compared. Right: The support vector classifier is compared to an SVM using a radial basis kernel with γ = 10 -3 , 10 -2 , and 10 -1 .  \n<!-- image -->  \nbehavior, in the sense that only nearby training observations have an effect on the class label of a test observation.  \nWhat is the advantage of using a kernel rather than simply enlarging the feature space using functions of the original features, as in (9.16)? One advantage is computational, and it amounts to the fact that using kernels, one need only compute K ( x i  x ′ i ) for all ( n 2 ) distinct pairs i i ′ . This can be done without explicitly working in the enlarged feature space. This is important because in many applications of SVMs, the enlarged feature space is so large that computations are intractable. For some kernels, such as the radial kernel (9.24), the feature space is implicit and infinite-dimensional, so we could never do the computations there anyway!', ""# 9 Support Vector Machines  \n<!-- image -->  \nIn this chapter, we discuss the support vector machine (SVM), an approach for classification that was developed in the computer science community in the 1990s and that has grown in popularity since then. SVMs have been shown to perform well in a variety of settings, and are often considered one of the best 'out of the box' classifiers.  \nPeople often loosely refer to the maximal margin classifier, the support vector classifier, and the support vector machine as 'support vector machines'. To avoid confusion, we will carefully distinguish between these three notions in this chapter.  \nThe support vector machine is a generalization of a simple and intuitive classifier called the maximal margin classifier , which we introduce in Section 9.1. Though it is elegant and simple, we will see that this classifier unfortunately cannot be applied to most data sets, since it requires that the classes be separable by a linear boundary. In Section 9.2, we introduce the support vector classifier , an extension of the maximal margin classifier that can be applied in a broader range of cases. Section 9.3 introduces the support vector machine , which is a further extension of the support vector classifier in order to accommodate non-linear class boundaries. Support vector machines are intended for the binary classification setting in which there are two classes; in Section 9.4 we discuss extensions of support vector machines to the case of more than two classes. In Section 9.5 we discuss the close connections between support vector machines and other statistical methods such as logistic regression.""]","In a Support Vector Machine (SVM), the role of a kernel is to enable the algorithm to operate in a high-dimensional, and potentially infinite-dimensional, feature space without explicitly computing the coordinates of the data in that space. This is achieved by using a kernel function, which quantifies the similarity between two observations.

The kernel function replaces the inner product of the observations in the feature space, allowing the SVM to find a non-linear decision boundary between classes. This is particularly useful when the classes are not linearly separable in the original feature space. By using a kernel, the SVM can efficiently compute the decision boundary in the transformed feature space, which can lead to a more flexible and accurate classification.

Some common types of kernels include:

1. **Linear Kernel**: This is the simplest form, which essentially uses the standard inner product of the observations.
2. **Polynomial Kernel**: This kernel allows for polynomial decision boundaries by raising the inner product to a specified degree.
3. **Radial Kernel (RBF)**: This kernel has a very local behavior, meaning that only nearby training observations have an effect on the class label of a test observation. It is particularly useful for capturing complex boundaries.

The use of kernels allows SVMs to handle complex and non-linear relationships in the data without the computational burden of explicitly working in a high-dimensional space.",A kernel in an SVM is a function that allows the algorithm to implicitly map the input data into a higher-dimensional feature space. This enables the SVM to find a non-linear decision boundary in the original feature space by constructing a linear separating hyperplane in the transformed space. Common kernels include polynomial and radial basis function (RBF) kernels.,1.0,,0.9999999999,1.0
What is the difference between a maximal margin classifier and a support vector classifier?,"[""# 9 Support Vector Machines  \n<!-- image -->  \nIn this chapter, we discuss the support vector machine (SVM), an approach for classification that was developed in the computer science community in the 1990s and that has grown in popularity since then. SVMs have been shown to perform well in a variety of settings, and are often considered one of the best 'out of the box' classifiers.  \nPeople often loosely refer to the maximal margin classifier, the support vector classifier, and the support vector machine as 'support vector machines'. To avoid confusion, we will carefully distinguish between these three notions in this chapter.  \nThe support vector machine is a generalization of a simple and intuitive classifier called the maximal margin classifier , which we introduce in Section 9.1. Though it is elegant and simple, we will see that this classifier unfortunately cannot be applied to most data sets, since it requires that the classes be separable by a linear boundary. In Section 9.2, we introduce the support vector classifier , an extension of the maximal margin classifier that can be applied in a broader range of cases. Section 9.3 introduces the support vector machine , which is a further extension of the support vector classifier in order to accommodate non-linear class boundaries. Support vector machines are intended for the binary classification setting in which there are two classes; in Section 9.4 we discuss extensions of support vector machines to the case of more than two classes. In Section 9.5 we discuss the close connections between support vector machines and other statistical methods such as logistic regression."", ""## 9.5 Relationship to Logistic Regression  \nWhen SVMs were first introduced in the mid-1990s, they made quite a splash in the statistical and machine learning communities. This was due in part to their good performance, good marketing, and also to the fact that the underlying approach seemed both novel and mysterious. The idea of finding a hyperplane that separates the data as well as possible, while allowing some violations to this separation, seemed distinctly different from classical approaches for classification, such as logistic regression and linear discriminant analysis. Moreover, the idea of using a kernel to expand the feature space in order to accommodate non-linear class boundaries appeared to be a unique and valuable characteristic.  \n$$x ) = \\beta _ { 0 } + \\beta _ { 1 } x _ { 1 } + \\dots + \\beta _ { p } x _ { p } \\text { as } \\\\ \\min _ { \\beta _ { 0 } , \\beta _ { 1 } , \\dots , \\beta _ { p } } \\left \\{ \\sum _ { i = 1 } ^ { n } \\max \\left [ 0 , 1 - y _ { i } f ( x _ { i } ) \\right ] + \\lambda \\sum _ { j = 1 } ^ { p } \\beta _ { j } ^ { 2 } \\right \\} , \\\\ \\intertext { s o r e } \\text { are } \\lambda \\text { is a nonnegative tuning parameter. } \\text { When } \\lambda \\text { is large then } \\beta _ { 1 } , \\dots , \\beta _ { p } \\\\ \\text {small } \\text { more volatile } \\text { to the margin are } \\text {tolerated } \\text { and } a \\text { low-variance }$$  \nHowever, since that time, deep connections between SVMs and other more classical statistical methods have emerged. It turns out that one can rewrite the criterion (9.12)-(9.15) for fitting the support vector classifier f ( X ) = β 0 + β 1 X 1 + · · · + β p X p as where λ is a nonnegative tuning parameter. When λ is large then β 1         β p are small, more violations to the margin are tolerated, and a low-variance but high-bias classifier will result. When λ is small then few violations to the margin will occur; this amounts to a high-variance but low-bias  \none-versusone one-versusall one-versusrest  \n<!-- image -->  \nclassifier. Thus, a small value of λ in (9.25) amounts to a small value of C in (9.15). Note that the λ ∑ p j =1 β 2 j term in (9.25) is the ridge penalty term from Section 6.2.1, and plays a similar role in controlling the bias-variance trade-off for the support vector classifier.  \nNow (9.25) takes the 'Loss + Penalty' form that we have seen repeatedly throughout this book:  \n$$\\min _ { \\beta _ { 0 } , \\beta _ { 1 } , \\dots , \\beta _ { p } } \\{ L ( X , y , \\beta ) + \\lambda P ( \\beta ) \\} \\, .$$  \nIn (9.26), L ( X  y  β ) is some loss function quantifying the extent to which the model, parametrized by β , fits the data ( X  y ) , and P ( β ) is a penalty function on the parameter vector β whose effect is controlled by a nonnegative tuning parameter λ . For instance, ridge regression and the lasso both take this form with  \n$$L ( X , y , \\beta ) \\equiv \\sum _ { i = 1 } ^ { n } \\left ( y _ { i } - \\beta _ { 0 } - \\sum _ { j = 1 } ^ { p } x _ { j } \\beta _ { j } \\right ) ^ { 2 } \\\\ P ( \\beta ) = \\sum _ { i = 1 } ^ { p } \\, , \\beta _ { i } ^ { 2 } \\text { for ridge regression and } P ( \\beta ) = \\sum _ { i = 1 } ^ { p }$$  \nand with P ( β ) = ∑ p j =1 β 2 j for ridge regression and P ( β ) = ∑ p j =1 ♣ β j ♣ for the lasso. In the case of (9.25) the loss function instead takes the form  \n$$L ( X , y , \\beta ) = \\sum _ { i = 1 } ^ { n } \\max \\left [ 0 , 1 - y _ { i } ( \\beta _ { 0 } + \\beta _ { 1 } x _ { i 1 } + \\cdots + \\beta _ { p } x _ { i p } ) \\right ] . \\\\$$  \nThis is known as hinge loss , and is depicted in Figure 9.12. However, it turns out that the hinge loss function is closely related to the loss function used in logistic regression, also shown in Figure 9.12.  \nWhen the support vector classifier and SVM were first introduced, it was thought that the tuning parameter C in (9.15) was an unimportant 'nuisance' parameter that could be set to some default value, like 1. However, the 'Loss + Penalty' formulation (9.25) for the support vector classifier indicates that this is not the case. The choice of tuning parameter is very important and determines the extent to which the model underfits or overfits the data, as illustrated, for example, in Figure 9.7.  \nAn interesting characteristic of the support vector classifier is that only support vectors play a role in the classifier obtained; observations on the correct side of the margin do not affect it. This is due to the fact that the loss function shown in Figure 9.12 is exactly zero for observations for which y i ( β 0 + β 1 x i 1 + · · · + β p x ip ) ≥ 1 ; these correspond to observations that are on the correct side of the margin. 3 In contrast, the loss function for logistic regression shown in Figure 9.12 is not exactly zero anywhere. But it is very small for observations that are far from the decision boundary. Due to the similarities between their loss functions, logistic regression and the support vector classifier often give very similar results. When the classes are well separated, SVMs tend to behave better than logistic regression; in more overlapping regimes, logistic regression is often preferred.  \n3 With this hinge-loss + penalty representation, the margin corresponds to the value one, and the width of the margin is determined by ∑ β 2 j .  \nhinge loss  \nFIGURE 9.12. The SVM and logistic regression loss functions are compared, as a function of y i ( β 0 + β 1 x i 1 + · · · + β p x ip ) . When y i ( β 0 + β 1 x i 1 + · · · + β p x ip ) is greater than 1, then the SVM loss is zero, since this corresponds to an observation that is on the correct side of the margin. Overall, the two loss functions have quite similar behavior.  \n<!-- image -->  \nWe have established that the support vector classifier is closely related to logistic regression and other preexisting statistical methods. Is the SVM unique in its use of kernels to enlarge the feature space to accommodate non-linear class boundaries? The answer to this question is 'no'. We could just as well perform logistic regression or many of the other classification methods seen in this book using non-linear kernels; this is closely related to some of the non-linear approaches seen in Chapter 7. However, for historical reasons, the use of non-linear kernels is much more widespread in the context of SVMs than in the context of logistic regression or other methods.  \nThough we have not addressed it here, there is in fact an extension of the SVM for regression (i.e. for a quantitative rather than a qualitative response), called support vector regression . In Chapter 3, we saw that least squares regression seeks coefficients β 0  β 1         β p such that the sum of squared residuals is as small as possible. (Recall from Chapter 3 that residuals are defined as y i -β 0 -β 1 x i 1 -· · · -β p x ip .) Support vector regression instead seeks coefficients that minimize a different type of loss, where only residuals larger in absolute value than some positive constant contribute to the loss function. This is an extension of the margin used in support vector classifiers to the regression setting.  \nsupport vector regression"", '### 9.1.5 The Non-separable Case  \nThe maximal margin classifier is a very natural way to perform classification, if a separating hyperplane exists . However, as we have hinted, in many cases no separating hyperplane exists, and so there is no maximal  \nFIGURE 9.4. There are two classes of observations, shown in blue and in purple. In this case, the two classes are not separable by a hyperplane, and so the maximal margin classifier cannot be used.  \n<!-- image -->  \nmargin classifier. In this case, the optimization problem (9.9)-(9.11) has no solution with M &gt; 0 . An example is shown in Figure 9.4. In this case, we cannot exactly separate the two classes. However, as we will see in the next section, we can extend the concept of a separating hyperplane in order to develop a hyperplane that almost separates the classes, using a so-called soft margin . The generalization of the maximal margin classifier to the non-separable case is known as the support vector classifier .', '## 9.2 Support Vector Classifiers  \n### 9.2.1 Overview of the Support Vector Classifier  \nIn Figure 9.4, we see that observations that belong to two classes are not necessarily separable by a hyperplane. In fact, even if a separating hyperplane does exist, then there are instances in which a classifier based on a separating hyperplane might not be desirable. A classifier based on a separating hyperplane will necessarily perfectly classify all of the training observations; this can lead to sensitivity to individual observations. An example is shown in Figure 9.5. The addition of a single observation in the right-hand panel of Figure 9.5 leads to a dramatic change in the maximal margin hyperplane. The resulting maximal margin hyperplane is not satisfactory-for one thing, it has only a tiny margin. This is problematic because as discussed previously, the distance of an observation from the hyperplane can be seen as a measure of our confidence that the observation was correctly classified. Moreover, the fact that the maximal margin hyperplane is extremely sensitive to a change in a single observation suggests that it may have overfit the training data.  \nIn this case, we might be willing to consider a classifier based on a hyperplane that does not perfectly separate the two classes, in the interest of  \nFIGURE 9.5. Left: Two classes of observations are shown in blue and in purple, along with the maximal margin hyperplane. Right: An additional blue observation has been added, leading to a dramatic shift in the maximal margin hyperplane shown as a solid line. The dashed line indicates the maximal margin hyperplane that was obtained in the absence of this additional point.  \n<!-- image -->  \n- Greater robustness to individual observations, and\n- Better classification of most of the training observations.  \nThat is, it could be worthwhile to misclassify a few training observations in order to do a better job in classifying the remaining observations.  \nAn observation can be not only on the wrong side of the margin, but also on the wrong side of the hyperplane. In fact, when there is no separating hyperplane, such a situation is inevitable. Observations on the wrong side of the hyperplane correspond to training observations that are misclassified by the support vector classifier. The right-hand panel of Figure 9.6 illustrates such a scenario.  \nThe support vector classifier , sometimes called a soft margin classifier , does exactly this. Rather than seeking the largest possible margin so that every observation is not only on the correct side of the hyperplane but also on the correct side of the margin, we instead allow some observations to be on the incorrect side of the margin, or even the incorrect side of the hyperplane. (The margin is soft because it can be violated by some of the training observations.) An example is shown in the left-hand panel of Figure 9.6. Most of the observations are on the correct side of the margin. However, a small subset of the observations are on the wrong side of the margin.']","The maximal margin classifier and the support vector classifier are both approaches used for classification, but they differ in their applicability and flexibility:

1. **Maximal Margin Classifier**:
   - It is a simple and intuitive classifier that seeks to find the hyperplane that maximizes the margin between two classes.
   - It requires that the classes be perfectly separable by a linear boundary, meaning there must exist a hyperplane that can completely separate the two classes without any errors.
   - Due to this requirement, the maximal margin classifier cannot be applied to most real-world datasets where classes are not perfectly separable.

2. **Support Vector Classifier**:
   - It is an extension of the maximal margin classifier designed to handle cases where the classes are not perfectly separable.
   - The support vector classifier introduces the concept of a ""soft margin,"" allowing some observations to be on the incorrect side of the margin or even the hyperplane.
   - This approach is more flexible and robust, as it can tolerate some misclassifications in the interest of better overall classification performance on the training data.

In summary, while the maximal margin classifier is limited to perfectly separable data, the support vector classifier can be applied to a broader range of datasets by allowing for some margin violations.","A maximal margin classifier requires that the data be perfectly separable by a hyperplane, and it finds the hyperplane that maximizes the margin (the distance to the nearest training points). A support vector classifier is an extension that allows for some misclassifications by introducing a 'soft margin', which makes it applicable to data that is not linearly separable.",0.9411764705882353,,0.999999999975,1.0
What are support vectors in the context of SVMs?,"['### 9.3.2 The Support Vector Machine  \nThe support vector machine (SVM) is an extension of the support vector classifier that results from enlarging the feature space in a specific way, using kernels . We will now discuss this extension, the details of which are somewhat complex and beyond the scope of this book. However, the main idea is described in Section 9.3.1: we may want to enlarge our feature space in order to accommodate a non-linear boundary between the classes. The kernel approach that we describe here is simply an efficient computational approach for enacting this idea.  \nWe have not discussed exactly how the support vector classifier is computed because the details become somewhat technical. However, it turns out that the solution to the support vector classifier problem (9.12)-(9.15) involves only the inner products of the observations (as opposed to the observations themselves). The inner product of two r -vectors a and b is defined as 〈 a b 〉 = ∑ r i =1 a i b i . Thus the inner product of two observations x i , x i ′ is given by  \nIt can be shown that  \n$$\\langle x _ { i } , x _ { i ^ { \\prime } } \\rangle = \\sum _ { j = 1 } ^ { p } x _ { i j } x _ { i ^ { \\prime } j } .$$  \n- The linear support vector classifier can be represented as  \n$$f ( x ) = \\beta _ { 0 } + \\sum _ { i = 1 } ^ { n } \\alpha _ { i } \\langle x , x _ { i } \\rangle ,$$  \nsupport vector machine kernel  \nwhere there are n parameters α i  i = 1         n , one per training observation.  \n- To estimate the parameters α 1         α n and β 0 , all we need are the ( n 2 ) inner products 〈 x i  x i ′ 〉 between all pairs of training observations. (The notation ( n 2 ) means n ( n -1) glyph[triangleleft] 2 , and gives the number of pairs among a set of n items.)  \nNotice that in (9.18), in order to evaluate the function f ( x ) , we need to compute the inner product between the new point x and each of the training points x i . However, it turns out that α i is nonzero only for the support vectors in the solution-that is, if a training observation is not a support vector, then its α i equals zero. So if S is the collection of indices of these support points, we can rewrite any solution function of the form (9.18) as  \n$$f ( x ) = \\beta _ { 0 } + \\sum _ { i \\in \\mathcal { S } } \\alpha _ { i } \\langle x , x _ { i } \\rangle ,$$  \nwhich typically involves far fewer terms than in (9.18). 2  \nNow suppose that every time the inner product (9.17) appears in the representation (9.18), or in a calculation of the solution for the support vector classifier, we replace it with a generalization of the inner product of the form  \nTo summarize, in representing the linear classifier f ( x ) , and in computing its coefficients, all we need are inner products.  \n$$K ( x _ { i } , x _ { i ^ { \\prime } } ) ,$$  \nwhere K is some function that we will refer to as a kernel . A kernel is a function that quantifies the similarity of two observations. For instance, we could simply take  \n$$K ( x _ { i } , x _ { i ^ { \\prime } } ) = \\sum _ { j = 1 } ^ { p } x _ { i j } x _ { i ^ { \\prime } j } , \\\\ \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad$$  \nwhich would just give us back the support vector classifier. Equation 9.21 is known as a linear kernel because the support vector classifier is linear in the features; the linear kernel essentially quantifies the similarity of a pair of observations using Pearson (standard) correlation. But one could instead choose another form for (9.20). For instance, one could replace every instance of ∑ p j =1 x ij x i ′ j with the quantity  \n$$K ( x _ { i } , x _ { i ^ { \\prime } } ) = ( 1 + \\sum _ { j = 1 } ^ { p } x _ { i j } x _ { i j ^ { \\prime } j } ) ^ { d } .$$  \nThis is known as a polynomial kernel of degree d , where d is a positive integer. Using such a kernel with d &gt; 1 , instead of the standard linear kernel (9.21), in the support vector classifier algorithm leads to a much more flexible decision boundary. It essentially amounts to fitting a support vector  \n2 By expanding each of the inner products in (9.19), it is easy to see that f ( x ) is a linear function of the coordinates of x . Doing so also establishes the correspondence between the α i and the original parameters β j .  \nkernel polynomial kernel  \nFIGURE 9.9. Left: An SVM with a polynomial kernel of degree 3 is applied to the non-linear data from Figure 9.8, resulting in a far more appropriate decision rule. Right: An SVM with a radial kernel is applied. In this example, either kernel is capable of capturing the decision boundary.  \n<!-- image -->  \nclassifier in a higher-dimensional space involving polynomials of degree d , rather than in the original feature space. When the support vector classifier is combined with a non-linear kernel such as (9.22), the resulting classifier is known as a support vector machine. Note that in this case the (non-linear) function has the form  \n$$f ( x ) = \\beta _ { 0 } + \\sum _ { i \\in \\mathcal { S } } \\alpha _ { i } K ( x , x _ { i } ) . \\\\$$  \nThe left-hand panel of Figure 9.9 shows an example of an SVM with a polynomial kernel applied to the non-linear data from Figure 9.8. The fit is a substantial improvement over the linear support vector classifier. When d = 1 , then the SVM reduces to the support vector classifier seen earlier in this chapter.  \nThe polynomial kernel shown in (9.22) is one example of a possible non-linear kernel, but alternatives abound. Another popular choice is the radial kernel , which takes the form  \n$$K ( x _ { i } , x _ { i ^ { \\prime } } ) = \\exp ( - \\gamma \\sum _ { j = 1 } ^ { p } ( x _ { i j } - x _ { i ^ { \\prime } j } ) ^ { 2 } ) .$$  \nIn (9.24), γ is a positive constant. The right-hand panel of Figure 9.9 shows an example of an SVM with a radial kernel on this non-linear data; it also does a good job in separating the two classes.  \nHow does the radial kernel (9.24) actually work? If a given test observation x ∗ = ( x ∗ 1         x ∗ p ) T is far from a training observation x i in terms of Euclidean distance, then ∑ p j =1 ( x ∗ j -x ij ) 2 will be large, and so K ( x ∗  x i ) = exp( -γ ∑ p j =1 ( x ∗ j -x ij ) 2 ) will be tiny. This means that in (9.23), x i will play virtually no role in f ( x ∗ ) . Recall that the predicted class label for the test observation x ∗ is based on the sign of f ( x ∗ ) . In other words, training observations that are far from x ∗ will play essentially no role in the predicted class label for x ∗ . This means that the radial kernel has very local radial kernel  \nFIGURE 9.10. ROC curves for the Heart data training set. Left: The support vector classifier and LDA are compared. Right: The support vector classifier is compared to an SVM using a radial basis kernel with γ = 10 -3 , 10 -2 , and 10 -1 .  \n<!-- image -->  \nbehavior, in the sense that only nearby training observations have an effect on the class label of a test observation.  \nWhat is the advantage of using a kernel rather than simply enlarging the feature space using functions of the original features, as in (9.16)? One advantage is computational, and it amounts to the fact that using kernels, one need only compute K ( x i  x ′ i ) for all ( n 2 ) distinct pairs i i ′ . This can be done without explicitly working in the enlarged feature space. This is important because in many applications of SVMs, the enlarged feature space is so large that computations are intractable. For some kernels, such as the radial kernel (9.24), the feature space is implicit and infinite-dimensional, so we could never do the computations there anyway!', ""# 9 Support Vector Machines  \n<!-- image -->  \nIn this chapter, we discuss the support vector machine (SVM), an approach for classification that was developed in the computer science community in the 1990s and that has grown in popularity since then. SVMs have been shown to perform well in a variety of settings, and are often considered one of the best 'out of the box' classifiers.  \nPeople often loosely refer to the maximal margin classifier, the support vector classifier, and the support vector machine as 'support vector machines'. To avoid confusion, we will carefully distinguish between these three notions in this chapter.  \nThe support vector machine is a generalization of a simple and intuitive classifier called the maximal margin classifier , which we introduce in Section 9.1. Though it is elegant and simple, we will see that this classifier unfortunately cannot be applied to most data sets, since it requires that the classes be separable by a linear boundary. In Section 9.2, we introduce the support vector classifier , an extension of the maximal margin classifier that can be applied in a broader range of cases. Section 9.3 introduces the support vector machine , which is a further extension of the support vector classifier in order to accommodate non-linear class boundaries. Support vector machines are intended for the binary classification setting in which there are two classes; in Section 9.4 we discuss extensions of support vector machines to the case of more than two classes. In Section 9.5 we discuss the close connections between support vector machines and other statistical methods such as logistic regression."", ""### 9.1.3 The Maximal Margin Classifier  \nIn general, if our data can be perfectly separated using a hyperplane, then there will in fact exist an infinite number of such hyperplanes. This is because a given separating hyperplane can usually be shifted a tiny bit up or down, or rotated, without coming into contact with any of the observations. Three possible separating hyperplanes are shown in the left-hand panel of Figure 9.2. In order to construct a classifier based upon a separating hyperplane, we must have a reasonable way to decide which of the infinite possible separating hyperplanes to use.  \nA natural choice is the maximal margin hyperplane (also known as the optimal separating hyperplane ), which is the separating hyperplane that is farthest from the training observations. That is, we can compute the (perpendicular) distance from each training observation to a given separating hyperplane; the smallest such distance is the minimal distance from the observations to the hyperplane, and is known as the margin . The maximal margin hyperplane is the separating hyperplane for which the margin is largest-that is, it is the hyperplane that has the farthest minimum distance to the training observations. We can then classify a test observation based on which side of the maximal margin hyperplane it lies. This is known  \nmaximal margin hyperplane optimal separating hyperplane margin  \nFIGURE 9.3. There are two classes of observations, shown in blue and in purple. The maximal margin hyperplane is shown as a solid line. The margin is the distance from the solid line to either of the dashed lines. The two blue points and the purple point that lie on the dashed lines are the support vectors, and the distance from those points to the hyperplane is indicated by arrows. The purple and blue grid indicates the decision rule made by a classifier based on this separating hyperplane.  \n<!-- image -->  \nas the maximal margin classifier . We hope that a classifier that has a large margin on the training data will also have a large margin on the test data, and hence will classify the test observations correctly. Although the maximal margin classifier is often successful, it can also lead to overfitting when p is large.  \nFigure 9.3 shows the maximal margin hyperplane on the data set of Figure 9.2. Comparing the right-hand panel of Figure 9.2 to Figure 9.3, we see that the maximal margin hyperplane shown in Figure 9.3 does indeed result in a greater minimal distance between the observations and the separating hyperplane-that is, a larger margin. In a sense, the maximal margin hyperplane represents the mid-line of the widest 'slab' that we can insert between the two classes.  \nIf β 0  β 1         β p are the coefficients of the maximal margin hyperplane, then the maximal margin classifier classifies the test observation x ∗ based on the sign of f ( x ∗ ) = β 0 + β 1 x ∗ 1 + β 2 x ∗ 2 + · · · + β p x ∗ p .  \nExamining Figure 9.3, we see that three training observations are equidistant from the maximal margin hyperplane and lie along the dashed lines indicating the width of the margin. These three observations are known as support vectors , since they are vectors in p -dimensional space (in Figure 9.3, p = 2 ) and they 'support' the maximal margin hyperplane in the sense that if these points were moved slightly then the maximal margin hyperplane would move as well. Interestingly, the maximal margin hyperplane depends directly on the support vectors, but not on the other observations: a movement to any of the other observations would not affect the separating hyperplane, provided that the observation's movement does not cause it to maximal margin classifier  \nsupport vector  \ncross the boundary set by the margin. The fact that the maximal margin hyperplane depends directly on only a small subset of the observations is an important property that will arise later in this chapter when we discuss the support vector classifier and support vector machines.""]","In the context of Support Vector Machines (SVMs), support vectors are the data points that lie closest to the decision boundary (or hyperplane). These points are crucial because they are the ones that define the position and orientation of the hyperplane. The support vectors are the observations that are equidistant from the maximal margin hyperplane and lie along the dashed lines indicating the width of the margin. They ""support"" the maximal margin hyperplane in the sense that if these points were moved slightly, the maximal margin hyperplane would move as well. Importantly, the maximal margin hyperplane depends directly on these support vectors, but not on the other observations. This property is significant because it means that the SVM model is robust to changes in the data that do not affect the support vectors.","Support vectors are the training data points that lie on or within the margin of the separating hyperplane. These are the critical points that define the hyperplane, and if they were to be moved, the hyperplane would also move.",0.875,,0.8333333332916666,1.0
¿Cuáles son las dos estrategias más comunes para extender SVMs a problemas de clasificación multiclase?,"['## 9.4 SVMs with More than Two Classes  \nSo far, our discussion has been limited to the case of binary classification: that is, classification in the two-class setting. How can we extend SVMs to the more general case where we have some arbitrary number of classes? It turns out that the concept of separating hyperplanes upon which SVMs are based does not lend itself naturally to more than two classes. Though a number of proposals for extending SVMs to the K -class case have been made, the two most popular are the one-versus-one and one-versus-all approaches. We briefly discuss those two approaches here.', ""# 9 Support Vector Machines  \n<!-- image -->  \nIn this chapter, we discuss the support vector machine (SVM), an approach for classification that was developed in the computer science community in the 1990s and that has grown in popularity since then. SVMs have been shown to perform well in a variety of settings, and are often considered one of the best 'out of the box' classifiers.  \nPeople often loosely refer to the maximal margin classifier, the support vector classifier, and the support vector machine as 'support vector machines'. To avoid confusion, we will carefully distinguish between these three notions in this chapter.  \nThe support vector machine is a generalization of a simple and intuitive classifier called the maximal margin classifier , which we introduce in Section 9.1. Though it is elegant and simple, we will see that this classifier unfortunately cannot be applied to most data sets, since it requires that the classes be separable by a linear boundary. In Section 9.2, we introduce the support vector classifier , an extension of the maximal margin classifier that can be applied in a broader range of cases. Section 9.3 introduces the support vector machine , which is a further extension of the support vector classifier in order to accommodate non-linear class boundaries. Support vector machines are intended for the binary classification setting in which there are two classes; in Section 9.4 we discuss extensions of support vector machines to the case of more than two classes. In Section 9.5 we discuss the close connections between support vector machines and other statistical methods such as logistic regression."", '### 9.3.2 The Support Vector Machine  \nThe support vector machine (SVM) is an extension of the support vector classifier that results from enlarging the feature space in a specific way, using kernels . We will now discuss this extension, the details of which are somewhat complex and beyond the scope of this book. However, the main idea is described in Section 9.3.1: we may want to enlarge our feature space in order to accommodate a non-linear boundary between the classes. The kernel approach that we describe here is simply an efficient computational approach for enacting this idea.  \nWe have not discussed exactly how the support vector classifier is computed because the details become somewhat technical. However, it turns out that the solution to the support vector classifier problem (9.12)-(9.15) involves only the inner products of the observations (as opposed to the observations themselves). The inner product of two r -vectors a and b is defined as 〈 a b 〉 = ∑ r i =1 a i b i . Thus the inner product of two observations x i , x i ′ is given by  \nIt can be shown that  \n$$\\langle x _ { i } , x _ { i ^ { \\prime } } \\rangle = \\sum _ { j = 1 } ^ { p } x _ { i j } x _ { i ^ { \\prime } j } .$$  \n- The linear support vector classifier can be represented as  \n$$f ( x ) = \\beta _ { 0 } + \\sum _ { i = 1 } ^ { n } \\alpha _ { i } \\langle x , x _ { i } \\rangle ,$$  \nsupport vector machine kernel  \nwhere there are n parameters α i  i = 1         n , one per training observation.  \n- To estimate the parameters α 1         α n and β 0 , all we need are the ( n 2 ) inner products 〈 x i  x i ′ 〉 between all pairs of training observations. (The notation ( n 2 ) means n ( n -1) glyph[triangleleft] 2 , and gives the number of pairs among a set of n items.)  \nNotice that in (9.18), in order to evaluate the function f ( x ) , we need to compute the inner product between the new point x and each of the training points x i . However, it turns out that α i is nonzero only for the support vectors in the solution-that is, if a training observation is not a support vector, then its α i equals zero. So if S is the collection of indices of these support points, we can rewrite any solution function of the form (9.18) as  \n$$f ( x ) = \\beta _ { 0 } + \\sum _ { i \\in \\mathcal { S } } \\alpha _ { i } \\langle x , x _ { i } \\rangle ,$$  \nwhich typically involves far fewer terms than in (9.18). 2  \nNow suppose that every time the inner product (9.17) appears in the representation (9.18), or in a calculation of the solution for the support vector classifier, we replace it with a generalization of the inner product of the form  \nTo summarize, in representing the linear classifier f ( x ) , and in computing its coefficients, all we need are inner products.  \n$$K ( x _ { i } , x _ { i ^ { \\prime } } ) ,$$  \nwhere K is some function that we will refer to as a kernel . A kernel is a function that quantifies the similarity of two observations. For instance, we could simply take  \n$$K ( x _ { i } , x _ { i ^ { \\prime } } ) = \\sum _ { j = 1 } ^ { p } x _ { i j } x _ { i ^ { \\prime } j } , \\\\ \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad \\vdots \\quad$$  \nwhich would just give us back the support vector classifier. Equation 9.21 is known as a linear kernel because the support vector classifier is linear in the features; the linear kernel essentially quantifies the similarity of a pair of observations using Pearson (standard) correlation. But one could instead choose another form for (9.20). For instance, one could replace every instance of ∑ p j =1 x ij x i ′ j with the quantity  \n$$K ( x _ { i } , x _ { i ^ { \\prime } } ) = ( 1 + \\sum _ { j = 1 } ^ { p } x _ { i j } x _ { i j ^ { \\prime } j } ) ^ { d } .$$  \nThis is known as a polynomial kernel of degree d , where d is a positive integer. Using such a kernel with d &gt; 1 , instead of the standard linear kernel (9.21), in the support vector classifier algorithm leads to a much more flexible decision boundary. It essentially amounts to fitting a support vector  \n2 By expanding each of the inner products in (9.19), it is easy to see that f ( x ) is a linear function of the coordinates of x . Doing so also establishes the correspondence between the α i and the original parameters β j .  \nkernel polynomial kernel  \nFIGURE 9.9. Left: An SVM with a polynomial kernel of degree 3 is applied to the non-linear data from Figure 9.8, resulting in a far more appropriate decision rule. Right: An SVM with a radial kernel is applied. In this example, either kernel is capable of capturing the decision boundary.  \n<!-- image -->  \nclassifier in a higher-dimensional space involving polynomials of degree d , rather than in the original feature space. When the support vector classifier is combined with a non-linear kernel such as (9.22), the resulting classifier is known as a support vector machine. Note that in this case the (non-linear) function has the form  \n$$f ( x ) = \\beta _ { 0 } + \\sum _ { i \\in \\mathcal { S } } \\alpha _ { i } K ( x , x _ { i } ) . \\\\$$  \nThe left-hand panel of Figure 9.9 shows an example of an SVM with a polynomial kernel applied to the non-linear data from Figure 9.8. The fit is a substantial improvement over the linear support vector classifier. When d = 1 , then the SVM reduces to the support vector classifier seen earlier in this chapter.  \nThe polynomial kernel shown in (9.22) is one example of a possible non-linear kernel, but alternatives abound. Another popular choice is the radial kernel , which takes the form  \n$$K ( x _ { i } , x _ { i ^ { \\prime } } ) = \\exp ( - \\gamma \\sum _ { j = 1 } ^ { p } ( x _ { i j } - x _ { i ^ { \\prime } j } ) ^ { 2 } ) .$$  \nIn (9.24), γ is a positive constant. The right-hand panel of Figure 9.9 shows an example of an SVM with a radial kernel on this non-linear data; it also does a good job in separating the two classes.  \nHow does the radial kernel (9.24) actually work? If a given test observation x ∗ = ( x ∗ 1         x ∗ p ) T is far from a training observation x i in terms of Euclidean distance, then ∑ p j =1 ( x ∗ j -x ij ) 2 will be large, and so K ( x ∗  x i ) = exp( -γ ∑ p j =1 ( x ∗ j -x ij ) 2 ) will be tiny. This means that in (9.23), x i will play virtually no role in f ( x ∗ ) . Recall that the predicted class label for the test observation x ∗ is based on the sign of f ( x ∗ ) . In other words, training observations that are far from x ∗ will play essentially no role in the predicted class label for x ∗ . This means that the radial kernel has very local radial kernel  \nFIGURE 9.10. ROC curves for the Heart data training set. Left: The support vector classifier and LDA are compared. Right: The support vector classifier is compared to an SVM using a radial basis kernel with γ = 10 -3 , 10 -2 , and 10 -1 .  \n<!-- image -->  \nbehavior, in the sense that only nearby training observations have an effect on the class label of a test observation.  \nWhat is the advantage of using a kernel rather than simply enlarging the feature space using functions of the original features, as in (9.16)? One advantage is computational, and it amounts to the fact that using kernels, one need only compute K ( x i  x ′ i ) for all ( n 2 ) distinct pairs i i ′ . This can be done without explicitly working in the enlarged feature space. This is important because in many applications of SVMs, the enlarged feature space is so large that computations are intractable. For some kernels, such as the radial kernel (9.24), the feature space is implicit and infinite-dimensional, so we could never do the computations there anyway!']","Las dos estrategias más comunes para extender las SVMs a problemas de clasificación multiclase son los enfoques de ""uno contra uno"" (one-versus-one) y ""uno contra todos"" (one-versus-all).","Las dos estrategias más comunes para extender las SVMs a problemas de clasificación multiclase son 'uno contra uno' (one-vs-one) y 'uno contra todos' (one-vs-all). El enfoque de uno contra uno construye un clasificador para cada par de clases, y el de uno contra todos construye un clasificador por cada clase, que la distingue de todas las demás clases combinadas.",0.3333333333333333,,0.8333333332916666,0.5
What are the advantages of a deep learning model compared to a simple neural network?,"[""## 10.6 When to Use Deep Learning  \nThe performance of deep learning in this chapter has been rather impressive. It nailed the digit classification problem, and deep CNNs have really revolutionized image classification. We see daily reports of new success stories for deep learning. Many of these are related to image classification tasks, such as machine diagnosis of mammograms or digital X-ray images, ophthalmology eye scans, annotations of MRI scans, and so on. Likewise there are numerous successes of RNNs in speech and language translation, forecasting, and document modeling. The question that then begs an answer is: should we discard all our older tools, and use deep learning on every problem with data? To address this question, we revisit our Hitters dataset from Chapter 6.  \nThis is a regression problem, where the goal is to predict the Salary of a baseball player in 1987 using his performance statistics from 1986. After removing players with missing responses, we are left with 263 players and 19 variables. We randomly split the data into a training set of 176 players (two thirds), and a test set of 87 players (one third). We used three methods for fitting a regression model to these data.  \n- A linear model was used to fit the training data, and make predictions on the test data. The model has 20 parameters.\n- The same linear model was fit with lasso regularization. The tuning parameter was selected by 10-fold cross-validation on the training data. It selected a model with 12 variables having nonzero coefficients.\n- A neural network with one hidden layer consisting of 64 ReLU units was fit to the data. This model has 1,345 parameters. 20  \n20 The model was fit by stochastic gradient descent with a batch size of 32 for 1,000 epochs, and 10% dropout regularization. The test error performance flattened out and started to slowly increase after 1,000 epochs. These fitting details are discussed in Section 10.7.  \nbidirectional  \nSeq2Seq  \nTABLE 10.2. Prediction results on the Hitters test data for linear models fit by ordinary least squares and lasso, compared to a neural network fit by stochastic gradient descent with dropout regularization.  \n| Model             |   # Parameters |   Mean Abs. Error |   Test Set R 2 |\n|-------------------|----------------|-------------------|----------------|\n| Linear Regression |             20 |             254.7 |           0.56 |\n| Lasso             |             12 |             252.3 |           0.51 |\n| Neural Network    |           1345 |             257.4 |           0.54 |  \nTABLE 10.3. Least squares coefficient estimates associated with the regression of Salary on four variables chosen by lasso on the Hitters data set. This model achieved the best performance on the test data, with a mean absolute error of 224.8. The results reported here were obtained from a regression on the test data, which was not used in fitting the lasso model.  \n|           |   Coefficient |   Std. error |   t -statistic | p -value                      |\n|-----------|---------------|--------------|----------------|-------------------------------|\n| Intercept |      -226.67  |        86.26 |          -2.63 | 0.0103                        |\n| Hits      |         3.06  |         1.02 |           3    | 0.0036                        |\n| Walks     |         0.181 |         2.04 |           0.09 | 0.9294                        |\n| CRuns     |         0.859 |         0.12 |           7.09 | < 0   0001 |\n| PutOuts   |         0.465 |         0.13 |           3.6  | 0.0005                        |  \nTable 10.2 compares the results. We see similar performance for all three models. We report the mean absolute error on the test data, as well as the test R 2 for each method, which are all respectable (see Exercise 5). We spent a fair bit of time fiddling with the configuration parameters of the neural network to achieve these results. It is possible that if we were to spend more time, and got the form and amount of regularization just right, that we might be able to match or even outperform linear regression and the lasso. But with great ease we obtained linear models that work well. Linear models are much easier to present and understand than the neural network, which is essentially a black box. The lasso selected 12 of the 19 variables in making its prediction. So in cases like this we are much better off following the Occam's razor principle: when faced with several methods that give roughly equivalent performance, pick the simplest.  \nWe have a number of very powerful tools at our disposal, including neural networks, random forests and boosting, support vector machines and generalized additive models, to name a few. And then we have linear models, and simple variants of these. When faced with new data modeling and prediction problems, it's tempting to always go for the trendy new methods. Often they give extremely impressive results, especially when the datasets are very large and can support the fitting of high-dimensional nonlinear models. However, if we can produce models with the simpler tools that  \nAfter a bit more exploration with the lasso model, we identified an even simpler model with four variables. We then refit the linear model with these four variables to the training data (the so-called relaxed lasso ), and achieved a test mean absolute error of 224.8, the overall winner! It is tempting to present the summary table from this fit, so we can see coefficients and pvalues; however, since the model was selected on the training data, there would be selection bias . Instead, we refit the model on the test data, which was not used in the selection. Table 10.3 shows the results.  \nOccam's razor  \nperform as well, they are likely to be easier to fit and understand, and potentially less fragile than the more complex approaches. Wherever possible, it makes sense to try the simpler models as well, and then make a choice based on the performance/complexity tradeoff.  \nTypically we expect deep learning to be an attractive choice when the sample size of the training set is extremely large, and when interpretability of the model is not a high priority."", '# 10 Deep Learning  \n<!-- image -->  \nThis chapter covers the important topic of deep learning . At the time of writing (2020), deep learning is a very active area of research in the machine learning and artificial intelligence communities. The cornerstone of deep learning is the neural network .  \nAll the while, though, a core group of neural-network enthusiasts were pushing their technology harder on ever-larger computing architectures and data sets. Neural networks resurfaced after 2010 with the new name deep learning , with new architectures, additional bells and whistles, and a string of success stories on some niche problems such as image and video classification, speech and text modeling. Many in the field believe that the major reason for these successes is the availability of ever-larger training datasets, made possible by the wide-scale use of digitization in science and industry.  \nNeural networks rose to fame in the late 1980s. There was a lot of excitement and a certain amount of hype associated with this approach, and they were the impetus for the popular Neural Information Processing Systems meetings (NeurIPS, formerly NIPS) held every year, typically in exotic places like ski resorts. This was followed by a synthesis stage, where the properties of neural networks were analyzed by machine learners, mathematicians and statisticians; algorithms were improved, and the methodology stabilized. Then along came SVMs, boosting, and random forests, and neural networks fell somewhat from favor. Part of the reason was that neural networks required a lot of tinkering, while the new methods were more automatic. Also, on many problems the new methods outperformed poorly-trained neural networks. This was the status quo for the first decade in the new millennium.  \nIn this chapter we discuss the basics of neural networks and deep learning, and then go into some of the specializations for specific problems, such as convolutional neural networks (CNNs) for image classification, and recurrent neural networks (RNNs) for time series and other sequences. We deep learning  \nneural network  \nFIGURE 10.1. Neural network with a single hidden layer. The hidden layer computes activations A k = h k ( X ) that are nonlinear transformations of linear combinations of the inputs X 1  X 2         X p . Hence these A k are not directly observed. The functions h k ( · ) are not fixed in advance, but are learned during the training of the network. The output layer is a linear model that uses these activations A k as inputs, resulting in a function f ( X ) .  \n<!-- image -->  \nwill also demonstrate these models using the Python torch package, along with a number of helper packages.  \nThe material in this chapter is slightly more challenging than elsewhere in this book.', ""## 10.8 Interpolation and Double Descent  \nThroughout this book, we have repeatedly discussed the bias-variance tradeoff, first presented in Section 2.2.2. This trade-off indicates that statistical learning methods tend to perform the best, in terms of test-set error, for an intermediate level of model complexity. In particular, if we plot 'flexibility' on the x -axis and error on the y -axis, then we generally expect to see that test error has a U-shape, whereas training error decreases monotonically. Two 'typical' examples of this behavior can be seen in the right-hand panel of Figure 2.9 on page 29, and in Figure 2.17 on page 39. One implication of the bias-variance trade-off is that it is generally not a good idea to interpolate the training data - that is, to get zero training error - since that will often result in very high test error.  \nWe now describe the set-up that resulted in Figure 10.20. We simulated n = 20 observations from the model  \nHowever, it turns out that in certain specific settings it can be possible for a statistical learning method that interpolates the training data to perform well - or at least, better than a slightly less complex model that does not quite interpolate the data. This phenomenon is known as double descent , and is displayed in Figure 10.20. 'Double descent' gets its name from the fact that the test error has a U-shape before the interpolation threshold is reached, and then it descends again (for a while, at least) as an increasingly flexible model is fit.  \n$$Y = \\sin ( X ) + \\epsilon ,$$  \nwhere X ∼ U [ -5  5] (uniform distribution), and /epsilon1 ∼ N (0  σ 2 ) with σ = 0   3 . We then fit a natural spline to the data, as described in Section 7.4, with d interpolate  \nFIGURE 10.21. Fitted functions ˆ f d ( X ) (orange), true function f ( X ) (black) and the observed 20 training data points. A different value of d (degrees of freedom) is used in each panel. For d ≥ 20 the orange curves all interpolate the training points, and hence the training error is zero.  \n<!-- image -->  \ndegrees of freedom. 22 Recall from Section 7.4 that fitting a natural spline with d degrees of freedom amounts to fitting a least-squares regression of the response onto a set of d basis functions. The upper-left panel of Figure 10.21 shows the data, the true function f ( X ) , and ˆ f 8 ( X ) , the fitted natural spline with d = 8 degrees of freedom.  \nWe now continue to fit natural splines to the data, with increasing values of d . For d &gt; 20 , the least squares regression of Y onto d basis functions is not unique: there are an infinite number of least squares coefficient estimates that achieve zero error. To select among them, we choose the one with the smallest sum of squared coefficients, ∑ d j =1 ˆ β 2 j . This is known as the minimum-norm solution.  \nNext, we fit a natural spline with d = 20 degrees of freedom. Since n = 20 , this means that n = d , and we have zero training error; in other words, we have interpolated the training data! We can see from the top-right panel of Figure 10.21 that ˆ f 20 ( X ) makes wild excursions, and hence the test error will be large.  \nThe two lower panels of Figure 10.21 show the minimum-norm natural spline fits with d = 42 and d = 80 degrees of freedom. Incredibly, ˆ f 42 ( X ) is quite a bit less less wild than ˆ f 20 ( X ) , even though it makes use of more degrees of freedom . And ˆ f 80 ( X ) is not much different. How can this be? Essentially, ˆ f 20 ( X ) is very wild because there is just a single way to interpolate n = 20 observations using d = 20 basis functions, and that single way results in a somewhat extreme fitted function. By contrast, there are an  \n22 This implies the choice of d knots, here chosen at d equi-probability quantiles of the training data. When d &gt; n , the quantiles are found by interpolation.  \ninfinite number of ways to interpolate n = 20 observations using d = 42 or d = 80 basis functions, and the smoothest of them - that is, the minimum norm solution - is much less wild than ˆ f 20 ( X ) !  \nIn Figures 10.20 and 10.21, we have illustrated the double descent phenomenon in a simple one-dimensional setting using natural splines. However, it turns out that the same phenomenon can arise for deep learning. Basically, when we fit neural networks with a huge number of parameters, we are sometimes able to get good results with zero training error. This is particularly true in problems with high signal-to-noise ratio, such as natural image recognition and language translation, for example. This is because the techniques used to fit neural networks, including stochastic gradient descent, naturally lend themselves to selecting a 'smooth' interpolating model that has good test-set performance on these kinds of problems.  \nIn Figure 10.20, we display the training error and test error associated with ˆ f d ( X ) , for a range of values of the degrees of freedom d . We see that the training error drops to zero once d = 20 and beyond; i.e. once the interpolation threshold is reached. By contrast, the test error shows a U -shape for d ≤ 20 , grows extremely large around d = 20 , and then shows a second region of descent for d &gt; 20 . For this example the signal-to-noise ratio Var( f ( X )) glyph[triangleleft] σ 2 - is 5   9 , which is quite high (the data points are close to the true curve). So an estimate that interpolates the data and does not wander too far inbetween the observed data points will likely do well.  \nSome points are worth emphasizing:  \n- The double-descent phenomenon does not contradict the bias-variance trade-off, as presented in Section 2.2.2. Rather, the double-descent curve seen in the right-hand side of Figure 10.20 is a consequence of the fact that the x -axis displays the number of spline basis functions used, which does not properly capture the true 'flexibility' of models that interpolate the training data. Stated another way, in this example, the minimum-norm natural spline with d = 42 has lower variance than the natural spline with d = 20 .\n- Most of the statistical learning methods seen in this book do not exhibit double descent. For instance, regularization approaches typically do not interpolate the training data, and thus double descent does not occur. This is not a drawback of regularized methods: they can give great results without interpolating the data !\n- In particular, in the examples here, if we had fit the natural splines using ridge regression with an appropriately-chosen penalty rather than least squares, then we would not have seen double descent, and in fact would have obtained better test error results.\n- In Chapter 9, we saw that maximal margin classifiers and SVMs that have zero training error nonetheless often achieve very good test error. This is in part because those methods seek smooth minimum norm solutions. This is similar to the fact that the minimum-norm natural spline can give good results with zero training error.\n- The double-descent phenomenon has been used by the machine learning community to explain the successful practice of using an over-  \nparametrized neural network (many layers, and many hidden units), and then fitting all the way to zero training error. However, fitting to zero error is not always optimal, and whether it is advisable depends on the signal-to-noise ratio. For instance, we may use ridge regularization to avoid overfitting a neural network, as in (10.31). In this case, provided that we use an appropriate choice for the tuning parameter λ , we will never interpolate the training data, and thus will not see the double descent phenomenon. Nonetheless we can get very good test-set performance, likely much better than we would have achieved had we interpolated the training data. Early stopping during stochastic gradient descent can also serve as a form of regularization that prevents us from interpolating the training data, while still getting very good results on test data.  \nTo summarize: though double descent can sometimes occur in neural networks, we typically do not want to rely on this behavior. Moreover, it is important to remember that the bias-variance trade-off always holds (though it is possible that test error as a function of flexibility may not exhibit a U-shape, depending on how we have parametrized the notion of 'flexibility' on the x -axis).""]","Deep learning models, which are essentially neural networks with multiple hidden layers, offer several advantages over simple neural networks (which typically have one or very few hidden layers):

1. **Ability to Learn Complex Patterns**: Deep learning models can capture complex patterns and representations in data due to their multiple layers. Each layer can learn different levels of abstraction, making them particularly effective for tasks like image and speech recognition.

2. **Feature Hierarchies**: Deep networks can automatically learn feature hierarchies from raw data. Lower layers might learn simple features, while higher layers can combine these into more complex features, which is beneficial for tasks like object detection in images.

3. **Scalability with Data**: Deep learning models tend to perform better as the amount of data increases. They can leverage large datasets to improve accuracy, which is why they are often used in big data applications.

4. **Versatility**: Deep learning models can be adapted to a wide range of tasks, including classification, regression, and unsupervised learning, by modifying the architecture and training process.

5. **State-of-the-Art Performance**: In many domains, such as image classification, natural language processing, and speech recognition, deep learning models have achieved state-of-the-art performance, often surpassing traditional machine learning methods.

6. **End-to-End Learning**: Deep learning models can often be trained in an end-to-end manner, meaning they can learn directly from raw inputs to desired outputs without the need for manual feature extraction.

However, it's important to note that deep learning models also come with challenges, such as requiring large amounts of data and computational resources, and they can be more difficult to interpret compared to simpler models.","Deep learning models, which are neural networks with multiple hidden layers, can learn more complex, hierarchical representations of the data. Each layer learns to extract features from the output of the previous layer, allowing the model to learn intricate patterns that might be missed by a shallow network. This often leads to superior performance on complex tasks like image and speech recognition.",0.7083333333333334,,0.9999999999666667,1.0
What is a convolutional neural network (CNN) and what kind of data is it particularly well-suited for?,"[""## 10.3 Convolutional Neural Networks  \nNeural networks rebounded around 2010 with big successes in image classification. Around that time, massive databases of labeled images were being accumulated, with ever-increasing numbers of classes. Figure 10.5 shows 75 images drawn from the CIFAR100 database. 4 This database consists of 60,000 images labeled according to 20 superclasses (e.g. aquatic mammals), with five classes per superclass (beaver, dolphin, otter, seal, whale). Each image has a resolution of 32 × 32 pixels, with three eight-bit numbers per pixel representing red, green and blue. The numbers for each image are organized in a three-dimensional array called a feature map . The first two  \n4 See Chapter 3 of Krizhevsky (2009) 'Learning multiple layers of features from tiny images', available at https://www.cs.toronto.edu/~kriz/ learning-features-2009-TR.pdf .  \ndropout feature map  \nFIGURE 10.6. Schematic showing how a convolutional neural network classifies an image of a tiger. The network takes in the image and identifies local features. It then combines the local features in order to create compound features, which in this example include eyes and ears. These compound features are used to output the label 'tiger'.  \n<!-- image -->  \naxes are spatial (both are 32 -dimensional), and the third is the channel axis, 5 representing the three colors. There is a designated training set of 50,000 images, and a test set of 10,000.  \nFigure 10.6 illustrates the idea behind a convolutional neural network on a cartoon image of a tiger. 6  \nA special family of convolutional neural networks (CNNs) has evolved for classifying images such as these, and has shown spectacular success on a wide range of problems. CNNs mimic to some degree how humans classify images, by recognizing specific features or patterns anywhere in the image that distinguish each particular object class. In this section we give a brief overview of how they work.  \nThe network first identifies low-level features in the input image, such as small edges, patches of color, and the like. These low-level features are then combined to form higher-level features, such as parts of ears, eyes, and so on. Eventually, the presence or absence of these higher-level features contributes to the probability of any given output class.  \nHow does a convolutional neural network build up this hierarchy? It combines two specialized types of hidden layers, called convolution layers and pooling layers. Convolution layers search for instances of small patterns in the image, whereas pooling layers downsample these to select a prominent subset. In order to achieve state-of-the-art results, contemporary neuralnetwork architectures make use of many convolution and pooling layers. We describe convolution and pooling layers next."", ""### 10.3.1 Convolution Layers  \nA convolution layer is made up of a large number of convolution filters , each  \n5 The term channel is taken from the signal-processing literature. Each channel is a distinct source of information.  \n6 Thanks to Elena Tuzhilina for producing the diagram and https://www. cartooning4kids.com/ for permission to use the cartoon tiger.  \nchannel convolutional neural networks  \nconvolution layer convolution filter  \nof which is a template that determines whether a particular local feature is present in an image. A convolution filter relies on a very simple operation, called a convolution , which basically amounts to repeatedly multiplying matrix elements and then adding the results.  \n$$3 \\, \\text {image} \\colon \\, \\begin{bmatrix} 3 & 3 & 3 & 3 & 3 & 3 & 3 \\\\ & & & & & & & \\begin{bmatrix} a & b & c \\\\ d & e & f \\\\ g & h & i \\\\ j & k & l \\end{bmatrix} & \\cdot \\\\ \\times 2 \\, \\text {filter of the form}$$  \nTo understand how a convolution filter works, consider a very simple example of a 4 × 3 image:  \nNow consider a 2 × 2 filter of the form  \n$$C o n v o l u t i o n F i r t e = \\left [ \\begin{matrix} \\alpha & \\beta \\\\ \\gamma & \\delta \\end{matrix} \\right ] .$$  \nWhen we convolve the image with the filter, we get the result 7  \n$$\\text {when we compute the image with this filter} , \\text { we get the same result} ^ { \\dagger } \\\\ \\text {Convolved Image} = \\begin{bmatrix} a \\alpha + b \\beta + d \\gamma + e \\delta & b \\alpha + c \\beta + e \\gamma + f \\delta \\\\ d \\alpha + e \\beta + g \\gamma + h \\delta & e \\alpha + f \\beta + h \\gamma + i \\delta \\\\ g \\alpha + h \\beta + j \\gamma + k \\delta & h \\alpha + i \\beta + k \\gamma + l \\delta \\end{bmatrix} . \\\\ \\text {For instance, the top-left element comes from multiplying each element in}$$  \nFigure 10.7 illustrates the application of two convolution filters to a 192 × 179 image of a tiger, shown on the left-hand side. 8 Each convolution filter is a 15 × 15 image containing mostly zeros (black), with a narrow strip of ones (white) oriented either vertically or horizontally within the image. When each filter is convolved with the image of the tiger, areas of the tiger that resemble the filter (i.e. that have either horizontal or vertical stripes or edges) are given large values, and areas of the tiger that do not resemble the feature are given small values. The convolved images are displayed on the right-hand side. We see that the horizontal stripe filter picks out horizontal stripes and edges in the original image, whereas the vertical stripe filter picks out vertical stripes and edges in the original image.  \nFor instance, the top-left element comes from multiplying each element in the 2 × 2 filter by the corresponding element in the top left 2 × 2 portion of the image, and adding the results. The other elements are obtained in a similar way: the convolution filter is applied to every 2 × 2 submatrix of the original image in order to obtain the convolved image. If a 2 × 2 submatrix of the original image resembles the convolution filter, then it will have a large value in the convolved image; otherwise, it will have a small value. Thus, the convolved image highlights regions of the original image that resemble the convolution filter. We have used 2 × 2 as an example; in general convolution filters are small /lscript 1 × /lscript 2 arrays, with /lscript 1 and /lscript 2 small positive integers that are not necessarily equal.  \n7 The convolved image is smaller than the original image because its dimension is given by the number of 2 × 2 submatrices in the original image. Note that 2 × 2 is the dimension of the convolution filter. If we want the convolved image to have the same dimension as the original image, then padding can be applied.  \n8 The tiger image used in Figures 10.7-10.9 was obtained from the public domain image resource https://www.needpix.com/ .  \nFIGURE 10.7. Convolution filters find local features in an image, such as edges and small shapes. We begin with the image of the tiger shown on the left, and apply the two small convolution filters in the middle. The convolved images highlight areas in the original image where details similar to the filters are found. Specifically, the top convolved image highlights the tiger's vertical stripes, whereas the bottom convolved image highlights the tiger's horizontal stripes. We can think of the original image as the input layer in a convolutional neural network, and the convolved images as the units in the first hidden layer.  \n<!-- image -->  \nWe have used a large image and two large filters in Figure 10.7 for illustration. For the CIFAR100 database there are 32 × 32 color pixels per image, and we use 3 × 3 convolution filters.  \nWe now give some additional details.  \nIn a convolution layer, we use a whole bank of filters to pick out a variety of differently-oriented edges and shapes in the image. Using predefined filters in this way is standard practice in image processing. By contrast, with CNNs the filters are learned for the specific classification task. We can think of the filter weights as the parameters going from an input layer to a hidden layer, with one hidden unit for each pixel in the convolved image. This is in fact the case, though the parameters are highly structured and constrained (see Exercise 4 for more details). They operate on localized patches in the input image (so there are many structural zeros), and the same weights in a given filter are reused for all possible patches in the image (so the weights are constrained). 9  \n- Since the input image is in color, it has three channels represented by a three-dimensional feature map (array). Each channel is a twodimensional ( 32 × 32 ) feature map - one for red, one for green, and one for blue. A single convolution filter will also have three channels, one per color, each of dimension 3 × 3 , with potentially different filter weights. The results of the three convolutions are summed to form a two-dimensional output feature map. Note that at this point the color information has been used, and is not passed on to subsequent layers except through its role in the convolution.  \n9 This used to be called weight sharing in the early years of neural networks.  \n- If we use K different convolution filters at this first hidden layer, we get K two-dimensional output feature maps, which together are treated as a single three-dimensional feature map. We view each of the K output feature maps as a separate channel of information, so now we have K channels in contrast to the three color channels of the original input feature map. The three-dimensional feature map is just like the activations in a hidden layer of a simple neural network, except organized and produced in a spatially structured way.\n- We typically apply the ReLU activation function (10.5) to the convolved image. This step is sometimes viewed as a separate layer in the convolutional neural network, in which case it is referred to as a detector layer .""]","A convolutional neural network (CNN) is a specialized type of neural network that is particularly well-suited for classifying images. CNNs have shown spectacular success on a wide range of image classification problems. They mimic, to some degree, how humans classify images by recognizing specific features or patterns anywhere in the image that distinguish each particular object class. CNNs are designed to identify low-level features in the input image, such as small edges and patches of color, and then combine these to form higher-level features, such as parts of ears or eyes. Eventually, the presence or absence of these higher-level features contributes to the probability of any given output class. CNNs are especially effective for image data due to their ability to capture spatial hierarchies in images through the use of convolution and pooling layers.","A convolutional neural network (CNN) is a type of deep learning model that uses convolutional and pooling layers to automatically and adaptively learn spatial hierarchies of features from data. They are particularly well-suited for processing grid-like data such as images and videos, where local patterns are important.",1.0,,0.99999999995,1.0
Explain the concept of 'dropout' in the context of training neural networks.,"[""### 10.7.3 Dropout Learning  \nThe second row in Table 10.1 is labeled dropout . This is a relatively new and efficient form of regularization, similar in some respects to ridge regularization. Inspired by random forests (Section 8.2), the idea is to randomly remove a fraction φ of the units in a layer when fitting the model. Figure 10.19 illustrates this. This is done separately each time a training observation is processed. The surviving units stand in for those missing, and their weights are scaled up by a factor of 1 glyph[triangleleft] (1 -φ ) to compensate. This prevents nodes from becoming over-specialized, and can be seen as a form of regularization. In practice dropout is achieved by randomly setting the activations for the 'dropped out' units to zero, while keeping the architecture intact."", '### 10.7.2 Regularization and Stochastic Gradient Descent  \nGradient descent usually takes many steps to reach a local minimum. In practice, there are a number of approaches for accelerating the process. Also, when n is large, instead of summing (10.29)-(10.30) over all n observations, we can sample a small fraction or minibatch of them each time we compute a gradient step. This process is known as stochastic gradient descent (SGD) and is the state of the art for learning deep neural networks. Fortunately, there is very good software for setting up deep learning models, and for fitting them to data, so most of the technicalities are hidden from the user.  \nWe now turn to the multilayer network (Figure 10.4) used in the digit recognition problem. The network has over 235,000 weights, which is around four times the number of training examples. Regularization is essential here learning rate  \nchain rule backpropagation  \nminibatch stochastic gradient descent  \nFIGURE 10.18. Evolution of training and validation errors for the MNIST neural network depicted in Figure 10.4, as a function of training epochs. The objective refers to the log-likelihood (10.14).  \n<!-- image -->  \nto avoid overfitting. The first row in Table 10.1 uses ridge regularization on the weights. This is achieved by augmenting the objective function (10.14) with a penalty term:  \n$$R ( \\theta ; \\lambda ) = - \\sum _ { i = 1 } ^ { n } \\sum _ { m = 0 } ^ { 9 } y _ { i m } \\log ( f _ { m } ( x _ { i } ) ) + \\lambda \\sum _ { j } \\theta _ { j } ^ { 2 } .$$  \nThe parameter λ is often preset at a small value, or else it is found using the validation-set approach of Section 5.3.1. We can also use different values of λ for the groups of weights from different layers; in this case W 1 and W 2 were penalized, while the relatively few weights B of the output layer were not penalized at all. Lasso regularization is also popular as an additional form of regularization, or as an alternative to ridge.  \nFigure 10.18 shows some metrics that evolve during the training of the network on the MNIST data. It turns out that SGD naturally enforces its own form of approximately quadratic regularization. 21 Here the minibatch size was 128 observations per gradient update. The term epochs labeling the horizontal axis in Figure 10.18 counts the number of times an equivalent of the full training set has been processed. For this network, 20% of the 60,000 training observations were used as a validation set in order to determine when training should stop. So in fact 48,000 observations were used for training, and hence there are 48  000 glyph[triangleleft] 128 ≈ 375 minibatch gradient updates per epoch. We see that the value of the validation objective actually starts to increase by 30 epochs, so early stopping can also be used as an additional form of regularization.  \n21 This and other properties of SGD for deep learning are the subject of much research in the machine learning literature at the time of writing.  \nepochs early stopping  \nFIGURE 10.19. Dropout Learning. Left: a fully connected network. Right: network with dropout in the input and hidden layer. The nodes in grey are selected at random, and ignored in an instance of training.  \n<!-- image -->', ""### 10.3.3 Architecture of a Convolutional Neural Network  \nSo far we have defined a single convolution layer - each filter produces a new two-dimensional feature map. The number of convolution filters in a convolution layer is akin to the number of units at a particular hidden layer in a fully-connected neural network of the type we saw in Section 10.2. This number also defines the number of channels in the resulting threedimensional feature map. We have also described a pooling layer, which reduces the first two dimensions of each three-dimensional feature map. Deep CNNs have many such layers. Figure 10.8 shows a typical architecture for a CNN for the CIFAR100 image classification task.  \nAt the input layer, we see the three-dimensional feature map of a color image, where the channel axis represents each color by a 32 × 32 twodimensional feature map of pixels. Each convolution filter produces a new channel at the first hidden layer, each of which is a 32 × 32 feature map (after some padding at the edges). After this first round of convolutions, we now have a new 'image'; a feature map with considerably more channels than the three color input channels (six in the figure, since we used six convolution filters).  \ndetector layer pooling  \nFIGURE 10.8. Architecture of a deep CNN for the CIFAR100 classification task. Convolution layers are interspersed with 2 × 2 max-pool layers, which reduce the size by a factor of 2 in both dimensions.  \n<!-- image -->  \nThis is followed by a max-pool layer, which reduces the size of the feature map in each channel by a factor of four: two in each dimension.  \nThis convolve-then-pool sequence is now repeated for the next two layers. Some details are as follows:  \n- Each subsequent convolve layer is similar to the first. It takes as input the three-dimensional feature map from the previous layer and treats it like a single multi-channel image. Each convolution filter learned has as many channels as this feature map.\n- Since the channel feature maps are reduced in size after each pool layer, we usually increase the number of filters in the next convolve layer to compensate.\n- Sometimes we repeat several convolve layers before a pool layer. This effectively increases the dimension of the filter.  \nThese operations are repeated until the pooling has reduced each channel feature map down to just a few pixels in each dimension. At this point the three-dimensional feature maps are flattened - the pixels are treated as separate units - and fed into one or more fully-connected layers before reaching the output layer, which is a softmax activation for the 100 classes (as in (10.13)).  \nThere are many tuning parameters to be selected in constructing such a network, apart from the number, nature, and sizes of each layer. Dropout learning can be used at each layer, as well as lasso or ridge regularization (see Section 10.7). The details of constructing a convolutional neural network can seem daunting. Fortunately, terrific software is available, with extensive examples and vignettes that provide guidance on sensible choices for the parameters. For the CIFAR100 official test set, the best accuracy as of this writing is just above 75%, but undoubtedly this performance will continue to improve."", ""## 10.2 Multilayer Neural Networks  \nModern neural networks typically have more than one hidden layer, and often many units per layer. In theory a single hidden layer with a large number of units has the ability to approximate most functions. However, the learning task of discovering a good solution is made much easier with multiple layers each of modest size.  \nWe will illustrate a large dense network on the famous and publicly available MNIST handwritten digit dataset. 1 Figure 10.3 shows examples of these digits. The idea is to build a model to classify the images into their correct digit class 0-9. Every image has p = 28 × 28 = 784 pixels, each of which is an eight-bit grayscale value between 0 and 255 representing  \n1 See LeCun, Cortes, and Burges (2010) 'The MNIST database of handwritten digits', available at http://yann.lecun.com/exdb/mnist .  \nFIGURE 10.3. Examples of handwritten digits from the MNIST corpus. Each grayscale image has 28 × 28 pixels, each of which is an eight-bit number (0-255) which represents how dark that pixel is. The first 3, 5, and 8 are enlarged to show their 784 individual pixel values.  \n<!-- image -->  \nthe relative amount of the written digit in that tiny square. 2 These pixels are stored in the input vector X (in, say, column order). The output is the class label, represented by a vector Y = ( Y 0  Y 1         Y 9 ) of 10 dummy variables, with a one in the position corresponding to the label, and zeros elsewhere. In the machine learning community, this is known as one-hot encoding . There are 60,000 training images, and 10,000 test images.  \nFigure 10.4 shows a multilayer network architecture that works well for solving the digit-classification task. It differs from Figure 10.1 in several ways:  \nOn a historical note, digit recognition problems were the catalyst that accelerated the development of neural network technology in the late 1980s at AT&amp;T Bell Laboratories and elsewhere. Pattern recognition tasks of this kind are relatively simple for humans. Our visual system occupies a large fraction of our brains, and good recognition is an evolutionary force for survival. These tasks are not so simple for machines, and it has taken more than 30 years to refine the neural-network architectures to match human performance.  \n- It has two hidden layers L 1 (256 units) and L 2 (128 units) rather than one. Later we will see a network with seven hidden layers.\n- It has ten output variables, rather than one. In this case the ten variables really represent a single qualitative variable and so are quite dependent. (We have indexed them by the digit class 0-9 rather than 1-10, for clarity.) More generally, in multi-task learning one can predict different responses simultaneously with a single network; they all have a say in the formation of the hidden layers.\n- The loss function used for training the network is tailored for the multiclass classification task.  \n2 In the analog-to-digital conversion process, only part of the written numeral may fall in the square representing a particular pixel.  \none-hot encoding multi-task learning  \nFIGURE 10.4. Neural network diagram with two hidden layers and multiple outputs, suitable for the MNIST handwritten-digit problem. The input layer has p = 784 units, the two hidden layers K 1 = 256 and K 2 = 128 units respectively, and the output layer 10 units. Along with intercepts (referred to as biases in the deep-learning community) this network has 235,146 parameters (referred to as weights).  \n<!-- image -->  \nThe first hidden layer is as in (10.2), with  \n$$\\begin{array} { r l r } { A _ { k } ^ { ( 1 ) } } & { = } & { h _ { k } ^ { ( 1 ) } ( X ) } \\\\ & { = } & { g ( w _ { k 0 } ^ { ( 1 ) } + \\sum _ { j = 1 } ^ { p } w _ { k j } ^ { ( 1 ) } X _ { j } ) } \\\\ { \\dots } & { K _ { 1 } \\colon T h e s e d h i n d e h i v e t r e a t i v a t i o n s \\ A _ { k } ^ { ( 1 ) } \\ o f } \\end{array}$$  \nfor k = 1         K 1 . The second hidden layer treats the activations A (1) k of the first hidden layer as inputs and computes new activations  \n$$\\begin{array} { r l r } { A _ { \\ell } ^ { ( 2 ) } } & { = } & { h _ { \\ell } ^ { ( 2 ) } ( X ) } \\\\ & { = } & { g ( w _ { \\ell 0 } ^ { ( 2 ) } + \\sum _ { k = 1 } ^ { K _ { 1 } } w _ { k } ^ { ( 2 ) } A _ { k } ^ { ( 1 ) } ) } \\\\ { \\dots , K _ { 2 } . \\, \\text {Notice that each of the activations in the second layer} } \\end{array}$$  \nfor /lscript = 1         K 2   Notice that each of the activations in the second layer A (2) /lscript = h (2) /lscript ( X ) is a function of the input vector X . This is the case because while they are explicitly a function of the activations A (1) k from layer L 1 , these in turn are functions of X . This would also be the case with more hidden layers. Thus, through a chain of transformations, the network is able to build up fairly complex transformations of X that ultimately feed into the output layer as features.  \nWe have introduced additional superscript notation such as h (2) /lscript ( X ) and w (2) /lscript j in (10.10) and (10.11) to indicate to which layer the activations and weights (coefficients) belong, in this case layer 2. The notation W 1 in Fig-  \nure 10.4 represents the entire matrix of weights that feed from the input layer to the first hidden layer L 1 . This matrix will have 785 × 256 = 200  960 elements; there are 785 rather than 784 because we must account for the intercept or bias term. 3  \nEach element A (1) k feeds to the second hidden layer L 2 via the matrix of weights W 2 of dimension 257 × 128 = 32  896 .  \nWe now get to the output layer, where we now have ten responses rather than one. The first step is to compute ten different linear models similar to our single model (10.1),  \n$$\\begin{array} { r l } { \\mathrm e } { \\mathrm e } { \\mathrm d } { \\mathrm m } & { Z _ { m } \\ = \\ \\beta _ { m 0 } + \\sum _ { \\ell = 1 } ^ { K _ { 2 } } \\beta _ { m \\ell } h _ { \\ell } ^ { ( 2 ) } ( X ) } \\\\ & { = \\ \\beta _ { m 0 } + \\sum _ { \\ell = 1 } ^ { K _ { 2 } } \\beta _ { m \\ell } A _ { \\ell } ^ { ( 2 ) } , } \\\\ { 1 , 1 , \\dots , 9 . } \\, \\mathrm T h i r x { \\mathrm B } { \\mathrm s o r s a l l } & { 1 2 9 \\times 1 0 = 1 , 2 9 0 \\, o f \\, \\mathrm s e } \\end{array}$$  \nIf these were all separate quantitative responses, we would simply set each f m ( X ) = Z m and be done. However, we would like our estimates to represent class probabilities f m ( X ) = Pr( Y = m ♣ X ) , just like in multinomial logistic regression in Section 4.3.5. So we use the special softmax activation function (see (4.13) on page 145), for m = 0  1         9 . The matrix B stores all 129 × 10 = 1  290 of these weights.  \n$$f _ { m } ( X ) = & \\Pr ( Y = m | X ) = \\frac { e ^ { Z _ { m } } } { \\sum _ { \\ell = 0 } ^ { 9 } e ^ { Z _ { \\ell } } } , \\\\ 0 , 1 , \\dots , 9 \\text {  This ensures that the 10 numbers behave like proba-} \\\\ \\text {non-negative and sum to one} \\ \\text {Even though the goal is to build}$$  \nTo train this network, since the response is qualitative, we look for coefficient estimates that minimize the negative multinomial log-likelihood for m = 0  1         9 . This ensures that the 10 numbers behave like probabilities (non-negative and sum to one). Even though the goal is to build a classifier, our model actually estimates a probability for each of the 10 classes. The classifier then assigns the image to the class with the highest probability.  \n$$- \\sum _ { i = 1 } ^ { n } \\sum _ { m = 0 } ^ { 9 } y _ { i m } \\log ( f _ { m } ( x _ { i } ) ) , \\\\ \\intertext { t h o w } \\intertext { a n d } \\intertext { s u n t w }$$  \nTable 10.1 compares the test performance of the neural network with two simple models presented in Chapter 4 that make use of linear decision boundaries: multinomial logistic regression and linear discriminant analysis. The improvement of neural networks over both of these linear methods is dramatic: the network with dropout regularization achieves a test error rate below 2% on the 10  000 test images. (We describe dropout regularization in Section 10.7.3.) In Section 10.9.2 of the lab, we present the code for fitting this model, which runs in just over two minutes on a laptop computer.  \nalso known as the cross-entropy . This is a generalization of the criterion (4.5) for two-class logistic regression. Details on how to minimize this objective are given in Section 10.7. If the response were quantitative, we would instead minimize squared-error loss as in (10.9).  \n3 The use of 'weights' for coefficients and 'bias' for the intercepts w k 0 in (10.2) is popular in the machine learning community; this use of bias is not to be confused with the 'bias-variance' usage elsewhere in this book.  \nbias softmax  \ncrossentropy  \nTABLE 10.1. Test error rate on the MNIST data, for neural networks with two forms of regularization, as well as multinomial logistic regression and linear discriminant analysis. In this example, the extra complexity of the neural network leads to a marked improvement in test error.  \n| Method                                  | Test Error                 |\n|-----------------------------------------|----------------------------|\n| Neural Network + Ridge Regularization   | 2   3%  |\n| Neural Network + Dropout Regularization | 1   8%  |\n| Multinomial Logistic Regression         | 7   2%  |\n| Linear Discriminant Analysis            | 12   7% |  \nFIGURE 10.5. A sample of images from the CIFAR100 database: a collection of natural images from everyday life, with 100 different classes represented.  \n<!-- image -->  \nAdding the number of coefficients in W 1 , W 2 and B , we get 235  146 in all, more than 33 times the number 785 × 9 = 7  065 needed for multinomial logistic regression. Recall that there are 60  000 images in the training set. While this might seem like a large training set, there are almost four times as many coefficients in the neural network model as there are observations in the training set! To avoid overfitting, some regularization is needed. In this example, we used two forms of regularization: ridge regularization, which is similar to ridge regression from Chapter 6, and dropout regularization. We discuss both forms of regularization in Section 10.7.""]","Dropout is a form of regularization used in training neural networks to prevent overfitting. The concept involves randomly removing a fraction \( \phi \) of the units in a layer during the training process. This is done separately for each training observation. The units that remain, or ""surviving units,"" take on the role of the missing ones, and their weights are scaled up by a factor of \( \frac{1}{1 - \phi} \) to compensate for the dropout. This technique helps prevent nodes from becoming overly specialized and acts as a regularization method.

In practice, dropout is implemented by setting the activations of the ""dropped out"" units to zero while keeping the network's architecture intact. This approach is inspired by the concept of random forests and is considered efficient for regularization, similar to ridge regularization.","Dropout is a regularization technique used in training neural networks to prevent overfitting. During training, a certain proportion of neurons (and their connections) are randomly ignored or 'dropped out' for each training sample. This forces the network to learn more robust features and prevents it from becoming overly reliant on any single neuron.",1.0,,0.999999999975,1.0
¿Qué es una red neuronal recurrente (RNN) y para qué tipo de datos es adecuada?,"[""## 10.5 Recurrent Neural Networks  \nMany data sources are sequential in nature, and call for special treatment when building predictive models. Examples include:  \n- Documents such as book and movie reviews, newspaper articles, and tweets. The sequence and relative positions of words in a document capture the narrative, theme and tone, and can be exploited in tasks such as topic classification, sentiment analysis, and language translation.\n- Time series of temperature, rainfall, wind speed, air quality, and so on. We may want to forecast the weather several days ahead, or climate several decades ahead.\n- Financial time series, where we track market indices, trading volumes, stock and bond prices, and exchange rates. Here prediction is often difficult, but as we will see, certain indices can be predicted with reasonable accuracy.\n- Recorded speech, musical recordings, and other sound recordings. We may want to give a text transcription of a speech, or perhaps a language translation. We may want to assess the quality of a piece of music, or assign certain attributes.\n- Handwriting, such as doctor's notes, and handwritten digits such as zip codes. Here we want to turn the handwriting into digital text, or read the digits (optical character recognition).  \nIn a recurrent neural network (RNN), the input object X is a sequence . Consider a corpus of documents, such as the collection of IMDb movie reviews. Each document can be represented as a sequence of L words, so X = ¶ X 1  X 2         X L ♦ , where each X /lscript represents a word. The order of the words, and closeness of certain words in a sentence, convey semantic meaning. RNNs are designed to accommodate and take advantage of the sequential nature of such input objects, much like convolutional neural networks accommodate the spatial structure of image inputs. The output Y can also be a sequence (such as in language translation), but often is a scalar, like the binary sentiment label of a movie review document.  \nrecurrent neural network  \nFIGURE 10.12. Schematic of a simple recurrent neural network. The input is a sequence of vectors ¶ X /lscript ♦ L 1 , and here the target is a single response. The network processes the input sequence X sequentially; each X /lscript feeds into the hidden layer, which also has as input the activation vector A /lscript -1 from the previous element in the sequence, and produces the current activation vector A /lscript . The same collections of weights W , U and B are used as each element of the sequence is processed. The output layer produces a sequence of predictions O /lscript from the current activation A /lscript , but typically only the last of these, O L , is of relevance. To the left of the equal sign is a concise representation of the network, which is unrolled into a more explicit version on the right.  \n<!-- image -->  \nFigure 10.12 illustrates the structure of a very basic RNN with a sequence X = ¶ X 1  X 2         X L ♦ as input, a simple output Y , and a hidden-layer sequence ¶ A /lscript ♦ L 1 = ¶ A 1  A 2         A L ♦ . Each X /lscript is a vector; in the document example X /lscript could represent a one-hot encoding for the /lscript th word based on the language dictionary for the corpus (see the top panel in Figure 10.13 for a simple example). As the sequence is processed one vector X /lscript at a time, the network updates the activations A /lscript in the hidden layer, taking as input the vector X /lscript and the activation vector A /lscript -1 from the previous step in the sequence. Each A /lscript feeds into the output layer and produces a prediction O /lscript for Y . O L , the last of these, is the most relevant.  \nIn detail, suppose each vector X /lscript of the input sequence has p components X T /lscript = ( X /lscript 1  X /lscript 2         X /lscript p ) , and the hidden layer consists of K units A T /lscript = ( A /lscript 1  A /lscript 2         A /lscript K ) . As in Figure 10.4, we represent the collection of K × ( p +1) shared weights w kj for the input layer by a matrix W , and similarly U is a K × K matrix of the weights u ks for the hidden-to-hidden layers, and B is a K +1 vector of weights β k for the output layer. Then  \n$$A _ { \\ell k } = g \\left ( w _ { k 0 } + \\sum _ { j = 1 } ^ { p } w _ { k j } X _ { \\ell j } + \\sum _ { s = 1 } ^ { K } u _ { k s } A _ { \\ell - 1 , s } \\right ) ,$$  \nand the output O /lscript is computed as  \n$$O _ { \\ell } = \\beta _ { 0 } + \\sum _ { k = 1 } ^ { K } \\beta _ { k } A _ { \\ell k } \\\\$$  \nfor a quantitative response, or with an additional sigmoid activation function for a binary response, for example. Here g ( · ) is an activation function such as ReLU. Notice that the same weights W , U and B are used as we  \nprocess each element in the sequence, i.e. they are not functions of /lscript . This is a form of weight sharing used by RNNs, and similar to the use of filters in convolutional neural networks (Section 10.3.1.) As we proceed from beginning to end, the activations A /lscript accumulate a history of what has been seen before, so that the learned context can be used for prediction.  \nFor regression problems the loss function for an observation ( XY ) is  \n$$( Y - O _ { L } ) ^ { 2 } ,$$  \nwhich only references the final output O L = β 0 + ∑ K k =1 β k A Lk . Thus O 1  O 2         O L -1 are not used. When we fit the model, each element X /lscript of the input sequence X contributes to O L via the chain (10.16), and hence contributes indirectly to learning the shared parameters W , U and B via the loss (10.18). With n input sequence/response pairs ( x i  y i )  the parameters are found by minimizing the sum of squares  \n$$\\sum _ { i = 1 } ^ { n } ( y _ { i } - & o _ { i L } ) ^ { 2 } = \\sum _ { i = 1 } ^ { n } \\left ( y _ { i } - ( \\beta _ { 0 } + \\sum _ { k = 1 } ^ { K } \\beta _ { k } g ( w _ { k 0 } + \\sum _ { j = 1 } ^ { p } w _ { k j } x _ { i L j } + \\sum _ { s = 1 } ^ { K } u _ { k s } a _ { i , L - 1 , s } ) ) \\right ) ^ { 2 } . \\\\ \\\\ \\text {Here we use} \\, \\text {lowercase letters for the observed} \\, u _ { i } \\text { and vector sequences}$$  \nSince the intermediate outputs O /lscript are not used, one may well ask why they are there at all. First of all, they come for free, since they use the same output weights B needed to produce O L , and provide an evolving prediction for the output. Furthermore, for some learning tasks the response is also a sequence, and so the output sequence ¶ O 1  O 2         O L ♦ is explicitly needed.  \nHere we use lowercase letters for the observed y i and vector sequences x i = ¶ x i 1  x i 2         x iL ♦ , 15 as well as the derived activations.  \nWhen used at full strength, recurrent neural networks can be quite complex. We illustrate their use in two simple applications. In the first, we continue with the IMDb sentiment analysis of the previous section, where we process the words in the reviews sequentially. In the second application, we illustrate their use in a financial time series forecasting problem."", '### 10.5.3 Summary of RNNs  \nWe have illustrated RNNs through two simple use cases, and have only scratched the surface.  \nOne can also have additional hidden layers in an RNN. For example, with two hidden layers, the sequence A /lscript is treated as an input sequence to the next hidden layer in an obvious fashion.  \nThere are many variations and enhancements of the simple RNN we used for sequence modeling. One approach we did not discuss uses a onedimensional convolutional neural network, treating the sequence of vectors (say words, as represented in the embedding space) as an image. The convolution filter slides along the sequence in a one-dimensional fashion, with the potential to learn particular phrases or short subsequences relevant to the learning task.  \nflattening  \nThe RNN we used scanned the document from beginning to end; alternative bidirectional RNNs scan the sequences in both directions.  \nAlgorithms used to fit RNNs can be complex and computationally costly. Fortunately, good software protects users somewhat from these complexities, and makes specifying and fitting these models relatively painless. Many of the models that we enjoy in daily life (like Google Translate ) use stateof-the-art architectures developed by teams of highly skilled engineers, and have been trained using massive computational and data resources.  \nIn language translation the target is also a sequence of words, in a language different from that of the input sequence. Both the input sequence and the target sequence are represented by a structure similar to Figure 10.12, and they share the hidden units. In this so-called Seq2Seq learning, the hidden units are thought to capture the semantic meaning of the sentences. Some of the big breakthroughs in language modeling and translation resulted from the relatively recent improvements in such RNNs.']","Una red neuronal recurrente (RNN) es un tipo de red neuronal diseñada para procesar secuencias de datos. Es especialmente adecuada para datos que tienen una naturaleza secuencial, donde el orden y la posición relativa de los elementos en la secuencia son importantes para capturar el significado o el contexto. Algunos ejemplos de este tipo de datos incluyen:

- **Documentos**: Como reseñas de libros y películas, artículos de periódicos y tweets, donde el orden de las palabras es crucial para tareas como la clasificación de temas, el análisis de sentimientos y la traducción de idiomas.
- **Series temporales**: Como la temperatura, la lluvia, la velocidad del viento y la calidad del aire, donde se puede querer predecir el clima o el clima futuro.
- **Series financieras**: Donde se rastrean índices de mercado, volúmenes de negociación, precios de acciones y bonos, y tasas de cambio.
- **Grabaciones de sonido**: Como discursos grabados y música, donde se puede querer transcribir un discurso o evaluar la calidad de una pieza musical.
- **Escritura a mano**: Como notas de médicos y dígitos escritos a mano, donde se busca convertir la escritura en texto digital o reconocer los dígitos.

Las RNNs son capaces de aprovechar la naturaleza secuencial de estos datos al procesar cada elemento de la secuencia uno a la vez, acumulando un historial de lo que se ha visto antes para usarlo en la predicción.","Una red neuronal recurrente (RNN) es un tipo de red neuronal diseñada para procesar datos secuenciales, como texto, series de tiempo o habla. A diferencia de las redes neuronales de avance, las RNN tienen conexiones recurrentes que les permiten mantener una memoria interna de la información procesada anteriormente, lo que las hace adecuadas para tareas en las que el contexto y el orden de los datos son importantes.",1.0,,0.99999999995,1.0
What is the difference between Principal Component Analysis (PCA) and clustering?,"['## 12.4 Clustering Methods  \nClustering refers to a very broad set of techniques for finding subgroups , or clusters , in a data set. When we cluster the observations of a data set, we seek to partition them into distinct groups so that the observations within each group are quite similar to each other, while observations in different groups are quite different from each other. Of course, to make this concrete, we must define what it means for two or more observations to be similar or different . Indeed, this is often a domain-specific consideration that must be made based on knowledge of the data being studied.  \nBoth clustering and PCA seek to simplify the data via a small number of summaries, but their mechanisms are different:  \nFor instance, suppose that we have a set of n observations, each with p features. The n observations could correspond to tissue samples for patients with breast cancer, and the p features could correspond to measurements collected for each tissue sample; these could be clinical measurements, such as tumor stage or grade, or they could be gene expression measurements. We may have a reason to believe that there is some heterogeneity among the n tissue samples; for instance, perhaps there are a few different unknown subtypes of breast cancer. Clustering could be used to find these subgroups. This is an unsupervised problem because we are trying to discover structure-in this case, distinct clusters-on the basis of a data set. The goal in supervised problems, on the other hand, is to try to predict some outcome vector such as survival time or response to drug treatment.  \nclustering  \n- PCA looks to find a low-dimensional representation of the observations that explain a good fraction of the variance;\n- Clustering looks to find homogeneous subgroups among the observations.  \nAnother application of clustering arises in marketing. We may have access to a large number of measurements (e.g. median household income, occupation, distance from nearest urban area, and so forth) for a large number of people. Our goal is to perform market segmentation by identifying subgroups of people who might be more receptive to a particular form of advertising, or more likely to purchase a particular product. The task of performing market segmentation amounts to clustering the people in the data set.  \nIn general, we can cluster observations on the basis of the features in order to identify subgroups among the observations, or we can cluster features on the basis of the observations in order to discover subgroups among the features. In what follows, for simplicity we will discuss clustering observations on the basis of the features, though the converse can be performed by simply transposing the data matrix.  \nSince clustering is popular in many fields, there exist a great number of clustering methods. In this section we focus on perhaps the two best-known clustering approaches: K -means clustering and hierarchical clustering . In K -means clustering, we seek to partition the observations into a pre-specified number of clusters. On the other hand, in hierarchical clustering, we do not know in advance how many clusters we want; in fact, we end up with a tree-like visual representation of the observations, called a dendrogram , that allows us to view at once the clusterings obtained for each possible number of clusters, from 1 to n . There are advantages and disadvantages to each of these clustering approaches, which we highlight in this chapter.', '## 12.2 Principal Components Analysis  \nPrincipal components are discussed in Section 6.3.1 in the context of principal components regression. When faced with a large set of correlated variables, principal components allow us to summarize this set with a smaller number of representative variables that collectively explain most of the variability in the original set. The principal component directions are presented in Section 6.3.1 as directions in feature space along which the original data are highly variable . These directions also define lines and subspaces that are as close as possible to the data cloud. To perform principal components regression, we simply use principal components as predictors in a regression model in place of the original larger set of variables.  \nPrincipal components analysis (PCA) refers to the process by which principal components are computed, and the subsequent use of these components in understanding the data. PCA is an unsupervised approach, since it involves only a set of features X 1  X 2         X p , and no associated response Y . Apart from producing derived variables for use in supervised learning problems, PCA also serves as a tool for data visualization (visualization of exploratory data analysis  \nprincipal components analysis  \nthe observations or visualization of the variables). It can also be used as a tool for data imputation - that is, for filling in missing values in a data matrix.  \nWe now discuss PCA in greater detail, focusing on the use of PCA as a tool for unsupervised data exploration, in keeping with the topic of this chapter.', '### 6.3.1 Principal Components Regression  \nPrincipal components analysis (PCA) is a popular approach for deriving  \na low-dimensional set of features from a large set of variables. PCA is discussed in greater detail as a tool for unsupervised learning in Chapter 12. Here we describe its use as a dimension reduction technique for regression.\nAn Overview of Principal Components Analysis  \nPCA is a technique for reducing the dimension of an n × p data matrix X . The first principal component direction of the data is that along which the observations vary the most . For instance, consider Figure 6.14, which shows population size ( pop ) in tens of thousands of people, and ad spending for a particular company ( ad ) in thousands of dollars, for 100 cities. 6 The green solid line represents the first principal component direction of the data. We can see by eye that this is the direction along which there is the greatest variability in the data. That is, if we projected the 100 observations onto this line (as shown in the left-hand panel of Figure 6.15), then the resulting projected observations would have the largest possible variance; projecting the observations onto any other line would yield projected observations with lower variance. Projecting a point onto a line simply involves finding the location on the line which is closest to the point.  \nThe first principal component is displayed graphically in Figure 6.14, but how can it be summarized mathematically? It is given by the formula  \n$$Z _ { 1 } = 0 . 8 3 9 \\times ( \\text {pop} - \\overline { \\text {pop} } ) + 0 . 5 4 4 \\times ( \\text {ad} - \\overline { \\text {ad} } ) .$$  \nHere φ 11 = 0   839 and φ 21 = 0   544 are the principal component loadings, which define the direction referred to above. In (6.19), pop indicates the mean of all pop values in this data set, and ad indicates the mean of all advertising spending. The idea is that out of every possible linear combination of pop and ad such that φ 2 11 + φ 2 21 = 1 , this particular linear combination yields the highest variance: i.e. this is the linear combination for which Var( φ 11 × ( pop -pop ) + φ 21 × ( ad -ad )) is maximized. It is necessary to consider only linear combinations of the form φ 2 11 + φ 2 21 = 1 , since otherwise we could increase φ 11 and φ 21 arbitrarily in order to blow up the variance. In (6.19), the two loadings are both positive and have similar size, and so Z 1 is almost an average of the two variables.  \nSince n = 100 , pop and ad are vectors of length 100 , and so is Z 1 in (6.19). For instance,  \n$$z _ { i 1 } = 0 . 8 3 9 \\times ( \\text {pop} _ { i } - \\overline { \\text {pop} } ) + 0 . 5 4 4 \\times ( \\text {ad} _ { i } - \\overline { \\text {ad} } ) .$$  \nThe values of z 11         z n 1 are known as the principal component scores , and can be seen in the right-hand panel of Figure 6.15.  \nThere is also another interpretation of PCA: the first principal component vector defines the line that is as close as possible to the data. For instance, in Figure 6.14, the first principal component line minimizes the sum of the squared perpendicular distances between each point and the line. These distances are plotted as dashed line segments in the left-hand  \n6 This dataset is distinct from the Advertising data discussed in Chapter 3.  \nFIGURE 6.15. A subset of the advertising data. The mean pop and ad budgets are indicated with a blue circle. Left: The first principal component direction is shown in green. It is the dimension along which the data vary the most, and it also defines the line that is closest to all n of the observations. The distances from each observation to the principal component are represented using the black dashed line segments. The blue dot represents ( pop  ad ) . Right: The left-hand panel has been rotated so that the first principal component direction coincides with the x-axis.  \n<!-- image -->  \npanel of Figure 6.15, in which the crosses represent the projection of each point onto the first principal component line. The first principal component has been chosen so that the projected observations are as close as possible to the original observations.  \nWe can think of the values of the principal component Z 1 as singlenumber summaries of the joint pop and ad budgets for each location. In this example, if z i 1 = 0   839 × ( pop i -pop ) + 0   544 × ( ad i -ad ) &lt; 0 , then this indicates a city with below-average population size and belowaverage ad spending. A positive score suggests the opposite. How well can a single number represent both pop and ad ? In this case, Figure 6.14 indicates that pop and ad have approximately a linear relationship, and so we might expect that a single-number summary will work well. Figure 6.16 displays z i 1 versus both pop and ad . 7 The plots show a strong relationship between the first principal component and the two features. In other words, the first principal component appears to capture most of the information contained in the pop and ad predictors.  \nIn the right-hand panel of Figure 6.15, the left-hand panel has been rotated so that the first principal component direction coincides with the x -axis. It is possible to show that the first principal component score for the i th observation, given in (6.20), is the distance in the x -direction of the i th cross from zero. So for example, the point in the bottom-left corner of the left-hand panel of Figure 6.15 has a large negative principal component score, z i 1 = -26   1 , while the point in the top-right corner has a large positive score, z i 1 = 18   7 . These scores can be computed directly using (6.20).  \nSo far we have concentrated on the first principal component. In general, one can construct up to p distinct principal components. The second  \n7 The principal components were calculated after first standardizing both pop and ad , a common approach. Hence, the x-axes on Figures 6.15 and 6.16 are not on the same scale.  \nFIGURE 6.16. Plots of the first principal component scores z i 1 versus pop and ad . The relationships are strong.  \n<!-- image -->  \nprincipal component Z 2 is a linear combination of the variables that is uncorrelated with Z 1 , and has largest variance subject to this constraint. The second principal component direction is illustrated as a dashed blue line in Figure 6.14. It turns out that the zero correlation condition of Z 1 with Z 2 is equivalent to the condition that the direction must be perpendicular , or orthogonal , to the first principal component direction. The second principal component is given by the formula  \n$$Z _ { 2 } = 0 . 5 4 4 \\times ( \\text {pop} - \\overline { \\text {pop} } ) - 0 . 8 3 9 \\times ( \\text {ad} - \\overline { \\text {ad} } ) .$$  \nSince the advertising data has two predictors, the first two principal components contain all of the information that is in pop and ad . However, by construction, the first component will contain the most information. Consider, for example, the much larger variability of z i 1 (the x -axis) versus z i 2 (the y -axis) in the right-hand panel of Figure 6.15. The fact that the second principal component scores are much closer to zero indicates that this component captures far less information. As another illustration, Figure 6.17 displays z i 2 versus pop and ad . There is little relationship between the second principal component and these two predictors, again suggesting that in this case, one only needs the first principal component in order to accurately represent the pop and ad budgets.  \nWith two-dimensional data, such as in our advertising example, we can construct at most two principal components. However, if we had other predictors, such as population age, income level, education, and so forth, then additional components could be constructed. They would successively maximize variance, subject to the constraint of being uncorrelated with the preceding components.\nThe Principal Components Regression Approach  \nThe principal components regression (PCR) approach involves constructing the first M principal components, Z 1         Z M , and then using these components as the predictors in a linear regression model that is fit using least squares. The key idea is that often a small number of principal components suffice to explain most of the variability in the data, as well as the relationship with the response. In other words, we assume that the perpendicular orthogonal  \nprincipal components regression  \nFIGURE 6.17. Plots of the second principal component scores z i 2 versus pop and ad . The relationships are weak.  \n<!-- image -->  \nFIGURE 6.18. PCR was applied to two simulated data sets. In each panel, the horizontal dashed line represents the irreducible error. Left: Simulated data from Figure 6.8. Right: Simulated data from Figure 6.9.  \n<!-- image -->  \ndirections in which X 1         X p show the most variation are the directions that are associated with Y . While this assumption is not guaranteed to be true, it often turns out to be a reasonable enough approximation to give good results.  \nFigure 6.18 displays the PCR fits on the simulated data sets from Figures 6.8 and 6.9. Recall that both data sets were generated using n = 50 observations and p = 45 predictors. However, while the response in the first data set was a function of all the predictors, the response in the second data set was generated using only two of the predictors. The curves are plotted as a function of M , the number of principal components used as predictors in the regression model. As more principal components are used  \nIf the assumption underlying PCR holds, then fitting a least squares model to Z 1         Z M will lead to better results than fitting a least squares model to X 1         X p , since most or all of the information in the data that relates to the response is contained in Z 1         Z M , and by estimating only M /lessmuch p coefficients we can mitigate overfitting. In the advertising data, the first principal component explains most of the variance in both pop and ad , so a principal component regression that uses this single variable to predict some response of interest, such as sales , will likely perform quite well.  \nFIGURE 6.19. PCR, ridge regression, and the lasso were applied to a simulated data set in which the first five principal components of X contain all the information about the response Y . In each panel, the irreducible error Var( /epsilon1 ) is shown as a horizontal dashed line. Left: Results for PCR. Right: Results for lasso (solid) and ridge regression (dotted). The x -axis displays the shrinkage factor of the coefficient estimates, defined as the /lscript 2 norm of the shrunken coefficient estimates divided by the /lscript 2 norm of the least squares estimate.  \n<!-- image -->  \nin the regression model, the bias decreases, but the variance increases. This results in a typical U-shape for the mean squared error. When M = p = 45 , then PCR amounts simply to a least squares fit using all of the original predictors. The figure indicates that performing PCR with an appropriate choice of M can result in a substantial improvement over least squares, especially in the left-hand panel. However, by examining the ridge regression and lasso results in Figures 6.5, 6.8, and 6.9, we see that PCR does not perform as well as the two shrinkage methods in this example.  \nWe note that even though PCR provides a simple way to perform regression using M &lt; p predictors, it is not a feature selection method. This is because each of the M principal components used in the regression is a linear combination of all p of the original features. For instance, in (6.19), Z 1 was a linear combination of both pop and ad . Therefore, while PCR often performs quite well in many practical settings, it does not result in the  \nThe relatively worse performance of PCR in Figure 6.18 is a consequence of the fact that the data were generated in such a way that many principal components are required in order to adequately model the response. In contrast, PCR will tend to do well in cases when the first few principal components are sufficient to capture most of the variation in the predictors as well as the relationship with the response. The left-hand panel of Figure 6.19 illustrates the results from another simulated data set designed to be more favorable to PCR. Here the response was generated in such a way that it depends exclusively on the first five principal components. Now the bias drops to zero rapidly as M , the number of principal components used in PCR, increases. The mean squared error displays a clear minimum at M = 5 . The right-hand panel of Figure 6.19 displays the results on these data using ridge regression and the lasso. All three methods offer a significant improvement over least squares. However, PCR and ridge regression slightly outperform the lasso.  \nFIGURE 6.20. Left: PCR standardized coefficient estimates on the Credit data set for different values of M . Right: The ten-fold cross-validation MSE obtained using PCR, as a function of M .  \n<!-- image -->  \ndevelopment of a model that relies upon a small set of the original features. In this sense, PCR is more closely related to ridge regression than to the lasso. In fact, one can show that PCR and ridge regression are very closely related. One can even think of ridge regression as a continuous version of PCR! 8  \nWhen performing PCR, we generally recommend standardizing each predictor, using (6.6), prior to generating the principal components. This standardization ensures that all variables are on the same scale. In the absence of standardization, the high-variance variables will tend to play a larger role in the principal components obtained, and the scale on which the variables are measured will ultimately have an effect on the final PCR model. However, if the variables are all measured in the same units (say, kilograms, or inches), then one might choose not to standardize them.  \nIn PCR, the number of principal components, M , is typically chosen by cross-validation. The results of applying PCR to the Credit data set are shown in Figure 6.20; the right-hand panel displays the cross-validation errors obtained, as a function of M . On these data, the lowest cross-validation error occurs when there are M = 10 components; this corresponds to almost no dimension reduction at all, since PCR with M = 11 is equivalent to simply performing least squares.']","Principal Component Analysis (PCA) and clustering are both techniques used for simplifying and analyzing data, but they serve different purposes and operate through different mechanisms:

1. **Objective**:
   - **PCA**: The primary goal of PCA is to reduce the dimensionality of a data set by finding a smaller number of uncorrelated variables, called principal components, that explain most of the variance in the original data. It is used to summarize and visualize data.
   - **Clustering**: The goal of clustering is to partition a data set into distinct groups or clusters such that observations within each group are similar to each other, while observations in different groups are dissimilar. It is used to discover structure in the data.

2. **Approach**:
   - **PCA**: PCA is an unsupervised linear transformation technique that identifies the directions (principal components) in which the data varies the most. It projects the data onto these directions to reduce dimensionality.
   - **Clustering**: Clustering involves grouping data points based on a measure of similarity or distance. Common clustering methods include K-means clustering, which partitions data into a pre-specified number of clusters, and hierarchical clustering, which creates a tree-like structure of clusters.

3. **Output**:
   - **PCA**: The output of PCA is a set of principal components, which are linear combinations of the original variables. These components can be used for data visualization, dimensionality reduction, or as inputs to other models.
   - **Clustering**: The output of clustering is a set of clusters, where each data point is assigned to one cluster. This can be used for identifying patterns, segmenting data, or further analysis.

4. **Use Cases**:
   - **PCA**: Often used for data visualization, noise reduction, and as a preprocessing step for other machine learning algorithms.
   - **Clustering**: Used in market segmentation, image segmentation, social network analysis, and any scenario where discovering natural groupings in data is beneficial.

In summary, while both PCA and clustering are used to simplify data, PCA focuses on reducing dimensionality by capturing variance, whereas clustering focuses on grouping similar observations together.","PCA is a dimensionality reduction technique that aims to find a low-dimensional representation of the data that captures the maximum amount of variance. Clustering, on the other hand, is an unsupervised learning method that aims to group similar data points together into clusters based on their features.",1.0,,0.8333333332916666,1.0
What is the main advantage of hierarchical clustering over K-means clustering?,"['## 12.4 Clustering Methods  \nClustering refers to a very broad set of techniques for finding subgroups , or clusters , in a data set. When we cluster the observations of a data set, we seek to partition them into distinct groups so that the observations within each group are quite similar to each other, while observations in different groups are quite different from each other. Of course, to make this concrete, we must define what it means for two or more observations to be similar or different . Indeed, this is often a domain-specific consideration that must be made based on knowledge of the data being studied.  \nBoth clustering and PCA seek to simplify the data via a small number of summaries, but their mechanisms are different:  \nFor instance, suppose that we have a set of n observations, each with p features. The n observations could correspond to tissue samples for patients with breast cancer, and the p features could correspond to measurements collected for each tissue sample; these could be clinical measurements, such as tumor stage or grade, or they could be gene expression measurements. We may have a reason to believe that there is some heterogeneity among the n tissue samples; for instance, perhaps there are a few different unknown subtypes of breast cancer. Clustering could be used to find these subgroups. This is an unsupervised problem because we are trying to discover structure-in this case, distinct clusters-on the basis of a data set. The goal in supervised problems, on the other hand, is to try to predict some outcome vector such as survival time or response to drug treatment.  \nclustering  \n- PCA looks to find a low-dimensional representation of the observations that explain a good fraction of the variance;\n- Clustering looks to find homogeneous subgroups among the observations.  \nAnother application of clustering arises in marketing. We may have access to a large number of measurements (e.g. median household income, occupation, distance from nearest urban area, and so forth) for a large number of people. Our goal is to perform market segmentation by identifying subgroups of people who might be more receptive to a particular form of advertising, or more likely to purchase a particular product. The task of performing market segmentation amounts to clustering the people in the data set.  \nIn general, we can cluster observations on the basis of the features in order to identify subgroups among the observations, or we can cluster features on the basis of the observations in order to discover subgroups among the features. In what follows, for simplicity we will discuss clustering observations on the basis of the features, though the converse can be performed by simply transposing the data matrix.  \nSince clustering is popular in many fields, there exist a great number of clustering methods. In this section we focus on perhaps the two best-known clustering approaches: K -means clustering and hierarchical clustering . In K -means clustering, we seek to partition the observations into a pre-specified number of clusters. On the other hand, in hierarchical clustering, we do not know in advance how many clusters we want; in fact, we end up with a tree-like visual representation of the observations, called a dendrogram , that allows us to view at once the clusterings obtained for each possible number of clusters, from 1 to n . There are advantages and disadvantages to each of these clustering approaches, which we highlight in this chapter.', '### 12.4.2 Hierarchical Clustering  \nOne potential disadvantage of K -means clustering is that it requires us to pre-specify the number of clusters K . Hierarchical clustering is an alternative approach which does not require that we commit to a particular choice of K . Hierarchical clustering has an added advantage over K -means clustering in that it results in an attractive tree-based representation of the observations, called a dendrogram .  \nIn this section, we describe bottom-up or agglomerative clustering. This is the most common type of hierarchical clustering, and refers to the fact that a dendrogram (generally depicted as an upside-down tree; see Figure 12.11) is built starting from the leaves and combining clusters up to the trunk. We will begin with a discussion of how to interpret a dendrogram bottom-up agglomerative  \nFIGURE 12.10. Forty-five observations generated in two-dimensional space. In reality there are three distinct classes, shown in separate colors. However, we will treat these class labels as unknown and will seek to cluster the observations in order to discover the classes from the data.  \n<!-- image -->  \nand then discuss how hierarchical clustering is actually performed-that is, how the dendrogram is built.\nInterpreting a Dendrogram  \nWe begin with the simulated data set shown in Figure 12.10, consisting of 45 observations in two-dimensional space. The data were generated from a three-class model; the true class labels for each observation are shown in distinct colors. However, suppose that the data were observed without the class labels, and that we wanted to perform hierarchical clustering of the data. Hierarchical clustering (with complete linkage, to be discussed later) yields the result shown in the left-hand panel of Figure 12.11. How can we interpret this dendrogram?  \nThis highlights a very important point in interpreting dendrograms that is often misunderstood. Consider the left-hand panel of Figure 12.12, which shows a simple dendrogram obtained from hierarchically clustering nine  \nIn the left-hand panel of Figure 12.11, each leaf of the dendrogram represents one of the 45 observations in Figure 12.10. However, as we move up the tree, some leaves begin to fuse into branches. These correspond to observations that are similar to each other. As we move higher up the tree, branches themselves fuse, either with leaves or other branches. The earlier (lower in the tree) fusions occur, the more similar the groups of observations are to each other. On the other hand, observations that fuse later (near the top of the tree) can be quite different. In fact, this statement can be made precise: for any two observations, we can look for the point in the tree where branches containing those two observations are first fused. The height of this fusion, as measured on the vertical axis, indicates how different the two observations are. Thus, observations that fuse at the very bottom of the tree are quite similar to each other, whereas observations that fuse close to the top of the tree will tend to be quite different.  \nFIGURE 12.11. Left: dendrogram obtained from hierarchically clustering the data from Figure 12.10 with complete linkage and Euclidean distance. Center: the dendrogram from the left-hand panel, cut at a height of nine (indicated by the dashed line). This cut results in two distinct clusters, shown in different colors. Right: the dendrogram from the left-hand panel, now cut at a height of five. This cut results in three distinct clusters, shown in different colors. Note that the colors were not used in clustering, but are simply used for display purposes in this figure.  \n<!-- image -->  \nobservations. One can see that observations 5 and 7 are quite similar to each other, since they fuse at the lowest point on the dendrogram. Observations 1 and 6 are also quite similar to each other. However, it is tempting but incorrect to conclude from the figure that observations 9 and 2 are quite similar to each other on the basis that they are located near each other on the dendrogram. In fact, based on the information contained in the dendrogram, observation 9 is no more similar to observation 2 than it is to observations 8  5  and 7 . (This can be seen from the right-hand panel of Figure 12.12, in which the raw data are displayed.) To put it mathematically, there are 2 n -1 possible reorderings of the dendrogram, where n is the number of leaves. This is because at each of the n -1 points where fusions occur, the positions of the two fused branches could be swapped without affecting the meaning of the dendrogram. Therefore, we cannot draw conclusions about the similarity of two observations based on their proximity along the horizontal axis . Rather, we draw conclusions about the similarity of two observations based on the location on the vertical axis where branches containing those two observations first are fused.  \nNow that we understand how to interpret the left-hand panel of Figure 12.11, we can move on to the issue of identifying clusters on the basis of a dendrogram. In order to do this, we make a horizontal cut across the dendrogram, as shown in the center and right-hand panels of Figure 12.11. The distinct sets of observations beneath the cut can be interpreted as clusters. In the center panel of Figure 12.11, cutting the dendrogram at a height of nine results in two clusters, shown in distinct colors. In the right-hand panel, cutting the dendrogram at a height of five results in three clusters. Further cuts can be made as one descends the dendrogram in order to obtain any number of clusters, between 1 (corresponding to no cut) and n  \nFIGURE 12.12. An illustration of how to properly interpret a dendrogram with nine observations in two-dimensional space. Left: a dendrogram generated using Euclidean distance and complete linkage. Observations 5 and 7 are quite similar to each other, as are observations 1 and 6 . However, observation 9 is no more similar to observation 2 than it is to observations 8  5  and 7 , even though observations 9 and 2 are close together in terms of horizontal distance. This is because observations 2  8  5  and 7 all fuse with observation 9 at the same height, approximately 1   8 . Right: the raw data used to generate the dendrogram can be used to confirm that indeed, observation 9 is no more similar to observation 2 than it is to observations 8  5  and 7 .  \n<!-- image -->  \n(corresponding to a cut at height 0, so that each observation is in its own cluster). In other words, the height of the cut to the dendrogram serves the same role as the K in K -means clustering: it controls the number of clusters obtained.  \nThe term hierarchical refers to the fact that clusters obtained by cutting the dendrogram at a given height are necessarily nested within the clusters obtained by cutting the dendrogram at any greater height. However, on an arbitrary data set, this assumption of hierarchical structure might be unrealistic. For instance, suppose that our observations correspond to a group of men and women, evenly split among Americans, Japanese, and French. We can imagine a scenario in which the best division into two groups might split these people by gender, and the best division into three groups might split them by nationality. In this case, the true clusters are not nested, in the sense that the best division into three groups does not result from taking the best division into two groups and splitting up one of those groups. Consequently, this situation could not be well-represented by hierarchical clustering. Due to situations such as this one, hierarchical clustering can sometimes yield worse (i.e. less accurate) results than K -means clustering for a given number of clusters.  \nFigure 12.11 therefore highlights a very attractive aspect of hierarchical clustering: one single dendrogram can be used to obtain any number of clusters. In practice, people often look at the dendrogram and select by eye a sensible number of clusters, based on the heights of the fusion and the number of clusters desired. In the case of Figure 12.11, one might choose to select either two or three clusters. However, often the choice of where to cut the dendrogram is not so clear.\nAlgorithm 12.3 Hierarchical Clustering  \n1. Begin with n observations and a measure (such as Euclidean distance) of all the ( n 2 ) = n ( n -1) glyph[triangleleft] 2 pairwise dissimilarities. Treat each observation as its own cluster.\n2. For i = n n -1         2 :\n3. (a) Examine all pairwise inter-cluster dissimilarities among the i clusters and identify the pair of clusters that are least dissimilar (that is, most similar). Fuse these two clusters. The dissimilarity between these two clusters indicates the height in the dendrogram at which the fusion should be placed.\n4. (b) Compute the new pairwise inter-cluster dissimilarities among the i -1 remaining clusters.\nThe Hierarchical Clustering Algorithm  \nThe hierarchical clustering dendrogram is obtained via an extremely simple algorithm. We begin by defining some sort of dissimilarity measure between each pair of observations. Most often, Euclidean distance is used; we will discuss the choice of dissimilarity measure later in this chapter. The algorithm proceeds iteratively. Starting out at the bottom of the dendrogram, each of the n observations is treated as its own cluster. The two clusters that are most similar to each other are then fused so that there now are n -1 clusters. Next the two clusters that are most similar to each other are fused again, so that there now are n -2 clusters. The algorithm proceeds in this fashion until all of the observations belong to one single cluster, and the dendrogram is complete. Figure 12.13 depicts the first few steps of the algorithm, for the data from Figure 12.12. To summarize, the hierarchical clustering algorithm is given in Algorithm 12.3.  \nThis algorithm seems simple enough, but one issue has not been addressed. Consider the bottom right panel in Figure 12.13. How did we determine that the cluster ¶ 5  7 ♦ should be fused with the cluster ¶ 8 ♦ ? We have a concept of the dissimilarity between pairs of observations, but how do we define the dissimilarity between two clusters if one or both of the clusters contains multiple observations? The concept of dissimilarity between a pair of observations needs to be extended to a pair of groups of observations . This extension is achieved by developing the notion of linkage , which defines the dissimilarity between two groups of observations. The four most common types of linkagecomplete , average , single , and centroid -are briefly described in Table 12.3. Average, complete, and single linkage are most popular among statisticians. Average and complete linkage are generally preferred over single linkage, as they tend to yield more balanced dendrograms. Centroid linkage is often used in genomics, but suffers from a major drawback in that an inversion can occur, whereby two clusters are fused at a height below either of the individual clusters in the dendrogram. This can lead to difficulties in visualization as well as in interpretation of the dendrogram. The dissimilarities computed in Step 2(b)  \nlinkage inversion  \nTABLE 12.3. A summary of the four most commonly-used types of linkage in hierarchical clustering.  \n| Linkage   | Description                                                                                                                                                                                                                                                                                                          |\n|-----------|----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|\n| Complete  | Maximal intercluster dissimilarity. Compute all pairwise dissimilarities between the observations in cluster A and the observations in cluster B, and record the largest of these dis- similarities.                                                                                                                 |\n| Single    | Minimal intercluster dissimilarity. Compute all pairwise dis- similarities between the observations in cluster A and the observations in cluster B, and record the smallest of these dissimilarities. Single linkage can result in extended, trailing clusters in which single observations are fused one-at-a-time. |\n| Average   | Mean intercluster dissimilarity. Compute all pairwise dis- similarities between the observations in cluster A and the observations in cluster B, and record the average of these dissimilarities.                                                                                                                    |\n| Centroid  | Dissimilarity between the centroid for cluster A (a mean vector of length p ) and the centroid for cluster B. Centroid linkage can result in undesirable inversions .                                                                                                                                                |  \nof the hierarchical clustering algorithm will depend on the type of linkage used, as well as on the choice of dissimilarity measure. Hence, the resulting dendrogram typically depends quite strongly on the type of linkage used, as is shown in Figure 12.14.\nChoice of Dissimilarity Measure  \nThus far, the examples in this chapter have used Euclidean distance as the dissimilarity measure. But sometimes other dissimilarity measures might be preferred. For example, correlation-based distance considers two observations to be similar if their features are highly correlated, even though the observed values may be far apart in terms of Euclidean distance. This is an unusual use of correlation, which is normally computed between variables; here it is computed between the observation profiles for each pair of observations. Figure 12.15 illustrates the difference between Euclidean and correlation-based distance. Correlation-based distance focuses on the shapes of observation profiles rather than their magnitudes.  \nFor instance, consider an online retailer interested in clustering shoppers based on their past shopping histories. The goal is to identify subgroups of similar shoppers, so that shoppers within each subgroup can be shown items and advertisements that are particularly likely to interest them. Suppose the data takes the form of a matrix where the rows are the shoppers and the columns are the items available for purchase; the elements of the data matrix indicate the number of times a given shopper has purchased a  \nThe choice of dissimilarity measure is very important, as it has a strong effect on the resulting dendrogram. In general, careful attention should be paid to the type of data being clustered and the scientific question at hand. These considerations should determine what type of dissimilarity measure is used for hierarchical clustering.  \nFIGURE 12.13. An illustration of the first few steps of the hierarchical clustering algorithm, using the data from Figure 12.12, with complete linkage and Euclidean distance. Top Left: initially, there are nine distinct clusters, ¶ 1 ♦  ¶ 2 ♦         ¶ 9 ♦ . Top Right: the two clusters that are closest together, ¶ 5 ♦ and ¶ 7 ♦ , are fused into a single cluster. Bottom Left: the two clusters that are closest together, ¶ 6 ♦ and ¶ 1 ♦ , are fused into a single cluster. Bottom Right: the two clusters that are closest together using complete linkage , ¶ 8 ♦ and the cluster ¶ 5  7 ♦ , are fused into a single cluster.  \n<!-- image -->  \ngiven item (i.e. a 0 if the shopper has never purchased this item, a 1 if the shopper has purchased it once, etc.) What type of dissimilarity measure should be used to cluster the shoppers? If Euclidean distance is used, then shoppers who have bought very few items overall (i.e. infrequent users of the online shopping site) will be clustered together. This may not be desirable. On the other hand, if correlation-based distance is used, then shoppers with similar preferences (e.g. shoppers who have bought items A and B but never items C or D) will be clustered together, even if some shoppers with these preferences are higher-volume shoppers than others. Therefore, for this application, correlation-based distance may be a better choice.  \nIn addition to carefully selecting the dissimilarity measure used, one must also consider whether or not the variables should be scaled to have standard deviation one before the dissimilarity between the observations is computed. To illustrate this point, we continue with the online shopping ex-  \nFIGURE 12.14. Average, complete, and single linkage applied to an example data set. Average and complete linkage tend to yield more balanced clusters.  \n<!-- image -->  \nample just described. Some items may be purchased more frequently than others; for instance, a shopper might buy ten pairs of socks a year, but a computer very rarely. High-frequency purchases like socks therefore tend to have a much larger effect on the inter-shopper dissimilarities, and hence on the clustering ultimately obtained, than rare purchases like computers. This may not be desirable. If the variables are scaled to have standard deviation one before the inter-observation dissimilarities are computed, then each variable will in effect be given equal importance in the hierarchical clustering performed. We might also want to scale the variables to have standard deviation one if they are measured on different scales; otherwise, the choice of units (e.g. centimeters versus kilometers) for a particular variable will greatly affect the dissimilarity measure obtained. It should come as no surprise that whether or not it is a good decision to scale the variables before computing the dissimilarity measure depends on the application at hand. An example is shown in Figure 12.16. We note that the issue of whether or not to scale the variables before performing clustering applies to K -means clustering as well.', ""### 12.4.3 Practical Issues in Clustering  \nClustering can be a very useful tool for data analysis in the unsupervised setting. However, there are a number of issues that arise in performing clustering. We describe some of these issues here.\nSmall Decisions with Big Consequences  \nIn order to perform clustering, some decisions must be made.  \nFIGURE 12.15. Three observations with measurements on 20 variables are shown. Observations 1 and 3 have similar values for each variable and so there is a small Euclidean distance between them. But they are very weakly correlated, so they have a large correlation-based distance. On the other hand, observations 1 and 2 have quite different values for each variable, and so there is a large Euclidean distance between them. But they are highly correlated, so there is a small correlation-based distance between them.  \n<!-- image -->  \n- Should the observations or features first be standardized in some way? For instance, maybe the variables should be scaled to have standard deviation one.\n- In the case of hierarchical clustering,\n- -What dissimilarity measure should be used?\n- -What type of linkage should be used?\n- -Where should we cut the dendrogram in order to obtain clusters?\n- In the case of K -means clustering, how many clusters should we look for in the data?  \nEach of these decisions can have a strong impact on the results obtained. In practice, we try several different choices, and look for the one with the most useful or interpretable solution. With these methods, there is no single right answer-any solution that exposes some interesting aspects of the data should be considered.\nValidating the Clusters Obtained  \nAny time clustering is performed on a data set we will find clusters. But we really want to know whether the clusters that have been found represent true subgroups in the data, or whether they are simply a result of clustering the noise . For instance, if we were to obtain an independent set of observations, then would those observations also display the same set of clusters? This is a hard question to answer. There exist a number of techniques for assigning a p-value to a cluster in order to assess whether there is more  \nFIGURE 12.16. An eclectic online retailer sells two items: socks and computers. Left: the number of pairs of socks, and computers, purchased by eight online shoppers is displayed. Each shopper is shown in a different color. If inter-observation dissimilarities are computed using Euclidean distance on the raw variables, then the number of socks purchased by an individual will drive the dissimilarities obtained, and the number of computers purchased will have little effect. This might be undesirable, since (1) computers are more expensive than socks and so the online retailer may be more interested in encouraging shoppers to buy computers than socks, and (2) a large difference in the number of socks purchased by two shoppers may be less informative about the shoppers' overall shopping preferences than a small difference in the number of computers purchased. Center: the same data are shown, after scaling each variable by its standard deviation. Now the two products will have a comparable effect on the inter-observation dissimilarities obtained. Right: the same data are displayed, but now the y -axis represents the number of dollars spent by each online shopper on socks and on computers. Since computers are much more expensive than socks, now computer purchase history will drive the inter-observation dissimilarities obtained.  \n<!-- image -->  \nevidence for the cluster than one would expect due to chance. However, there has been no consensus on a single best approach. More details can be found in ESL. 8\nOther Considerations in Clustering  \nBoth K -means and hierarchical clustering will assign each observation to a cluster. However, sometimes this might not be appropriate. For instance, suppose that most of the observations truly belong to a small number of (unknown) subgroups, and a small subset of the observations are quite different from each other and from all other observations. Then since K -means and hierarchical clustering force every observation into a cluster, the clusters found may be heavily distorted due to the presence of outliers that do not belong to any cluster. Mixture models are an attractive approach for accommodating the presence of such outliers. These amount to a soft version of K -means clustering, and are described in ESL.  \n8 ESL: The Elements of Statistical Learning by Hastie, Tibshirani and Friedman.  \nIn addition, clustering methods generally are not very robust to perturbations to the data. For instance, suppose that we cluster n observations, and then cluster the observations again after removing a subset of the n observations at random. One would hope that the two sets of clusters obtained would be quite similar, but often this is not the case!\nA Tempered Approach to Interpreting the Results of Clustering  \nWe have described some of the issues associated with clustering. However, clustering can be a very useful and valid statistical tool if used properly. We mentioned that small decisions in how clustering is performed, such as how the data are standardized and what type of linkage is used, can have a large effect on the results. Therefore, we recommend performing clustering with different choices of these parameters, and looking at the full set of results in order to see what patterns consistently emerge. Since clustering can be non-robust, we recommend clustering subsets of the data in order to get a sense of the robustness of the clusters obtained. Most importantly, we must be careful about how the results of a clustering analysis are reported. These results should not be taken as the absolute truth about a data set. Rather, they should constitute a starting point for the development of a scientific hypothesis and further study, preferably on an independent data set.""]","The main advantage of hierarchical clustering over K-means clustering is that hierarchical clustering does not require the pre-specification of the number of clusters \( K \). Additionally, hierarchical clustering results in an attractive tree-based representation of the observations, called a dendrogram, which allows for the visualization of clusterings obtained for each possible number of clusters, from 1 to \( n \). This provides flexibility in exploring the data structure without committing to a specific number of clusters upfront.","The main advantage of hierarchical clustering is that it does not require the number of clusters (K) to be specified in advance. It produces a dendrogram, which is a tree-like diagram that allows for the visualization of the data's hierarchical structure and helps in choosing the number of clusters after the fact.",1.0,,0.9999999999666667,1.0
Explain the concept of a 'scree plot' and how it is used in PCA.,"[""### 12.2.4 More on PCA  \nScaling the Variables  \nWe have already mentioned that before PCA is performed, the variables should be centered to have mean zero. Furthermore, the results obtained when we perform PCA will also depend on whether the variables have been individually scaled (each multiplied by a different constant). This is in contrast to some other supervised and unsupervised learning techniques, such as linear regression, in which scaling the variables has no effect. (In linear regression, multiplying a variable by a factor of c will simply lead to multiplication of the corresponding coefficient estimate by a factor of 1 glyph[triangleleft]c , and thus will have no substantive effect on the model obtained.)  \nFor instance, Figure 12.1 was obtained after scaling each of the variables to have standard deviation one. This is reproduced in the left-hand plot in Figure 12.4. Why does it matter that we scaled the variables? In these data, the variables are measured in different units; Murder , Rape , and Assault are reported as the number of occurrences per 100  000 people, and UrbanPop is the percentage of the state's population that lives in an urban area. These four variables have variances of 18   97 , 87   73 , 6945   16 , and 209   5 , respectively. Consequently, if we perform PCA on the unscaled variables, then scree plot  \nFIGURE 12.4. Two principal component biplots for the USArrests data. Left: the same as Figure 12.1, with the variables scaled to have unit standard deviations. Right: principal components using unscaled data. Assault has by far the largest loading on the first principal component because it has the highest variance among the four variables. In general, scaling the variables to have standard deviation one is recommended.  \n<!-- image -->  \nthe first principal component loading vector will have a very large loading for Assault , since that variable has by far the highest variance. The righthand plot in Figure 12.4 displays the first two principal components for the USArrests data set, without scaling the variables to have standard deviation one. As predicted, the first principal component loading vector places almost all of its weight on Assault , while the second principal component loading vector places almost all of its weight on UrbanPop . Comparing this to the left-hand plot, we see that scaling does indeed have a substantial effect on the results obtained.  \nIn certain settings, however, the variables may be measured in the same units. In this case, we might not wish to scale the variables to have standard deviation one before performing PCA. For instance, suppose that the variables in a given data set correspond to expression levels for p genes. Then since expression is measured in the same 'units' for each gene, we might choose not to scale the genes to each have standard deviation one.  \nHowever, this result is simply a consequence of the scales on which the variables were measured. For instance, if Assault were measured in units of the number of occurrences per 100 people (rather than number of occurrences per 100  000 people), then this would amount to dividing all of the elements of that variable by 1  000 . Then the variance of the variable would be tiny, and so the first principal component loading vector would have a very small value for that variable. Because it is undesirable for the principal components obtained to depend on an arbitrary choice of scaling, we typically scale each variable to have standard deviation one before we perform PCA.\nUniqueness of the Principal Components  \nWhile in theory the principal components need not be unique, in almost all practical settings they are (up to sign flips). This means that two different software packages will yield the same principal component loading vectors, although the signs of those loading vectors may differ. The signs may differ because each principal component loading vector specifies a direction in p -dimensional space: flipping the sign has no effect as the direction does not change. (Consider Figure 6.14-the principal component loading vector is a line that extends in either direction, and flipping its sign would have no effect.) Similarly, the score vectors are unique up to a sign flip, since the variance of Z is the same as the variance of -Z . It is worth noting that when we use (12.5) to approximate x ij we multiply z im by φ jm . Hence, if the sign is flipped on both the loading and score vectors, the final product of the two quantities is unchanged.\nDeciding How Many Principal Components to Use  \nIn general, an n × p data matrix X has min( n -1  p ) distinct principal components. However, we usually are not interested in all of them; rather, we would like to use just the first few principal components in order to visualize or interpret the data. In fact, we would like to use the smallest number of principal components required to get a good understanding of the data. How many principal components are needed? Unfortunately, there is no single (or simple!) answer to this question.  \nHowever, this type of visual analysis is inherently ad hoc . Unfortunately, there is no well-accepted objective way to decide how many principal components are enough . In fact, the question of how many principal components are enough is inherently ill-defined, and will depend on the specific area of application and the specific data set. In practice, we tend to look at the first few principal components in order to find interesting patterns in the data. If no interesting patterns are found in the first few principal components, then further principal components are unlikely to be of interest. Conversely, if the first few principal components are interesting, then we typically continue to look at subsequent principal components until no further interesting patterns are found. This is admittedly a subjective ap-  \nWe typically decide on the number of principal components required to visualize the data by examining a scree plot , such as the one shown in the left-hand panel of Figure 12.3. We choose the smallest number of principal components that are required in order to explain a sizable amount of the variation in the data. This is done by eyeballing the scree plot, and looking for a point at which the proportion of variance explained by each subsequent principal component drops off. This drop is often referred to as an elbow in the scree plot. For instance, by inspection of Figure 12.3, one might conclude that a fair amount of variance is explained by the first two principal components, and that there is an elbow after the second component. After all, the third principal component explains less than ten percent of the variance in the data, and the fourth principal component explains less than half that and so is essentially worthless.  \nproach, and is reflective of the fact that PCA is generally used as a tool for exploratory data analysis.  \nOn the other hand, if we compute principal components for use in a supervised analysis, such as the principal components regression presented in Section 6.3.1, then there is a simple and objective way to determine how many principal components to use: we can treat the number of principal component score vectors to be used in the regression as a tuning parameter to be selected via cross-validation or a related approach. The comparative simplicity of selecting the number of principal components for a supervised analysis is one manifestation of the fact that supervised analyses tend to be more clearly defined and more objectively evaluated than unsupervised analyses."", '## 12.2 Principal Components Analysis  \nPrincipal components are discussed in Section 6.3.1 in the context of principal components regression. When faced with a large set of correlated variables, principal components allow us to summarize this set with a smaller number of representative variables that collectively explain most of the variability in the original set. The principal component directions are presented in Section 6.3.1 as directions in feature space along which the original data are highly variable . These directions also define lines and subspaces that are as close as possible to the data cloud. To perform principal components regression, we simply use principal components as predictors in a regression model in place of the original larger set of variables.  \nPrincipal components analysis (PCA) refers to the process by which principal components are computed, and the subsequent use of these components in understanding the data. PCA is an unsupervised approach, since it involves only a set of features X 1  X 2         X p , and no associated response Y . Apart from producing derived variables for use in supervised learning problems, PCA also serves as a tool for data visualization (visualization of exploratory data analysis  \nprincipal components analysis  \nthe observations or visualization of the variables). It can also be used as a tool for data imputation - that is, for filling in missing values in a data matrix.  \nWe now discuss PCA in greater detail, focusing on the use of PCA as a tool for unsupervised data exploration, in keeping with the topic of this chapter.', '### 12.2.3 The Proportion of Variance Explained  \nIn Figure 12.2, we performed PCA on a three-dimensional data set (lefthand panel) and projected the data onto the first two principal component loading vectors in order to obtain a two-dimensional view of the data (i.e. the principal component score vectors; right-hand panel). We see that this two-dimensional representation of the three-dimensional data does successfully capture the major pattern in the data: the orange, green, and cyan observations that are near each other in three-dimensional space remain nearby in the two-dimensional representation. Similarly, we have seen on the USArrests data set that we can summarize the 50 observations and 4 variables using just the first two principal component score vectors and the first two principal component loading vectors.  \nWe can now ask a natural question: how much of the information in a given data set is lost by projecting the observations onto the first few principal components? That is, how much of the variance in the data is not contained in the first few principal components? More generally, we are interested in knowing the proportion of variance explained (PVE) by each  \n4 Technically, the solution to (12.6) is not unique. Thus, it is more precise to state that any solution to (12.6) can be easily transformed to yield the principal components.  \nprincipal component. The total variance present in a data set (assuming that the variables have been centered to have mean zero) is defined as  \n$$\\sum _ { j = 1 } ^ { p } \\text {Var} ( X _ { j } ) = \\sum _ { j = 1 } ^ { p } \\frac { 1 } { n } \\sum _ { i = 1 } ^ { n } x _ { i j } ^ { 2 } ,$$  \nand the variance explained by the m th principal component is  \n$$\\frac { 1 } { n } \\sum _ { i = 1 } ^ { n } z _ { i m } ^ { 2 } = \\frac { 1 } { n } \\sum _ { i = 1 } ^ { n } \\left ( \\sum _ { j = 1 } ^ { p } \\phi _ { j m } x _ { i j } \\right ) ^ { 2 } \\, . \\\\ \\intertext { t h e P V e f o r $ t h e m t h r i n c i p l a n c o m p e n t $ i s given by }$$  \nTherefore, the PVE of the m th principal component is given by  \n$$\\frac { \\sum _ { i = 1 } ^ { n } z _ { i m } ^ { 2 } } { \\sum _ { j = 1 } ^ { p } \\sum _ { i = 1 } ^ { m } x _ { i j } ^ { 2 } } = \\frac { \\sum _ { i = 1 } ^ { n } \\left ( \\sum _ { j = 1 } ^ { p } \\phi _ { j m } x _ { i j } \\right ) ^ { 2 } } { \\sum _ { j = 1 } ^ { p } \\sum _ { i = 1 } ^ { n } x _ { i j } ^ { 2 } } .$$  \nIn Section 12.2.2, we showed that the first M principal component loading and score vectors can be interpreted as the best M -dimensional approximation to the data, in terms of residual sum of squares. It turns out that the variance of the data can be decomposed into the variance of the first M principal components plus the mean squared error of this M -dimensional approximation, as follows:  \nThe PVE of each principal component is a positive quantity. In order to compute the cumulative PVE of the first M principal components, we can simply sum (12.10) over each of the first M PVEs. In total, there are min( n -1  p ) principal components, and their PVEs sum to one.  \n$$\\begin{array} { r l } & { \\text {approximation} , \\, a s \\, i n o w s . } \\\\ & { \\underbrace { \\sum _ { j = 1 } ^ { p } \\frac { 1 } { n } \\sum _ { i = 1 } ^ { n } x _ { i j } ^ { 2 } = \\sum _ { m = 1 } ^ { M } \\frac { 1 } { n } \\sum _ { i = 1 } ^ { n } z _ { i m } ^ { 2 } } _ { V a r . o f i t a r { M P C S } } \\underbrace { + \\frac { 1 } { n } \\sum _ { j = 1 } ^ { p } \\sum _ { i = 1 } ^ { n } \\left ( x _ { i j } - \\sum _ { m = 1 } ^ { M } z _ { i m } \\phi _ { j m } \\right ) } _ { M S E E } \\quad ( 1 2 . 1 1 ) } \\\\ & { \\underbrace { T h e r e t h e r e m s i n c h o d i s o n i t i o n a r e d i s u c h e s s e d i n v e r s . } _ { ( 1 2 . 8 ) , ( 1 2 . 9 ) , a n d } \\quad ( 1 2 . 1 8 ) , \\, ( 1 2 . 9 ) , \\, a n d } \\end{array}$$  \nMoreover, we can use (12.11) to see that the PVE defined in (12.10) equals  \nThe three terms in this decomposition are discussed in (12.8), (12.9), and (12.7), respectively. Since the first term is fixed, we see that by maximizing the variance of the first M principal components, we minimize the mean squared error of the M -dimensional approximation, and vice versa. This explains why principal components can be equivalently viewed as minimizing the approximation error (as in Section 12.2.2) or maximizing the variance (as in Section 12.2.1).  \n$$1 - \\frac { \\sum _ { j = 1 } ^ { p } \\sum _ { i = 1 } ^ { n } \\left ( x _ { i j } - \\sum _ { m = 1 } ^ { M } z _ { i m } \\phi _ { j m } \\right ) ^ { 2 } } { \\sum _ { j = 1 } ^ { p } \\sum _ { i = 1 } ^ { n } x _ { i j } ^ { 2 } } = 1 - \\frac { R S S } { T S S } , \\\\ \\intertext { h e r s t s r e p r e s e n t s the t o tal sum of s q u a r e d e l e m e t s of X , a n d R S S } \\intertext { p e r s u s s e r d e c t i o n s u s s e r f o r s u s s e r d e c t i o n a n d r a p r o w i m a t i o n }$$  \nwhere TSS represents the total sum of squared elements of X , and RSS represents the residual sum of squares of the M -dimensional approximation given by the principal components. Recalling the definition of R 2 from (3.17), this means that we can interpret the PVE as the R 2 of the approximation for X given by the first M principal components.  \nFIGURE 12.3. Left: a scree plot depicting the proportion of variance explained by each of the four principal components in the USArrests data. Right: the cumulative proportion of variance explained by the four principal components in the USArrests data.  \n<!-- image -->  \nIn the USArrests data, the first principal component explains 62.0 % of the variance in the data, and the next principal component explains 24.7 % of the variance. Together, the first two principal components explain almost 87 % of the variance in the data, and the last two principal components explain only 13 % of the variance. This means that Figure 12.1 provides a pretty accurate summary of the data using just two dimensions. The PVE of each principal component, as well as the cumulative PVE, is shown in Figure 12.3. The left-hand panel is known as a scree plot , and will be discussed later in this chapter.']","A scree plot is a graphical representation used in Principal Component Analysis (PCA) to help determine the number of principal components to retain for analysis. It depicts the proportion of variance explained by each principal component. Here's how it is used:

1. **Plotting the Variance**: The scree plot displays the proportion of variance explained by each principal component on the y-axis, with the principal components themselves on the x-axis.

2. **Identifying the 'Elbow'**: The main purpose of the scree plot is to identify a point where the proportion of variance explained by each subsequent principal component drops off significantly. This point is often referred to as the ""elbow"" of the plot.

3. **Deciding the Number of Components**: By examining the scree plot, one can choose the smallest number of principal components that are required to explain a sizable amount of the variation in the data. The components before the elbow are typically retained, as they explain most of the variance.

4. **Subjective Analysis**: The process of determining the number of components to retain using a scree plot is somewhat subjective, as it involves visually inspecting the plot to find the elbow.

In summary, a scree plot is a useful tool in PCA for visualizing the variance explained by each principal component and aiding in the decision of how many components to retain for further analysis.","A scree plot is a graphical representation of the eigenvalues or the proportion of variance explained (PVE) by each principal component. It is used to determine the optimal number of principal components to retain by identifying the 'elbow' in the plot, which is the point where the PVE drops off significantly.",0.8571428571428571,,0.8333333332916666,1.0
¿Qué significa que los datos están 'censurados' en el análisis de supervivencia?,"['# Contents  \nPreface\nvii\n1\nIntroduction\n1\n2\nStatistical Learning\n15\n2.1\nWhat Is Statistical Learning? . . . . . . . . . . . . . . . . .\n15\n2.1.1\nWhy Estimate f? . . . . . . . . . . . . . . . . . . .\n17\n2.1.2\nHow Do We Estimate f? . . . . . . . . . . . . . . .\n20\n2.1.3\nThe Trade-Off Between Prediction Accuracy\nand Model Interpretability . . . . . . . . . . . . . .\n23\n2.1.4\nSupervised Versus Unsupervised Learning . . . . .\n25\n2.1.5\nRegression Versus Classification Problems . . . . .\n27\n2.2\nAssessing Model Accuracy\n. . . . . . . . . . . . . . . . . .\n27\n2.2.1\nMeasuring the Quality of Fit\n. . . . . . . . . . . .\n28\n2.2.2\nThe Bias-Variance Trade-Off . . . . . . . . . . . . .\n31\n2.2.3\nThe Classification Setting\n. . . . . . . . . . . . . .\n34\n2.3\nLab: Introduction to Python . . . . . . . . . . . . . . . . .\n40\n2.3.1\nGetting Started . . . . . . . . . . . . . . . . . . . .\n40\n2.3.2\nBasic Commands . . . . . . . . . . . . . . . . . . .\n40\n2.3.3\nIntroduction to Numerical Python\n. . . . . . . . .\n42\n2.3.4\nGraphics . . . . . . . . . . . . . . . . . . . . . . . .\n48\n2.3.5\nSequences and Slice Notation\n. . . . . . . . . . . .\n51\n2.3.6\nIndexing Data . . . . . . . . . . . . . . . . . . . . .\n51\n2.3.7\nLoading Data . . . . . . . . . . . . . . . . . . . . .\n55\n2.3.8\nFor Loops . . . . . . . . . . . . . . . . . . . . . . .\n59\n2.3.9\nAdditional Graphical and Numerical Summaries . .\n61\n2.4\nExercises . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n63\n3\nLinear Regression\n69\n3.1\nSimple Linear Regression . . . . . . . . . . . . . . . . . . .\n70\n3.1.1\nEstimating the Coefficients\n. . . . . . . . . . . . .\n71\n3.1.2\nAssessing the Accuracy of the Coefficient\nEstimates . . . . . . . . . . . . . . . . . . . . . . .\n72\n3.1.3\nAssessing the Accuracy of the Model . . . . . . . .\n77\n3.2\nMultiple Linear Regression . . . . . . . . . . . . . . . . . .\n80\n3.2.1\nEstimating the Regression Coefficients . . . . . . .\n81\nix  \n<!-- FIN DEL BLOQUE DE FALLBACK -->  \n4  \n|                    | 3.2.2                                                                                  | Some Important Questions 83                                                             |\n|--------------------|----------------------------------------------------------------------------------------|-----------------------------------------------------------------------------------------|\n| 3.3                | . . . . . . . . . . . . . . Other Considerations in the Regression Model . . . . . . . | 91                                                                                      |\n|                    | 3.3.1                                                                                  | Qualitative Predictors . . . . . . . . . . . . . . . . 91                               |\n|                    | 3.3.2                                                                                  | Extensions of the Linear Model . . . . . . . . . . . 94                                 |\n|                    | 3.3.3                                                                                  | Potential Problems . . . . . . . . 100                                                  |\n| 3.4                | . . . . . . . . . . The Marketing Plan . . . . . . . . . . . . . . . .                 | . . . . . . 109                                                                         |\n| 3.5                | Comparison of Linear Regression with K -Nearest                                        | Comparison of Linear Regression with K -Nearest                                         |\n|                    | Neighbors . . . . . . . . . . . . . . . . . . . . . . . . . . . . 111                  | Neighbors . . . . . . . . . . . . . . . . . . . . . . . . . . . . 111                   |\n| 3.6                | Lab: Linear Regression . . . . . . . . . . . . . . . . . . . .                         | 116                                                                                     |\n|                    | 3.6.1                                                                                  | Importing packages . . . . . . . . . . . . . . . . . . 116                              |\n|                    | 3.6.2                                                                                  | Simple Linear Regression . . . . . . . . . . . . . . . 117                              |\n|                    | 3.6.3                                                                                  | Multiple Linear Regression . . . . . . . . . . . . . . 122 . . . . . . . .              |\n|                    | 3.6.4                                                                                  | Multivariate Goodness of Fit . . . . 123                                                |\n|                    | 3.6.5                                                                                  | Interaction Terms . . . . . . . . . . . . . . . . . . . 124                             |\n|                    | 3.6.6                                                                                  | Non-linear Transformations of the Predictors . . . 125                                  |\n|                    | 3.6.7                                                                                  | Qualitative Predictors . . . . . . . . . . . . . . . . 126                              |\n| 3.7                | Exercises . . . . . . . . . . . . . . . . . . . . . . . . . . . . 127                  | Exercises . . . . . . . . . . . . . . . . . . . . . . . . . . . . 127                   |\n| Classification 135 | Classification 135                                                                     | Classification 135                                                                      |\n| 4.1                | An Overview of Classification . . . . . . . . . . . . . . . . . 135                    | An Overview of Classification . . . . . . . . . . . . . . . . . 135                     |\n| 4.2                | Why Not Linear Regression? . . . . . . . . . . . . . . . . . 136 . 138                 | Why Not Linear Regression? . . . . . . . . . . . . . . . . . 136 . 138                  |\n| 4.3                | Logistic Regression . . . . . . . . . . . . . . . . . . . . . . . . . . . . .          | Logistic Regression . . . . . . . . . . . . . . . . . . . . . . . . . . . . .           |\n|                    | 4.3.1                                                                                  | The Logistic Model . . . . . . . . . . 139                                              |\n|                    | 4.3.2                                                                                  | Estimating the Regression Coefficients . . . . . . . 140                                |\n|                    | 4.3.3                                                                                  | Making Predictions . . . . . . . . . . . . . . . . . . 141                              |\n|                    | 4.3.4                                                                                  | Multiple Logistic Regression . . . . . . . . . . . . . 142                              |\n|                    | 4.3.5                                                                                  | Multinomial Logistic Regression . . . . . . . . . . . 144                               |\n| 4.4                | Generative Models for Classification . . . . . . . . . . . . . 146 . . . . . . . 147   | Generative Models for Classification . . . . . . . . . . . . . 146 . . . . . . . 147    |\n|                    | 4.4.1                                                                                  | Linear Discriminant Analysis for p = 1                                                  |\n|                    | 4.4.2                                                                                  | Linear Discriminant Analysis for p > 1 . . . . . . . 150                                |\n|                    | 4.4.3                                                                                  | Quadratic Discriminant Analysis . . . . . . . . . . 156 . . . . . . . . . . . . . . . . |\n|                    | 4.4.4                                                                                  | Naive Bayes . . . . . . 158                                                             |\n| 4.5                | A Comparison of Classification Methods . . . . . . . . . . . . . . . . . .             | . . 161                                                                                 |\n|                    | 4.5.1                                                                                  | An Analytical Comparison . . . . 161                                                    |\n|                    | 4.5.2                                                                                  | An Empirical Comparison . . . . . . . . . . . . . . 164                                 |\n| 4.6                | Generalized Linear Models . . . . . . . . . . . . . . . . .                            | . 167                                                                                   |\n|                    | 4.6.1                                                                                  | Linear Regression on the Bikeshare Data . . . . . . 167                                 |\n|                    | 4.6.2                                                                                  | Poisson Regression on the Bikeshare Data . . . . . 169                                  |\n|                    | 4.6.3                                                                                  | Generalized Linear Models in Greater Generality . 172                                   |\n| 4.7                | Lab: Logistic Regression, LDA, QDA, and KNN . .                                        | . . . . 173                                                                             |\n|                    | 4.7.1                                                                                  | The Stock Market Data . . . . . . . . . . . . . . . 173                                 |\n|                    | 4.7.2                                                                                  | Logistic Regression . . . . . . . . . . . . . . . . . . 174                             |\n|                    | 4.7.3                                                                                  | Linear Discriminant Analysis . . . . . . . . . . . . 179                                |\n|                    | 4.7.4                                                                                  | Quadratic Discriminant Analysis . . . . . . 181                                         |\n|                    | 4.7.5                                                                                  | . . . . Naive Bayes . . . . . . . . . . . . . . . . . . . . . . 182                     |\n|                    | 4.7.6                                                                                  | K-Nearest Neighbors . . . . . . . . . . . . . . . . . 183                               |\n|                    | 4.7.7                                                                                  | Linear and Poisson Regression on the Bikeshare Data188                                  |\n| 4.8                | Exercises . . . . . . . . . . . . . . . . . . . . . . . . . . .                        | . 193                                                                                   |  \n| 5 Resampling Methods   | 5 Resampling Methods                      | 5 Resampling Methods                                                                                                 | 5 Resampling Methods                                                                                                      | 201         |\n|------------------------|-------------------------------------------|----------------------------------------------------------------------------------------------------------------------|---------------------------------------------------------------------------------------------------------------------------|-------------|\n|                        | 5.1                                       | Cross-Validation . . . .                                                                                             | . . . . . . . . . . . . . . . . . . .                                                                                     | 202         |\n|                        |                                           | 5.1.1 The Validation Set                                                                                             | Approach . . . . . . . . . . .                                                                                            | 202         |\n|                        |                                           | 5.1.2                                                                                                                | Leave-One-Out Cross-Validation . . . . . . . . .                                                                          | 204         |\n|                        |                                           | 5.1.3                                                                                                                | k -Fold Cross-Validation . . . . . . . . . .                                                                              | 206         |\n|                        |                                           | 5.1.4                                                                                                                | . . . . Bias-Variance Trade-Off for k -Fold Cross-Validation . . . . . . . . . . . . . . . . . .                          | 208         |\n|                        |                                           | 5.1.5                                                                                                                | Cross-Validation on Classification Problems . . .                                                                         | 209         |\n|                        | 5.2                                       | The Bootstrap . . . . .                                                                                              | . . . . . . . . . . . . . . . . . . .                                                                                     | 212         |\n|                        | 5.3                                       | Lab: Cross-Validation and the Bootstrap                                                                              | . . . . . . . . .                                                                                                         | 215         |\n|                        |                                           | 5.3.1                                                                                                                | The Validation Set Approach . . . . . . . . . . .                                                                         | 216         |\n|                        |                                           | 5.3.2                                                                                                                | Cross-Validation . . . . . . . . . . . . . . . . . .                                                                      | 217         |\n|                        |                                           | 5.3.3                                                                                                                | The Bootstrap . . . . . . . . . . . . . . . . . . .                                                                       | 220         |\n|                        | 5.4                                       | Exercises . . . . . . . . . . . . . . . . . . . . . . . . . . .                                                      | Exercises . . . . . . . . . . . . . . . . . . . . . . . . . . .                                                           | 224         |\n| 6                      | Linear Model Selection and Regularization | Linear Model Selection and Regularization                                                                            | Linear Model Selection and Regularization                                                                                 | 229         |\n|                        | 6.1                                       | Subset Selection . . . . . . .                                                                                       | . . . . . . . . . . . . . . . .                                                                                           | 231         |\n|                        |                                           | 6.1.1                                                                                                                | Best Subset Selection . . . . . . . . . . . . . . . .                                                                     | 231         |\n|                        |                                           | 6.1.2                                                                                                                | Stepwise Selection . . . . . . . . . . . . . . . . .                                                                      | 233         |\n|                        |                                           | 6.1.3                                                                                                                | Choosing the Optimal Model                                                                                                | 235         |\n|                        | 6.2                                       | . . . . . . . . . . . Shrinkage Methods . . . . . . . . . . . . . . . . . . . . .                                    | . . . . . . . . . . . Shrinkage Methods . . . . . . . . . . . . . . . . . . . . .                                         | 240         |\n|                        |                                           | 6.2.1                                                                                                                | Ridge Regression . . . . . . . . . . . . . . . . . . . .                                                                  | 240         |\n|                        |                                           | 6.2.2                                                                                                                | The Lasso . . . . . . . . . . . . . . . . . . . .                                                                         | 244         |\n|                        |                                           | 6.2.3                                                                                                                | Selecting the Tuning Parameter . . . . . . .                                                                              | 252         |\n|                        | 6.3                                       | . . . . . . . . .                                                                                                    | . . . . . . .                                                                                                             | 253         |\n|                        |                                           | Dimension Reduction Methods . .                                                                                      | Dimension Reduction Methods . .                                                                                           |             |\n|                        |                                           | 6.3.1                                                                                                                | Principal Components Regression . . . . . . . . .                                                                         | 254         |\n|                        |                                           | 6.3.2                                                                                                                | Partial Least Squares . . . . . . . . . . . . . . . .                                                                     | 260         |\n|                        | 6.4                                       | 6.4.1 High-Dimensional                                                                                               | Considerations in High Dimensions . . . . . . . . . . . . Data . . . . . . . . . . . . . . .                              | 262 262     |\n|                        |                                           | 6.4.2                                                                                                                | What Goes Wrong in High Dimensions? . . . . .                                                                             | 263         |\n|                        |                                           | 6.4.3                                                                                                                | . . . . . . . . . .                                                                                                       |             |\n|                        |                                           |                                                                                                                      | Regression in High Dimensions                                                                                             | 265         |\n|                        |                                           | 6.4.4                                                                                                                | Interpreting Results in High Dimensions                                                                                   | 266         |\n|                        | 6.5                                       | . . . . . Lab: Linear Models and Regularization Methods . . . . . 6.5.1                                              | . . . . . . . . . . .                                                                                                     | 267 268     |\n|                        |                                           | 6.5.2                                                                                                                | Subset Selection Methods . . Ridge Regression and the Lasso . . . . . . . . .                                             | 273         |\n|                        |                                           | .                                                                                                                    | . . . . . .                                                                                                               | 283         |\n|                        |                                           | 6.5.3                                                                                                                | PCR and PLS Regression . . . . . . . . . . . . .                                                                          | 280         |\n| 7                      | 6.6                                       | Exercises . . . . . . . . . . . . . . . . . . . . . Moving Beyond Linearity                                          | Exercises . . . . . . . . . . . . . . . . . . . . . Moving Beyond Linearity                                               | 289 290     |\n|                        | 7.1 7.2                                   | Polynomial Regression .                                                                                              | . . . . . . . . . . . . . . . . . . .                                                                                     |             |\n|                        |                                           | Step Functions . . . . . . . . . . . . . . . . . . . . . . . . Basis . . . . . . .                                   | Step Functions . . . . . . . . . . . . . . . . . . . . . . . . Basis . . . . . . .                                        | 292         |\n|                        | 7.3                                       | . . .                                                                                                                | . . .                                                                                                                     | 293         |\n|                        | 7.4                                       | Functions . . . . . . . . . . . . . Regression Splines . . . . . . . . . . . . 7.4.1 Piecewise Polynomials . . . . . | . . . . . . . . . .                                                                                                       | 294         |\n|                        |                                           | 7.4.2 7.4.3                                                                                                          | . . . . . . . . . . Constraints and Splines . . . . . . . . . . . . . . The Spline Basis Representation . . . . . . . . . | 294 296 296 |\n|                        |                                           | 7.4.4                                                                                                                | Choosing the Number and Locations                                                                                         | 297         |\n|                        |                                           |                                                                                                                      | of the Knots . . . . . . . . . . . . . . . . . . . . .                                                                    |             |\n|                        |                                           | 7.4.5                                                                                                                | Comparison to Polynomial Regression . . . . . .                                                                           | 299         |\nxii Contents  \n|     | 7.5 Smoothing Splines . . . . . . . . . . . . . . . . . .                                                     | 7.5 Smoothing Splines . . . . . . . . . . . . . . . . . .                                                     |\n|-----|---------------------------------------------------------------------------------------------------------------|---------------------------------------------------------------------------------------------------------------|\n|     | 7.5.1                                                                                                         | An Overview of Smoothing Splines . . . .                                                                      |\n|     | 7.5.2                                                                                                         | Smoothing Parameter λ . .                                                                                     |\n|     |                                                                                                               | Choosing the                                                                                                  |\n| 7.7 | 7.6 Local                                                                                                     | Regression . . . . . . . . . . . . . . . . . . . Generalized Additive Models . . . . . . . . . . . .          |\n|     | 7.7.1                                                                                                         | GAMs for Regression Problems . . . . . .                                                                      |\n|     | 7.7.2                                                                                                         | GAMs for Classification Problems . . . . .                                                                    |\n| 7.8 | Lab:                                                                                                          | Non-Linear Modeling                                                                                           |\n|     | 7.8.1                                                                                                         | . . . . . . . . . . . . . Polynomial Regression and Step Functions                                            |\n|     | 7.8.2                                                                                                         | Splines . . . . . . . . . . . . . . . . . . . .                                                               |\n|     | 7.8.3                                                                                                         | Smoothing Splines and GAMs . . . . . . . . . .                                                                |\n|     | 7.8.4                                                                                                         | Local Regression . . . . . . .                                                                                |\n|     |                                                                                                               | . . . . . . . . . . . .                                                                                       |\n|     | 7.9 Exercises                                                                                                 | . . . . . .                                                                                                   |\n|     | . . . . . . . . . . .                                                                                         | . . . . . . . . . . .                                                                                         |\n| 8   | Tree-Based Methods                                                                                            | Tree-Based Methods                                                                                            |\n|     | 8.1 The Basics                                                                                                | of Decision Trees . . . . . . . . . . . . . . . . . .                                                         |\n|     | 8.1.1                                                                                                         | Regression Trees . . . . . . . .                                                                              |\n|     | 8.1.2                                                                                                         | Classification Trees . . . . . . . . . . . . .                                                                |\n|     | 8.1.3                                                                                                         | Trees Versus Linear Models . . . . . . . .                                                                    |\n|     | 8.1.4                                                                                                         | Advantages and Disadvantages of Trees . .                                                                     |\n| 8.2 | Bagging, Random Forests, Boosting, and Bayesian Additive Regression . . . . . . . . . . . . . . . . . . . . . | Bagging, Random Forests, Boosting, and Bayesian Additive Regression . . . . . . . . . . . . . . . . . . . . . |\n|     |                                                                                                               | Trees                                                                                                         |\n|     | 8.2.1 .                                                                                                       | Bagging . . . . . . . . . . . . . . . . . .                                                                   |\n|     | 8.2.2                                                                                                         | Random Forests . . . . . . . . . . . . . . .                                                                  |\n|     | 8.2.3 8.2.4                                                                                                   | Boosting . . . . . . . . . . . . . . . . . . . Bayesian Additive Regression Trees . . . .                     |\n| 8.3 | Lab: Tree-Based Methods . . . Fitting Classification Trees                                                    | . . . . . . . . . . . . . . . . . . . .                                                                       |\n|     | Vector 9.1 Maximal                                                                                            | Machines . . . . . .                                                                                          |\n|     | 8.3.1 8.3.5 Bayesian Exercises . . . . .                                                                      | Additive Regression Trees . . . . . . . . . . . . . . . . . . . . . .                                         |\n|     | 8.3.2                                                                                                         | Fitting Regression Trees . . . . . . . . . .                                                                  |\n| 8.4 | 8.3.3 8.3.4                                                                                                   | Bagging and Random Forests . . . . . . . Boosting . . . . . . . . . . . . . . . . . . .                       |\n| 9   |                                                                                                               |                                                                                                               |\n|     | Support                                                                                                       | Margin Classifier . . . . . . .                                                                               |\n|     | 9.1.1                                                                                                         | What Is a Hyperplane? . . . . . . . . . . .                                                                   |\n|     | 9.1.2 9.1.3                                                                                                   | Classification Using a Separating Hyperplane The Maximal Margin Classifier . . . . .                          |\n|     |                                                                                                               | . Classifier                                                                                                  |\n|     |                                                                                                               | Margin                                                                                                        |\n|     | . .                                                                                                           | . . . . . .                                                                                                   |\n|     | 9.1.4                                                                                                         | . . . . . . . .                                                                                               |\n|     |                                                                                                               | Construction of the Maximal The Non-separable Case . . . . . .                                                |\n|     | 9.1.5                                                                                                         |                                                                                                               |\n| 9.2 | Support Vector                                                                                                | Classifiers . .                                                                                               |\n|     | 9.2.1                                                                                                         | Overview of the Support                                                                                       |\n|     | 9.2.2                                                                                                         | of the Support Vector Classifier . .                                                                          |\n| 9.3 | Details Support Vector Machines . . . . . . . . . . . .                                                       | .                                                                                                             |\n|     |                                                                                                               | Vector Classifier .                                                                                           |\n|     | 9.3.1                                                                                                         | Classification with Non-Linear Decision                                                                       |\n|     |                                                                                                               | Boundaries . . . . . . . . . . . . . . . . .                                                                  |\n|     | 9.3.2                                                                                                         | The Support Vector Machine . . . . . . .                                                                      |  \n|                                                                                                                                       | 9.3.3                                                                                                                                 | An Application to the Heart                                                                                                           | Disease                                                                                                                               |                                                                                                                                       | Data .                                                                                                                                | . .                                                                                                                                   | . .                                                                                                                                   | 382                                                                   | 382                                                                   | 382                                                                   |\n|---------------------------------------------------------------------------------------------------------------------------------------|---------------------------------------------------------------------------------------------------------------------------------------|---------------------------------------------------------------------------------------------------------------------------------------|---------------------------------------------------------------------------------------------------------------------------------------|---------------------------------------------------------------------------------------------------------------------------------------|---------------------------------------------------------------------------------------------------------------------------------------|---------------------------------------------------------------------------------------------------------------------------------------|---------------------------------------------------------------------------------------------------------------------------------------|-----------------------------------------------------------------------|-----------------------------------------------------------------------|-----------------------------------------------------------------------|\n| 9.4                                                                                                                                   | SVMs                                                                                                                                  | with More than Two Classes                                                                                                            | . .                                                                                                                                   | . . .                                                                                                                                 | . . . . .                                                                                                                             | . .                                                                                                                                   | . .                                                                                                                                   | 383                                                                   | 383                                                                   | 383                                                                   |\n|                                                                                                                                       | 9.4.1                                                                                                                                 | One-Versus-One Classification                                                                                                         | . .                                                                                                                                   | . . .                                                                                                                                 | . . .                                                                                                                                 | .                                                                                                                                     | . .                                                                                                                                   | 384                                                                   | 384                                                                   | 384                                                                   |\n|                                                                                                                                       | 9.4.2                                                                                                                                 | One-Versus-All Classification                                                                                                         | . . .                                                                                                                                 | . .                                                                                                                                   | . . . .                                                                                                                               | .                                                                                                                                     | . .                                                                                                                                   | 384                                                                   | 384                                                                   | 384                                                                   |\n| 9.5                                                                                                                                   | Relationship                                                                                                                          | to Logistic Regression                                                                                                                | . .                                                                                                                                   | . . . .                                                                                                                               | . . . .                                                                                                                               | .                                                                                                                                     | . .                                                                                                                                   | 384                                                                   | 384                                                                   | 384                                                                   |\n| 9.6                                                                                                                                   | Lab: Support Vector                                                                                                                   | Machines                                                                                                                              | . . . . .                                                                                                                             | . . .                                                                                                                                 | . . . . .                                                                                                                             | .                                                                                                                                     | . .                                                                                                                                   | 387                                                                   | 387                                                                   | 387                                                                   |\n|                                                                                                                                       | 9.6.1                                                                                                                                 | Support Vector Classifier                                                                                                             | . . . .                                                                                                                               | . . .                                                                                                                                 | . . . . .                                                                                                                             | .                                                                                                                                     | . .                                                                                                                                   | 387                                                                   | 387                                                                   | 387                                                                   |\n|                                                                                                                                       | 9.6.2                                                                                                                                 | Support Vector Machine                                                                                                                | . . . .                                                                                                                               | . .                                                                                                                                   | . . . . . .                                                                                                                           | .                                                                                                                                     | . .                                                                                                                                   | 390                                                                   | 390                                                                   | 390                                                                   |\n|                                                                                                                                       | 9.6.3                                                                                                                                 | ROC Curves . . .                                                                                                                      | . . . . .                                                                                                                             | . . .                                                                                                                                 | . . . . . .                                                                                                                           | . . .                                                                                                                                 | . .                                                                                                                                   | 392                                                                   | 392                                                                   | 392                                                                   |\n|                                                                                                                                       | 9.6.4                                                                                                                                 | SVM with Multiple Classes                                                                                                             | . . .                                                                                                                                 | . .                                                                                                                                   | . . . . .                                                                                                                             | .                                                                                                                                     | . .                                                                                                                                   | 393                                                                   | 393                                                                   | 393                                                                   |\n|                                                                                                                                       | 9.6.5                                                                                                                                 | Application to Gene Expression                                                                                                        | Data                                                                                                                                  | . .                                                                                                                                   | . . .                                                                                                                                 | .                                                                                                                                     | . .                                                                                                                                   | 394                                                                   | 394                                                                   | 394                                                                   |\n| 9.7                                                                                                                                   | Exercises . . . . . . . . . . . . . . . . . . . . . . . . . . . . 395                                                                 | Exercises . . . . . . . . . . . . . . . . . . . . . . . . . . . . 395                                                                 | Exercises . . . . . . . . . . . . . . . . . . . . . . . . . . . . 395                                                                 | Exercises . . . . . . . . . . . . . . . . . . . . . . . . . . . . 395                                                                 | Exercises . . . . . . . . . . . . . . . . . . . . . . . . . . . . 395                                                                 | Exercises . . . . . . . . . . . . . . . . . . . . . . . . . . . . 395                                                                 | Exercises . . . . . . . . . . . . . . . . . . . . . . . . . . . . 395                                                                 | Exercises . . . . . . . . . . . . . . . . . . . . . . . . . . . . 395 | Exercises . . . . . . . . . . . . . . . . . . . . . . . . . . . . 395 | Exercises . . . . . . . . . . . . . . . . . . . . . . . . . . . . 395 |\n| 10 Deep Learning                                                                                                                      | 10 Deep Learning                                                                                                                      | 10 Deep Learning                                                                                                                      | 10 Deep Learning                                                                                                                      | 10 Deep Learning                                                                                                                      | 10 Deep Learning                                                                                                                      | 10 Deep Learning                                                                                                                      | 10 Deep Learning                                                                                                                      | 399                                                                   | 399                                                                   | 399                                                                   |\n| 10.1 Single Layer Neural Networks . . . . . . . . . . . . . . . .                                                                     | 10.1 Single Layer Neural Networks . . . . . . . . . . . . . . . .                                                                     | 10.1 Single Layer Neural Networks . . . . . . . . . . . . . . . .                                                                     | 10.1 Single Layer Neural Networks . . . . . . . . . . . . . . . .                                                                     | 10.1 Single Layer Neural Networks . . . . . . . . . . . . . . . .                                                                     | 10.1 Single Layer Neural Networks . . . . . . . . . . . . . . . .                                                                     | 10.1 Single Layer Neural Networks . . . . . . . . . . . . . . . .                                                                     | 10.1 Single Layer Neural Networks . . . . . . . . . . . . . . . .                                                                     | 400                                                                   | 400                                                                   | 400                                                                   |\n| 10.2 Multilayer Neural Networks . . . . . . . . . . . . . . . . . .                                                                   | 10.2 Multilayer Neural Networks . . . . . . . . . . . . . . . . . .                                                                   | 10.2 Multilayer Neural Networks . . . . . . . . . . . . . . . . . .                                                                   | 10.2 Multilayer Neural Networks . . . . . . . . . . . . . . . . . .                                                                   | 10.2 Multilayer Neural Networks . . . . . . . . . . . . . . . . . .                                                                   | 10.2 Multilayer Neural Networks . . . . . . . . . . . . . . . . . .                                                                   | 10.2 Multilayer Neural Networks . . . . . . . . . . . . . . . . . .                                                                   | 10.2 Multilayer Neural Networks . . . . . . . . . . . . . . . . . .                                                                   | 402                                                                   | 402                                                                   | 402                                                                   |\n| 10.3 Convolutional Neural Networks . . . . . . . . . . . . . . . .                                                                    | 10.3 Convolutional Neural Networks . . . . . . . . . . . . . . . .                                                                    | 10.3 Convolutional Neural Networks . . . . . . . . . . . . . . . .                                                                    | 10.3 Convolutional Neural Networks . . . . . . . . . . . . . . . .                                                                    | 10.3 Convolutional Neural Networks . . . . . . . . . . . . . . . .                                                                    | 10.3 Convolutional Neural Networks . . . . . . . . . . . . . . . .                                                                    | 10.3 Convolutional Neural Networks . . . . . . . . . . . . . . . .                                                                    | 10.3 Convolutional Neural Networks . . . . . . . . . . . . . . . .                                                                    | 406                                                                   | 406                                                                   | 406                                                                   |\n|                                                                                                                                       | Convolution Layers . . . . . . . . . . . . . . . . . . . . .                                                                          | Convolution Layers . . . . . . . . . . . . . . . . . . . . .                                                                          | Convolution Layers . . . . . . . . . . . . . . . . . . . . .                                                                          | Convolution Layers . . . . . . . . . . . . . . . . . . . . .                                                                          | Convolution Layers . . . . . . . . . . . . . . . . . . . . .                                                                          | Convolution Layers . . . . . . . . . . . . . . . . . . . . .                                                                          | Convolution Layers . . . . . . . . . . . . . . . . . . . . .                                                                          | 407                                                                   | 407                                                                   | 407                                                                   |\n|                                                                                                                                       | 10.3.1                                                                                                                                |                                                                                                                                       |                                                                                                                                       |                                                                                                                                       |                                                                                                                                       |                                                                                                                                       |                                                                                                                                       | 410                                                                   | 410                                                                   | 410                                                                   |\n|                                                                                                                                       | 10.3.2                                                                                                                                | Pooling Layers                                                                                                                        | . . . . . . . . . . . . . . .                                                                                                         |                                                                                                                                       | .                                                                                                                                     |                                                                                                                                       | .                                                                                                                                     |                                                                       |                                                                       |                                                                       |\n|                                                                                                                                       | 10.3.3                                                                                                                                | Architecture of a Convolutional                                                                                                       | Neural                                                                                                                                |                                                                                                                                       | Network                                                                                                                               |                                                                                                                                       | . .                                                                                                                                   | 410                                                                   | 410                                                                   | 410                                                                   |\n|                                                                                                                                       | 10.3.4                                                                                                                                | Data Augmentation .                                                                                                                   | . . . . . .                                                                                                                           | . .                                                                                                                                   | . . . . . .                                                                                                                           | .                                                                                                                                     | . .                                                                                                                                   | 411                                                                   | 411                                                                   | 411                                                                   |\n|                                                                                                                                       | 10.3.5                                                                                                                                | Results Using a Pretrained                                                                                                            | Classifier .                                                                                                                          | .                                                                                                                                     | . . .                                                                                                                                 | .                                                                                                                                     | . .                                                                                                                                   | 412                                                                   | 412                                                                   | 412                                                                   |\n| 10.4                                                                                                                                  | Document                                                                                                                              | Classification . . . .                                                                                                                |                                                                                                                                       |                                                                                                                                       |                                                                                                                                       | .                                                                                                                                     | .                                                                                                                                     | 413                                                                   | 413                                                                   | 413                                                                   |\n|                                                                                                                                       | . . . . . . . . . . . . . .                                                                                                           | . . . . . . . . . . . . . .                                                                                                           | . . . . . . . . . . . . . .                                                                                                           | . . . . . . . . . . . . . .                                                                                                           | . . . . . . . . . . . . . .                                                                                                           | . . . . . . . . . . . . . .                                                                                                           | . . . . . . . . . . . . . .                                                                                                           | 416                                                                   | 416                                                                   | 416                                                                   |\n| 10.5                                                                                                                                  | Recurrent Neural Networks . . . . . . . . . . . . . . . . . . .                                                                       | Recurrent Neural Networks . . . . . . . . . . . . . . . . . . .                                                                       | Recurrent Neural Networks . . . . . . . . . . . . . . . . . . .                                                                       | Recurrent Neural Networks . . . . . . . . . . . . . . . . . . .                                                                       | Recurrent Neural Networks . . . . . . . . . . . . . . . . . . .                                                                       | Recurrent Neural Networks . . . . . . . . . . . . . . . . . . .                                                                       | Recurrent Neural Networks . . . . . . . . . . . . . . . . . . .                                                                       | 418                                                                   | 418                                                                   | 418                                                                   |\n|                                                                                                                                       | Sequential Models for Document Classification . . . . .                                                                               | Sequential Models for Document Classification . . . . .                                                                               | Sequential Models for Document Classification . . . . .                                                                               | Sequential Models for Document Classification . . . . .                                                                               | Sequential Models for Document Classification . . . . .                                                                               | Sequential Models for Document Classification . . . . .                                                                               | Sequential Models for Document Classification . . . . .                                                                               |                                                                       |                                                                       |                                                                       |\n|                                                                                                                                       | 10.5.1                                                                                                                                |                                                                                                                                       |                                                                                                                                       |                                                                                                                                       |                                                                                                                                       | . .                                                                                                                                   | .                                                                                                                                     | 420                                                                   | 420                                                                   | 420                                                                   |\n|                                                                                                                                       | 10.5.2                                                                                                                                | Time Series Forecasting                                                                                                               | . . . . . . . . . . .                                                                                                                 |                                                                                                                                       | .                                                                                                                                     | .                                                                                                                                     |                                                                                                                                       | 424                                                                   | 424                                                                   | 424                                                                   |\n|                                                                                                                                       | 10.5.3                                                                                                                                | Summary of RNNs .                                                                                                                     | . . . . . . . . . . . . .                                                                                                             | . . .                                                                                                                                 | . . . . . . .                                                                                                                         | . .                                                                                                                                   | . . . .                                                                                                                               | 425                                                                   | 425                                                                   | 425                                                                   |\n| 10.6 10.7                                                                                                                             | When to Fitting                                                                                                                       | Use Deep Learning a Neural Network .                                                                                                  | . .                                                                                                                                   | .                                                                                                                                     | .                                                                                                                                     | .                                                                                                                                     | .                                                                                                                                     | 427                                                                   | 427                                                                   | 427                                                                   |\n|                                                                                                                                       | . . . . . . . . . . . . . . . . . . . . . . . . . . . . .                                                                             | . . . . . . . . . . . . . . . . . . . . . . . . . . . . .                                                                             | . . . . . . . . . . . . . . . . . . . . . . . . . . . . .                                                                             | . . . . . . . . . . . . . . . . . . . . . . . . . . . . .                                                                             | . . . . . . . . . . . . . . . . . . . . . . . . . . . . .                                                                             | . . . . . . . . . . . . . . . . . . . . . . . . . . . . .                                                                             | . . . . . . . . . . . . . . . . . . . . . . . . . . . . .                                                                             | 428                                                                   | 428                                                                   | 428                                                                   |\n|                                                                                                                                       | 10.7.1                                                                                                                                | Backpropagation                                                                                                                       | . . Stochastic Gradient                                                                                                               |                                                                                                                                       |                                                                                                                                       | Descent .                                                                                                                             | . .                                                                                                                                   | 429                                                                   | 429                                                                   | 429                                                                   |\n|                                                                                                                                       | 10.7.2                                                                                                                                | Regularization and Dropout Learning .                                                                                                 | . . . . . . .                                                                                                                         | . .                                                                                                                                   | . . . .                                                                                                                               | . .                                                                                                                                   | . .                                                                                                                                   | 431                                                                   | 431                                                                   | 431                                                                   |\n|                                                                                                                                       | 10.7.3                                                                                                                                |                                                                                                                                       | .                                                                                                                                     | .                                                                                                                                     | .                                                                                                                                     | .                                                                                                                                     |                                                                                                                                       |                                                                       |                                                                       |                                                                       |\n|                                                                                                                                       | 10.7.4                                                                                                                                | Network Tuning .                                                                                                                      | . . . . . .                                                                                                                           | .                                                                                                                                     | . . . .                                                                                                                               | . .                                                                                                                                   | . .                                                                                                                                   | 431                                                                   | 431                                                                   | 431                                                                   |\n| 10.8                                                                                                                                  | Interpolation and Double Descent . . . . . . . . . . . . .                                                                            | Interpolation and Double Descent . . . . . . . . . . . . .                                                                            | Interpolation and Double Descent . . . . . . . . . . . . .                                                                            | Interpolation and Double Descent . . . . . . . . . . . . .                                                                            | Interpolation and Double Descent . . . . . . . . . . . . .                                                                            | Interpolation and Double Descent . . . . . . . . . . . . .                                                                            | .                                                                                                                                     | 432                                                                   | 432                                                                   | 432                                                                   |\n| 10.9                                                                                                                                  | Lab: Deep Learning . . . . . . . . . . . . . . . . . . . . .                                                                          | Lab: Deep Learning . . . . . . . . . . . . . . . . . . . . .                                                                          | Lab: Deep Learning . . . . . . . . . . . . . . . . . . . . .                                                                          | Lab: Deep Learning . . . . . . . . . . . . . . . . . . . . .                                                                          | Lab: Deep Learning . . . . . . . . . . . . . . . . . . . . .                                                                          | Lab: Deep Learning . . . . . . . . . . . . . . . . . . . . .                                                                          | Lab: Deep Learning . . . . . . . . . . . . . . . . . . . . .                                                                          | 435                                                                   | 435                                                                   | 435                                                                   |\n|                                                                                                                                       | . on Hitters . . .                                                                                                                    | . on Hitters . . .                                                                                                                    | . on Hitters . . .                                                                                                                    | . on Hitters . . .                                                                                                                    | . on Hitters . . .                                                                                                                    | . on Hitters . . .                                                                                                                    | . on Hitters . . .                                                                                                                    |                                                                       |                                                                       |                                                                       |\n|                                                                                                                                       | 10.9.1                                                                                                                                | Single Layer Network                                                                                                                  | Data                                                                                                                                  | .                                                                                                                                     | .                                                                                                                                     | .                                                                                                                                     | .                                                                                                                                     | 437                                                                   | 437                                                                   | 437                                                                   |\n|                                                                                                                                       | 10.9.2                                                                                                                                | Multilayer                                                                                                                            | Network on the MNIST                                                                                                                  | Digit                                                                                                                                 | Data                                                                                                                                  | . .                                                                                                                                   | . .                                                                                                                                   | 444                                                                   | 444                                                                   | 444                                                                   |\n|                                                                                                                                       | 10.9.3                                                                                                                                | Convolutional                                                                                                                         | Neural Networks . . .                                                                                                                 | .                                                                                                                                     | . .                                                                                                                                   | .                                                                                                                                     | . .                                                                                                                                   | 448                                                                   | 448                                                                   | 448                                                                   |\n|                                                                                                                                       | 10.9.4                                                                                                                                | Using Pretrained CNN Models                                                                                                           | . . . . .                                                                                                                             | .                                                                                                                                     | . .                                                                                                                                   | .                                                                                                                                     | .                                                                                                                                     | 452                                                                   | 452                                                                   | 452                                                                   |\n|                                                                                                                                       |                                                                                                                                       | Classification                                                                                                                        | . .                                                                                                                                   | . .                                                                                                                                   | .                                                                                                                                     | .                                                                                                                                     | . .                                                                                                                                   | 454                                                                   | 454                                                                   | 454                                                                   |\n|                                                                                                                                       | 10.9.5 10.9.6 Recurrent                                                                                                               | IMDB Document Neural                                                                                                                  | . . Networks                                                                                                                          | . . .                                                                                                                                 | . . . . . . . . .                                                                                                                     | .                                                                                                                                     | . . .                                                                                                                                 | 458                                                                   | 458                                                                   | 458                                                                   |\n| 10.10 Exercises . . . . . . . . . . . . . . . . . . . . . . . . . . . . 11                                                            | 10.10 Exercises . . . . . . . . . . . . . . . . . . . . . . . . . . . . 11                                                            | 10.10 Exercises . . . . . . . . . . . . . . . . . . . . . . . . . . . . 11                                                            | 10.10 Exercises . . . . . . . . . . . . . . . . . . . . . . . . . . . . 11                                                            | 10.10 Exercises . . . . . . . . . . . . . . . . . . . . . . . . . . . . 11                                                            | 10.10 Exercises . . . . . . . . . . . . . . . . . . . . . . . . . . . . 11                                                            | 10.10 Exercises . . . . . . . . . . . . . . . . . . . . . . . . . . . . 11                                                            | 10.10 Exercises . . . . . . . . . . . . . . . . . . . . . . . . . . . . 11                                                            | 469                                                                   | 469                                                                   | 469                                                                   |\n| Survival Analysis and Censored Data                                                                                                   | Survival Analysis and Censored Data                                                                                                   | Survival Analysis and Censored Data                                                                                                   | Survival Analysis and Censored Data                                                                                                   | Survival Analysis and Censored Data                                                                                                   | Survival Analysis and Censored Data                                                                                                   | Survival Analysis and Censored Data                                                                                                   | Survival Analysis and Censored Data                                                                                                   |                                                                       |                                                                       |                                                                       |\n|                                                                                                                                       |                                                                                                                                       |                                                                                                                                       |                                                                                                                                       |                                                                                                                                       |                                                                                                                                       |                                                                                                                                       |                                                                                                                                       | 470                                                                   | 470                                                                   | 470                                                                   |\n| 11.1 Survival and Censoring Times . . . . . . . . . . . . . . . . 11.2 A Closer Look at Censoring . . . . . . . . . . . . . . . . . . | 11.1 Survival and Censoring Times . . . . . . . . . . . . . . . . 11.2 A Closer Look at Censoring . . . . . . . . . . . . . . . . . . | 11.1 Survival and Censoring Times . . . . . . . . . . . . . . . . 11.2 A Closer Look at Censoring . . . . . . . . . . . . . . . . . . | 11.1 Survival and Censoring Times . . . . . . . . . . . . . . . . 11.2 A Closer Look at Censoring . . . . . . . . . . . . . . . . . . | 11.1 Survival and Censoring Times . . . . . . . . . . . . . . . . 11.2 A Closer Look at Censoring . . . . . . . . . . . . . . . . . . | 11.1 Survival and Censoring Times . . . . . . . . . . . . . . . . 11.2 A Closer Look at Censoring . . . . . . . . . . . . . . . . . . | 11.1 Survival and Censoring Times . . . . . . . . . . . . . . . . 11.2 A Closer Look at Censoring . . . . . . . . . . . . . . . . . . | 11.1 Survival and Censoring Times . . . . . . . . . . . . . . . . 11.2 A Closer Look at Censoring . . . . . . . . . . . . . . . . . . | 470                                                                   | 470                                                                   | 470                                                                   |\n| The                                                                                                                                   | The                                                                                                                                   | The                                                                                                                                   | The                                                                                                                                   | The                                                                                                                                   |                                                                                                                                       | 11.3                                                                                                                                  |                                                                                                                                       | Kaplan-Meier Survival Curve . . . . . . . . . . . . . .               | 472                                                                   |                                                                       |\n| .                                                                                                                                     | .                                                                                                                                     | .                                                                                                                                     | .                                                                                                                                     |                                                                                                                                       | . .                                                                                                                                   | .                                                                                                                                     | .                                                                                                                                     | 474                                                                   | 474                                                                   | 474                                                                   |\n| 11.4 The Log-Rank Test . . . . . . . . . . . . . . . . 11.5 Regression Models With a Survival Response . .                            | 11.4 The Log-Rank Test . . . . . . . . . . . . . . . . 11.5 Regression Models With a Survival Response . .                            | 11.4 The Log-Rank Test . . . . . . . . . . . . . . . . 11.5 Regression Models With a Survival Response . .                            | 11.4 The Log-Rank Test . . . . . . . . . . . . . . . . 11.5 Regression Models With a Survival Response . .                            |                                                                                                                                       | . . . .                                                                                                                               | . .                                                                                                                                   | . 476                                                                                                                                 | . 476                                                                 | . 476                                                                 | . 476                                                                 |  \n|              | 11.5.1                                                                                                                        | The Hazard Function . . . . . . . . . . . . . . . . . 476                                                                                         |\n|--------------|-------------------------------------------------------------------------------------------------------------------------------|---------------------------------------------------------------------------------------------------------------------------------------------------|\n|              | 11.5.2                                                                                                                        | Proportional Hazards . . . . . . . . . . . . . . . . . 478                                                                                        |\n|              | 11.5.3                                                                                                                        | Example: Brain Cancer Data . . . . . . . . . . . . 482                                                                                            |\n|              | 11.5.4                                                                                                                        | Example: Publication Data . . . . . . . . . . . . . 482                                                                                           |\n| 11.6         | Shrinkage for the Cox Model . . . . . . . . . . . . . . . . . 484 486                                                         | Shrinkage for the Cox Model . . . . . . . . . . . . . . . . . 484 486                                                                             |\n| 11.7         | Additional Topics . . . . . . . . . . . . . . . . . . . . . . .                                                               | Additional Topics . . . . . . . . . . . . . . . . . . . . . . .                                                                                   |\n|              | 11.7.1                                                                                                                        | Area Under the Curve for Survival Analysis . . . . 486                                                                                            |\n|              | 11.7.2                                                                                                                        | Choice of Time Scale . . . . . . . . . . . . . . . . . 487                                                                                        |\n|              | 11.7.3                                                                                                                        | Time-Dependent Covariates . . . . . . . . . . . . . 488                                                                                           |\n|              | 11.7.4                                                                                                                        | Checking the Proportional Hazards Assumption . . 488                                                                                              |\n|              | 11.7.5                                                                                                                        | Survival Trees . . . . . . . . . . . . . . . . . . . . . 488                                                                                      |\n| 11.8         | Lab: Survival Analysis . . . . . . . . . . . . . . . . . . . . . 489                                                          | Lab: Survival Analysis . . . . . . . . . . . . . . . . . . . . . 489                                                                              |\n|              |                                                                                                                               | Brain Cancer Data . . . . . . . . . . . . . . . . . 489                                                                                           |\n|              | 11.8.1                                                                                                                        | .                                                                                                                                                 |\n|              | 11.8.2                                                                                                                        | Publication Data . . . . . . . . . . . . . . . . . . . 493                                                                                        |\n| 11.9         | 11.8.3                                                                                                                        | Call Center Data . . . . . . . . . . . . . . . . . . . 494 . .                                                                                    |\n|              | Exercises                                                                                                                     | . . . . . . . . . . . . . . . . . . . . . . . . . . 498                                                                                           |\n| Unsupervised |                                                                                                                               |                                                                                                                                                   |\n| 12           | Learning 503                                                                                                                  | Learning 503                                                                                                                                      |\n| 12.1 12.2    | The Challenge of Unsupervised Learning                                                                                        | . . . . . . . . . . 503 . . . .                                                                                                                   |\n|              | Principal                                                                                                                     | Components Analysis . . . . . . . . . . . . 504                                                                                                   |\n|              | 12.2.1 12.2.2                                                                                                                 | What Are Principal Components? . . . . . . . . . . 505 Another Interpretation of Principal Components . 508                                       |\n|              | 12.2.3                                                                                                                        | The Proportion of Variance Explained . . . . . . . 510                                                                                            |\n|              | 12.2.4                                                                                                                        | More on PCA . . . . . . . . . . . . . . . . . . . . . 512                                                                                         |\n| 12.3         | 12.2.5 Other Uses for Principal Components . . . . . . . . 515 Missing Values and Matrix Completion . . . . . . . . . . . 515 | 12.2.5 Other Uses for Principal Components . . . . . . . . 515 Missing Values and Matrix Completion . . . . . . . . . . . 515                     |\n| 12.4         |                                                                                                                               | Methods . . . . . . . . . . . . . . . . . . . . . . 520                                                                                           |\n|              | Clustering 12.4.1                                                                                                             | K -Means Clustering . . . . . . . . . . . . . . . . . 521 .                                                                                       |\n| 12.5         | 12.4.2 12.4.3                                                                                                                 | Hierarchical Clustering . . . . . . . . . . . . . . . 525 Practical Issues in Clustering . . . . . . . . . . . . 532                              |\n|              | Lab: Unsupervised Learning . . . . . . . . . . . . . . . . . 535                                                              | Lab: Unsupervised Learning . . . . . . . . . . . . . . . . . 535                                                                                  |\n|              | 12.5.1                                                                                                                        | Principal Components Analysis . . . . . . . . . . 535                                                                                             |\n|              | 12.5.2                                                                                                                        | . Matrix Completion . . . . . . . . . . . . . . . . . . 539                                                                                       |\n|              | 12.5.3 12.5.4                                                                                                                 | Clustering . . . . . . . . . . . . . . . . . . . . . . . 542 NCI60 Data Example . . . . . . . . . . . . . . . . . 546                             |\n| 13           | Multiple Testing 557 . . . . . . . . . . . 558                                                                                | Multiple Testing 557 . . . . . . . . . . . 558                                                                                                    |\n| 13.1         | A Quick                                                                                                                       | Review of Hypothesis Testing                                                                                                                      |\n|              |                                                                                                                               | Testing a Hypothesis . . . . . . . . . . . . . 558                                                                                                |\n|              | 13.1.1                                                                                                                        | .                                                                                                                                                 |\n| 13.2 13.3    | 13.1.2                                                                                                                        | . . . Type I and Type II Errors . . . . . . . . . . . 562                                                                                         |\n|              | The The                                                                                                                       | . . . Challenge of Multiple Testing . . . . . . . . . . . . . . 563                                                                               |\n| 13.4         | 13.3.1 13.3.2 Approaches to Control                                                                                           | Family-Wise Error Rate . . . . . . . . . . . . . . . . . 565 What is the Family-Wise Error Rate? . . . . . . . 565 the Family-Wise Error Rate 567 |  \n| 13.5   | A Re-Sampling Approach to p -Values and False Discovery                                |\n|--------|----------------------------------------------------------------------------------------|\n|        | Rates . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 577                  |\n|        | 13.5.1 A Re-Sampling Approach to the p -Value . . . . . . 578                          |\n|        | 13.5.2 A Re-Sampling Approach to the False Discovery Rate579                           |\n|        | 13.5.3 When Are Re-Sampling Approaches Useful? . . . . 581 . . . . . . . . . . . . 583 |\n| 13.6   | Lab: Multiple Testing . . . . . . . . .                                                |\n|        | 13.6.1 Review of Hypothesis Tests . . . . . . . . . . . . . 583                        |\n|        | 13.6.2 Family-Wise Error Rate . . . . . . . . . . . . . . . 585                        |\n|        | 13.6.3 False Discovery Rate . . . . . . . . . . . . . . . . . 588                      |\n|        | 13.6.4 A Re-Sampling Approach . . . . . . . . . . . . . . 590                          |\n| 13.7   | Exercises . . . . . . . . . . . . . . . . . . . . . . . . . . . . 593                  |\n| Index  | Index                                                                                  |', '## 11.2 A Closer Look at Censoring  \nIn order to analyze survival data, we need to make some assumptions about why censoring has occurred. For instance, suppose that a number of patients drop out of a cancer study early because they are very sick. An analysis that does not take into consideration the reason why the patients dropped out will likely overestimate the true average survival time. Similarly, suppose that males who are very sick are more likely to drop out of the study than survival time censoring time failure time event time  \nFIGURE 11.1. Illustration of censored survival data. For patients 1 and 3, the event was observed. Patient 2 was alive when the study ended. Patient 4 dropped out of the study.  \n<!-- image -->  \nfemales who are very sick. Then a comparison of male and female survival times may wrongly suggest that males survive longer than females.  \nIn this chapter, we focus on right censoring , which occurs when T ≥ Y , i.e. the true event time T is at least as large as the observed time Y . (Notice that T ≥ Y is a consequence of (11.1). Right censoring derives its name from the fact that time is typically displayed from left to right, as in Figure 11.1.) However, other types of censoring are possible. For instance, in left censoring , the true event time T is less than or equal to the observed time Y . For example, in a study of pregnancy duration, suppose that we survey patients 250 days after conception, when some have already had their babies. Then we know that for those patients, pregnancy duration is less than 250 days. More generally, interval censoring refers to the setting in which we do not know the exact event time, but we know that it falls in some interval. For instance, this setting arises if we survey patients once per week in order to determine whether the event has occurred. While left censoring and interval censoring can be accommodated using variants of the ideas presented in this chapter, in what follows we focus specifically on right censoring.  \nIn general, we need to assume that the censoring mechanism is independent : conditional on the features, the event time T is independent of the censoring time C . The two examples above violate the assumption of independent censoring. Typically, it is not possible to determine from the data itself whether the censoring mechanism is independent. Instead, one has to carefully consider the data collection process in order to determine whether independent censoring is a reasonable assumption. In the remainder of this chapter, we will assume that the censoring mechanism is independent. 1  \n1 The assumption of independent censoring can be relaxed somewhat using the notion of non-informative censoring ; however, the definition of non-informative censoring is too technical for this book.', ""# 11 Survival Analysis and Censored Data  \nIn this chapter, we will consider the topics of survival analysis and censored data . These arise in the analysis of a unique kind of outcome variable: the time until an event occurs .  \nThough the phrase 'survival analysis' evokes a medical study, the applications of survival analysis extend far beyond medicine. For example, consider a company that wishes to model churn , the process by which customers cancel subscription to a service. The company might collect data on customers over some time period, in order to model each customer's time to cancellation as a function of demographics or other predictors. However, presumably not all customers will have canceled their subscription by the end of this time period; for such customers, the time to cancellation is censored.  \nFor example, suppose that we have conducted a five-year medical study, in which patients have been treated for cancer. We would like to fit a model to predict patient survival time, using features such as baseline health measurements or type of treatment. At first pass, this may sound like a regression problem of the kind discussed in Chapter 3. But there is an important complication: hopefully some or many of the patients have survived until the end of the study. Such a patient's survival time is said to be censored : we know that it is at least five years, but we do not know its true value. We do not want to discard this subset of surviving patients, as the fact that they survived at least five years amounts to valuable information. However, it is not clear how to make use of this information using the techniques covered thus far in this textbook.  \nIn fact, survival analysis is relevant even in application areas that are unrelated to time. For instance, suppose we wish to model a person's weight as a function of some covariates, using a dataset with measurements for a large number of people. Unfortunately, the scale used to weigh those people is unable to report weights above a certain number. Then, any weights that  \nhttps://doi.org/10.1007/978-3-031-38747-0\\_11  \nsurvival analysis censored data  \nexceed that number are censored. The survival analysis methods presented in this chapter could be used to analyze this dataset.  \nSurvival analysis is a very well-studied topic within statistics, due to its critical importance in a variety of applications, both in and out of medicine. However, it has received relatively little attention in the machine learning community.""]","En el análisis de supervivencia, los datos se consideran 'censurados' cuando no se conoce el tiempo exacto en el que ocurre un evento de interés, pero se tiene información parcial sobre ese tiempo. Esto puede suceder en varias situaciones:

1. **Censura por la derecha**: Ocurre cuando el tiempo verdadero del evento \( T \) es al menos tan grande como el tiempo observado \( Y \). Por ejemplo, en un estudio médico de cinco años, si un paciente sobrevive hasta el final del estudio, su tiempo de supervivencia es censurado porque sabemos que es al menos cinco años, pero no conocemos su valor exacto.

2. **Censura por la izquierda**: Ocurre cuando el tiempo verdadero del evento \( T \) es menor o igual al tiempo observado \( Y \). Por ejemplo, en un estudio sobre la duración del embarazo, si se encuesta a las pacientes 250 días después de la concepción y algunas ya han dado a luz, se sabe que la duración del embarazo es menor a 250 días para esas pacientes.

3. **Censura por intervalo**: Se refiere a la situación en la que no se conoce el tiempo exacto del evento, pero se sabe que cae dentro de un intervalo. Esto puede ocurrir si se encuesta a los pacientes una vez por semana para determinar si el evento ha ocurrido.

La censura es un aspecto importante en el análisis de supervivencia porque proporciona información valiosa sobre los datos, incluso cuando el evento de interés no se ha observado directamente.","En el análisis de supervivencia, los datos están censurados cuando el evento de interés no se ha observado para un individuo durante el período de estudio. Esto puede suceder si el estudio finaliza antes de que ocurra el evento para ese individuo o si se pierde el seguimiento de ese individuo. La información de que el evento no ha ocurrido hasta un cierto punto en el tiempo se utiliza en el análisis.",1.0,,0.9999999999666667,1.0
What is the family-wise error rate (FWER) and why is it important in multiple hypothesis testing?,"['### 13.3.1 What is the Family-Wise Error Rate?  \nRecall that the Type I error rate is the probability of rejecting H 0 if H 0 is true. The family-wise error rate (FWER) generalizes this notion to the setting of m null hypotheses, H 01         H 0 m , and is defined as the probability of making at least one Type I error. To state this idea more formally, consider Table 13.2, which summarizes the possible outcomes when performing m hypothesis tests. Here, V represents the number of Type I errors (also known as false positives or false discoveries), S the number of true positives, U the number of true negatives, and W the number of Type II errors (also known as false negatives). Then the family-wise error rate is given by  \n$$F W E R = \\Pr ( V \\geq 1 ) .$$  \nA strategy of rejecting any null hypothesis for which the p -value is below α (i.e. controlling the Type I error for each null hypothesis at level α ) leads to a FWER of  \n$$F W E ( \\alpha ) & \\ = \\ 1 - \\Pr ( V = 0 ) \\\\ & = \\ 1 - \\Pr ( \\text {do not falsely reject any null hypotheses} ) \\\\ & = \\ 1 - \\Pr \\left ( \\bigcap _ { j = 1 } ^ { m } \\{ \\text {do not falsely reject } H _ { 0 j } \\} \\right ) . \\\\ \\intertext { R ecall from basic probability that if two events $ A $ and $ B $ are independent, }$$  \nRecall from basic probability that if two events A and B are independent, then Pr( A ∩ B ) = Pr( A ) Pr( B ) . Therefore, if we make the additional rather strong assumptions that the m tests are independent and that all m null hypotheses are true, then  \n$$F W E R ( \\alpha ) = 1 - \\prod _ { j = 1 } ^ { m } ( 1 - \\alpha ) = 1 - ( 1 - \\alpha ) ^ { m } .$$  \nfamily-wise error rate  \nFIGURE 13.2. The family-wise error rate, as a function of the number of hypotheses tested (displayed on the log scale), for three values of α : α = 0   05 (orange), α = 0   01 (blue), and α = 0   001 (purple). The dashed line indicates 0   05 . For example, in order to control the FWER at 0   05 when testing m = 50 null hypotheses, we must control the Type I error for each null hypothesis at level α = 0   001 .  \n<!-- image -->  \nHence, if we test only one null hypothesis, then FWER ( α ) = 1 -(1 -α ) 1 = α , so the Type I error rate and the FWER are equal. However, if we perform m = 100 independent tests, then FWER ( α ) = 1 -(1 -α ) 100 . For instance, taking α = 0   05 leads to a FWER of 1 -(1 -0   05) 100 = 0   994 . In other words, we are virtually guaranteed to make at least one Type I error!  \nWe now briefly return to the example in Section 13.1.1, in which we consider testing a single null hypothesis of the form H 0 : θ t = θ c using a two-sample t -statistic. Recall from Figure 13.1 that in order to guarantee that the Type I error does not exceed 0   02 , we decide whether or not to reject H 0 using a cutpoint of 2   33 (i.e. we reject H 0 if ♣ T ♣ ≥ 2   33 ). Now, what if we wish to test 10 null hypotheses using two-sample t -statistics, instead of just one? We will see in Section 13.3.2 that we can guarantee that the FWER does not exceed 0   02 by rejecting only null hypotheses for which the p -value falls below 0   002 . This corresponds to a much more stringent cutpoint of 3   09 (i.e. we should reject H 0 j only if its test statistic ♣ T j ♣ ≥ 3   09 , for j = 1         10 ). In other words, controlling the FWER at level α amounts to a much higher bar, in terms of evidence required to reject any given null hypothesis, than simply controlling the Type I error for each null hypothesis at level α .  \nFigure 13.2 displays (13.5) for various values of m , the number of hypotheses, and α , the Type I error. We see that setting α = 0   05 results in a high FWER even for moderate m . With α = 0   01 , we can test no more than five null hypotheses before the FWER exceeds 0   05 . Only for very small values, such as α = 0   001 , do we manage to ensure a small FWER, at least for moderately-sized m .  \nTABLE 13.3. The first two columns correspond to the sample mean and sample standard deviation of the percentage excess return, over n = 50 months, for the first five managers in the Fund dataset. The last two columns provide the t -statistic ( √ n · ¯ Xglyph[triangleleft]S ) and associated p -value for testing H 0 j : θ j = 0 , the null hypothesis that the (population) mean return for the j th hedge fund manager equals zero.  \n| Manager   |   Mean, ¯ x |   Standard Deviation, s |   t -statistic |   p -value |\n|-----------|-------------|-------------------------|----------------|------------|\n| One       |         3   |                     7.4 |           2.86 |      0.006 |\n| Two       |        -0.1 |                     6.9 |          -0.1  |      0.918 |\n| Three     |         2.8 |                     7.5 |           2.62 |      0.012 |\n| Four      |         0.5 |                     6.7 |           0.53 |      0.601 |\n| Five      |         0.3 |                     6.8 |           0.31 |      0.756 |', ""### 13.3.2 Approaches to Control the Family-Wise Error Rate  \nIn this section, we briefly survey some approaches to control the FWER. We will illustrate these approaches on the Fund dataset, which records the monthly percentage excess returns for 2,000 fund managers over n = 50 months. 13 Table 13.3 provides relevant summary statistics for the first five managers.  \nWe first present the Bonferroni method and Holm's step-down procedure, which are very general-purpose approaches for controlling the FWER that can be applied whenever m p -values have been computed, regardless of the form of the null hypotheses, the choice of test statistics, or the (in)dependence of the p -values. We then briefly discuss Tukey's method and Scheffé's method in order to illustrate the fact that, in certain situations, more specialized approaches for controlling the FWER may be preferable.\nThe Bonferroni Method  \nAs in the previous section, suppose we wish to test H 01         H 0 m . Let A j denote the event that we make a Type I error for the j th null hypothesis, for j = 1         m . Then  \n$$f & = 1 , \\dots , m \\, \\text {.} \\\\ & \\quad \\text {FWER} = \\Pr ( \\text {falsy} ) \\, \\text {reject at least one null hypothesis} \\\\ & = \\Pr _ { m } ( \\bigcup _ { j = 1 } ^ { m } A _ { j } ) \\\\ & \\leq \\sum _ { j = 1 } ^ { m } \\Pr ( A _ { j } ) . \\\\$$  \nIn (13.6), the inequality results from the fact that for any two events A and B , Pr( A ∪ B ) ≤ Pr( A ) + Pr( B ) , regardless of whether A and B are independent. The Bonferroni method , or Bonferroni correction , sets the threshold for rejecting each hypothesis test to α glyph[triangleleft]m , so that Pr( A j ) ≤ α glyph[triangleleft]m . Equation 13.6 implies that  \n$$F W E R ( \\alpha / m ) \\leq m \\times \\frac { \\alpha } { m } = \\alpha ,$$  \n13 Excess returns correspond to the additional return the fund manager achieves beyond the market's overall return. So if the market increases by 5% during a given period and the fund manager achieves a 7% return, their excess return would be 7% -5% = 2% .  \nso this procedure controls the FWER at level α . For instance, in order to control the FWER at level 0   1 while testing m = 100 null hypotheses, the Bonferroni procedure requires us to control the Type I error for each null hypothesis at level 0   1 glyph[triangleleft] 100 = 0   001 , i.e. to reject all null hypotheses for which the p -value is below 0   001 .  \nThe Bonferroni correction is by far the best-known and most commonlyused multiplicity correction in all of statistics. Its ubiquity is due in large part to the fact that it is very easy to understand and simple to implement, and also from the fact that it successfully controls Type I error regardless of whether the m hypothesis tests are independent. However, as we will see, it is typically neither the most powerful nor the best approach for multiple testing correction. In particular, the Bonferroni correction can be quite conservative, in the sense that the true FWER is often quite a bit lower than the nominal (or target) FWER; this results from the inequality in (13.6). By contrast, a less conservative procedure might allow us to control the FWER while rejecting more null hypotheses, and therefore making fewer Type II errors.  \nWe now consider the Fund dataset in Table 13.3. If we control the Type I error at level α = 0   05 for each fund manager separately, then we will conclude that the first and third managers have significantly non-zero excess returns; in other words, we will reject H 01 : θ 1 = 0 and H 03 : θ 3 = 0 . However, as discussed in previous sections, this procedure does not account for the fact that we have tested multiple hypotheses, and therefore it will lead to a FWER greater than 0   05 . If we instead wish to control the FWER at level 0   05 , then, using a Bonferroni correction, we must control the Type I error for each individual manager at level α glyph[triangleleft]m = 0   05 glyph[triangleleft] 5 = 0   01 . Consequently, we will reject the null hypothesis only for the first manager, since the p -values for all other managers exceed 0   01 . The Bonferroni correction gives us peace of mind that we have not falsely rejected too many null hypotheses, but for a price: we reject few null hypotheses, and thus will typically make quite a few Type II errors.\nHolm's Step-Down Procedure  \nHolm's method , also known as Holm's step-down procedure or the HolmBonferroni method, is an alternative to the Bonferroni procedure. Holm's method controls the FWER, but it is less conservative than Bonferroni, in the sense that it will reject more null hypotheses, typically resulting in fewer Type II errors and hence greater power. The procedure is summarized in Algorithm 13.1. The proof that this method controls the FWER is similar to, but slightly more complicated than, the argument in (13.6) that the Bonferroni method controls the FWER. It is worth noting that in Holm's procedure, the threshold that we use to reject each null hypothesis p ( L ) in Step 5 - actually depends on the values of all m of the p -values. (See the definition of L in (13.7).) This is in contrast to the Bonferroni procedure, in which to control the FWER at level α , we reject any null hypotheses for which the p -value is below α glyph[triangleleft]m , regardless of the other p -values. Holm's method makes no independence assumptions about the m hypothesis tests, and is uniformly more powerful than the Bonferroni method - it will  \nHolm's method\nAlgorithm 13.1 Holm's Step-Down Procedure to Control the FWER  \n1. Specify α , the level at which to control the FWER.\n2. Compute p -values, p 1         p m , for the m null hypotheses H 01         H 0 m .\n3. Order the m p -values so that p (1) ≤ p (2) ≤ · · · ≤ p ( m ) .\n4. Define  \n$$L = \\min \\left \\{ j \\colon p _ { ( j ) } > \\frac { \\alpha } { m + 1 - j } \\right \\} . \\\\$$  \n5. Reject all null hypotheses H 0 j for which p j &lt; p ( L ) .  \nalways reject at least as many null hypotheses as Bonferroni - and so it should always be preferred.  \nFigure 13.3 provides an illustration of the Bonferroni and Holm methods on three simulated data sets in a setting involving m = 10 hypothesis tests, of which m 0 = 2 of the null hypotheses are true. Each panel displays the ten corresponding p -values, ordered from smallest to largest, and plotted on a log scale. The eight red points represent the false null hypotheses, and the two black points represent the true null hypotheses. We wish to control the FWER at level 0   05 . The Bonferroni procedure requires us to reject all null hypotheses for which the p -value is below 0   005 ; this is represented by the black horizontal line. The Holm procedure requires us to reject all null hypotheses that fall below the blue line. The blue line always lies above the black line, so Holm will always reject more tests than Bonferroni; the region between the two lines corresponds to the hypotheses that are only rejected by Holm. In the left-hand panel, both Bonferroni and Holm successfully reject seven of the eight false null hypotheses. In the center panel, Holm successfully rejects all eight of the false null hypotheses, while Bonferroni fails to reject one. In the right-hand panel, Bonferroni only rejects three of the false null hypotheses, while Holm rejects all eight. Neither Bonferroni nor Holm makes any Type I errors in these examples.  \nWe now consider applying Holm's method to the first five fund managers in the Fund dataset in Table 13.3, while controlling the FWER at level 0   05 . The ordered p -values are p (1) = 0   006  p (2) = 0   012  p (3) = 0   601  p (4) = 0   756 and p (5) = 0   918 . The Holm procedure rejects the first two null hypotheses, because p (1) = 0   006 &lt; 0   05 glyph[triangleleft] (5 + 1 -1) = 0   01 and p (2) = 0   012 &lt; 0   05 glyph[triangleleft] (5 + 1 -2) = 0   0125 , but p (3) = 0   601 &gt; 0   05 glyph[triangleleft] (5 + 1 -3) = 0   0167 , which implies that L = 3 . We note that, in this setting, Holm is more powerful than Bonferroni: the former rejects the null hypotheses for the first and third managers, whereas the latter rejects the null hypothesis only for the first manager.\nTwo Special Cases: Tukey's Method and Scheffé's Method  \nBonferroni's method and Holm's method can be used in virtually any setting in which we wish to control the FWER for m null hypotheses: they  \n<!-- image -->  \nFIGURE 13.3. Each panel displays, for a separate simulation, the sorted p -values for tests of m = 10 null hypotheses. The p -values corresponding to the m 0 = 2 true null hypotheses are displayed in black, and the rest are in red. When controlling the FWER at level 0   05 , the Bonferroni procedure rejects all null hypotheses that fall below the black line, and the Holm procedure rejects all null hypotheses that fall below the blue line. The region between the blue and black lines indicates null hypotheses that are rejected using the Holm procedure but not using the Bonferroni procedure. In the center panel, the Holm procedure rejects one more null hypothesis than the Bonferroni procedure. In the right-hand panel, it rejects five more null hypotheses.  \n<!-- image -->  \nmake no assumptions about the nature of the null hypotheses, the type of test statistic used, or the (in)dependence of the p -values. However, in certain very specific settings, we can achieve higher power by controlling the FWER using approaches that are more tailored to the task at hand. Tukey's method and Scheffé's method provide two such examples.  \nHowever, in this setting, a Bonferroni correction is actually a bit too stringent, since it fails to consider the fact that the m = 10 hypothesis tests are all somewhat related: for instance, Managers Two and Five have similar mean returns, as do Managers Two and Four; this guarantees that the mean returns of Managers Four and Five are similar. Stated another way, the m p -values for the m pairwise comparisons are not independent. Therefore, it should be possible to control the FWER in a way that is  \nTable 13.3 indicates that for the Fund dataset, Managers One and Two have the greatest difference in their sample mean returns. This finding might motivate us to test the null hypothesis H 0 : θ 1 = θ 2 , where θ j is the (population) mean return for the j th fund manager. A two-sample t -test (13.1) for H 0 yields a p -value of 0   0349 , suggesting modest evidence against H 0 . However, this p -value is misleading, since we decided to compare the average returns of Managers One and Two only after having examined the returns for all five managers; this essentially amounts to having performed m = 5 × (5 -1) glyph[triangleleft] 2 = 10 hypothesis tests, and selecting the one with the smallest p -value. This suggests that in order to control the FWER at level 0   05 , we should make a Bonferroni correction for m = 10 hypothesis tests, and therefore should only reject a null hypothesis for which the p -value is below 0   005 . If we do this, then we will be unable to reject the null hypothesis that Managers One and Two have identical performance.  \nFIGURE 13.4. Each panel displays, for a separate simulation, the sorted p -values for tests of m = 15 hypotheses, corresponding to pairwise tests for the equality of G = 6 means. The m 0 = 10 true null hypotheses are displayed in black, and the rest are in red. When controlling the FWER at level 0   05 , the Bonferroni procedure rejects all null hypotheses that fall below the black line, whereas Tukey rejects all those that fall below the blue line. Thus, Tukey's method has slightly higher power than Bonferroni's method. Controlling the Type I error without adjusting for multiple testing involves rejecting all those that fall below the green line.  \n<!-- image -->  \nless conservative. This is exactly the idea behind Tukey's method : when performing m = G ( G -1) glyph[triangleleft] 2 pairwise comparisons of G means, it allows us to control the FWER at level α while rejecting all null hypotheses for which the p -value falls below α T , for some α T &gt; α glyph[triangleleft]m .  \n/negationslash  \nNow, suppose that we once again examine the data in Table 13.3, and notice that Managers One and Three have higher mean returns than Managers Two, Four, and Five. This might motivate us to test the null hypothesis  \nFigure 13.4 illustrates Tukey's method on three simulated data sets in a setting with G = 6 means, with θ 1 = θ 2 = θ 3 = θ 4 = θ 5 = θ 6 . Therefore, of the m = G ( G -1) glyph[triangleleft] 2 = 15 null hypotheses of the form H 0 : θ j = θ k , ten are true and five are false. In each panel, the true null hypotheses are displayed in black, and the false ones are in red. The horizontal lines indicate that Tukey's method always results in at least as many rejections as Bonferroni's method. In the left-hand panel, Tukey correctly rejects two more null hypotheses than Bonferroni.  \n$$H _ { 0 } \\colon \\frac { 1 } { 2 } \\left ( \\mu _ { 1 } + \\mu _ { 3 } \\right ) = \\frac { 1 } { 3 } \\left ( \\mu _ { 2 } + \\mu _ { 4 } + \\mu _ { 5 } \\right ) .$$  \n(Recall that θ j is the population mean return for the j th hedge fund manager.) It turns out that we could test (13.8) using a variant of the twosample t -test presented in (13.1), leading to a p -value of 0   004 . This suggests strong evidence of a difference between Managers One and Three compared to Managers Two, Four, and Five. However, there is a problem: we decided to test the null hypothesis in (13.8) only after peeking at the data in Table 13.3. In a sense, this means that we have conducted multiple testing. In this setting, using Bonferroni to control the FWER at level α  \nTukey's method  \nwould require a p -value threshold of α glyph[triangleleft]m , for an extremely large value of m 14 .  \nTo summarize, Holm's procedure and Bonferroni's procedure are very general approaches for multiple testing correction that can be applied under all circumstances. However, in certain special cases, more powerful procedures for multiple testing correction may be available, in order to control the FWER while achieving higher power (i.e. committing fewer Type II errors) than would be possible using Holm or Bonferroni. In this section, we have illustrated two such examples.  \nScheffé's method is designed for exactly this setting. It allows us to compute a value α S such that rejecting the null hypothesis H 0 in (13.8) if the p -value is below α S will control the Type I error at level α . It turns out that for the Fund example, in order to control the Type I error at level α = 0   05 , we must set α S = 0   002 . Therefore, we are unable to reject H 0 in (13.8), despite the apparently very small p -value of 0   004 . An important advantage of Scheffé's method is that we can use this same threshold of α S = 0   002 in order to perform a pairwise comparison of any split of the managers into two groups: for instance, we could also test H 0 : 1 3 ( θ 1 + θ 2 + θ 3 ) = 1 2 ( θ 4 + θ 5 ) and H 0 : 1 4 ( θ 1 + θ 2 + θ 3 + θ 4 ) = θ 5 using the same threshold of 0   002 , without needing to further adjust for multiple testing.""]","The family-wise error rate (FWER) is a statistical concept that generalizes the notion of the Type I error rate to the setting of multiple hypothesis tests. Specifically, it is defined as the probability of making at least one Type I error (false positive) when testing multiple null hypotheses. Mathematically, it is expressed as:

\[ FWER = \Pr(V \geq 1) \]

where \( V \) represents the number of Type I errors.

FWER is important in multiple hypothesis testing because when multiple tests are conducted, the chance of incorrectly rejecting at least one true null hypothesis increases. If each test is conducted at a significance level \(\alpha\), the probability of making at least one Type I error across all tests can be much higher than \(\alpha\). Controlling the FWER ensures that the overall probability of making one or more Type I errors is kept at a desired level, providing more reliable results when multiple hypotheses are being tested. This is crucial in fields like genomics, clinical trials, and other areas where multiple comparisons are common.","The family-wise error rate (FWER) is the probability of making at least one Type I error (falsely rejecting a true null hypothesis) among a set of hypothesis tests. It's important to control the FWER in multiple testing scenarios to avoid an inflated probability of making false discoveries, which can happen if you only control the individual Type I error rate for each test.",1.0,,0.99999999995,1.0
How does the Benjamini-Hochberg procedure differ from the Bonferroni correction for multiple testing?,"['### 13.4.2 The Benjamini-Hochberg Procedure  \nWe now focus on the task of controlling the FDR: that is, deciding which null hypotheses to reject while guaranteeing that the FDR, E( Vglyph[triangleleft]R ) , is less than or equal to some pre-specified value q . In order to do this, we need some way to connect the p -values, p 1         p m , from the m null hypotheses to the desired FDR value, q . It turns out that a very simple procedure, outlined in Algorithm 13.2, can be used to control the FDR.\nAlgorithm 13.2 Benjamini-Hochberg Procedure to Control the FDR  \n1. Specify q , the level at which to control the FDR.\n2. Compute p -values, p 1         p m , for the m null hypotheses H 01         H 0 m .\n3. Order the m p -values so that p (1) ≤ p (2) ≤ · · · ≤ p ( m ) .\n4. Define  \n$$L = \\max \\{ j \\colon p _ { ( j ) } < q j / m \\} .$$  \n5. Reject all null hypotheses H 0 j for which p j ≤ p ( L ) .  \nAlgorithm 13.2 is known as the Benjamini-Hochberg procedure . The crux of this procedure lies in (13.10). For example, consider again the first five managers in the Fund dataset, presented in Table 13.3. (In this example, m = 5 , although typically we control the FDR in settings involving a much greater number of null hypotheses.) We see that p (1) = 0   006 &lt; 0   05 × 1 glyph[triangleleft] 5 , p (2) = 0   012 &lt; 0   05 × 2 glyph[triangleleft] 5 , p (3) = 0   601 &gt; 0   05 × 3 glyph[triangleleft] 5 , p (4) = 0   756 &gt; 0   05 × 4 glyph[triangleleft] 5 , and p (5) = 0   918 &gt; 0   05 × 5 glyph[triangleleft] 5 . Therefore, to control the FDR at 5% , we reject the null hypotheses that the first and third fund managers perform no better than chance.  \nAs long as the m p -values are independent or only mildly dependent, then the Benjamini-Hochberg procedure guarantees 16 that  \n$$F D R \\leq q .$$  \nIn other words, this procedure ensures that, on average, no more than a fraction q of the rejected null hypotheses are false positives. Remarkably, this holds regardless of how many null hypotheses are true, and regardless of the distribution of the p -values for the null hypotheses that are false. Therefore, the Benjamini-Hochberg procedure gives us a very easy way to determine, given a set of m p -values, which null hypotheses to reject in order to control the FDR at any pre-specified level q .  \n16 However, the proof is well beyond the scope of this book.  \nBenjaminiHochberg procedure  \nFIGURE 13.6. Each panel displays the same set of m = 2  000 ordered p -values for the Fund data. The green lines indicate the p -value thresholds corresponding to FWER control, via the Bonferroni procedure, at levels α = 0   05 (left) , α = 0   1 (center) , and α = 0   3 (right) . The orange lines indicate the p -value thresholds corresponding to FDR control, via Benjamini-Hochberg, at levels q = 0   05 (left) , q = 0   1 (center) , and q = 0   3 (right) . When the FDR is controlled at level q = 0   1 , 146 null hypotheses are rejected (center) ; the corresponding p -values are shown in blue. When the FDR is controlled at level q = 0   3 , 279 null hypotheses are rejected (right) ; the corresponding p -values are shown in blue.  \n<!-- image -->  \nThere is a fundamental difference between the Bonferroni procedure of Section 13.3.2 and the Benjamini-Hochberg procedure. In the Bonferroni procedure, in order to control the FWER for m null hypotheses at level α , we must simply reject null hypotheses for which the p -value is below α glyph[triangleleft]m . This threshold of α glyph[triangleleft]m does not depend on anything about the data (beyond the value of m ), and certainly does not depend on the p -values themselves. By contrast, the rejection threshold used in the BenjaminiHochberg procedure is more complicated: we reject all null hypotheses for which the p -value is less than or equal to the L th smallest p -value, where L is itself a function of all m p -values, as in (13.10). Therefore, when conducting the Benjamini-Hochberg procedure, we cannot plan out in advance what threshold we will use to reject p -values; we need to first see our data. For instance, in the abstract, there is no way to know whether we will reject a null hypothesis corresponding to a p -value of 0.01 when using an FDR threshold of 0   1 with m = 100 ; the answer depends on the values of the other m -1 p -values. This property of the Benjamini-Hochberg procedure is shared by the Holm procedure, which also involves a data-dependent p -value threshold.  \nFigure 13.6 displays the results of applying the Bonferroni and BenjaminiHochberg procedures on the Fund data set, using the full set of m = 2  000 fund managers, of which the first five were displayed in Table 13.3. When the FWER is controlled at level 0   3 using Bonferroni, only one null hypothesis is rejected; that is, we can conclude only that a single fund manager is beating the market. This is despite the fact that a substantial portion of  \nthe m = 2  000 fund managers appear to have beaten the market without performing correction for multiple testing - for instance, 13 of them have p -values below 0   001 . By contrast, when the FDR is controlled at level 0   3 , we can conclude that 279 fund managers are beating the market: we expect that no more than around 279 × 0   3 = 83   7 of these fund managers had good performance only due to chance. Thus, we see that FDR control is much milder - and more powerful - than FWER control, in the sense that it allows us to reject many more null hypotheses, with a cost of substantially more false positives.  \nThe Benjamini-Hochberg procedure has been around since the mid1990s. While a great many papers have been published since then proposing alternative approaches for FDR control that can perform better in particular scenarios, the Benjamini-Hochberg procedure remains a very useful and widely-applicable approach.', ""### 13.3.2 Approaches to Control the Family-Wise Error Rate  \nIn this section, we briefly survey some approaches to control the FWER. We will illustrate these approaches on the Fund dataset, which records the monthly percentage excess returns for 2,000 fund managers over n = 50 months. 13 Table 13.3 provides relevant summary statistics for the first five managers.  \nWe first present the Bonferroni method and Holm's step-down procedure, which are very general-purpose approaches for controlling the FWER that can be applied whenever m p -values have been computed, regardless of the form of the null hypotheses, the choice of test statistics, or the (in)dependence of the p -values. We then briefly discuss Tukey's method and Scheffé's method in order to illustrate the fact that, in certain situations, more specialized approaches for controlling the FWER may be preferable.\nThe Bonferroni Method  \nAs in the previous section, suppose we wish to test H 01         H 0 m . Let A j denote the event that we make a Type I error for the j th null hypothesis, for j = 1         m . Then  \n$$f & = 1 , \\dots , m \\, \\text {.} \\\\ & \\quad \\text {FWER} = \\Pr ( \\text {falsy} ) \\, \\text {reject at least one null hypothesis} \\\\ & = \\Pr _ { m } ( \\bigcup _ { j = 1 } ^ { m } A _ { j } ) \\\\ & \\leq \\sum _ { j = 1 } ^ { m } \\Pr ( A _ { j } ) . \\\\$$  \nIn (13.6), the inequality results from the fact that for any two events A and B , Pr( A ∪ B ) ≤ Pr( A ) + Pr( B ) , regardless of whether A and B are independent. The Bonferroni method , or Bonferroni correction , sets the threshold for rejecting each hypothesis test to α glyph[triangleleft]m , so that Pr( A j ) ≤ α glyph[triangleleft]m . Equation 13.6 implies that  \n$$F W E R ( \\alpha / m ) \\leq m \\times \\frac { \\alpha } { m } = \\alpha ,$$  \n13 Excess returns correspond to the additional return the fund manager achieves beyond the market's overall return. So if the market increases by 5% during a given period and the fund manager achieves a 7% return, their excess return would be 7% -5% = 2% .  \nso this procedure controls the FWER at level α . For instance, in order to control the FWER at level 0   1 while testing m = 100 null hypotheses, the Bonferroni procedure requires us to control the Type I error for each null hypothesis at level 0   1 glyph[triangleleft] 100 = 0   001 , i.e. to reject all null hypotheses for which the p -value is below 0   001 .  \nThe Bonferroni correction is by far the best-known and most commonlyused multiplicity correction in all of statistics. Its ubiquity is due in large part to the fact that it is very easy to understand and simple to implement, and also from the fact that it successfully controls Type I error regardless of whether the m hypothesis tests are independent. However, as we will see, it is typically neither the most powerful nor the best approach for multiple testing correction. In particular, the Bonferroni correction can be quite conservative, in the sense that the true FWER is often quite a bit lower than the nominal (or target) FWER; this results from the inequality in (13.6). By contrast, a less conservative procedure might allow us to control the FWER while rejecting more null hypotheses, and therefore making fewer Type II errors.  \nWe now consider the Fund dataset in Table 13.3. If we control the Type I error at level α = 0   05 for each fund manager separately, then we will conclude that the first and third managers have significantly non-zero excess returns; in other words, we will reject H 01 : θ 1 = 0 and H 03 : θ 3 = 0 . However, as discussed in previous sections, this procedure does not account for the fact that we have tested multiple hypotheses, and therefore it will lead to a FWER greater than 0   05 . If we instead wish to control the FWER at level 0   05 , then, using a Bonferroni correction, we must control the Type I error for each individual manager at level α glyph[triangleleft]m = 0   05 glyph[triangleleft] 5 = 0   01 . Consequently, we will reject the null hypothesis only for the first manager, since the p -values for all other managers exceed 0   01 . The Bonferroni correction gives us peace of mind that we have not falsely rejected too many null hypotheses, but for a price: we reject few null hypotheses, and thus will typically make quite a few Type II errors.\nHolm's Step-Down Procedure  \nHolm's method , also known as Holm's step-down procedure or the HolmBonferroni method, is an alternative to the Bonferroni procedure. Holm's method controls the FWER, but it is less conservative than Bonferroni, in the sense that it will reject more null hypotheses, typically resulting in fewer Type II errors and hence greater power. The procedure is summarized in Algorithm 13.1. The proof that this method controls the FWER is similar to, but slightly more complicated than, the argument in (13.6) that the Bonferroni method controls the FWER. It is worth noting that in Holm's procedure, the threshold that we use to reject each null hypothesis p ( L ) in Step 5 - actually depends on the values of all m of the p -values. (See the definition of L in (13.7).) This is in contrast to the Bonferroni procedure, in which to control the FWER at level α , we reject any null hypotheses for which the p -value is below α glyph[triangleleft]m , regardless of the other p -values. Holm's method makes no independence assumptions about the m hypothesis tests, and is uniformly more powerful than the Bonferroni method - it will  \nHolm's method\nAlgorithm 13.1 Holm's Step-Down Procedure to Control the FWER  \n1. Specify α , the level at which to control the FWER.\n2. Compute p -values, p 1         p m , for the m null hypotheses H 01         H 0 m .\n3. Order the m p -values so that p (1) ≤ p (2) ≤ · · · ≤ p ( m ) .\n4. Define  \n$$L = \\min \\left \\{ j \\colon p _ { ( j ) } > \\frac { \\alpha } { m + 1 - j } \\right \\} . \\\\$$  \n5. Reject all null hypotheses H 0 j for which p j &lt; p ( L ) .  \nalways reject at least as many null hypotheses as Bonferroni - and so it should always be preferred.  \nFigure 13.3 provides an illustration of the Bonferroni and Holm methods on three simulated data sets in a setting involving m = 10 hypothesis tests, of which m 0 = 2 of the null hypotheses are true. Each panel displays the ten corresponding p -values, ordered from smallest to largest, and plotted on a log scale. The eight red points represent the false null hypotheses, and the two black points represent the true null hypotheses. We wish to control the FWER at level 0   05 . The Bonferroni procedure requires us to reject all null hypotheses for which the p -value is below 0   005 ; this is represented by the black horizontal line. The Holm procedure requires us to reject all null hypotheses that fall below the blue line. The blue line always lies above the black line, so Holm will always reject more tests than Bonferroni; the region between the two lines corresponds to the hypotheses that are only rejected by Holm. In the left-hand panel, both Bonferroni and Holm successfully reject seven of the eight false null hypotheses. In the center panel, Holm successfully rejects all eight of the false null hypotheses, while Bonferroni fails to reject one. In the right-hand panel, Bonferroni only rejects three of the false null hypotheses, while Holm rejects all eight. Neither Bonferroni nor Holm makes any Type I errors in these examples.  \nWe now consider applying Holm's method to the first five fund managers in the Fund dataset in Table 13.3, while controlling the FWER at level 0   05 . The ordered p -values are p (1) = 0   006  p (2) = 0   012  p (3) = 0   601  p (4) = 0   756 and p (5) = 0   918 . The Holm procedure rejects the first two null hypotheses, because p (1) = 0   006 &lt; 0   05 glyph[triangleleft] (5 + 1 -1) = 0   01 and p (2) = 0   012 &lt; 0   05 glyph[triangleleft] (5 + 1 -2) = 0   0125 , but p (3) = 0   601 &gt; 0   05 glyph[triangleleft] (5 + 1 -3) = 0   0167 , which implies that L = 3 . We note that, in this setting, Holm is more powerful than Bonferroni: the former rejects the null hypotheses for the first and third managers, whereas the latter rejects the null hypothesis only for the first manager.\nTwo Special Cases: Tukey's Method and Scheffé's Method  \nBonferroni's method and Holm's method can be used in virtually any setting in which we wish to control the FWER for m null hypotheses: they  \n<!-- image -->  \nFIGURE 13.3. Each panel displays, for a separate simulation, the sorted p -values for tests of m = 10 null hypotheses. The p -values corresponding to the m 0 = 2 true null hypotheses are displayed in black, and the rest are in red. When controlling the FWER at level 0   05 , the Bonferroni procedure rejects all null hypotheses that fall below the black line, and the Holm procedure rejects all null hypotheses that fall below the blue line. The region between the blue and black lines indicates null hypotheses that are rejected using the Holm procedure but not using the Bonferroni procedure. In the center panel, the Holm procedure rejects one more null hypothesis than the Bonferroni procedure. In the right-hand panel, it rejects five more null hypotheses.  \n<!-- image -->  \nmake no assumptions about the nature of the null hypotheses, the type of test statistic used, or the (in)dependence of the p -values. However, in certain very specific settings, we can achieve higher power by controlling the FWER using approaches that are more tailored to the task at hand. Tukey's method and Scheffé's method provide two such examples.  \nHowever, in this setting, a Bonferroni correction is actually a bit too stringent, since it fails to consider the fact that the m = 10 hypothesis tests are all somewhat related: for instance, Managers Two and Five have similar mean returns, as do Managers Two and Four; this guarantees that the mean returns of Managers Four and Five are similar. Stated another way, the m p -values for the m pairwise comparisons are not independent. Therefore, it should be possible to control the FWER in a way that is  \nTable 13.3 indicates that for the Fund dataset, Managers One and Two have the greatest difference in their sample mean returns. This finding might motivate us to test the null hypothesis H 0 : θ 1 = θ 2 , where θ j is the (population) mean return for the j th fund manager. A two-sample t -test (13.1) for H 0 yields a p -value of 0   0349 , suggesting modest evidence against H 0 . However, this p -value is misleading, since we decided to compare the average returns of Managers One and Two only after having examined the returns for all five managers; this essentially amounts to having performed m = 5 × (5 -1) glyph[triangleleft] 2 = 10 hypothesis tests, and selecting the one with the smallest p -value. This suggests that in order to control the FWER at level 0   05 , we should make a Bonferroni correction for m = 10 hypothesis tests, and therefore should only reject a null hypothesis for which the p -value is below 0   005 . If we do this, then we will be unable to reject the null hypothesis that Managers One and Two have identical performance.  \nFIGURE 13.4. Each panel displays, for a separate simulation, the sorted p -values for tests of m = 15 hypotheses, corresponding to pairwise tests for the equality of G = 6 means. The m 0 = 10 true null hypotheses are displayed in black, and the rest are in red. When controlling the FWER at level 0   05 , the Bonferroni procedure rejects all null hypotheses that fall below the black line, whereas Tukey rejects all those that fall below the blue line. Thus, Tukey's method has slightly higher power than Bonferroni's method. Controlling the Type I error without adjusting for multiple testing involves rejecting all those that fall below the green line.  \n<!-- image -->  \nless conservative. This is exactly the idea behind Tukey's method : when performing m = G ( G -1) glyph[triangleleft] 2 pairwise comparisons of G means, it allows us to control the FWER at level α while rejecting all null hypotheses for which the p -value falls below α T , for some α T &gt; α glyph[triangleleft]m .  \n/negationslash  \nNow, suppose that we once again examine the data in Table 13.3, and notice that Managers One and Three have higher mean returns than Managers Two, Four, and Five. This might motivate us to test the null hypothesis  \nFigure 13.4 illustrates Tukey's method on three simulated data sets in a setting with G = 6 means, with θ 1 = θ 2 = θ 3 = θ 4 = θ 5 = θ 6 . Therefore, of the m = G ( G -1) glyph[triangleleft] 2 = 15 null hypotheses of the form H 0 : θ j = θ k , ten are true and five are false. In each panel, the true null hypotheses are displayed in black, and the false ones are in red. The horizontal lines indicate that Tukey's method always results in at least as many rejections as Bonferroni's method. In the left-hand panel, Tukey correctly rejects two more null hypotheses than Bonferroni.  \n$$H _ { 0 } \\colon \\frac { 1 } { 2 } \\left ( \\mu _ { 1 } + \\mu _ { 3 } \\right ) = \\frac { 1 } { 3 } \\left ( \\mu _ { 2 } + \\mu _ { 4 } + \\mu _ { 5 } \\right ) .$$  \n(Recall that θ j is the population mean return for the j th hedge fund manager.) It turns out that we could test (13.8) using a variant of the twosample t -test presented in (13.1), leading to a p -value of 0   004 . This suggests strong evidence of a difference between Managers One and Three compared to Managers Two, Four, and Five. However, there is a problem: we decided to test the null hypothesis in (13.8) only after peeking at the data in Table 13.3. In a sense, this means that we have conducted multiple testing. In this setting, using Bonferroni to control the FWER at level α  \nTukey's method  \nwould require a p -value threshold of α glyph[triangleleft]m , for an extremely large value of m 14 .  \nTo summarize, Holm's procedure and Bonferroni's procedure are very general approaches for multiple testing correction that can be applied under all circumstances. However, in certain special cases, more powerful procedures for multiple testing correction may be available, in order to control the FWER while achieving higher power (i.e. committing fewer Type II errors) than would be possible using Holm or Bonferroni. In this section, we have illustrated two such examples.  \nScheffé's method is designed for exactly this setting. It allows us to compute a value α S such that rejecting the null hypothesis H 0 in (13.8) if the p -value is below α S will control the Type I error at level α . It turns out that for the Fund example, in order to control the Type I error at level α = 0   05 , we must set α S = 0   002 . Therefore, we are unable to reject H 0 in (13.8), despite the apparently very small p -value of 0   004 . An important advantage of Scheffé's method is that we can use this same threshold of α S = 0   002 in order to perform a pairwise comparison of any split of the managers into two groups: for instance, we could also test H 0 : 1 3 ( θ 1 + θ 2 + θ 3 ) = 1 2 ( θ 4 + θ 5 ) and H 0 : 1 4 ( θ 1 + θ 2 + θ 3 + θ 4 ) = θ 5 using the same threshold of 0   002 , without needing to further adjust for multiple testing.""]","The Benjamini-Hochberg procedure and the Bonferroni correction are both methods used to address the problem of multiple testing, but they differ in their approach and objectives:

1. **Objective**:
   - **Benjamini-Hochberg Procedure**: Aims to control the False Discovery Rate (FDR), which is the expected proportion of false positives among the rejected hypotheses. It is less conservative and allows for more rejections, which can lead to more false positives but also more power to detect true effects.
   - **Bonferroni Correction**: Aims to control the Family-Wise Error Rate (FWER), which is the probability of making at least one Type I error among all the hypotheses. It is more conservative, reducing the chance of any false positives but often at the cost of reduced power to detect true effects.

2. **Rejection Threshold**:
   - **Benjamini-Hochberg Procedure**: The rejection threshold is data-dependent and involves ordering the p-values and comparing them to a threshold that increases with the rank of the p-value. Specifically, it rejects all null hypotheses for which the p-value is less than or equal to the largest p-value that satisfies \( p_{(j)} < \frac{qj}{m} \), where \( q \) is the desired FDR level, \( j \) is the rank, and \( m \) is the total number of hypotheses.
   - **Bonferroni Correction**: The rejection threshold is fixed and independent of the data. It rejects any null hypothesis for which the p-value is below \( \frac{\alpha}{m} \), where \( \alpha \) is the desired FWER level and \( m \) is the total number of hypotheses.

3. **Conservativeness**:
   - **Benjamini-Hochberg Procedure**: Less conservative, allowing for more rejections and thus more power, but with a higher risk of false positives.
   - **Bonferroni Correction**: More conservative, minimizing false positives but often at the expense of power, leading to more Type II errors.

4. **Use Cases**:
   - **Benjamini-Hochberg Procedure**: Preferred when the goal is to identify as many true effects as possible while controlling the proportion of false discoveries.
   - **Bonferroni Correction**: Preferred when the goal is to minimize the chance of any false positives, often used in settings where false positives are particularly costly.

In summary, the Benjamini-Hochberg procedure is more powerful and less conservative than the Bonferroni correction, making it suitable for situations where controlling the proportion of false discoveries is more important than minimizing the chance of any false positives.","The Bonferroni correction controls the FWER by applying a very stringent significance level (alpha divided by the number of tests) to each individual test, making it conservative. The Benjamini-Hochberg procedure, on the other hand, controls the False Discovery Rate (FDR) and is generally more powerful. It orders the p-values and rejects hypotheses based on a dynamic threshold that depends on the rank of the p-value.",1.0,,0.99999999995,1.0
"Describe a scenario where a re-sampling approach, like permutation testing, would be preferred over a traditional parametric test.","[""### 13.5.1 A Re-Sampling Approach to the p -Value  \n/negationslash  \nWereturn to the example of Section 13.1.1, in which we wish to test whether the mean of a random variable X equals the mean of a random variable Y , i.e. H 0 : E( X ) = E( Y ) , against the alternative H a : E( X ) = E( Y ) . Given n X independent observations from X and n Y independent observations from Y , the two-sample t -statistic takes the form  \n$$T = \\frac { \\hat { \\mu } _ { X } - \\hat { \\mu } _ { Y } } { s \\sqrt { \\frac { 1 } { n _ { X } } + \\frac { 1 } { n _ { Y } } } } \\\\$$  \nIf n X and n Y are large, then T in (13.11) approximately follows a N (0  1) distribution. But if n X and n Y are small, then in the absence of a strong assumption about the distribution of X and Y , we do not know the theoretical null distribution of T . 17 In this case, it turns out that we can approximate the null distribution of T using a re-sampling approach, or more specifically, a permutation approach.  \nwhere ˆ θ X = 1 n X ∑ n X i =1 x i , ˆ θ Y = 1 n Y ∑ n Y i =1 y i , s = √ ( n X -1) s 2 X +( n Y -1) s 2 Y n X + n Y -2 , and s 2 X and s 2 Y are unbiased estimators of the variances in the two groups. A large (absolute) value of T provides evidence against H 0 .  \nTo do this, we conduct a thought experiment. If H 0 holds, so that E( X ) = E( Y ) , and we make the stronger assumption that the distributions of X and Y are the same, then the distribution of T is invariant under swapping observations of X with observations of Y . That is, if we randomly swap some of the observations in X with the observations in Y , then the test statistic T in (13.11) computed based on this swapped data has the same distribution as T based on the original data. This is true only if H 0 holds, and the distributions of X and Y are the same.  \nThis suggests that in order to approximate the null distribution of T , we can take the following approach. We randomly permute the n X + n Y observations B times, for some large value of B , and each time we compute (13.11). We let T ∗ 1         T ∗ B denote the values of (13.11) on the permuted data. These can be viewed as an approximation of the null distribution of T under H 0 . Recall that by definition, a p -value is the probability of observing a test statistic at least this extreme under H 0 . Therefore, to compute a p -value for T , we can simply compute  \n$$p \\text {value} = \\frac { \\sum _ { b = 1 } ^ { B } 1 _ { ( | T ^ { * b } | \\geq | T | ) } } { B } ,$$  \nthe fraction of permuted datasets for which the value of the test statistic is at least as extreme as the value observed on the original data. This procedure is summarized in Algorithm 13.3.  \n17 If we assume that X and Y are normally distributed, then T in (13.11) follows a t -distribution with n X + n Y -2 degrees of freedom under H 0 . However, in practice, the distribution of random variables is rarely known, and so it can be preferable to perform a re-sampling approach instead of making strong and unjustified assumptions. If the results of the re-sampling approach disagree with the results of assuming a theoretical null distribution, then the results of the re-sampling approach are more trustworthy.  \nre-sampling permutation\nAlgorithm 13.3 Re-Sampling p -Value for a Two-Sample t -Test  \n1. Compute T , defined in (13.11), on the original data x 1         x n X and y 1         y n Y .\n2. For b = 1         B , where B is a large number (e.g. B = 10  000 ):\n3. (a) Permute the n X + n Y observations at random. Call the first n X permuted observations x ∗ 1         x ∗ n X , and call the remaining n Y observations y ∗ 1         y ∗ n Y .\n4. (b) Compute (13.11) on the permuted data x ∗ 1         x ∗ n X and y ∗ 1         y ∗ n Y , and call the result T ∗ b .\n3. The p -value is given by ∑ B b =1 1 ( ♣ T ∗ b ♣ ≥ ♣ T ♣ ) B .  \nWe try out this procedure on the Khan dataset, which consists of expression measurements for 2  308 genes in four sub-types of small round blood cell tumors, a type of cancer typically seen in children. This dataset is part of the ISLR2 package. We restrict our attention to the two sub-types for which the most observations are available: rhabdomyosarcoma ( n X = 29 ) and Burkitt's lymphoma ( n Y = 25 ).  \nBy contrast, Figure 13.8 shows an analogous set of results for the 877 th gene. In this case, there is a substantial difference between the theoretical and re-sampling null distributions, which results in a difference between their p -values.  \nA two-sample t -test for the null hypothesis that the 11 th gene's mean expression values are equal in the two groups yields T = -2   09 . Using the theoretical null distribution, which is a t 52 distribution (since n X + n Y -2 = 52 ), we obtain a p -value of 0   041 . (Note that a t 52 distribution is virtually indistinguishable from a N (0  1) distribution.) If we instead apply Algorithm 13.3 with B = 10  000 , then we obtain a p -value of 0   042 . Figure 13.7 displays the theoretical null distribution, the re-sampling null distribution, and the actual value of the test statistic ( T = -2   09 ) for this gene. In this example, we see very little difference between the p -values obtained using the theoretical null distribution and the re-sampling null distribution.  \nIn general, in settings with a smaller sample size or a more skewed data distribution (so that the theoretical null distribution is less accurate), the difference between the re-sampling and theoretical p -values will tend to be more pronounced. In fact, the substantial difference between the resampling and theoretical null distributions in Figure 13.8 is due to the fact that a single observation in the 877 th gene is very far from the other observations, leading to a very skewed distribution."", ""### 13.5.3 When Are Re-Sampling Approaches Useful?  \nIn Sections 13.5.1 and 13.5.2, we considered testing null hypotheses of the form H 0 : E( X ) = E( Y ) using a two-sample t -statistic (13.11), for which we  \n18 To implement Algorithm 13.4 efficiently, the same set of permutations in Step 2(b)i. should be used for all m null hypotheses.\nAlgorithm 13.4 Plug-In FDR for a Two-Sample T -Test  \n1. Select a threshold c , where c &gt; 0 .\n2. For j = 1         m :\n3. (a) Compute T ( j ) , the two-sample t -statistic (13.11) for the null hypothesis H 0 j on the basis of the original data, x ( j ) 1         x ( j ) n X and y ( j ) 1         y ( j ) n Y .\n4. (b) For b = 1         B , where B is a large number (e.g. B = 10  000 ):\n- i. Permute the n X + n Y observations at random. Call the first n X observations x ∗ ( j ) 1         x ∗ ( j ) n X , and call the remaining observations y ∗ ( j ) 1         y ∗ ( j ) n Y .\n- ii. Compute (13.11) on the permuted data x ∗ ( j ) 1         x ∗ ( j ) n X and y ∗ ( j ) 1         y ∗ ( j ) n Y , and call the result T ( j )  ∗ b .\n3. Compute R = ∑ m j =1 1 ( ♣ T ( j ) ♣ ≥ c ) .\n5. The estimated FDR associated with the threshold c is ̂ V glyph[triangleleft]R .\n4. Compute ̂ V = ∑ B b =1 ∑ j m =1 1 ( ♣ T ( j )  ∗ b ♣ ≥ c ) B .  \napproximated the null distribution via a re-sampling approach. We saw that using the re-sampling approach gave us substantially different results from using the theoretical p -value approach in Figure 13.8, but not in Figure 13.7.  \nIn general, there are two settings in which a re-sampling approach is particularly useful:  \n1. Perhaps no theoretical null distribution is available. This may be the case if you are testing an unusual null hypothesis H 0 , or using an unsual test statistic T .\n2. Perhaps a theoretical null distribution is available, but the assumptions required for its validity do not hold. For instance, the twosample t -statistic in (13.11) follows a t n X + n Y -2 distribution only if the observations are normally distributed. Furthermore, it follows a N (0  1) distribution only if n X and n Y are quite large. If the data are non-normal and n X and n Y are small, then p -values that make use of the theoretical null distribution will not be valid (i.e. they will not properly control the Type I error).  \nIn general, if you can come up with a way to re-sample or permute your observations in order to generate data that follow the null distribution, then you can compute p -values or estimate the FDR using variants of Algorithms 13.3 and 13.4. In many real-world settings, this provides a powerful tool for hypothesis testing when no out-of-box hypothesis tests are available, or when the key assumptions underlying those out-of-box tests are violated.  \nFIGURE 13.9. For j = 1         m = 2  308 , we tested the null hypothesis that for the j th gene in the Khan dataset, the mean expression in Burkitt's lymphoma equals the mean expression in rhabdomyosarcoma. For each value of k from 1 to 2  308 , the y -axis displays the estimated FDR associated with rejecting the null hypotheses corresponding to the k smallest p -values. The orange dashed curve shows the FDR obtained using the Benjamini-Hochberg procedure, whereas the blue solid curve shows the FDR obtained using the re-sampling approach of Algorithm 13.4, with B = 10  000 . There is very little difference between the two FDR estimates. According to either estimate, rejecting the null hypothesis for the 500 genes with the smallest p -values corresponds to an FDR of around 17.7%.  \n<!-- image -->"", '### 13.5.2 A Re-Sampling Approach to the False Discovery Rate  \nNow, suppose that we wish to control the FDR for m null hypotheses, H 01         H 0 m , in a setting in which either no theoretical null distribution is available, or else we simply prefer to avoid the use of a theoretical null  \n<!-- image -->  \n400  \n300  \n200  \n100  \n0  \nFIGURE 13.7. The 11 th gene in the Khan dataset has a test statistic of T = -2   09 . Its theoretical and re-sampling null distributions are almost identical. The theoretical p -value equals 0   041 and the re-sampling p -value equals 0   042 .  \n<!-- image -->  \n400  \n300  \n200  \n100  \n0  \nFIGURE 13.8. The 877 th gene in the Khan dataset has a test statistic of T = -0   57 . Its theoretical and re-sampling null distributions are quite different. The theoretical p -value equals 0   571 , and the re-sampling p -value equals 0   673 .  \n<!-- image -->  \ndistribution. As in Section 13.5.1, we make use of a two-sample t -statistic for each hypothesis, leading to the test statistics T 1         T m . We could simply compute a p -value for each of the m null hypotheses, as in Section 13.5.1, and then apply the Benjamini-Hochberg procedure of Section 13.4.2 to these p -values. However, it turns out that we can do this in a more direct way, without even needing to compute p -values.  \nRecall from Section 13.4 that the FDR is defined as E( Vglyph[triangleleft]R ) , using the notation in Table 13.2. In order to estimate the FDR via re-sampling, we first make the following approximation:  \n$$\\ F D R = E \\left ( \\frac { V } { R } \\right ) \\approx \\frac { E ( V ) } { R } .$$  \nNow suppose we reject any null hypothesis for which the test statistic exceeds c in absolute value. Then computing R in the denominator on the right-hand side of (13.13) is straightforward: R = ∑ m j =1 1 ( ♣ T j ♣ ≥ c ) .  \nHowever, the numerator E( V ) on the right-hand side of (13.13) is more challenging. This is the expected number of false positives associated with rejecting any null hypothesis for which the test statistic exceeds c in absolute value. At the risk of stating the obvious, estimating V is challenging because we do not know which of H 01         H 0 m are really true, and so we do not know which rejected hypotheses are false positives. To overcome this problem, we take a re-sampling approach, in which we simulate data under H 01         H 0 m , and then compute the resulting test statistics. The number of re-sampled test statistics that exceed c provides an estimate of V .  \nAlgorithm 13.4 details this procedure. 18 It provides what is known as a plug-in estimate of the FDR, because the approximation in (13.13) allows us to estimate the FDR by plugging R into the denominator and an estimate for E( V ) into the numerator.  \nIn greater detail, in the case of a two-sample t -statistic (13.11) for each of the null hypotheses H 01         H 0 m , we can estimate E( V ) as follows. Let x ( j ) 1         x ( j ) n X and y ( j ) 1         y ( j ) n Y denote the data associated with the j th null hypothesis, j = 1         m . We permute these n X + n Y observations at random, and then compute the t -statistic on the permuted data. For this permuted data, we know that all of the null hypotheses H 01         H 0 m hold; therefore, the number of permuted t -statistics that exceed the threshold c in absolute value provides an estimate for E( V ) . This estimate can be further improved by repeating the permutation process B times, for a large value of B , and averaging the results.  \nWe apply the re-sampling approach to the FDR from Algorithm 13.4, as well as the Benjamini-Hochberg approach from Algorithm 13.2 using theoretical p -values, to the m = 2  308 genes in the Khan dataset. Results are shown in Figure 13.9. We see that for a given number of rejected hypotheses, the estimated FDRs are almost identical for the two methods.  \nWe began this section by noting that in order to control the FDR for m hypothesis tests using a re-sampling approach, we could simply compute m re-sampling p -values as in Section 13.5.1, and then apply the BenjaminiHochberg procedure of Section 13.4.2 to these p -values. It turns out that if we define the j th re-sampling p -value as  \n$$p _ { j } & = \\frac { \\sum _ { j ^ { \\prime } = 1 } ^ { m } \\sum _ { b = 1 } ^ { B } 1 _ { ( | T _ { j ^ { \\prime } } ^ { * } b | \\geq | T _ { j } | ) } } { B m } \\\\$$  \nfor j = 1         m , instead of as in (13.12), then applying the BenjaminiHochberg procedure to these re-sampled p -values is exactly equivalent to Algorithm 13.4. Note that (13.14) is an alternative to (13.12) that pools the information across all m hypothesis tests in approximating the null distribution.']","A re-sampling approach, such as permutation testing, would be preferred over a traditional parametric test in scenarios where the assumptions required for the validity of the parametric test do not hold. For instance, consider a situation where you are comparing the means of two groups using a two-sample t-test. The traditional t-test assumes that the data in each group are normally distributed and that the sample sizes are sufficiently large. However, if the data are non-normal and the sample sizes are small, these assumptions are violated, and the p-values obtained from the traditional t-test may not be valid, as they will not properly control the Type I error.

In such cases, a re-sampling approach like permutation testing is useful because it does not rely on these distributional assumptions. Instead, it approximates the null distribution by randomly permuting the data and recalculating the test statistic multiple times. This provides a more accurate p-value that reflects the actual data distribution, making it a powerful tool for hypothesis testing when the assumptions of parametric tests are not met.","A re-sampling approach, like permutation testing, is preferred when the assumptions of a parametric test are not met. For example, if the data is not normally distributed, especially with small sample sizes, the results of a t-test might not be valid. In such cases, permutation tests provide a more robust way to assess statistical significance without relying on distributional assumptions.",1.0,,0.9999999999666667,1.0
¿Qué es la 'maldición de la dimensionalidad' y cómo afecta a los métodos de aprendizaje estadístico?,"[""## 3.5 Comparison of Linear Regression with K -Nearest Neighbors  \nAs discussed in Chapter 2, linear regression is an example of a parametric approach because it assumes a linear functional form for f ( X ) . Parametric methods have several advantages. They are often easy to fit, because one need estimate only a small number of coefficients. In the case of linear regression, the coefficients have simple interpretations, and tests of statistical significance can be easily performed. But parametric methods do have a disadvantage: by construction, they make strong assumptions about the form of f ( X ) . If the specified functional form is far from the truth, and prediction accuracy is our goal, then the parametric method will perform poorly. For instance, if we assume a linear relationship between X and Y but the true relationship is far from linear, then the resulting model will provide a poor fit to the data, and any conclusions drawn from it will be suspect.  \nIn contrast, non-parametric methods do not explicitly assume a parametric form for f ( X ) , and thereby provide an alternative and more flexible approach for performing regression. We discuss various non-parametric methods in this book. Here we consider one of the simplest and best-known non-parametric methods, K -nearest neighbors regression (KNN regression). The KNN regression method is closely related to the KNN classifier discussed in Chapter 2. Given a value for K and a prediction point x 0 , KNN regression first identifies the K training observations that are closest to x 0 , represented by N 0 . It then estimates f ( x 0 ) using the average of all the training responses in N 0 . In other words,  \n$$\\hat { f } ( x _ { 0 } ) = \\frac { 1 } { K } \\sum _ { x _ { i } \\in \\mathcal { N } _ { 0 } } y _ { i } .$$  \nFigure 3.16 illustrates two KNN fits on a data set with p = 2 predictors. The fit with K = 1 is shown in the left-hand panel, while the right-hand panel corresponds to K = 9 . We see that when K = 1 , the KNN fit perfectly interpolates the training observations, and consequently takes the form of a step function. When K = 9 , the KNN fit still is a step function, but averaging over nine observations results in much smaller regions of constant prediction, and consequently a smoother fit. In general, the optimal value for K will depend on the bias-variance tradeoff , which we introduced in Chapter 2. A small value for K provides the most flexible fit, which will have low bias but high variance. This variance is due to the fact that the prediction in a given region is entirely dependent on just one observation.  \nK -nearest neighbors regression  \nFIGURE 3.16. Plots of ˆ f ( X ) using KNN regression on a two-dimensional data set with 64 observations (orange dots). Left: K = 1 results in a rough step function fit. Right: K = 9 produces a much smoother fit.  \n<!-- image -->  \nIn contrast, larger values of K provide a smoother and less variable fit; the prediction in a region is an average of several points, and so changing one observation has a smaller effect. However, the smoothing may cause bias by masking some of the structure in f ( X ) . In Chapter 5, we introduce several approaches for estimating test error rates. These methods can be used to identify the optimal value of K in KNN regression.  \nIn practice, the true relationship between X and Y is rarely exactly linear. Figure 3.19 examines the relative performances of least squares regression and KNN under increasing levels of non-linearity in the relationship between X and Y . In the top row, the true relationship is nearly linear. In this case we see that the test MSE for linear regression is still superior  \nIn what setting will a parametric approach such as least squares linear regression outperform a non-parametric approach such as KNN regression? The answer is simple: the parametric approach will outperform the nonparametric approach if the parametric form that has been selected is close to the true form of f . Figure 3.17 provides an example with data generated from a one-dimensional linear regression model. The black solid lines represent f ( X ) , while the blue curves correspond to the KNN fits using K = 1 and K = 9 . In this case, the K = 1 predictions are far too variable, while the smoother K = 9 fit is much closer to f ( X ) . However, since the true relationship is linear, it is hard for a non-parametric approach to compete with linear regression: a non-parametric approach incurs a cost in variance that is not offset by a reduction in bias. The blue dashed line in the lefthand panel of Figure 3.18 represents the linear regression fit to the same data. It is almost perfect. The right-hand panel of Figure 3.18 reveals that linear regression outperforms KNN for this data. The green solid line, plotted as a function of 1 glyph[triangleleft]K , represents the test set mean squared error (MSE) for KNN. The KNN errors are well above the black dashed line, which is the test MSE for linear regression. When the value of K is large, then KNN performs only a little worse than least squares regression in terms of MSE. It performs far worse when K is small.  \n<!-- image -->  \nx  \nx  \nFIGURE 3.17. Plots of ˆ f ( X ) using KNN regression on a one-dimensional data set with 50 observations. The true relationship is given by the black solid line. Left: The blue curve corresponds to K = 1 and interpolates (i.e. passes directly through) the training data. Right: The blue curve corresponds to K = 9 , and represents a smoother fit.  \n<!-- image -->  \nFIGURE 3.18. The same data set shown in Figure 3.17 is investigated further. Left: The blue dashed line is the least squares fit to the data. Since f ( X ) is in fact linear (displayed as the black line), the least squares regression line provides a very good estimate of f ( X ) . Right: The dashed horizontal line represents the least squares test set MSE, while the green solid line corresponds to the MSE for KNN as a function of 1 glyph[triangleleft]K (on the log scale). Linear regression achieves a lower test MSE than does KNN regression, since f ( X ) is in fact linear. For KNN regression, the best results occur with a very large value of K , corresponding to a small value of 1 glyph[triangleleft]K .  \nFIGURE 3.19. Top Left: In a setting with a slightly non-linear relationship between X and Y (solid black line), the KNN fits with K = 1 (blue) and K = 9 (red) are displayed. Top Right: For the slightly non-linear data, the test set MSE for least squares regression (horizontal black) and KNN with various values of 1 glyph[triangleleft]K (green) are displayed. Bottom Left and Bottom Right: As in the top panel, but with a strongly non-linear relationship between X and Y .  \n<!-- image -->  \nto that of KNN for low values of K . However, for K ≥ 4 , KNN outperforms linear regression. The second row illustrates a more substantial deviation from linearity. In this situation, KNN substantially outperforms linear regression for all values of K . Note that as the extent of non-linearity increases, there is little change in the test set MSE for the non-parametric KNN method, but there is a large increase in the test set MSE of linear regression.  \nFigures 3.18 and 3.19 display situations in which KNN performs slightly worse than linear regression when the relationship is linear, but much better than linear regression for nonlinear situations. In a real life situation in which the true relationship is unknown, one might suspect that KNN should be favored over linear regression because it will at worst be slightly inferior to linear regression if the true relationship is linear, and may give substantially better results if the true relationship is non-linear. But in reality, even when the true relationship is highly non-linear, KNN may still provide inferior results to linear regression. In particular, both Figures 3.18  \nFIGURE 3.20. Test MSE for linear regression (black dashed lines) and KNN (green curves) as the number of variables p increases. The true function is nonlinear in the first variable, as in the lower panel in Figure 3.19, and does not depend on the additional variables. The performance of linear regression deteriorates slowly in the presence of these additional noise variables, whereas KNN's performance degrades much more quickly as p increases.  \n<!-- image -->  \nand 3.19 illustrate settings with p = 1 predictor. But in higher dimensions, KNN often performs worse than linear regression.  \nEven when the dimension is small, we might prefer linear regression to KNN from an interpretability standpoint. If the test MSE of KNN is only slightly lower than that of linear regression, we might be willing to forego a little bit of prediction accuracy for the sake of a simple model that can be described in terms of just a few coefficients, and for which p -values are available.  \nFigure 3.20 considers the same strongly non-linear situation as in the second row of Figure 3.19, except that we have added additional noise predictors that are not associated with the response. When p = 1 or p = 2 , KNN outperforms linear regression. But for p = 3 the results are mixed, and for p ≥ 4 linear regression is superior to KNN. In fact, the increase in dimension has only caused a small deterioration in the linear regression test set MSE, but it has caused more than a ten-fold increase in the MSE for KNN. This decrease in performance as the dimension increases is a common problem for KNN, and results from the fact that in higher dimensions there is effectively a reduction in sample size. In this data set there are 50 training observations; when p = 1 , this provides enough information to accurately estimate f ( X ) . However, spreading 50 observations over p = 20 dimensions results in a phenomenon in which a given observation has no nearby neighbors -this is the so-called curse of dimensionality . That is, the K observations that are nearest to a given test observation x 0 may be very far away from x 0 in p -dimensional space when p is large, leading to a very poor prediction of f ( x 0 ) and hence a poor KNN fit. As a general rule, parametric methods will tend to outperform non-parametric approaches when there is a small number of observations per predictor.  \ncurse of dimensionality"", ""## 4.8 Exercises  \n### Conceptual  \n1. Using a little bit of algebra, prove that (4.2) is equivalent to (4.3). In other words, the logistic function representation and logit representation for the logistic regression model are equivalent.\n2. It was stated in the text that classifying an observation to the class for which (4.17) is largest is equivalent to classifying an observation to the class for which (4.18) is largest. Prove that this is the case. In other words, under the assumption that the observations in the k th class are drawn from a N ( θ k  σ 2 ) distribution, the Bayes classifier assigns an observation to the class for which the discriminant function is maximized.\n3. This problem relates to the QDA model, in which the observations within each class are drawn from a normal distribution with a classspecific mean vector and a class specific covariance matrix. We consider the simple case where p = 1 ; i.e. there is only one feature.\n4. Suppose that we have classes, and that if an observation belongs  \nK to the k th class then X comes from a one-dimensional normal distribution, X ∼ N ( θ k  σ 2 k ) . Recall that the density function for the one-dimensional normal distribution is given in (4.16). Prove that in this case, the Bayes classifier is not linear. Argue that it is in fact quadratic.  \nHint: For this problem, you should follow the arguments laid out in Section 4.4.1, but without making the assumption that σ 2 1 = · · · = σ 2 K .  \n4. When the number of features p is large, there tends to be a deterioration in the performance of KNN and other local approaches that perform prediction using only observations that are near the test observation for which a prediction must be made. This phenomenon is known as the curse of dimensionality , and it ties into the fact that non-parametric approaches often perform poorly when p is large. We will now investigate this curse.  \n<!-- image -->  \ncurse of dimensionality  \n- (a) Suppose that we have a set of observations, each with measurements on p = 1 feature, X . We assume that X is uniformly (evenly) distributed on [0  1] . Associated with each observation is a response value. Suppose that we wish to predict a test observation's response using only observations that are within 10 % of the range of X closest to that test observation. For instance, in order to predict the response for a test observation with X = 0   6 , we will use observations in the range [0   55  0   65] . On average, what fraction of the available observations will we use to make the prediction?\n- (b) Now suppose that we have a set of observations, each with measurements on p = 2 features, X 1 and X 2 . We assume that ( X 1  X 2 ) are uniformly distributed on [0  1] × [0  1] . We wish to predict a test observation's response using only observations that are within 10 % of the range of X 1 and within 10 % of the range of X 2 closest to that test observation. For instance, in order to predict the response for a test observation with X 1 = 0   6 and X 2 = 0   35 , we will use observations in the range [0   55  0   65] for X 1 and in the range [0   3  0   4] for X 2 . On average, what fraction of the available observations will we use to make the prediction?\n- (c) Now suppose that we have a set of observations on p = 100 features. Again the observations are uniformly distributed on each feature, and again each feature ranges in value from 0 to 1. We wish to predict a test observation's response using observations within the 10 % of each feature's range that is closest to that test observation. What fraction of the available observations will we use to make the prediction?\n- (d) Using your answers to parts (a)-(c), argue that a drawback of KNN when p is large is that there are very few training observations 'near' any given test observation.\n- (e) Now suppose that we wish to make a prediction for a test observation by creating a p -dimensional hypercube centered around the test observation that contains, on average, 10 % of the training observations. For p = 1  2 , and 100 , what is the length of each side of the hypercube? Comment on your answer.  \nNote: A hypercube is a generalization of a cube to an arbitrary number of dimensions. When p = 1 , a hypercube is simply a line segment, when p = 2 it is a square, and when p = 100 it is a 100-dimensional cube.  \n5. We now examine the differences between LDA and QDA.\n2. (a) If the Bayes decision boundary is linear, do we expect LDA or QDA to perform better on the training set? On the test set?\n3. (b) If the Bayes decision boundary is non-linear, do we expect LDA or QDA to perform better on the training set? On the test set?  \n- (c) In general, as the sample size n increases, do we expect the test prediction accuracy of QDA relative to LDA to improve, decline, or be unchanged? Why?\n- (d) True or False: Even if the Bayes decision boundary for a given problem is linear, we will probably achieve a superior test error rate using QDA rather than LDA because QDA is flexible enough to model a linear decision boundary. Justify your answer.\n6. Suppose we collect data for a group of students in a statistics class with variables X 1 =hours studied, X 2 =undergrad GPA, and Y = receive an A. We fit a logistic regression and produce estimated coefficient, ˆ β 0 = -6  ˆ β 1 = 0   05  ˆ β 2 = 1 .\n- (a) Estimate the probability that a student who studies for 40 h and has an undergrad GPA of 3   5 gets an A in the class.\n- (b) How many hours would the student in part (a) need to study to have a 50 % chance of getting an A in the class?\n7. Suppose that we wish to predict whether a given stock will issue a dividend this year ('Yes' or 'No') based on X , last year's percent profit. We examine a large number of companies and discover that the mean value of X for companies that issued a dividend was ¯ X = 10 , while the mean for those that didn't was ¯ X = 0 . In addition, the variance of X for these two sets of companies was ˆ σ 2 = 36 . Finally, 80 % of companies issued dividends. Assuming that X follows a normal distribution, predict the probability that a company will issue a dividend this year given that its percentage profit was X = 4 last year.  \nHint: Recall that the density function for a normal random variable is f ( x ) = 1 √ 2 πσ 2 e -( x -θ ) 2 glyph[triangleleft] 2 σ 2 . You will need to use Bayes' theorem.  \n8. Suppose that we take a data set, divide it into equally-sized training and test sets, and then try out two different classification procedures. First we use logistic regression and get an error rate of 20 % on the training data and 30 % on the test data. Next we use 1-nearest neighbors (i.e. K = 1 ) and get an average error rate (averaged over both test and training data sets) of 18 %. Based on these results, which method should we prefer to use for classification of new observations? Why?\n9. This problem has to do with odds .\n3. (a) On average, what fraction of people with an odds of 0.37 of defaulting on their credit card payment will in fact default?\n4. (b) Suppose that an individual has a 16 % chance of defaulting on her credit card payment. What are the odds that she will default?  \n10. Equation 4.32 derived an expression for log ( Pr( Y = k ♣ X = x ) Pr( Y = K ♣ X = x ) ) in the setting where p &gt; 1 , so that the mean for the k th class, θ k , is a p -dimensional vector, and the shared covariance Σ is a p × p matrix. However, in the setting with p = 1 , (4.32) takes a simpler form, since the means θ 1         θ K and the variance σ 2 are scalars. In this simpler setting, repeat the calculation in (4.32), and provide expressions for a k and b kj in terms of π k , π K , θ k , θ K , and σ 2 .\n11. Work out the detailed forms of a k , b kj , and b kjl in (4.33). Your answer should involve π k , π K , θ k , θ K , Σ k , and Σ K .\n12. Suppose that you wish to classify an observation X ∈ R into apples and oranges . You fit a logistic regression model and find that  \n$$\\widehat { \\Pr } ( Y = \\text {orange} | X = x ) = \\frac { \\exp ( \\hat { \\beta } _ { 0 } + \\hat { \\beta } _ { 1 } x ) } { 1 + \\exp ( \\hat { \\beta } _ { 0 } + \\hat { \\beta } _ { 1 } x ) } . \\\\ \\text {fr} \\text { friend fits a logistic regression model to the same data using}$$  \nYour friend fits a logistic regression model to the same data using the softmax formulation in (4.13), and finds that  \n$$\\widehat { \\Pr } ( Y = \\text {orange} | X = x ) = \\\\ \\frac { \\exp ( \\hat { \\alpha } _ { \\text {orange} } 0 + \\hat { \\alpha } _ { \\text {orange} } 1 x ) } { \\exp ( \\hat { \\alpha } _ { \\text {orange} } 0 + \\hat { \\alpha } _ { \\text {orange} } 1 x ) + \\exp ( \\hat { \\alpha } _ { \\text {apple} } 0 + \\hat { \\alpha } _ { \\text {apple} } 1 x ) } .$$  \n- (a) What is the log odds of orange versus apple in your model?\n- (b) What is the log odds of orange versus apple in your friend's model?\n- (c) Suppose that in your model, ˆ β 0 = 2 and ˆ β 1 = -1 . What are the coefficient estimates in your friend's model? Be as specific as possible.\n- (d) Now suppose that you and your friend fit the same two models on a different data set. This time, your friend gets the coefficient estimates ˆ α orange 0 = 1   2 , ˆ α orange 1 = -2 , ˆ α orange 0 = 3 , ˆ α orange 1 = 0   6 . What are the coefficient estimates in your model?\n- (e) Finally, suppose you apply both models from (d) to a data set with 2,000 test observations. What fraction of the time do you expect the predicted class labels from your model to agree with those from your friend's model? Explain your answer."", ""# Index  \n| accuracy, 415 activation, 400 activation function, 401 additive, 11, 94-98, 110-111 additivity, 305, 306 adjusted R 2 , 87, 231, 232, 236- 238 Advertising data set, 15, 16, 19, 69, 71-73, 77, 78, 80, 82, 83, 85, 87-90, 95, 96, 109- 111 agglomerative clustering, 525 Akaike information criterion, 87, 231, 232, 236-238 alternative hypothesis, 76, 559 analysis of variance, 312 ANOVA, 587 area under the curve, 155, 486- 487 argument, 40 array, 42 attribute, 42 AUC, 155 Auto data set, 12, 66, 98-101, 129, 197, 202-207, 327, 398 auto-correlation, 421 autoregression, 423 axes, 48 backfitting, 307, 328   | backpropagation, 429 backward stepwise selection, 87, 234-235 bag-of- n -grams, 415 bag-of-words, 414 bagging, 11, 24, 331, 343-346, 354, 360-361 BART, 343, 350, 353, 354, 362- 363 baseline, 93, 145, 161 basis function, 293-294, 296 Bayes classifier, 35-37, 147 decision boundary, 148 error, 35-37 Bayes' theorem, 146, 250 Bayesian, 250-251, 353 Bayesian additive regression trees, 331, 343, 350, 350, 353, 354, 362-363 Bayesian information criterion, 87, 231, 232, 236-238 Benjamini-Hochberg procedure, 577 Bernoulli distribution, 172   |\n|----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|\n| ©Springer Nature Switzerland AG 2023 Learning                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                              | Springer Texts in Statistics, 597                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                         |\n| G. James et al., An Introduction to Statistical                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                            | 575- best subset selection, 231, 246 bias, 31-34, 74, 90, 159, 405 bias-variance decomposition, 32                                                                                                                                                                                                                                                                                                                                                                                                                                                        |  \n| trade-off, 31-34, 38, 111-112, 157, 159, 163, 164, 242, 254, 263, 266, 301, 336, 376, 385 bidirectional, 425 Bikeshare data set, 12, 167-172 binary, 27, 138 biplot, 507, 508 Bonferroni method, 575-577, 585 Boolean, 53, 176 boosting, 11, 24, 331, 343, 347- 350, 354, 361-362 bootstrap, 11, 201, 212-214, 343 Boston data set, 12, 67, 117, 122, 133, 199, 227, 287, 327, 364, 556   | bottom-up, 525 hierarchical, 521, 525-535 K -means, 11, 521-524 Cochran-Mantel-Haenszel test, 475 coefficient, 71 College data set, 12, 65, 286, 328 collinearity, 106-110 concatenation, 41 conditional probability, 35 confidence interval, 75-76, 90, 110, 292 confounding, 144 confusion matrix, 153, 176   | bottom-up, 525 hierarchical, 521, 525-535 K -means, 11, 521-524 Cochran-Mantel-Haenszel test, 475 coefficient, 71 College data set, 12, 65, 286, 328 collinearity, 106-110 concatenation, 41 conditional probability, 35 confidence interval, 75-76, 90, 110, 292 confounding, 144 confusion matrix, 153, 176   | bottom-up, 525 hierarchical, 521, 525-535 K -means, 11, 521-524 Cochran-Mantel-Haenszel test, 475 coefficient, 71 College data set, 12, 65, 286, 328 collinearity, 106-110 concatenation, 41 conditional probability, 35 confidence interval, 75-76, 90, 110, 292 confounding, 144 confusion matrix, 153, 176   | bottom-up, 525 hierarchical, 521, 525-535 K -means, 11, 521-524 Cochran-Mantel-Haenszel test, 475 coefficient, 71 College data set, 12, 65, 286, 328 collinearity, 106-110 concatenation, 41 conditional probability, 35 confidence interval, 75-76, 90, 110, 292 confounding, 144 confusion matrix, 153, 176   |\n|-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|\n|                                                                                                                                                                                                                                                                                                                                                                                           | continuous, 2 contour, 246                                                                                                                                                                                                                                                                                      | continuous, 2 contour, 246                                                                                                                                                                                                                                                                                      | continuous, 2 contour, 246                                                                                                                                                                                                                                                                                      | continuous, 2 contour, 246                                                                                                                                                                                                                                                                                      |\n| bottom-up clustering, 525                                                                                                                                                                                                                                                                                                                                                                 | contour plot, 50                                                                                                                                                                                                                                                                                                | contour plot, 50                                                                                                                                                                                                                                                                                                | contour plot, 50                                                                                                                                                                                                                                                                                                | contour plot, 50                                                                                                                                                                                                                                                                                                |\n| boxplot, 62                                                                                                                                                                                                                                                                                                                                                                               | contrast, 94                                                                                                                                                                                                                                                                                                    | contrast, 94                                                                                                                                                                                                                                                                                                    | contrast, 94                                                                                                                                                                                                                                                                                                    | contrast, 94                                                                                                                                                                                                                                                                                                    |\n| BrainCancer data set, 12, 472-                                                                                                                                                                                                                                                                                                                                                            | convenience function, 53                                                                                                                                                                                                                                                                                        | convenience function, 53                                                                                                                                                                                                                                                                                        | convenience function, 53                                                                                                                                                                                                                                                                                        | convenience function, 53                                                                                                                                                                                                                                                                                        |\n| 474, 476, 482                                                                                                                                                                                                                                                                                                                                                                             | convolution filter, 407                                                                                                                                                                                                                                                                                         | convolution filter, 407                                                                                                                                                                                                                                                                                         | convolution filter, 407                                                                                                                                                                                                                                                                                         | convolution filter, 407                                                                                                                                                                                                                                                                                         |\n| branch, 333                                                                                                                                                                                                                                                                                                                                                                               | convolution layer, 407 convolutional neural network,                                                                                                                                                                                                                                                            | convolution layer, 407 convolutional neural network,                                                                                                                                                                                                                                                            | convolution layer, 407 convolutional neural network,                                                                                                                                                                                                                                                            | convolution layer, 407 convolutional neural network,                                                                                                                                                                                                                                                            |\n| burn-in, 352                                                                                                                                                                                                                                                                                                                                                                              | 406- 413                                                                                                                                                                                                                                                                                                        | 406- 413                                                                                                                                                                                                                                                                                                        | 406- 413                                                                                                                                                                                                                                                                                                        | 406- 413                                                                                                                                                                                                                                                                                                        |\n| C-index, 487                                                                                                                                                                                                                                                                                                                                                                              | correlation, 79, 82-83, 530                                                                                                                                                                                                                                                                                     | correlation, 79, 82-83, 530                                                                                                                                                                                                                                                                                     | correlation, 79, 82-83, 530                                                                                                                                                                                                                                                                                     | correlation, 79, 82-83, 530                                                                                                                                                                                                                                                                                     |\n| Caravan data set, 12, 184, 366                                                                                                                                                                                                                                                                                                                                                            | count data, 167, 170                                                                                                                                                                                                                                                                                            | count data, 167, 170                                                                                                                                                                                                                                                                                            | count data, 167, 170                                                                                                                                                                                                                                                                                            | count data, 167, 170                                                                                                                                                                                                                                                                                            |\n| Carseats data set, 12, 126, 130, 364                                                                                                                                                                                                                                                                                                                                                      | Cox's proportional hazards model, 480, 483-486                                                                                                                                                                                                                                                                  | Cox's proportional hazards model, 480, 483-486                                                                                                                                                                                                                                                                  | Cox's proportional hazards model, 480, 483-486                                                                                                                                                                                                                                                                  | Cox's proportional hazards model, 480, 483-486                                                                                                                                                                                                                                                                  |\n| categorical, 2, 27                                                                                                                                                                                                                                                                                                                                                                        | C p , 87, 231, 232, 236-238                                                                                                                                                                                                                                                                                     | C p , 87, 231, 232, 236-238                                                                                                                                                                                                                                                                                     | C p , 87, 231, 232, 236-238                                                                                                                                                                                                                                                                                     | C p , 87, 231, 232, 236-238                                                                                                                                                                                                                                                                                     |\n| censored data, 469-502                                                                                                                                                                                                                                                                                                                                                                    | Credit data set, 12, 91, 92, 94,                                                                                                                                                                                                                                                                                | Credit data set, 12, 91, 92, 94,                                                                                                                                                                                                                                                                                | Credit data set, 12, 91, 92, 94,                                                                                                                                                                                                                                                                                | Credit data set, 12, 91, 92, 94,                                                                                                                                                                                                                                                                                |\n| censoring independent, 471                                                                                                                                                                                                                                                                                                                                                                | 97, 98, 106-109 cross-entropy, 405                                                                                                                                                                                                                                                                              | 97, 98, 106-109 cross-entropy, 405                                                                                                                                                                                                                                                                              | 97, 98, 106-109 cross-entropy, 405                                                                                                                                                                                                                                                                              | 97, 98, 106-109 cross-entropy, 405                                                                                                                                                                                                                                                                              |\n| interval, 471                                                                                                                                                                                                                                                                                                                                                                             | cross-validation, 11, 31, 34, 201-                                                                                                                                                                                                                                                                              | cross-validation, 11, 31, 34, 201-                                                                                                                                                                                                                                                                              | cross-validation, 11, 31, 34, 201-                                                                                                                                                                                                                                                                              | cross-validation, 11, 31, 34, 201-                                                                                                                                                                                                                                                                              |\n| left, 471                                                                                                                                                                                                                                                                                                                                                                                 | 211, 231, 252, 270                                                                                                                                                                                                                                                                                              | 211, 231, 252, 270                                                                                                                                                                                                                                                                                              | 211, 231, 252, 270                                                                                                                                                                                                                                                                                              | 211, 231, 252, 270                                                                                                                                                                                                                                                                                              |\n| mechanism, 471                                                                                                                                                                                                                                                                                                                                                                            | k -fold, 206-209                                                                                                                                                                                                                                                                                                | k -fold, 206-209                                                                                                                                                                                                                                                                                                | k -fold, 206-209                                                                                                                                                                                                                                                                                                | k -fold, 206-209                                                                                                                                                                                                                                                                                                |\n| 471 right, 471                                                                                                                                                                                                                                                                                                                                                                            |                                                                                                                                                                                                                                                                                                                 |                                                                                                                                                                                                                                                                                                                 |                                                                                                                                                                                                                                                                                                                 |                                                                                                                                                                                                                                                                                                                 |\n|                                                                                                                                                                                                                                                                                                                                                                                           | curse of dimensionality, 115, 193,                                                                                                                                                                                                                                                                              | curse of dimensionality, 115, 193,                                                                                                                                                                                                                                                                              | curse of dimensionality, 115, 193,                                                                                                                                                                                                                                                                              | curse of dimensionality, 115, 193,                                                                                                                                                                                                                                                                              |\n| chain                                                                                                                                                                                                                                                                                                                                                                                     |                                                                                                                                                                                                                                                                                                                 |                                                                                                                                                                                                                                                                                                                 |                                                                                                                                                                                                                                                                                                                 |                                                                                                                                                                                                                                                                                                                 |\n|                                                                                                                                                                                                                                                                                                                                                                                           | Data Advertising , 15, 16, 19,                                                                                                                                                                                                                                                                                  | Data Advertising , 15, 16, 19,                                                                                                                                                                                                                                                                                  | Data Advertising , 15, 16, 19,                                                                                                                                                                                                                                                                                  | Data Advertising , 15, 16, 19,                                                                                                                                                                                                                                                                                  |\n| tree, 337-341, 355-358                                                                                                                                                                                                                                                                                                                                                                    | 69, 71-73, 77, 78, 80, 82, 83,                                                                                                                                                                                                                                                                                  | 69, 71-73, 77, 78, 80, 82, 83,                                                                                                                                                                                                                                                                                  | 69, 71-73, 77, 78, 80, 82, 83,                                                                                                                                                                                                                                                                                  | 69, 71-73, 77, 78, 80, 82, 83,                                                                                                                                                                                                                                                                                  |\n| classifier, 135                                                                                                                                                                                                                                                                                                                                                                           | 85, 87-90, 95, 96, 109-                                                                                                                                                                                                                                                                                         | 85, 87-90, 95, 96, 109-                                                                                                                                                                                                                                                                                         | 85, 87-90, 95, 96, 109-                                                                                                                                                                                                                                                                                         | 85, 87-90, 95, 96, 109-                                                                                                                                                                                                                                                                                         |\n|                                                                                                                                                                                                                                                                                                                                                                                           | Auto , 12, 66, 98-101, 129, 197,                                                                                                                                                                                                                                                                                | Auto , 12, 66, 98-101, 129, 197,                                                                                                                                                                                                                                                                                | Auto , 12, 66, 98-101, 129, 197,                                                                                                                                                                                                                                                                                | Auto , 12, 66, 98-101, 129, 197,                                                                                                                                                                                                                                                                                |\n| clustering, 4, 25-26,                                                                                                                                                                                                                                                                                                                                                                     | 202-207, 327, 398                                                                                                                                                                                                                                                                                               | 202-207, 327, 398                                                                                                                                                                                                                                                                                               | 202-207, 327, 398                                                                                                                                                                                                                                                                                               | 202-207, 327, 398                                                                                                                                                                                                                                                                                               |\n|                                                                                                                                                                                                                                                                                                                                                                                           | Bikeshare , 12,                                                                                                                                                                                                                                                                                                 | Bikeshare , 12,                                                                                                                                                                                                                                                                                                 | Bikeshare , 12,                                                                                                                                                                                                                                                                                                 | Bikeshare , 12,                                                                                                                                                                                                                                                                                                 |\n| agglomerative, 525                                                                                                                                                                                                                                                                                                                                                                        |                                                                                                                                                                                                                                                                                                                 |                                                                                                                                                                                                                                                                                                                 |                                                                                                                                                                                                                                                                                                                 |                                                                                                                                                                                                                                                                                                                 |\n|                                                                                                                                                                                                                                                                                                                                                                                           | 167-172                                                                                                                                                                                                                                                                                                         | 167-172                                                                                                                                                                                                                                                                                                         | 167-172                                                                                                                                                                                                                                                                                                         | 167-172                                                                                                                                                                                                                                                                                                         |\n| cluster analysis, 25-26                                                                                                                                                                                                                                                                                                                                                                   |                                                                                                                                                                                                                                                                                                                 |                                                                                                                                                                                                                                                                                                                 |                                                                                                                                                                                                                                                                                                                 |                                                                                                                                                                                                                                                                                                                 |\n|                                                                                                                                                                                                                                                                                                                                                                                           | frame, 55 sets                                                                                                                                                                                                                                                                                                  | frame, 55 sets                                                                                                                                                                                                                                                                                                  | frame, 55 sets                                                                                                                                                                                                                                                                                                  | frame, 55 sets                                                                                                                                                                                                                                                                                                  |\n|                                                                                                                                                                                                                                                                                                                                                                                           | data augmentation, 411 data                                                                                                                                                                                                                                                                                     | data augmentation, 411 data                                                                                                                                                                                                                                                                                     | data augmentation, 411 data                                                                                                                                                                                                                                                                                     | data augmentation, 411 data                                                                                                                                                                                                                                                                                     |\n| error rate, 338                                                                                                                                                                                                                                                                                                                                                                           | 266                                                                                                                                                                                                                                                                                                             | 266                                                                                                                                                                                                                                                                                                             | 266                                                                                                                                                                                                                                                                                                             | 266                                                                                                                                                                                                                                                                                                             |\n| rule, 429 407 data set,                                                                                                                                                                                                                                                                                                                                                                   |                                                                                                                                                                                                                                                                                                                 |                                                                                                                                                                                                                                                                                                                 |                                                                                                                                                                                                                                                                                                                 |                                                                                                                                                                                                                                                                                                                 |\n| 199,                                                                                                                                                                                                                                                                                                                                                                                      | leave-one-out, 204-206                                                                                                                                                                                                                                                                                          | leave-one-out, 204-206                                                                                                                                                                                                                                                                                          | leave-one-out, 204-206                                                                                                                                                                                                                                                                                          | leave-one-out, 204-206                                                                                                                                                                                                                                                                                          |\n| 448, 449 2, 11, 34-39, 135-                                                                                                                                                                                                                                                                                                                                                               |                                                                                                                                                                                                                                                                                                                 |                                                                                                                                                                                                                                                                                                                 |                                                                                                                                                                                                                                                                                                                 |                                                                                                                                                                                                                                                                                                                 |\n| classification, 27, 367-382                                                                                                                                                                                                                                                                                                                                                               |                                                                                                                                                                                                                                                                                                                 |                                                                                                                                                                                                                                                                                                                 |                                                                                                                                                                                                                                                                                                                 |                                                                                                                                                                                                                                                                                                                 |\n| CIFAR100                                                                                                                                                                                                                                                                                                                                                                                  |                                                                                                                                                                                                                                                                                                                 |                                                                                                                                                                                                                                                                                                                 |                                                                                                                                                                                                                                                                                                                 |                                                                                                                                                                                                                                                                                                                 |\n| time, 470                                                                                                                                                                                                                                                                                                                                                                                 |                                                                                                                                                                                                                                                                                                                 |                                                                                                                                                                                                                                                                                                                 |                                                                                                                                                                                                                                                                                                                 |                                                                                                                                                                                                                                                                                                                 |\n| channel,                                                                                                                                                                                                                                                                                                                                                                                  |                                                                                                                                                                                                                                                                                                                 |                                                                                                                                                                                                                                                                                                                 |                                                                                                                                                                                                                                                                                                                 |                                                                                                                                                                                                                                                                                                                 |\n| 406, 409-411,                                                                                                                                                                                                                                                                                                                                                                             |                                                                                                                                                                                                                                                                                                                 |                                                                                                                                                                                                                                                                                                                 |                                                                                                                                                                                                                                                                                                                 |                                                                                                                                                                                                                                                                                                                 |\n| non-informative,                                                                                                                                                                                                                                                                                                                                                                          |                                                                                                                                                                                                                                                                                                                 |                                                                                                                                                                                                                                                                                                                 |                                                                                                                                                                                                                                                                                                                 |                                                                                                                                                                                                                                                                                                                 |\n|                                                                                                                                                                                                                                                                                                                                                                                           | 111                                                                                                                                                                                                                                                                                                             | 111                                                                                                                                                                                                                                                                                                             | 111                                                                                                                                                                                                                                                                                                             | 111                                                                                                                                                                                                                                                                                                             |\n|                                                                                                                                                                                                                                                                                                                                                                                           | 520-535                                                                                                                                                                                                                                                                                                         | 520-535                                                                                                                                                                                                                                                                                                         | 520-535                                                                                                                                                                                                                                                                                                         | 520-535                                                                                                                                                                                                                                                                                                         |  \n| Boston , 12, 67, 117, 122, 133, 199, 227, 287, 327, 364, 556                                                                                                                                                                                                                                                    | Default data set, 12, 136-139, 141- 144, 152-156, 160, 161, 225, 226, 466                                                                                                                                                                             | Default data set, 12, 136-139, 141- 144, 152-156, 160, 161, 225, 226, 466                                                                                                                                                                             | Default data set, 12, 136-139, 141- 144, 152-156, 160, 161, 225, 226, 466                                                                                                                                                                             | Default data set, 12, 136-139, 141- 144, 152-156, 160, 161, 225, 226, 466                                                                                                                                                                             |\n|-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|\n| BrainCancer , 12, 472-474, 476, 482 Caravan , 12, 184, 366                                                                                                                                                                                                                                                      | degrees of freedom, 30, 266, 295, 296, 301 dendrogram, 521, 525-530                                                                                                                                                                                   | degrees of freedom, 30, 266, 295, 296, 301 dendrogram, 521, 525-530                                                                                                                                                                                   | degrees of freedom, 30, 266, 295, 296, 301 dendrogram, 521, 525-530                                                                                                                                                                                   | degrees of freedom, 30, 266, 295, 296, 301 dendrogram, 521, 525-530                                                                                                                                                                                   |\n| Hitters , 12, 332, 333, 336, 338, 339, 366, 425, 426, 437, 446 IMDb , 413, 415, 416, 418, 420, 437, 458, 467 Income , 16-18, 21-23 Khan , 12, 579-581, 583, 590, 593 MNIST , 402-404, 406, 430, 441, 444, 445, 448 NCI60 , 4, 5, 12, 546, 548-550 NYSE , 12, 422-424, 466, 467 OJ , 12, 365, 398 Portfolio , 12 | derivative, 296, 300 detector layer, 410 deviance, 232 dictionary, 66 dimension reduction, 230, discriminant function, 149 discriminant method, 146-161 dissimilarity, 530-532 distance correlation-based, 530-532, 554 Euclidean, 509, 522, 523, 532 | derivative, 296, 300 detector layer, 410 deviance, 232 dictionary, 66 dimension reduction, 230, discriminant function, 149 discriminant method, 146-161 dissimilarity, 530-532 distance correlation-based, 530-532, 554 Euclidean, 509, 522, 523, 532 | derivative, 296, 300 detector layer, 410 deviance, 232 dictionary, 66 dimension reduction, 230, discriminant function, 149 discriminant method, 146-161 dissimilarity, 530-532 distance correlation-based, 530-532, 554 Euclidean, 509, 522, 523, 532 | derivative, 296, 300 detector layer, 410 deviance, 232 dictionary, 66 dimension reduction, 230, discriminant function, 149 discriminant method, 146-161 dissimilarity, 530-532 distance correlation-based, 530-532, 554 Euclidean, 509, 522, 523, 532 |\n|                                                                                                                                                                                                                                                                                                                 | density function, 146 dependent variable, 15                                                                                                                                                                                                          | density function, 146 dependent variable, 15                                                                                                                                                                                                          | density function, 146 dependent variable, 15                                                                                                                                                                                                          | density function, 146 dependent variable, 15                                                                                                                                                                                                          |\n| Carseats , 12, 126, 130, 364 CIFAR100 , 406, 409-411, 448, 449                                                                                                                                                                                                                                                  |                                                                                                                                                                                                                                                       |                                                                                                                                                                                                                                                       |                                                                                                                                                                                                                                                       |                                                                                                                                                                                                                                                       |\n| College , 12, 65, 286, 328 , 12, 91, 92, 94, 97, 98,                                                                                                                                                                                                                                                            |                                                                                                                                                                                                                                                       |                                                                                                                                                                                                                                                       |                                                                                                                                                                                                                                                       |                                                                                                                                                                                                                                                       |\n| Credit 106-109 Default , 12, 136-139, 141- 144, 152-156, 160,                                                                                                                                                                                                                                                   |                                                                                                                                                                                                                                                       |                                                                                                                                                                                                                                                       |                                                                                                                                                                                                                                                       |                                                                                                                                                                                                                                                       |\n| USArrests , 12, 507, 508, 510, 512, 513, 515, 516, 518,                                                                                                                                                                                                                                                         | 253-262                                                                                                                                                                                                                                               | 253-262                                                                                                                                                                                                                                               | 253-262                                                                                                                                                                                                                                               | 253-262                                                                                                                                                                                                                                               |\n| 161, 225, 226, 466                                                                                                                                                                                                                                                                                              |                                                                                                                                                                                                                                                       |                                                                                                                                                                                                                                                       |                                                                                                                                                                                                                                                       |                                                                                                                                                                                                                                                       |\n| Fund , 12, 567-570, 572, 575, 576, 585, 588, 589                                                                                                                                                                                                                                                                |                                                                                                                                                                                                                                                       |                                                                                                                                                                                                                                                       |                                                                                                                                                                                                                                                       |                                                                                                                                                                                                                                                       |\n| Heart , 339, 340, 344-347, 352,                                                                                                                                                                                                                                                                                 |                                                                                                                                                                                                                                                       |                                                                                                                                                                                                                                                       |                                                                                                                                                                                                                                                       |                                                                                                                                                                                                                                                       |\n| 353, 382, 383                                                                                                                                                                                                                                                                                                   | 529-                                                                                                                                                                                                                                                  | 529-                                                                                                                                                                                                                                                  | 529-                                                                                                                                                                                                                                                  | 529-                                                                                                                                                                                                                                                  |\n|                                                                                                                                                                                                                                                                                                                 | double descent, 431-435                                                                                                                                                                                                                               | double descent, 431-435                                                                                                                                                                                                                               | double descent, 431-435                                                                                                                                                                                                                               | double descent, 431-435                                                                                                                                                                                                                               |\n|                                                                                                                                                                                                                                                                                                                 | double-exponential distribution, 251                                                                                                                                                                                                                  | double-exponential distribution, 251                                                                                                                                                                                                                  | double-exponential distribution, 251                                                                                                                                                                                                                  | double-exponential distribution, 251                                                                                                                                                                                                                  |\n|                                                                                                                                                                                                                                                                                                                 | dropout, 406, 431 dummy variable, 91-94, 138, 142,                                                                                                                                                                                                    | dropout, 406, 431 dummy variable, 91-94, 138, 142,                                                                                                                                                                                                    | dropout, 406, 431 dummy variable, 91-94, 138, 142,                                                                                                                                                                                                    | dropout, 406, 431 dummy variable, 91-94, 138, 142,                                                                                                                                                                                                    |\n|                                                                                                                                                                                                                                                                                                                 | 292                                                                                                                                                                                                                                                   | 292                                                                                                                                                                                                                                                   | 292                                                                                                                                                                                                                                                   | 292                                                                                                                                                                                                                                                   |\n|                                                                                                                                                                                                                                                                                                                 | early stopping, 430 effective degrees of freedom, 301                                                                                                                                                                                                 | early stopping, 430 effective degrees of freedom, 301                                                                                                                                                                                                 | early stopping, 430 effective degrees of freedom, 301                                                                                                                                                                                                 | early stopping, 430 effective degrees of freedom, 301                                                                                                                                                                                                 |\n| 431,                                                                                                                                                                                                                                                                                                            | eigen decomposition, 506, 516 elbow, 548 embedding, 418 embedding layer, 419                                                                                                                                                                          | eigen decomposition, 506, 516 elbow, 548 embedding, 418 embedding layer, 419                                                                                                                                                                          | eigen decomposition, 506, 516 elbow, 548 embedding, 418 embedding layer, 419                                                                                                                                                                          | eigen decomposition, 506, 516 elbow, 548 embedding, 418 embedding layer, 419                                                                                                                                                                          |\n|                                                                                                                                                                                                                                                                                                                 | ensemble, 343-354 entropy, 337-339, 363                                                                                                                                                                                                               | ensemble, 343-354 entropy, 337-339, 363                                                                                                                                                                                                               | ensemble, 343-354 entropy, 337-339, 363                                                                                                                                                                                                               | ensemble, 343-354 entropy, 337-339, 363                                                                                                                                                                                                               |\n|                                                                                                                                                                                                                                                                                                                 | epochs, 430                                                                                                                                                                                                                                           | epochs, 430                                                                                                                                                                                                                                           | epochs, 430                                                                                                                                                                                                                                           | epochs, 430                                                                                                                                                                                                                                           |\n| Publication , 12, 482-487                                                                                                                                                                                                                                                                                       |                                                                                                                                                                                                                                                       |                                                                                                                                                                                                                                                       |                                                                                                                                                                                                                                                       |                                                                                                                                                                                                                                                       |\n| Smarket , 2, 3, 12, 173, 184,                                                                                                                                                                                                                                                                                   | irreducible, 17, rate, 34                                                                                                                                                                                                                             | irreducible, 17, rate, 34                                                                                                                                                                                                                             | irreducible, 17, rate, 34                                                                                                                                                                                                                             | irreducible, 17, rate, 34                                                                                                                                                                                                                             |\n| 196                                                                                                                                                                                                                                                                                                             | reducible, 17                                                                                                                                                                                                                                         | reducible, 17                                                                                                                                                                                                                                         | reducible, 17                                                                                                                                                                                                                                         | reducible, 17                                                                                                                                                                                                                                         |\n|                                                                                                                                                                                                                                                                                                                 | term, 16 distance,                                                                                                                                                                                                                                    | term, 16 distance,                                                                                                                                                                                                                                    | term, 16 distance,                                                                                                                                                                                                                                    | term, 16 distance,                                                                                                                                                                                                                                    |\n|                                                                                                                                                                                                                                                                                                                 | 509,                                                                                                                                                                                                                                                  | 509,                                                                                                                                                                                                                                                  | 509,                                                                                                                                                                                                                                                  | 509,                                                                                                                                                                                                                                                  |\n| 519                                                                                                                                                                                                                                                                                                             | 522, 554                                                                                                                                                                                                                                              | 522, 554                                                                                                                                                                                                                                              | 522, 554                                                                                                                                                                                                                                              | 522, 554                                                                                                                                                                                                                                              |\n| Wage , 1, 2, 8, 9, 12, 290, 291, 293, 295, 297-300, 302-                                                                                                                                                                                                                                                        | event time, 470 exception, 45                                                                                                                                                                                                                         | event time, 470 exception, 45                                                                                                                                                                                                                         | event time, 470 exception, 45                                                                                                                                                                                                                         | event time, 470 exception, 45                                                                                                                                                                                                                         |\n| 306, 309, 315, 327                                                                                                                                                                                                                                                                                              | expected value, 18                                                                                                                                                                                                                                    | expected value, 18                                                                                                                                                                                                                                    | expected value, 18                                                                                                                                                                                                                                    | expected value, 18                                                                                                                                                                                                                                    |\n| , 12, 196, 226                                                                                                                                                                                                                                                                                                  | exploratory data analysis,                                                                                                                                                                                                                            | exploratory data analysis,                                                                                                                                                                                                                            | exploratory data analysis,                                                                                                                                                                                                                            | exploratory data analysis,                                                                                                                                                                                                                            |\n| Weekly                                                                                                                                                                                                                                                                                                          |                                                                                                                                                                                                                                                       |                                                                                                                                                                                                                                                       |                                                                                                                                                                                                                                                       |                                                                                                                                                                                                                                                       |\n| data type, 42                                                                                                                                                                                                                                                                                                   | exponential, 173 173                                                                                                                                                                                                                                  | exponential, 173 173                                                                                                                                                                                                                                  | exponential, 173 173                                                                                                                                                                                                                                  | exponential, 173 173                                                                                                                                                                                                                                  |\n| function, 387                                                                                                                                                                                                                                                                                                   | exponential                                                                                                                                                                                                                                           | exponential                                                                                                                                                                                                                                           | exponential                                                                                                                                                                                                                                           | exponential                                                                                                                                                                                                                                           |\n| decision                                                                                                                                                                                                                                                                                                        |                                                                                                                                                                                                                                                       |                                                                                                                                                                                                                                                       |                                                                                                                                                                                                                                                       |                                                                                                                                                                                                                                                       |\n| tree, 11, 331-342                                                                                                                                                                                                                                                                                               | family,                                                                                                                                                                                                                                               | family,                                                                                                                                                                                                                                               | family,                                                                                                                                                                                                                                               | family,                                                                                                                                                                                                                                               |\n| decision                                                                                                                                                                                                                                                                                                        |                                                                                                                                                                                                                                                       |                                                                                                                                                                                                                                                       |                                                                                                                                                                                                                                                       |                                                                                                                                                                                                                                                       |\n|                                                                                                                                                                                                                                                                                                                 | F-statistic, 84                                                                                                                                                                                                                                       | F-statistic, 84                                                                                                                                                                                                                                       | F-statistic, 84                                                                                                                                                                                                                                       | F-statistic, 84                                                                                                                                                                                                                                       |\n| deep learning, 399                                                                                                                                                                                                                                                                                              |                                                                                                                                                                                                                                                       |                                                                                                                                                                                                                                                       |                                                                                                                                                                                                                                                       |                                                                                                                                                                                                                                                       |\n|                                                                                                                                                                                                                                                                                                                 | 529-532,                                                                                                                                                                                                                                              | 529-532,                                                                                                                                                                                                                                              | 529-532,                                                                                                                                                                                                                                              | 529-532,                                                                                                                                                                                                                                              |\n|                                                                                                                                                                                                                                                                                                                 | error                                                                                                                                                                                                                                                 | error                                                                                                                                                                                                                                                 | error                                                                                                                                                                                                                                                 | error                                                                                                                                                                                                                                                 |\n|                                                                                                                                                                                                                                                                                                                 | Euclidean                                                                                                                                                                                                                                             | Euclidean                                                                                                                                                                                                                                             | Euclidean                                                                                                                                                                                                                                             | Euclidean                                                                                                                                                                                                                                             |  \n| factor, 92 factorial, 170 failure time, 470 false discovery proportion, 155, 573 discovery rate, 558, 573-577,   | heteroscedasticity, 103, 168           |\n|------------------------------------------------------------------------------------------------------------------|----------------------------------------|\n|                                                                                                                  | hidden layer, 400                      |\n|                                                                                                                  | hidden units, 400                      |\n|                                                                                                                  | hierarchical clustering, 525-530       |\n|                                                                                                                  | dendrogram, 525-528                    |\n|                                                                                                                  | inversion, 529                         |\n| 579-582                                                                                                          | linkage, 529-530                       |\n| negative, 155, 562                                                                                               | hierarchical principle, 96             |\n| positive, 155, 562, 563                                                                                          | high-dimensional, 86, 234, 263         |\n| positive rate, 155, 156, 382                                                                                     | hinge loss, 385                        |\n| family-wise error rate, 565-573, 577                                                                             | Hitters data set, 12, 332, 333,        |\n| feature, 15                                                                                                      | 336, 338, 339, 366, 425,               |\n| feature map, 406                                                                                                 | 426, 437, 446                          |\n| feature selection, 230                                                                                           | hold-out set, 202                      |\n| featurize, 414                                                                                                   | Holm's method, 568, 576,               |\n| feed-forward neural network, 400                                                                                 | 585 hypergeometric distribution, 501   |\n| figure, 48                                                                                                       | hyperparameter, 187                    |\n| fit, 21                                                                                                          | hyperplane, 367-372                    |\n| fitted value, 101                                                                                                | hypothesis test, 76-77, 84, 103,       |\n| flattening, 424                                                                                                  | 558-583                                |\n| 43                                                                                                               | IMDb data set, 413, 415, 416, 418,     |\n| flexible, 21 floating point, forward stepwise selection, 86, 87,                                                 | 420, 437, 458, 467 imputation, 515     |\n| 233-234, 268 function, 40                                                                                        | Income data set, 16-18,                |\n|                                                                                                                  | 21-23                                  |\n| Fund data set, 12, 567-570, 572,                                                                                 | increment, 60 independent variable, 15 |\n| 575, 576, 585, 588, 589                                                                                          | indexable, 186 292                     |\n| Gamma, 173                                                                                                       | indicator function,                    |\n| Gaussian (normal) distribution, 146, 147, 150, 172, 561                                                          | inference, 17, 18                      |\n| generalized additive model, 5, 24,                                                                               | inner product, 379, 380                |\n| 162, 289, 290, 305-309,                                                                                          | input layer, 400 input variable, 15    |\n| 319 generalized linear model, 5, 135,                                                                            | integral, 301                          |\n| 167-174, 217                                                                                                     | interaction, 70, 89, 95-98, 110-       |\n| generative model, 146-161                                                                                        | 111, 308 intercept, 71, 72             |\n| Gini index, 337-339, 345, 346, 363 global minimum, 427 gradient, 428                                             | interpolate, 432                       |\n|                                                                                                                  | interpretability, 229                  |\n| gradient descent,                                                                                                | inversion, 529                         |\n| 427                                                                                                              | irreducible error, 17, 36, 90, 110     |\n| Harrell's concordance index, 487                                                                                 | iterator, 312                          |\n| hazard function, 476-478                                                                                         |                                        |\n| baseline, 478                                                                                                    | joint distribution, 158                |\n| hazard rate, 476                                                                                                 | neighbors, 135, 164-167                |\n| 352, 353, 382, 383                                                                                               | classifier, 11, 36-37                  |\n| heatmap, 50                                                                                                      |                                        |\n| helper, 311                                                                                                      |                                        |\n|                                                                                                                  | regression, 111-115                    |\n|                                                                                                                  | K-nearest                              |\n| Heart                                                                                                            |                                        |\n|                                                                                                                  | 521-524                                |\n| data set, 339, 340,                                                                                              |                                        |\n|                                                                                                                  | 344-347,                               |\n|                                                                                                                  | K-means clustering, 11,                |  \n| Kaplan-Meier survival curve, 472- 474, 483 kernel, 379-382, 384, 394 linear, 380 non-linear, 377-382 polynomial, 380, 382 radial, 381-383, 390 kernel density estimator, 159 keyword, 46 Khan data set, 12, 579-581, 583, 590, 593 knot, 290, 294, 296-299 /lscript 1 norm, 244 norm, 242   | local regression, 290 log odds, 145 log-rank test, 474-476, 483 logistic function, 139 logistic regression, 5, 11, 25, 135, 138-144, 164-167, 172- 173, 308-309, 377, 384- 385 multinomial, 145, 163 multiple, 142-144 logit, 140 loss function, 300, 385 low-dimensional, 262 LSTM RNN, 420   |\n|---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|  \n| multivariate normal, 150                                         | overfitting, 21, 23, 25, 30-31, 88, 152, 233, 371   |\n|------------------------------------------------------------------|-----------------------------------------------------|\n| naive Bayes, 135, 158-161, 164- 167                              | p-value, 77, 82, 560-562, 578-579                   |\n| namespace, 116                                                   | adjusted, 586                                       |\n| natural spline, 297, 298, 301, 317                               | package, 42                                         |\n| NCI60 data set, 4, 5, 12, 546, 548-                              | parameter, 71                                       |\n| 550                                                              | parametric, 20-22, 111-115                          |\n| negative binomial, 173 negative predictive value, 155,           | partial least squares, 254, 260-262,                |\n| 156 neural network, 5, 399                                       | 282                                                 |\n|                                                                  | partial likelihood, 480                             |\n| node                                                             | path algorithm, 249                                 |\n| internal, 333 purity, 337-339                                    | permutation, 578                                    |\n| terminal, 333                                                    | permutation approach, 577-582                       |\n|                                                                  | perpendicular, 257                                  |\n| noise, 21, 252                                                   | Poisson distribution, 169, 172                      |\n| non-linear, 2, 11, 289-329 377-382                               | Poisson regression, 135, 167-173                    |\n| decision boundary, 377-382                                       | polynomial                                          |\n| kernel, non-parametric, 20, 22-23, 111-                          | kernel, 380, 382                                    |\n| 115, 193                                                         | regression, 98-99, 289-292, 294- 295                |\n| normal (Gaussian) distribution, 146,                             | pooling, 410                                        |\n| 147, 150, 172, 476, 561                                          | population regression line, 73                      |\n| notebook, 40                                                     | Portfolio data set, 12                              |\n| null, 152                                                        | positive predictive value, 155, 156                 |\n| distribution, 561, 578 hypothesis, 76, 559                       | posterior                                           |\n| model, 87, 231, 245                                              | distribution, 251                                   |\n| null rate, 186                                                   | mode, 251                                           |\n| NYSE data set, 12, 422-424, 466,                                 | probability, 147 power, 108, 155, 563               |\n| 467                                                              | precision, 155                                      |\n| Occam's razor, 426                                               | prediction, 17                                      |\n| odds, 140, 145, 195                                              | interval, 90, 110                                   |\n| OJ data set, 12, 365, 398                                        | predictor, 15                                       |\n| one-hot encoding, 92, 126, 403                                   | principal components, 505                           |\n| one-standard-error rule, 240                                     | analysis, 11, 254-260, 504-515                      |\n| one-versus-all, 384                                              | loading vector, 505, 506                            |\n| one-versus-one, 384                                              | missing values, 515-520                             |\n| one-versus-rest, 384                                             | proportion of variance explained,                   |\n| optimal separating hyperplane, 370                               | 510-515, 547                                        |\n| optimism of training error, 30 ordered categorical variable, 315 | regression, 11, 254-260, 280- 282, 504, 515         |\n| orthogonal, 257, 506                                             | score vector, 506                                   |\n| basis, 125                                                       | 514-515                                             |\n|                                                                  | scree plot, prior                                   |\n| out-of-bag, 345                                                  | distribution, 251                                   |\n| outlier, 103-104                                                 | probability, 146                                    |\n| output variable, 15                                              |                                                     |\n| over-parametrized, 465                                           | probability density function, 477,                  |\n| overdispersion, 172                                              | 478                                                 |\nIndex 603  \n```\nprojection, 230 proportional hazards assumption, 478 pruning, 336 cost complexity, 336 weakest link, 336 Publication data set, 12, 482487 Python objects and functions %%capture , 458 iloc[] , 58 loc[] , 57 AgglomerativeClustering() , 543 anova() , 313 anova_lm() , 125, 129, 312, 313 axhline() , 122, 551 axline() , 121, 129, 329 BART() , 362 biplot , 537 boot_SE() , 223 boxplot() , 62, 66 bs() , 315, 327 BSpline() , 315 clone() , 222 columns.drop() , 122 compute_linkage() , 544 confusion_table() , 176 contour() , 50 corr() , 129, 174 cost_complexity_pruning_path() , 357 CoxPHFitter() , 491 cross_val_predict() , 270 cross_validate() , 218, 219, 226 cumsum() , 539 cut_tree() , 545 data.frame() , 227 Dataset , 440 decision_function() , 392 DecisionTreeClassifier() , 354, 355 DecisionTreeRegressor() , 354 def , 121 dendrogram() , 544 describe() , 62, 66 dir() , 116 drop() , 179 dropna() , 56, 268, 461 DTC() , see DecisionTreeClassifier() DTR() , see DecisionTreeRegressor() dtype , 43 ElasticNetCV() , 279 enumerate() , 217 export_text() , 356 export_tree() , 365 fit() , 118, 181, 218 fit_transform() , 119 for , 59 GaussianNB() , 182 GBR() , see GradientBoostingRegressor() get_dummies() , 461 get_influence() , 121 get_prediction() , 120, 314 get_rdataset() , 535 glm() , 313 glob() , 437 GradientBoostingClassifier() , 361 GradientBoostingRegressor() , 354, 361 GridSearchCV() , 276 groupby() , 490 hist() , 62 iloc[] , 58, 59 import , 42 imshow() , 50, 449 ISLP.bart , 362 ISLP.cluster , 544 json , 437 KaplanMeierFitter() , 502 keras , 437 KFold() , 219 KMeans() , 542, 543 Kmeans() , 542 KNeighborsClassifier() , 183 lambda , 58 LDA() , see LinearDiscriminantAnalysis() legend() , 132 lifelines , 490 LinearDiscriminantAnalysis() , 174, 179 LinearGAM() , 317 LinearRegression() , 280\n```  \n```\nload_data() , 117 loc[] , 58, 59, 177 log_loss() , 355 LogisticGAM() , 323 logrank_test() , 490 lowess() , 324 matplotlib , 48 max() , 66 mean() , 48 median() , 197 min() , 66 MNIST() , 444 ModelSpec() , 116-118, 122, 124, 267 MS() , see ModelSpec() mult_test() , see multipletests() multipletests() , 586 multipletests() , 583, 589 multivariate_logrank_test() , 496 NaturalSpline() , 317, 319 ndim , 42 nn.RNN() , 461 normal() , 132, 286, 555 np , see numpy np.all() , 54, 180 np.allclose() , 190 np.any() , 54 np.arange() , 51 np.argmax() , 122 np.array() , 42 np.concatenate() , 133 np.corrcoef() , 46, 554 np.empty() , 224 np.isnan() , 268 np.ix_() , 53 np.linalg.svd() , 539 np.linspace() , 50 np.logspace() , 318 np.mean() , 47, 176 np.nan , 60 np.nanmean() , 541 np.percentile() , 228 np.power() , 219 np.random.choice() , 553 np.random.default_rng() , 46, 47 np.random.normal() , 45 np.sqrt() , 45 np.squeeze() , 457 np.std() , 47 np.sum() , 43 np.var() , 47 np.where() , 180 ns() , 317 numpy , 42, 555 os.chdir() , 55 outer() , 219 pairwise_distances() , 554 pairwise_tukeyhsd() , 587 pandas , 55 params , 175 partial() , 222, 269 PCA() , 280, 537, 540, 554 pd , see pandas pd.crosstab() , 555 pd.cut() , 315 pd.get_dummies() , 314 pd.plotting.scatter_matrix() , 62 pd.qcut() , 314, 315 pd.read_csv() , 55, 556 pd.Series() , 62 Pipeline() , 275 plot() , 48, 61, 356, 490 plot.scatter() , 120 plot_gam() , 321 plot_svm() , 398 PLSRegression() , 282 poly() , 125, 313, 327 predict() , 175, 178, 181, 216, 218, 323, 358 predict_survival_function() , 493 print() , 40 pvalues , 175 pygam , 307, 317 pytorch_lightning , 435 QDA() , see QuadraticDiscriminantAnalysis() QuadraticDiscriminantAnalysis() , 174, 181 random() , 555 RandomForestRegressor() , 354, 360 read_image() , 436 reindex() , 461 reshape() , 43\n```  \n```\nreturn , 198 RF() , see RandomForestRegressor() rng , see np.random.default_rng() rng.choice() , 60 rng.standard_normal() , 60 roc_curve() , 392 RocCurveDisplay.from_estimator() , 387 savefig() , 50 scatter() , 49, 61 scipy.interpolate , 315 score() , 218, 461 seed_everything() , 436 set_index() , 57 set_title() , 49 set_xlabel() , 49 set_xscale() , 198 set_ylabel() , 49 set_yscale() , 198 shape , 43 ShuffleSplit() , 219 sim_time() , 495 SimpleDataModule() , 441 SimpleModule.classification() , 446 SimpleModule.regression() , 442 skl , see sklearn.linear_model skl.ElasticNet() , 273, 277 skl.ElasticNet.path , 274 skl.ElasticNet.path() , 273 sklearn , 118, 181 sklearn.ensemble , 360 sklearn.linear_model , 267 sklearn.model_selection , 267 sklearn_selected() , 269 sklearn_selection_path() , 270 sklearn_sm() , 218 skm , see sklearn.model_selection skm.cross_val_predict() , 271 skm.KFold() , 271 skm.ShuffleSplit() , 272 slice() , 51, 462 sm , see statsmodels sm.GLM() , 174, 192, 226 sm.Logit() , 174 sm.OLS() , 118, 129, 174, 319 StandardScaler() , 185, 438, 537, 555 statsmodels , 116, 173 std() , 186 Stepwise() , 269 str.contains() , 59 subplots() , 48 sum() , 43, 268 summarize() , 118, 129, 223, 226 summary() , 119, 322, 587 super() , 440 SupportVectorClassifier() , 387, 389-391, 393 SupportVectorRegression() , 394 SVC() , see SupportVectorClassifier() svd() , 539 SVR() , see SupportVectorRegression() TensorDataset() , 441 to_numpy() , 437 torch , 435 torchinfo , 436 torchmetrics , 436 torchvision , 436 ToTensor() , 444 train_test_split() , 186, 216 transform() , 118, 119 ttest_1samp() , 584 ttest_ind() , 590 ttest_rel() , 587 tuple , 43 uniform() , 555 value_counts() , 66 var() , 536 variance_inflation_factor() , 116, 124 VIF() , see variance_inflation-_factor() where() , 355 zip() , 60, 312 q-values, 589 quadratic, 98 quadratic discriminant analysis, 4, 135, 156-157, 164-167\n```  \n| qualitative, 2, 27, 91, 135, 167, 202 variable, 91-94 quantitative, 2, 27, 91, 135, 167, 202 radial kernel, 381, 383, 390 random forest, 11, 331, 343, 346- 347, 354, 360-361 random seed, 46 re-sampling, 577-582 recall, 155 receiver operating characteristic (ROC), 154, 382-383 recommender systems, 516 rectified linear unit, 401 recurrent neural network, 416-427 recursive binary splitting, 334, 337, 338 reducible error, 17, 90 regression, 2, 11, 27 local, 289, 290, 304-305 piecewise polynomial, 294-295 polynomial, 289-292, 299 spline, 289, 294 tree, 331-337, 358-360 regularization, 230, 240, 406, 484- 486 ReLU, 401 resampling, 201-214 residual, 71, 81 plot, 100 standard error, 75, 77-78, 88- 89, 109 studentized, 104 sum of squares, 71, 79, 81 residuals, 263, 348 response, 15   | elbow, 514 semi-supervised learning, 27 sensitivity, 153, 155, 156 separating hyperplane, 367-372 Seq2Seq, 425 sequence, 41 shrinkage, 230, 240, 484-486 penalty, 240 sigmoid, 401 signal, 252 signature, 45 singular value decomposition, 539 slack variable, 375 slice, 51 slope, 71, 72 Smarket data set, 2, 3, 12, 173, 184, 196 smoother, 308 smoothing spline, 290, 300-303 soft margin classifier, 372-374 soft-thresholding, 250 softmax, 145, 405 sparse, 244, 252 sparse matrix format, 414 sparsity, 244 specificity, 153, 155, 156 spline, 289, 294-303 cubic, 296 linear, 296 natural, 297, 301 regression, 289, 294-299 smoothing, 30, 290, 300-303 thin-plate, 22 standard error, 75, 101 standardize, 185 statistical model, 1 step function, 111, 289, 292-293   |\n|-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|  \n| regression, 386 survival analysis, 469-502 curve, 472, 483 function, 472 time, 470 synergy, 70, 89, 95-98, 110-111 systematic, 16 t-distribution, 77, 165 t-statistic, 76 t-test one-sample, 583, 584, 588 paired, 587 two-sample, 559, 570, 571, 577- 581, 584, 590 test error, 35, 37, 176 MSE, 28-32 observations, 28 set, 30 statistic, 559 theoretical null distribution, 577 time series, 101 total sum of squares, 79 tracking, 102 train, 21 training data, 20 error, 35, 37, 176 MSE, 28-31 transformer, 311 tree, 331-342 tree-based method, 331 true negative, 155 true positive, 155 true positive rate, 155, 156, 382 truncated power basis, 296 Tukey's method, 571, 585, 587 tuning parameter, 187, 240, 484 two-sample t -test, 474 Type I error, 155, 562-565 Type I error rate, 563 Type II error, 155, 563, 568, 584   | unsupervised learning, 25-27, 255, 260, 503-552 USArrests data set, 12, 507, 508, 510, 512, 513, 515, 516, 518, 519 validation set, 202 approach, 202-204 variable, 15 dependent, 15 dummy, 91-94, 97-98 importance, 346, 360 independent, 15 indicator, 35 input, 15 output, 15 qualitative, 91-94, 97-98 selection, 86, 230, 244 variance, 18, 31-34, 159 inflation factor, 108-110, 123 varying coefficient model, 305 Wage data set, 1, 2, 8, 9, 12, 290, 291, 293, 295, 297-300, 302-306, 309, 315, 327 weak learner, 343 weakest link pruning, 336 Weekly data set, 12, 196, 226 weight freezing, 412, 419 weight sharing, 418 weighted least squares, 103, 304 weights, 404 with replacement, 214 within class covariance, 150 wrapper, 217   |\n|-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|""]","La 'maldición de la dimensionalidad' se refiere a varios fenómenos que ocurren cuando se trabaja con datos en espacios de alta dimensión. A medida que el número de dimensiones (o características) aumenta, el volumen del espacio crece de manera exponencial, lo que provoca que los datos se vuelvan escasos. Esto tiene varias implicaciones para los métodos de aprendizaje estadístico, especialmente para los métodos no paramétricos como el K-Nearest Neighbors (KNN).

En el contexto de KNN y otros métodos locales que realizan predicciones utilizando solo observaciones cercanas a la observación de prueba, la maldición de la dimensionalidad puede causar problemas significativos. A medida que aumenta el número de dimensiones, las observaciones tienden a estar más alejadas unas de otras en el espacio de características. Esto significa que, para una observación de prueba dada, puede no haber suficientes observaciones de entrenamiento cercanas para hacer una predicción confiable. En otras palabras, en un espacio de alta dimensión, las observaciones que son ""vecinas"" pueden estar realmente muy lejos en términos de distancia euclidiana, lo que lleva a predicciones pobres.

En general, los métodos paramétricos tienden a superar a los métodos no paramétricos en situaciones donde hay un pequeño número de observaciones por predictor, ya que los métodos paramétricos hacen suposiciones sobre la forma de la función subyacente y, por lo tanto, pueden ser más robustos en espacios de alta dimensión.","La 'maldición de la dimensionalidad' se refiere al hecho de que a medida que aumenta el número de características (dimensiones), los datos se vuelven cada vez más dispersos en el espacio de alta dimensión. Esto hace que los métodos de aprendizaje estadístico, especialmente los no paramétricos como k-vecinos más cercanos (KNN), sean menos efectivos, ya que la distancia entre los puntos de datos aumenta y la noción de 'vecindad' se vuelve menos significativa.",1.0,,0.9999999999666667,1.0
